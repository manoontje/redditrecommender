{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': 'A quarry', 'subreddit': 'sandiego'}\n",
      "{'body': \"[Salutations! I'm not sure what you said.](http://imgur.com/9TtaInH) \\n \\n\", 'subreddit': 'RWBY'}\n",
      "{'body': 'I got into baseball at about he same time Matt Cain started playing in the majors. Crazy to see him go. I teared up a bit too.', 'subreddit': 'baseball'}\n",
      "{'body': 'FUCKING TORY', 'subreddit': '2007scape'}\n",
      "{'body': 'I see a water dragon ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'Wait. The Michigan what? Where is this? Is this like U of M club or a just state of Michigan?', 'subreddit': 'Cubers'}\n",
      "{'body': 'ye fam', 'subreddit': 'teenagers'}\n",
      "{'body': '143417804| &gt; United States Anonymous (ID: LIAKFEVH)\\n\\n&gt;&gt;143412250 (OP)\\noldfag here\\n\\n2016: Hillary\\n2012: Obama\\n2008: Obama\\n2004: Kerry\\n2000: Buchanan\\n1996: Dole\\n1992: Bush\\n1988: Bush\\n1984: Reagan\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'That is some chicken salad outta chicken shit running. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Does he even know the rules?', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Tequila.', 'subreddit': 'CFB'}\n",
      "{'body': \"your heart beats fast when running to provide your muscles with oxygen because they are mainly the things burning calories. In EC it is adrenaline making your hearrt race. Your muscles are not doing anything like they would when exercising. I know coz i'm unfit but can EC for hours without getting out of breath!\", 'subreddit': 'EchoArena'}\n",
      "{'body': '&gt; Subscribe: /clivecummings\\n\\n', 'subreddit': 'HFY'}\n",
      "{'body': \"you're really ignorant of history then.  like grotesquely ignorant. \", 'subreddit': 'The_Donald'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"You are arguing to argue, I don't mind a story, I don't want it to be so long I can't play for 60 minutes to start a player, is that hard to understand? \", 'subreddit': 'NBA2k'}\n",
      "{'body': \"I'm thinking about ending my life right now. Godspeed, you're not alone. \", 'subreddit': 'opiates'}\n",
      "{'body': '[Original post](https://www.reddit.com/r/EarthPorn/comments/73ig6e/lake_tekapo_new_zealand_oc4288x2848/) by /u/PowderDirtRock in /r/EarthPorn\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#match \"Lake Tekapo\")\\n', 'subreddit': 'ImagesOfNewZealand'}\n",
      "{'body': \"I think that's a bug, personally from a lot of campaign and a bit of custom battles night runners don't feel like a replacement for slingers.\\n\\nSlingers feel far more useful for harassing units around your army and getting in that morale damage.(Some Skaven clan rats and a unit of slingers can make Kroxigores route with a good surround and focus fire, especially with a spell in the middle of them or sniping the lord.)\\n\\nWhile Nightrunners are better at dedicated skirmishers, harass arches with them, force the enemy to draw units away and pepper them, and once they're out of ammo just use them to flank for your clan rats and break units.\\n\\nI haven't really tried out the more expensive variants of the night runners, but I imagine they play the same but just a bit more annoying.\\n\\nSo, vs AI you can break up their army and possibly even pull half of it away with just a few nightrunners while constantly plinking at their health. Vs players they seem like an annoyance and way to distract/take away some of their micro or punish them if they're not paying attention.\", 'subreddit': 'totalwar'}\n",
      "{'body': \"Harp absolutelly. \\nHarp is a warrior of blood with broken heart. He does not fight for money, anger or glory, he fights for honor. He's so broken inside that is much more touching than Delany. Delany is a man of honor too but he plays in a different league. \", 'subreddit': 'tvcrossovers'}\n",
      "{'body': 'CP3, Steph, IT,  Lowry, Westbrook   ', 'subreddit': 'nba'}\n",
      "{'body': \"Nah, it's like this every season in recent times. Really sad and annoying.\", 'subreddit': 'realmadrid'}\n",
      "{'body': 'They act like it’s some crazy idea, and the Foo Fighters are famous for it. ', 'subreddit': 'CFB'}\n",
      "{'body': '***Welcome to /r/edc_raffle, please read the rules in the sidebar.***\\n\\n**General rules for this raffle:**\\n\\n**1.** Please comment to request slot(s). Only TOP COMMENTS count. No replies to comments or automod. OP will reply to your comment to confirm your slot(s).\\n\\n**2.** Please pay within the timeframe established by OP. If you anticipate you will not be able to pay for your slot(s) in that timeframe, please arrange with OP to pay for your slot(s) early.\\n\\n**3.** ***The only accepted payment method is PayPal Friends &amp; Family.  DO NOT SEND YOUR PAYMENT AS AN eCHECK!*** *Do not write anything in the payment notes section.* ***Report to mods anyone that refuses to pay via paypal friends &amp; family***\\n\\n**4.** After you have paid, please PM OP with the **NAME and EMAIL ADDRESS** associated with your PayPal. Do not publicly post your name or email address.\\n\\n**5.** All raffles &gt;$500 are approved by mods. Please PM mods with any question of price or authenticity.\\n\\n***MODS TRY TO KEEP YOU SAFE BUT SCAMS ARE ALWAYS POSSIBLE, TRUST YOUR GUT***\\n\\n***hstexan*** details: \\n\\n1. [/r/raffle_feedback feedback](https://www.reddit.com/r/raffle_feedback/search?q=hstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n2. [/r/edc_raffle posts](https://www.reddit.com/r/edc_raffle/search?q=author%3Ahstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n3. [/r/kniferaffle posts](https://www.reddit.com/r/kniferaffle/search?q=author%3Ahstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\nIf modmail is too slow, pm /u/EDCRaffleAdmin /u/EDCRaffleMod or /u/EDCRaffleMod1\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/edc_raffle) if you have any questions or concerns.*', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Indeed', 'subreddit': 'DrugStashes'}\n",
      "{'body': \"He's gorgeous, what kind of tail is that, rounded? very pretty\", 'subreddit': 'bettafish'}\n",
      "{'body': 'No it is not open. Before we started to date she told me that if I cheat on her that she will never talk to me again and I told her the same. Her passed relationships ended because he guy cheated on her so I would think she would do that to someone because she knows how that feels. ', 'subreddit': 'relationships'}\n",
      "{'body': 'Wow, what a body!!', 'subreddit': 'gonewild'}\n",
      "{'body': 'Well you got a pretty good laugh out of me so good fucking job lol', 'subreddit': 'KCRoyals'}\n",
      "{'body': \"I'm not really asking how to do it, I'm just asking if it would be possible/make sense in a world.\", 'subreddit': 'dndnext'}\n",
      "{'body': '[removed]', 'subreddit': 'DIY'}\n",
      "{'body': 'OP, what did you do?', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Yes, also if you get P grade in all levels (not bosses) you unlock a filter', 'subreddit': 'Cuphead'}\n",
      "{'body': \"That part couldn't be anymore irrelevant. I'm talking about how she would probably find him doing whatever it is you suggested he should have done weird\", 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'Thanks. It looks like some Amazon merchants are comparable to Uline.', 'subreddit': 'Entrepreneur'}\n",
      "{'body': 'Sauce?', 'subreddit': 'Roughsex'}\n",
      "{'body': \"Fuck I got nervous and spent $87 on books. I should've actually read this post first lmao.\", 'subreddit': 'Android'}\n",
      "{'body': '[deleted]', 'subreddit': 'MapPorn'}\n",
      "{'body': '\"NOTCH NOTCH NOTCH\"', 'subreddit': 'technology'}\n",
      "{'body': 'You are looking to create a reddit bot? You will want to check out the [reddit API](https://www.reddit.com/dev/api). Many people do this in Python and there are many tutorials on the internet showing how to do so.', 'subreddit': 'learnprogramming'}\n",
      "{'body': 'The outcome of this roll should determine some of the results of this post. [Read more »](https://www.reddit.com/r/worldpowers/wiki/codeofethics#wiki_rng)\\n\\n/u/rollme [[1d20 /u/Razqn **Overall Success**]] [[1d20 /u/Razqn **Secrecy**]] \\n\\n#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/worldpowers) if you have any questions or concerns.*', 'subreddit': 'worldpowers'}\n",
      "{'body': \"This shits so güd it's from the future.\", 'subreddit': 'listentothis'}\n",
      "{'body': '* **[Ice Block](https://media-Hearth.cursecdn.com/avatars/330/731/28.png)** Mage Spell Epic Classic 🐘 ^[HP](http://www.hearthpwn.com/cards/28), ^[HH](http://www.hearthhead.com/cards/ice-block), ^[Wiki](http://hearthstone.gamepedia.com/Ice_Block)  \\n3 Mana - Secret: When your hero takes fatal damage, prevent it and become Immune this turn.  \\n\\n^(Call/)^[PM](https://www.reddit.com/message/compose/?to=hearthscan-bot) ^( me with up to 7 [[cardname]]. )^[About.](https://www.reddit.com/message/compose/?to=hearthscan-bot&amp;message=Tell%20me%20more%20[[info]]&amp;subject=hi)', 'subreddit': 'hearthstone'}\n",
      "{'body': '10 team standard. Who is my best play?\\n\\nFleener vs MIA\\n\\nClay @ATL\\n\\nMartavis @BAL\\n\\nDuke Johnson vs CIN\\n\\n', 'subreddit': 'fantasyfootball'}\n",
      "{'body': \"There's also a bit hacky way to use Google maps location sharing api which is less configurable but for just very basic tracking requires no other apps and pretty much zero battery overhead (as long as you were using Google maps location services already) \", 'subreddit': 'homeautomation'}\n",
      "{'body': '[deleted]', 'subreddit': 'gaming'}\n",
      "{'body': 'LOL agreed.\\n\\n No perfect angel then? Atleast they fixed it fast? Could have told people to fuck off with spin statements', 'subreddit': 'xboxone'}\n",
      "{'body': 'Welcome to r/mma this must be your first time', 'subreddit': 'MMA'}\n",
      "{'body': '2 posts in a row asking very similar vague homework-sounding questions. \\nPeople here are passionate about their work and love to share information but they aren’t going to do your homework for you!', 'subreddit': 'techtheatre'}\n",
      "{'body': \"Probably not, but 24 hours is doable!\\n\\nYou'd get Mind Controller to onyx(!) by the end if you kept up that pace fielding for 19 hours, and if you do it in the right spot (or just put in one last field at the end) you could pretty easily get Illuminator too. You'd *just* get Connector platinum by the end of it, and could get Recruiter platinum with virtually no effort on your part if it's prepped in advance.\\n\\nThe 3 extra golds is where things start to break down a bit. Spec Ops is pretty easy, but you'd need to have a lot of easy 4 portal missions in the same area to do it and still have time for your last two. Translator gold is doable in that timeframe, but would take up a majority of that time and leave you one gold short. I expect Spec Ops/Pioneer/Explorer would be the best combo, since doing Pioneer would inherently get you half-way to Explorer. It's plausible that Spec Ops/Pioneer/Recharger might be faster with some good key management, but with how temperamental Niantic is for recharging I'd rather just go for the thousand additional unique hacks.\\n\\nI'd be astonished if it ever happened, but it is theoretically possible.\", 'subreddit': 'Ingress'}\n",
      "{'body': \"I don't have much interest in consoles anymore, so if they want my money they'll have to port it...\", 'subreddit': 'gaming'}\n",
      "{'body': 'Like David Hasselhoff. ', 'subreddit': 'DunderMifflin'}\n",
      "{'body': \"Ditto - I do some events work on the side of clinical school and love how it gets me out and meeting non-veterinary-related people every so often. I'm a bit of a gearhead, and I get a lot of enjoyment reading about different people's rigs, newly released gear etc.\\n\\n\\n...oh, and I guess making images I'm really satisfied with is neat too ;)\", 'subreddit': 'photography'}\n",
      "{'body': \"please tell me there's video\\n\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Just raptor and the secret one (requires beating pof)', 'subreddit': 'Guildwars2'}\n",
      "{'body': 'holy. crap. I want to wait to tell DH in the morning and make sure I get a positive then too. Longest 13 hours of my life!', 'subreddit': 'TFABLinePorn'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'She is pretty amazing. I get her to close to 100% ta and she hits pretty close to dmg cap while also being somewhat tanky. Of course needs atleast 3 turns to get going. Also nice as you can choose if you want aggro up or down in combat. She is also pretty easy to use, as you basically just use her #3 turn 1 and if you dont need to never touch her again, which is very nice to attack mashing. And she cant lose her buffs as opposed to six', 'subreddit': 'Granblue_en'}\n",
      "{'body': \"Could you let us see the data when you're finished please?\", 'subreddit': 'startrek'}\n",
      "{'body': '10,000 career total yards for J.T. Barrett', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'relationships'}\n",
      "{'body': 'It will be, bow battles day 1 was everything ', 'subreddit': 'playrust'}\n",
      "{'body': 'gg', 'subreddit': '2007scape'}\n",
      "{'body': 'But the US states are similar economically developed. Ignore DC and you have GDPs/capita ranging between 65k to 30k dollars. In Europe ignoring Luxemburg you have GDPs/capita ranging between 79k to 1,900 dollar. \\n\\nIt\\'s obvious that countries, that have such a low GDPs/capita aren\\'t sensitive towards social issues, because they\\'re not even first world countries, but emerging countries. Also the cultural difference is more severe, not only because of the culture itself, but the fact that those countries had until the 90s totalitarian governments, therefore they have experience with democracy (including minority rights) not even for 30 years now. \\n\\n\"Therefore to say \"Europe is...\" and than referring to those countries is a double standard. It\\'s the same like looking at Singapore and say \"Asia is rich\" ', 'subreddit': 'AskAnAmerican'}\n",
      "{'body': 'Why is Peyper even mentioning repeated infringements to Read?', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'After a touchdown, people would think the player is Teebowing. Pass out a paper, people toss it. \\n\\nThis was the best way to do what they meant to do', 'subreddit': 'news'}\n",
      "{'body': 'I should add that I am in love with this phone. It’s everything I wanted and it runs beautifully.', 'subreddit': 'iphone'}\n",
      "{'body': '**Remember OP is a real person who has taken a risk by posting photos of herself to the internet. Please keep your comments respectful.** If in doubt, ask yourself \"how would I feel if someone posted this comment about me?\" Rude comments may result in a ban.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/curvy) if you have any questions or concerns.*', 'subreddit': 'curvy'}\n",
      "{'body': '\\\\- a guard at a Siberian labour camp, AD 1921 ', 'subreddit': 'ukpolitics'}\n",
      "{'body': 'Ask them as nicely as you can, amd offer to share profits.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"NOW LISTEN HERE YOU KNIFE-EARED PIECE OF SHIT!\\n\\nIF YOU GO ANY FURTHER WITH YOUR PISS-STAINED PUBIC HAIR YOU CALL A WIG, I'M GONNA WRECK YOUR SHIT SO HARD YOU WON'T EVEN BE ABLE TO WALK WITH YOUR **LIMP DICK!** \\n\\nI'M GONNA SHOVE MY FOOT SO FAR UP YOUR SHAVEN PERFECT LITTLE ARSE THAT YOUR BREATH IS GONNA SMELL LIKE SHOE POLISH! THEN I'M GONNA TAKE THAT LITTLE RED ANAL BEAD ON YOUR BELT AND PUSH IT IN YOUR FACE! \\n\\n**I'M GONNA FLAGELLATE YOU WITH MY FUCKIN' BEARD!**\\n\\nI'M GONNA BUILD A PAIR OF RUNIC MECHANICAL BALLS, AND USE SURGICAL PRECISION TO SEW THEM TO YOUR GROIN WHERE YOUR MANHOOD OUGHT TO BE JUST SO I CAN KICK THEM WITH MY IRON FUCKIN' FEET, YOU TWAT!\\n\\n\\nLISTEN TO ME YOU POLE-PROPORTIONED DENDROPHILES! I'M GONNA TAKE YOUR FUCKIN' ARROWS, AND SHOVE THEM IN BETWEEN YOUR POLISHED FINGERNAILS! \\n\\nI'M GONNA TAKE THAT BOWSTRING OF YOURS AND STRING YOU UP BY YOUR FUCKIN' FORESKIN, UNTIL GRAVITY GIVES YOU A BOTCHED CIRCUMCISION! AND PLAY IT LIKE A FUCKIN' VIOLIN!\\n\\n\\nI'M GOING TO HEADBUTT YOU UNTIL THERE'S NOTHING BUT A BUTT LEFT! \\n\\nI'M GONNA COLOUR THAT PANSY WHITE SKIRT RED, USING YOUR FUCKIN' BONE MARROW! \\n\\nYOU BETTER BELIEVE YOU'RE GOING TO WISH YOU WERE NEVER BORN, 'CAUSE I'LL MAKE IT SEEM LIKE YOU NEVER WERE, YE ATROCIOUS FUCKIN' BOIL ON THE FACE OF REALITY! \\n\\nI WILL-\", 'subreddit': 'totalwar'}\n",
      "{'body': 'So what are you doing on an Australia specific subreddit? Just avoid anything that will likely spoil it for you', 'subreddit': 'australia'}\n",
      "{'body': 'Oh, I have been going crazy on that report button.', 'subreddit': 'duolingo'}\n",
      "{'body': \"Videos in this thread:\\n\\n[Watch Playlist &amp;#9654;](http://subtletv.com/_r73i549?feature=playlist&amp;nline=1)\\n\\nVIDEO|COMMENT\\n-|-\\n[Zeno Appears First Time  Dragon Ball Super Episode 41 English Sub](http://www.youtube.com/watch?v=pmXIbiHSyDw)|[+2](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqgh8c?context=10#dnqgh8c) - Erasing is super high tier reality warping so he can logically do anything below it. He also created the multiverse so it would make sense he can do lots of other hax. Also, the time rings make them acausal so GER won't work. Here is a source on the ...\\n[Zeno destroys Zamasu](http://www.youtube.com/watch?v=iTIef3cRFVM)|[+2](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqi5sd?context=10#dnqi5sd) - Here.\\n[How Powerful is Zeno in Dragon Ball Super?](http://www.youtube.com/watch?v=wGGKmp6xRX4)|[+1](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqijnw?context=10#dnqijnw) - Here is a video that describes how powerful Zeno is.\\nI'm a bot working hard to help Redditors find related videos to watch. I'll keep this updated as long as I can.\\n***\\n[Play All](http://subtletv.com/_r73i549?feature=playlist&amp;ftrlnk=1) | [Info](https://np.reddit.com/r/SubtleTV/wiki/mentioned_videos) | Get me on [Chrome](https://chrome.google.com/webstore/detail/mentioned-videos-for-redd/fiimkmdalmgffhibfdjnhljpnigcmohf) / [Firefox](https://addons.mozilla.org/en-US/firefox/addon/mentioned-videos-for-reddit)\", 'subreddit': 'whowouldwin'}\n",
      "{'body': \"Exactly. Generation Z already gets this. See this \\nhttps://www.reddit.com/r/MGTOW/comments/73cbkv/not_drinking_or_driving_teens_increasingly_put/?st=J87ZADUH&amp;sh=a8ba3e7c\\n\\n\\nThe ones bitching about that article don't get it. They're still trapped in the Matrix.\", 'subreddit': 'MGTOW'}\n",
      "{'body': 'WTF. I thought u had 1 year from your most recent survey. So stocking up is impossible now?', 'subreddit': 'Android'}\n",
      "{'body': '[deleted]', 'subreddit': 'thewalkingdead'}\n",
      "{'body': 'I thought it was always possible, just not worth the cost of processing until recently (referring to the Canadian oil sands).', 'subreddit': 'AskScienceFiction'}\n",
      "{'body': \"143413486| &gt; United States Anonymous (ID: QRDnEOyg)\\n\\n2016==&gt; Trump (and loving the shitstorm)\\n2012==&gt; Mitt (didn't like him but it was a vote against Obama)\\n2008==&gt; 3rd party\\n2004==&gt; Bushy\\n2000==&gt; Bushy\\n1996==&gt; 3rd party\\n1992==&gt; Ross Perot (woot)\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': \"Didn't know Marlins had downchop fairies...\", 'subreddit': 'Braves'}\n",
      "{'body': \"I can drive stick and have a CDL, it's class B though, but I can still move trailers to where they are needed.\", 'subreddit': 'The_Donald'}\n",
      "{'body': \"You don't lose any vertical space if you paint the ceiling, and it just kinda fades away when it's painted like that. It also fits with the more modern look we are shooting for in the basement. It's a really common way to do ceilings in commercial spaces- you may have been in a store or restaurant that has an all black ceiling and not even noticed. \", 'subreddit': 'HomeImprovement'}\n",
      "{'body': 'His dad was actually a contestant on the life-or-death game show *Wheel of Fortune* as a kid. When he failed to guess the puzzle, he was dumped into a pit of hungry crocodiles and Pat Sajak claimed the \"i\" in Firk\\'s surname as his personal trophy. His son Martin, distraught at losing both his father *and* his vowels to that madman Saijak vowed revenge and has spent the last 15 years honing his skills with the blade and plotting the moment in which he is able to crush Saijak and his henchman Vanna White and reclaim the letter that is rightfully his. ', 'subreddit': 'hockey'}\n",
      "{'body': 'Maritime provinces would make it to the end, New Brunswick being completely looked over by the others. \\n\\nSaskatchewan would go first. We would feel too guilty to vote off the territories. Quebec Ontario and BC would all try to be alphas, make alliances and the screw each other over ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Definitely better than huffing glue. Don't huff glue. \", 'subreddit': 'howtonotgiveafuck'}\n",
      "{'body': \"Which always seemed silly to me. Either the position specific numbers are important enough that he should have to change now that he is a RB, or they aren't important so why bother with them at all?\", 'subreddit': 'nfl'}\n",
      "{'body': 'I heard Mete got hit and it sounded dirty. Is he okay? Is the other guy still breathing? Please tell me Price dusted off his blocker!', 'subreddit': 'hockey'}\n",
      "{'body': \"I think being out to each other is really important, so good to hear you both are.\\n\\nAnd I'm certainly not trying to push you into or away from hormones or anything else. I'm just warning against slipping into denial or repression. As long as you're honest with yourself, you can make an informed (if scary!) decision.\\n\\nFBI...Female But Incognito? I might steal that...\", 'subreddit': 'asktransgender'}\n",
      "{'body': \"I'm a void walker for everything. \", 'subreddit': 'destiny2'}\n",
      "{'body': '* **[Dragonfire Potion](https://media-Hearth.cursecdn.com/avatars/329/334/49648.png)** Priest Spell Epic MSoG 🐘 ^[HP](http://www.hearthpwn.com/cards/49648), ^[HH](http://www.hearthhead.com/cards/dragonfire-potion), ^[Wiki](http://hearthstone.gamepedia.com/Dragonfire_Potion)  \\n6 Mana - Deal 5 damage to all minions except Dragons.  \\n\\n^(Call/)^[PM](https://www.reddit.com/message/compose/?to=hearthscan-bot) ^( me with up to 7 [[cardname]]. )^[About.](https://www.reddit.com/message/compose/?to=hearthscan-bot&amp;message=Tell%20me%20more%20[[info]]&amp;subject=hi)', 'subreddit': 'hearthstone'}\n",
      "{'body': 'The adoption fees for my cats :)', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'You may find these links helpful:\\n\\n- [Budgeting wiki page](http://www.reddit.com/r/personalfinance/wiki/budgeting)\\n- [Finance spreadsheets](http://www.reddit.com/r/personalfinance/wiki/tools#wiki_redditor_created.3A)\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/personalfinance) if you have any questions or concerns.*', 'subreddit': 'personalfinance'}\n",
      "{'body': 'V1 = we won', 'subreddit': 'LivestreamFail'}\n",
      "{'body': 'that would be 9:30 EST, its 8:30 CST', 'subreddit': 'Dodgers'}\n",
      "{'body': 'Or at least let Arcia wear his glasses.', 'subreddit': 'Brewers'}\n",
      "{'body': 'Josh Jackson refused to practice for Boston. Didnt wanna be a 3rd option.', 'subreddit': 'nba'}\n",
      "{'body': \"I haven't watched much of you guys this season. I'm guessing this is normal?\", 'subreddit': 'CFB'}\n",
      "{'body': 'what no? im playing titanfall 2 while sinking some tins listening to a podcsat, ive been top spot in my team pretty much every game', 'subreddit': 'science'}\n",
      "{'body': 'TheFear 290 titan', 'subreddit': 'Fireteams'}\n",
      "{'body': 'ahhhh i remember the days of water-bottles, pen tubes, tape and sockets....', 'subreddit': 'trees'}\n",
      "{'body': \"She looks beautiful. I hope she's feeling beautiful, too.\", 'subreddit': 'TeenMomOGandTeenMom2'}\n",
      "{'body': \"Type ''screenshot.render'' in the console and press enter, the console can be opened by pressing ` under the esc key.\\nYou can then find the screenshot in documents -&gt; battlefield 1 -&gt; screenshots.\", 'subreddit': 'battlefield_live'}\n",
      "{'body': \"That's not really a punishment. I guess detention would be a better approach.\\n\\nEdit: Man, I sure hope this comment is controversial because of some well-considered opinion on discipline and teaching and not, say, because [71%](https://www.reddit.com/r/dataisbeautiful/comments/5700sj/octhe_results_of_the_reddit_demographics_survey/) of of reddit users are either in highschool, college, or recently graduated and are salty about not getting to use their toy.\", 'subreddit': 'videos'}\n",
      "{'body': \"I was constantly bullied as a child for crying. Nowadays I can't cry at all, even when I desperately need it because my body *needs* to get stress out.\\n\\nSo instead I've bottled bad emotions for years and now they're manifesting with mind-shattering panic attacks.\\n\\nOh joy.\", 'subreddit': 'iamverybadass'}\n",
      "{'body': \"Thank you. Idk why people won't accept this as an answer it fucking works.\", 'subreddit': 'runescape'}\n",
      "{'body': 'Yeah I mean we are talking about a guy who jumps and squishes people. https://youtu.be/nfLS4nt5aQw', 'subreddit': 'nottheonion'}\n",
      "{'body': '[removed]', 'subreddit': 'WarCollege'}\n",
      "{'body': '\"It worked for me and it worked for Conor, but he added the part where you fight in a completely other sport\"', 'subreddit': 'MMA'}\n",
      "{'body': '&gt;  the story was completely over my expectations, it fluctuates to pretty entertaining to burst-out-laughing hilarious, and a slightly-more-than-healthy dose of cringy one-liners\\n\\nRULUE LOVES THE DARK PRINCE.\\n\\nRULUE LOVES THE DARK PRINCE.\\n\\nRULUE LOVES THE DARK PRINCE.', 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'Um ok....', 'subreddit': 'vainglorygame'}\n",
      "{'body': 'Read the sidebar. It meets none of the criteria. ', 'subreddit': 'ThreadKillers'}\n",
      "{'body': 'GODS, SHE HAS TANLINES NOW.', 'subreddit': 'SaraJUnderwood'}\n",
      "{'body': 'Hows he doing?', 'subreddit': 'frogdogs'}\n",
      "{'body': \"Long as your BAC wasn't above .06 you're good. \", 'subreddit': 'news'}\n",
      "{'body': '[deleted]', 'subreddit': 'Philippines'}\n",
      "{'body': 'Id recommend that you go \"porn style\" and pull out all the time until you would welcome a baby....why add that stress to sex?  ', 'subreddit': 'sex'}\n",
      "{'body': \"And we always see it coming first. We are the canary in the societal coal mine. They call us the religious right for a reason - we're usually right. \\n\\nRemember the alleged *Gay Agenda?* A conspiracy to queer up your kids?? Lol, not lol :'( \\n\\nBy the by... ISLAM!!!! AHHHH THE HORROR!!!!! STOP IT NOW BEFORE IT'S TOO LATE!!!! 🐥 😱💀\\n\\nEdit: http://www.thegatewaypundit.com/2017/09/report-france-western-europe-majority-muslim-40-years/\\n\\nChoose to be Muslim: freedom of religion, no problem.\\n\\nChoose to inflict Islam on others: act of war, terror, and genocide.\", 'subreddit': 'JordanPeterson'}\n",
      "{'body': 'Yup, I read the whole thing, but unlike the first analysis, the whole \"crying triggered a crying triggering another feeling of being defeated\" is kind of... odd and extremely forced. I mean, it\\'s not some random crying but Goten\\'s, who at that point of the series was meant to be a \"mini-Goku\" is the one awakening Broly. It\\'s just too much coincidence, and also going by Occam\\'s razor, it\\'s just the \"more fitting\" explanation.', 'subreddit': 'TeamFourStar'}\n",
      "{'body': \"ppl can say what they want about YES announcers, but they're always extremely courteous and professional\", 'subreddit': 'NYYankees'}\n",
      "{'body': 'Hot damn. (งツ)ว', 'subreddit': 'gonewild'}\n",
      "{'body': 'Yes, the iPad 3 is supported, not the Mini 3.', 'subreddit': 'jailbreak'}\n",
      "{'body': 'pretty much same.....was actually just putting on heavy slayer with seraph tears on it,,,,,,,,,but I don\\'t understand the seaph defence and seraph pd.....I can only refit after I take out of garage and dismnted them but cant equip or sell....and their description is just \"too good\" any thoughts??\\n', 'subreddit': 'spacerpg3'}\n",
      "{'body': 'Those massive parking lots are packed with Schlitz Park commuters Monday through Friday.  It took them years to get agreement from their lessees to repave that big lot. Can you imagine the jockeying required to shut it down long enough to build a parking garage? Plus the impact to both Pleasant and Cherry during construction would be incredibly inconvenient for residents and commuting employees alike.', 'subreddit': 'milwaukee'}\n",
      "{'body': 'First, try using your controller in another game and try using another controller, this will determine whether this is a problem with the controller or the game. If it’s fine then try resetting your controls to default in rocket league. Also try reinserting your controller into its port if you’re on pc. Honestly, you should be posting this onto the tomshardware forums.\\n', 'subreddit': 'RocketLeague'}\n",
      "{'body': \"Paediatric intubation is *much* harder because of their differing anatomy, the risk for barotrauma is higher in kids, their soft tissues are more easily traumatised and they're easier to manage with less invasive methods anyway. Unless you need the tube because it's an asthmatic arrest, or because of developing airway oedema, ETT should be a very low priority in children. I would argue that paramedics should almost never really be considering paediatric intubation if other methods are available, and never electively outside of airway oedema.\", 'subreddit': 'ems'}\n",
      "{'body': 'As others have suggested i would say ranger. Probably hunter because the beastmaster in my opinion, still after the rework, leves alot to be desired. Also with a few levels of Druid for shapechange and spells. ', 'subreddit': 'DnD'}\n",
      "{'body': 'Hallelujah. Some sense!', 'subreddit': 'wolfalice'}\n",
      "{'body': 'The title, it is backwards. You say \"Other people say s2x will cripple bitcoin core\" while these people say that /r/bitcoin. You say \"This subreddit says the opposite\" While other people say the opposite /r/btc. ', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'Well if you like it you\\'re not part of a \"problem\". Buy what you enjoy, just because I find it a little boring doesn\\'t mean I think its a problem if others like it and buy it. The bigger issue is when people who don\\'t like it, or actively hate it, continue buying it. Because fuck, thats not good for anyone', 'subreddit': 'comicbooks'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"If you have a melee build, just run under the dragon and put yourself behind his hind legs. He can't do the stomp attack backwards and if you aren't between his legs he can't hit you with the backward fire attack. Make sure you have something with stamina regen on and save some up as well. He will take to the air, and you have to sprint towards his tail, as when is done with his AOE, he will dro0 straight down. If you ran far enough you will be behind his back legs again. Repeat until desired results come.\", 'subreddit': 'DarkSouls2'}\n",
      "{'body': 'Do you run some special rune setup to secure.targon kills?', 'subreddit': 'RakanMains'}\n",
      "{'body': 'I have crimson halo + hexed', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Nice pull!', 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': \"So you know, 'Do your own research' is a tagline of flattards everywhere.\\n\\nGuy can ball though!  Needs a new agent to encourage him to keep his 'woke' in check.\", 'subreddit': 'nba'}\n",
      "{'body': 'It makes you seem more assertive.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'hm? my budget is? what do you mean like my budget isnt enough or something like that', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'I just pretend that he most likely failed at knocking the last guy off, and got knocked off himself.', 'subreddit': 'gaming'}\n",
      "{'body': \" To become a true Yanomamo warrior, you must prove that you can withstand **tits so small they're concave** without crying out.\\n\\nI&amp;apos;m sorry, sir, but we don't allow **tits so small they're concave** at the country club.\\n\\nAre you thinking what I&amp;apos;m thinking, B1? I think I am, B2: it&amp;apos;s **tits so small they're concave** time!\\n\\nDuring his midlife crisis, my dad got really into **tits so small they're concave**.\\n\\nChannel 5&amp;apos;s new reality show features eight washed-up celebrities living with **tits so small they're concave**.\\n\\nThis holiday season, Tim Allen must overcome his fear of **tits so small they're concave** to save Christmas.\\n\\nIF you like **tits so small they're concave**, YOU MIGHT BE A REDNECK.\\n\\nHe who controls **tits so small they're concave** controls the world.\\n\\nWhen all else fails, I can always masturbate to **tits so small they're concave**.\\n\\nHere at the Academy for Gifted Children, we all students to explore **tits so small they're concave** at their own pace.\", 'subreddit': 'cahideas'}\n",
      "{'body': \"I'm sorry if thats how this came across but i was just trying to figure out how my grandparents can prevent losing everything to pay for care as they are not wealthy at all. They have paid taxes all their life and will continue to do so, i was just wondering whats the best way to go into care/set up a trust fund to try and protect some of their assets, i was not asking how to avoid paying anything and make the government pay it all\", 'subreddit': 'UKPersonalFinance'}\n",
      "{'body': 'Goddamn.  The front fell off.', 'subreddit': 'CatastrophicFailure'}\n",
      "{'body': 'will fix ', 'subreddit': 'Cash4Cash'}\n",
      "{'body': '*Cho went over the injuries. The knife wounds were easy enough to fix, and luckily it seemed the blast had missed anything important. An hour later she was ready to wake Kin up.*', 'subreddit': 'galactic_senate'}\n",
      "{'body': 'Roto/8Cat/10Teams \\nMason Plumlee or Nikola Mirotic? \\n\\nThoughts now that Wade left the bulls? ', 'subreddit': 'fantasybball'}\n",
      "{'body': 'I member', 'subreddit': 'CFB'}\n",
      "{'body': 'Yeah I believe so', 'subreddit': 'Dallas'}\n",
      "{'body': 'https://twitter.com/xxbufsiz/status/913022619900796928', 'subreddit': 'yuri_jp'}\n",
      "{'body': \"Totally agreed. He's looking a little stiff in this colder weather. We don't need another injury. \", 'subreddit': 'AtlantaUnited'}\n",
      "{'body': \"I mean, it is a beta so anything can happen, but it worked for me.. I know this is bad advice, but I don't think a lot of bad things can really happen unless you're really unlucky.\", 'subreddit': 'jailbreak'}\n",
      "{'body': '[removed]', 'subreddit': 'theydidthemath'}\n",
      "{'body': '143413618| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413486\\n&gt;2016==&gt; Trump (and loving the shitstorm)\\n\\n#justdeplorablethings\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'It\\'s supposed to mimic this https://cdn.inquisitr.com/wp-content/uploads/2017/05/KKK-Torches-Charlottesville.jpg\\n\\nAdditionally, there\\'s not a lot of ambiguity around chanting \"Blood and soil\" along with other Nazi slogans.', 'subreddit': 'worldnews'}\n",
      "{'body': \"That's something I considered - he was in his 70s and didn't have many reviews (though the 2 he had were favorable). \\n\\nBut I mean...you don't pay Motel 6 price and expect the Hilton. Haha. \\n\\nETA: Of course, I wouldn't mind honest and in depth reviews if the AirBnb algorithms didn't fuck you over for having anything less that a perfect 5.0. \", 'subreddit': 'AirBnB'}\n",
      "{'body': 'Buttery males', 'subreddit': 'askgaybros'}\n",
      "{'body': 'LOL', 'subreddit': 'ElectricSkateboarding'}\n",
      "{'body': \"That's the pot calling the kettle black.\", 'subreddit': 'Jokes'}\n",
      "{'body': 'Nice 10k pc you got there.... cant even produce its own power', 'subreddit': 'DatGuyLirik'}\n",
      "{'body': 'I was you, then after getting hammered one too many times 4v3 or 4v2 and watching my stats take a big dip I now leave as soon as someone drops', 'subreddit': 'CruciblePlaybook'}\n",
      "{'body': 'I only like the Rockies when they wear purple', 'subreddit': 'Cardinals'}\n",
      "{'body': 'What’s brown and sticky?\\n\\nA stick.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Shaven Haven ', 'subreddit': 'pussy'}\n",
      "{'body': 'It is racist. Liberals are too dumb to understand. She doesnt want to date a white male because of the white male patriarchy she claims exists.', 'subreddit': 'Tinder'}\n",
      "{'body': 'Performance is balanced, you can take any car and succeeed probably.', 'subreddit': 'The_Crew'}\n",
      "{'body': 'Man, just wait for them to play and show themselves, same for every team. ', 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': 'What am I supposed to watch now? :( someone please suggest something.', 'subreddit': 'anime'}\n",
      "{'body': 'You’re 19, calm the fuck down.', 'subreddit': 'relationships'}\n",
      "{'body': 'Yeah pretty much. Not proud of my reaction but the sheer effort put into telling me to fuck off was crazy.\\n\\nNote: this was my ex-girlfriend from when I was in my early 20s, not my ex-wife - she certainly made mistakes but nothing like *that*.', 'subreddit': 'anime'}\n",
      "{'body': '10 Team, 1 PPR,\\n\\n- I have a very young team, I am also not competitive this year. (Almost guaranteed to start 0-4 after my opponents Rodgers/Nelson stack on Thursday).\\n\\n- **Trade:** Travis Kelce, Rawls, 2019 2nd (not mine, probably a mid) \\n\\n- **Receive:** OJ Howard, David Njoku, 2018 First (Likely mid).\\n\\n&amp;nbsp;\\n\\n- Rawls is just being throw in since I would drop him anyway. It\\'s either him or Clement and I\\'m probably dropping Clement for Marlon Mack.\\n\\n- My TE\\'s after this trade would be: Njoku, Jonnu Smith, and OJ Howard\\n\\n- It would also be my **fifth** 2018 first round pick. (Mine which will be 1.01 or 1.02 at the worst, two probable mids, and two probable late).\\n\\n&amp;nbsp;\\n\\n- I realize Kelce is a great asset. I do have Mahomes, which would make for a great stack in the future. But Kelce will probably be at the end of his prime by the time I am truly competitive. Kelce is my second oldest player. (Luck is my oldest). Kelce has also only had 1 elite year as a TE in fantasy. 2014/2015 were decent, but nothing like last year. They also didn\\'t have Kareem Hunt last year and Hill was a \"gadget\" rookie.', 'subreddit': 'DynastyFF'}\n",
      "{'body': \"A lot of these guys shouldn't be calling out Tony but the guys Tony should be fighting are AFK... Conor, Nate, Khabib, Eddie, etc.\", 'subreddit': 'MMA'}\n",
      "{'body': \"it's already open.\", 'subreddit': 'HFY'}\n",
      "{'body': 'Bent over I see. Pin you against that wall. Squeeze them tit from behind. Slide one hand down and rub you till you start getting wet. Slide the panty to the side and push myself in. Just the beginning', 'subreddit': 'gonewild'}\n",
      "{'body': \"&gt;**I can't help but feel like the principle is inspired by him in some way**\\n\\nYeah I can read. Apparently you can't. Don't worry, it's ok that you are a little bit behind for your age. I'm sure you'll catch up one day.\", 'subreddit': 'BokuNoHeroAcademia'}\n",
      "{'body': \"Power cords across the floor, cushioned chair, feet up... couldn't be at a salon... couldn't be.\", 'subreddit': 'thatHappened'}\n",
      "{'body': 'Ill give a try to Moda and phenyl soon, Ive been recommended for these. Others can be Adrafinil or Theacrine, but got no experience with them.', 'subreddit': 'StackAdvice'}\n",
      "{'body': \"Alwady.\\n\\n***\\n\\n^(Bleep-bloop, I'm a bot. This )^[portmanteau](https://en.wikipedia.org/wiki/Portmanteau) ^( was created from the phrase 'Always ready!'.)\", 'subreddit': 'Incels'}\n",
      "{'body': 'Were you winning? Could make sense then ', 'subreddit': 'FIFA'}\n",
      "{'body': \"Still close to a friend I made in 5th grade, but she lives in another state. Still closest to someone I have known for 30 years, but I moved to another state. Drove back when she was in an accident and stayed with her for 4 weeks till she was better. Locally, some one I have known for seven years is very close to me. A neighbor I have known for three years. Not sure which one of these woman are my 'best' friends, each of them had their strong points. \", 'subreddit': 'AskReddit'}\n",
      "{'body': \"$100 says none of them have any strong male characters in their lives. I smell 'daddy issues'\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'sold google pixel to /u/yarudl', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'What type of equipment here?', 'subreddit': 'SkyPorn'}\n",
      "{'body': '/u/amazingpikachu_qw', 'subreddit': 'amazingpikachu_38'}\n",
      "{'body': '&gt; East Tea Can\\nInteresting, I shop at the nearby Longos and just noticed this place. Will definitely try it - the menu looks great!\\nETA - Thank you!', 'subreddit': 'mississauga'}\n",
      "{'body': 'Is that a view of philly? Looks like one of the Liberty Towers. I love the monitors too. I’d love that setup boss. ', 'subreddit': 'battlestations'}\n",
      "{'body': '[removed]', 'subreddit': 'TheRedPill'}\n",
      "{'body': 'Might you be able to create a zip file with those?  This is a good strategy!', 'subreddit': 'PipeTobacco'}\n",
      "{'body': 'Yes. As long as you have a key you can open the chest. ', 'subreddit': 'Fireteams'}\n",
      "{'body': 'I found you can say \"Hey Google, sleep\". Then the SHIELD sleeps. You can wake it up if you have enabled always listening. Then you can say \"Hey Google, launch YouTube\". The SHIELD will turn on.', 'subreddit': 'AndroidTV'}\n",
      "{'body': 'Tbh, with a quality loss in Pullman, I think he has it locked up.  The dude just comes alive in the final 2 minutes of a game.  Seriously.  The dude is money for 2 minutes out of 60 in 80% of games this year, how do you deny him the Heisman?', 'subreddit': 'CFB'}\n",
      "{'body': \"oh fuck i didn't realise that was laura palmer and thought it was a mirrored steel toaster reflecting OP's smile\", 'subreddit': 'LiverpoolFC'}\n",
      "{'body': \"I think you've got NV turned on \", 'subreddit': 'AskPhotography'}\n",
      "{'body': 'The word “dragon” comes from the Greek word “draconta,” which means “to watch.” The Greeks saw dragons as beasts that guarded valuable items. In fact, many cultures depict dragons as hoarding treasure.', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'Have 2, 299 titan, 302 warlock GT: Slenderstalker1', 'subreddit': 'Fireteams'}\n",
      "{'body': 'https://www.reddit.com/r/KarmaCourt/comments/70ooah/the_people_of_numerous_subs_vs_unotkennyloggins/?st=J85RS0J8&amp;sh=cd30a395', 'subreddit': 'Prematurecelebration'}\n",
      "{'body': \"Phone screens still do scratch. Glass has a 5.5 mohs rating and is made from sand. Sand can and will scratch your screen. I've seen plenty of screens with scratched edges and fronts, even if the phones have cases. It's impossible to avoid scratches without a screen protector.\\n\\nThe reason you listed isn't a reason. Screen protectors are super easy to replace and they protect your screen. I always have and always will use a screen protector. \", 'subreddit': 'LifeProTips'}\n",
      "{'body': 'The room moves when I close my eyes and makes me dizzy', 'subreddit': 'AskEurope'}\n",
      "{'body': 'Classic.', 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': '\"Hey man, I\\'m looking to pick up within x days. Are you in stock yet or should I hit someone else up?\"', 'subreddit': 'saplings'}\n",
      "{'body': \"I know there's a French vanilla flavoured coffee that is available in fine grind and k-cups (not sure if it's in Tassimo but if anyone knows that please let me know!) that looks pretty similar to the other coffee packages but has a blue box behind the writing where the normal coffee has a red box on it. I can tell you with absolute certainty that that is not what you're looking for. The French vanilla cappuccino sounds like it should be what you're looking for, but I haven't seen it so I wouldn't be able to say for sure. \", 'subreddit': 'TimHortons'}\n",
      "{'body': '4 is not more absurd than 5. It had dark, foreboding Spanish villages, dark lightning, forests, moody castles and dim lighting. Only a segment on the Island towards the last leg of the game does it begin to twist into a more RE5/6 action-esque chapter. \\n\\n5 however, from the getgo, is action packed and horror lite. Crowds of enemies in broad daylight in the  African heat, followed by silly tentacle bosses like Excella or dragon sea monsters like Irving, huge minigun fights on a boat, plus other things. A co-op partner in the form of Sheva also helps eliminate any pure traces of survival horror.\\n\\nDon’t get me wrong, I like RE4 and RE5  a lot. Both of them are great games, I actually prefer 5 over 4, but you can’t deny.', 'subreddit': 'residentevil'}\n",
      "{'body': \"Not really. He may have been a little dismissive of Schrab, but Schrab thrives on playing the heel, so I didn't feel it was an issue. Dan verbally abusing the audience, however...\", 'subreddit': 'Harmontown'}\n",
      "{'body': 'Do good christians sexually assault girls at Disneyland?', 'subreddit': 'dankchristianmemes'}\n",
      "{'body': 'How about durability? Mac air came in aluminum body. I never dropped it or anything, but even though it had several minor crash on to wall, this laptop just never failed on any thing. No virus, no display failure, no internal problems, nothing. Do you think Aero 15X can last long without severe problem?\\n', 'subreddit': 'SuggestALaptop'}\n",
      "{'body': '16,186', 'subreddit': 'antimatterindustries'}\n",
      "{'body': \"It probably doesn't do much to assuage your worries about the way that you represent veganism (even though I don't think you need to worry about that), but if someone says something along those lines to you, you can just point out that veganism didn't give you an autoimmune disease. And by caring about your own health, about animals and about the environment, you *are* being a good example. You're doing what you can with what you have, which is all that can be asked of anyone. \", 'subreddit': 'vegan'}\n",
      "{'body': '[deleted]', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Cool. You should focus on the fact that your dad loves you dispite him being ignorant of modern gender/sexuality expression.', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': \"I agree, it's just that ethically speaking, this is bs\", 'subreddit': 'electronic_cigarette'}\n",
      "{'body': 'Omg what a cutie :D', 'subreddit': 'aww'}\n",
      "{'body': \"Maybe you'd be more artistic if you didn't spend all your time playing Rocket League. /s\", 'subreddit': 'AnimalCrossing'}\n",
      "{'body': '###[Here\\'s your Reddit Silver, Grimey_ass_nigga!](http://i.imgur.com/x0jw93q.png \"Reddit Silver\") \\n***\\n/u/Grimey_ass_nigga has received silver 1 time. (given by /u/lemmelickurcucumber) __[info](http://reddit.com/r/RedditSilverRobot)__', 'subreddit': 'Drugs'}\n",
      "{'body': 'And the \"mainland\" probably refer to England, Wales, &amp; Scotland, but not North Ireland, the Island of Man, or all these other enclaves and islands', 'subreddit': 'MapPorn'}\n",
      "{'body': 'Sweet sweet release', 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '\\n[I live near them ](http://www.achtypistours.gr/sites/default/files/imagecache/galleryformatter_slide/12/08/pyramids_and_sphinx.jpg)', 'subreddit': 'Wishlist'}\n",
      "{'body': 'They actually do pay federal taxes. Personal income tax is the one major exception.\\n\\n', 'subreddit': 'Ask_Politics'}\n",
      "{'body': 'I\\'m usually cynical as fuck about everything I read, but this just screams corruption. She\\'s probably getting  other forms of \"aid\" delivered straight into her personal bank account, or she\\'s just trying to save her own ass by blaming it all on Trump. The man sent 10k people and flooded their ports with supplies, what the fuck else do you want', 'subreddit': 'The_Donald'}\n",
      "{'body': \"Yep I just don't see all of the hype around Rahm when he really hasn't done anything of note\", 'subreddit': 'golf'}\n",
      "{'body': 'Thats rough', 'subreddit': 'unt'}\n",
      "{'body': \"Yes, I was very confused at first, and not really interested in it, but the shades of green were perfect for my room, and Michaels had them on sale, so I figured I'd give it a shot. I used a P hook, mesh stitch, and it was pretty easy to work with! I'm pleased with the overall result. It's cozy, sorta hippie-ish. 😃 \", 'subreddit': 'crochet'}\n",
      "{'body': \"Are you still married?  Are you still being abused?  Your post made it sound that way.  \\n\\n&gt;Over the years, he's convinced them I'm the abuser and that an example of my abuse is calling the police\\n\\nAbusers often accuse their victims of being abusive.  Your children are adults.  At this point in their lives, they should recognize his bullshit for what it is.  If they really think that you are the problem, then it sounds like they take after their father.  I'm sorry to say that.  \\n\\nIs that the reason you don't want to go against him?  Because you're worried you'll alienate your children?\", 'subreddit': 'domesticviolence'}\n",
      "{'body': \"For when they masturbate. Most people don't want to clean up with there under garments when they still want to use them that day. \", 'subreddit': 'techsupportgore'}\n",
      "{'body': \"If it doesn't have screen on the lid with which I can stream all my favorite Geek Franchises from beyond the grave, then they can FUCK RIGHT OFF.\", 'subreddit': 'RedLetterMedia'}\n",
      "{'body': 'One random', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'I love your post!! \\n\"It\\'s messed up that alcohol is the problem and it also convinces us it\\'s the solution.\"\\nTHIS. Sooo true. \\n', 'subreddit': 'stopdrinking'}\n",
      "{'body': 'I use genned Pokémon for Masuda then relates after I get my shiny :)', 'subreddit': 'CasualPokemonTrades'}\n",
      "{'body': 'I bought the same set. I love them!', 'subreddit': 'electricians'}\n",
      "{'body': \"My company matches dollar for dollar up to 6% of my gross pay.  They also throw an additional 3% for free because we're a non-union site.  \", 'subreddit': 'personalfinance'}\n",
      "{'body': \"I'd say june at the latest without having a real impact\", 'subreddit': 'premed'}\n",
      "{'body': 'You need to increase the frequency of your DC power.', 'subreddit': 'MechanicAdvice'}\n",
      "{'body': '[removed]', 'subreddit': 'recipe'}\n",
      "{'body': '[removed]', 'subreddit': 'beermoney'}\n",
      "{'body': 'What problems are you speaking of, specifically?', 'subreddit': 'AskReddit'}\n",
      "{'body': 'It is all on Hulu. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"This was sparked out of conversation with a friend about the current mindset of the average American. The general distrust of our government, growing inequality(including the rise of AI and the inevitable massive loss of jobs), mass surveillance and consistent nuclear threat by delusional leaders being the main catalysts. And by media portrayal and tribe mentality among social media, these issues and the likelihood of revolution seem to be growing everyday.  It seems this form of corrupt government certainly has a limited life-span. It also seems relatively certain there lies a tipping point at which the majority of Americans will be so fed up with the failing system that unification to overthrow the gov is inevitable. Do you think it's likely, or even possible? Is it preventable or inevitable? What do you think this could look like? A lot of big, unanswerable questions here. Just curious what your thoughts are!\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'I would be careful with advising the p47m model for the US. Its flight model gets borked every other patch and this patch it seems the M is not very good at the moment. Better off putting a tali on the p47N or like you advised the US spit.', 'subreddit': 'Warthunder'}\n",
      "{'body': \"Very pretty. I've made a couple hats this week and hated how they came out. I'm jealous of you! Ha ha. \", 'subreddit': 'crochet'}\n",
      "{'body': \"I take 2 trains to work and 2 trains home.  That's $7 a day for 5 days a week for $35.  \", 'subreddit': 'LosAngeles'}\n",
      "{'body': 'esoognoM-', 'subreddit': 'Fireteams'}\n",
      "{'body': \"What a battle. Still confused by several of the officials calls but I'll take it. \\n\\nAlso, we hit the most unlikely 50 yard field goal of all time. I don't even trust the kickers to make an XP\", 'subreddit': 'CFB'}\n",
      "{'body': 'She owns everything! Everything is hers! ', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Make em yourself.', 'subreddit': 'ottawa'}\n",
      "{'body': \"Fucking awesome. Unexpected awesome.\\n\\nDefinitely best new animated show since rick &amp; morty. IMO it's already on that level.\\n\\nWhen a 12 yr old kid's hairy hormone monster skull-fucked a decapitated head, I was sold.\", 'subreddit': 'television'}\n",
      "{'body': 'https://www.blackvue.com/shop/\\n\\nBlackVue DR750S-2CH\\n\\nDual Front and Rear full HD dash cam with Sony STARVIS Image Sensor, GPS, Wi-Fi, Cloud connectivity, impact and motion detection.\\n\\n\\n\\n', 'subreddit': 'Denver'}\n",
      "{'body': \"Ammonia is quite water soluble and the amounts we measure in aquariums are pretty small so you may actually be able to detect ammonia from the air in the water.  \\n\\nOf course, it doesn't matter where it comes from as it has the same effect on fish. But if your tank is nowhere near your litter box or bird cage but you run the test near one, it could affect the results.\", 'subreddit': 'Aquariums'}\n",
      "{'body': 'Removed.  Video deleted.', 'subreddit': 'AsianAmericanPorn'}\n",
      "{'body': 'BAARLEEY PRIMOOOO', 'subreddit': 'Brawlstars'}\n",
      "{'body': '[removed]', 'subreddit': 'osugame'}\n",
      "{'body': 'Fitness pf the storm were bsicly a cho gall only stream and had around 300 viewrs alwys so yeah', 'subreddit': 'heroesofthestorm'}\n",
      "{'body': '&gt;He said he supported the pair protesting, but wanted them to do it in other ways - kneeling after a touchdown in the end zone or writing and passing out a paper about the issues.\\n\\n\"I support a protest, but only when and how I say.\" Just sack up and say you don\\'t support it--it\\'s not support if you dictate how it\\'s done.  I\\'d still think it\\'s stupid, but could respect the honesty at least.\\n\\nThat said, there\\'s really nothing anyone can do since it sounds like the program is private.  Such programs can set whatever standards they like as long as they don\\'t discriminate against a protected class. A public school would be a different story I think.\\n      ', 'subreddit': 'news'}\n",
      "{'body': \"We're dying! Please, for the love of God, SEND CAN OPENERS!\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'Added', 'subreddit': 'DirtySnapchat'}\n",
      "{'body': 'Ms st course to ruining this for you. ', 'subreddit': 'CFB'}\n",
      "{'body': \"That's beautiful! I always loved the card, but the art never did a damn thing for me.\", 'subreddit': 'magicTCG'}\n",
      "{'body': '&gt; This makes no logical, rhetorical, or grammatical sense\\n\\nYes it does, because when you say the reasoning behind the action is fair but the action as taken is not you\\'re defending the intent to take action, not the specific action itself. In this sense you defend an action taken in response to their thinking, just not the kind of action. The kind of thinking that leads to genocide is not rational, its not sensible, and its not fair. There is no just motive that makes people believe in mass rape. That is hate. Your entire premise is that there\\'s a just way to think of another people in a way that can make you want to do this.\\n\\n&gt;which is that you think I\\'m excusing rape\\n\\nOn some level you are to an extent. Its like if someone said \"so and so got raped by her ex\" and someone replied \"well she did screw him over hard in the divorce\". You know saying that is fucked up and you know the impulse to consider the action that way and then express it says something terrible about how the speaker is envisioning the dynamics of the relationship.\\n\\nWhen people are tried for crimes the things you\\'re saying are used as mitigation. The reasoning behind an act is used to lessen or strengthen the condemnation and ensuing punishment for the act. On any level where you look at how our value system manifests judgment for actions taken you are mitigating it.\\n\\n&gt;The problem with your argument is that nowhere did I say I supported the how of the Burmese actions\\n\\nI already acknowledged that. I said that doesn\\'t matter and I have explained why. Your prejudice against Islam allows you to justify however thinking in these grotesque terms and think you\\'re clean in doing so.', 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': 'added', 'subreddit': 'mechmarket'}\n",
      "{'body': 'I like the mood you have with this beat. The bells are dope at creating an eerie dark feeling. I do think that there are a couple of mixing issues though. Like u/AnythingIsBad said, the 808 and kick don’t really punch. The snare also seems a little weak as well. Other than that I think this is great. Keep it up!', 'subreddit': 'makinghiphop'}\n",
      "{'body': \"It depends on whether the toll collection authority has a reciprocal agreement with the state/province.\\n\\nI have used British Columbia plates in Washington and California, and successfully evaded the tolls because while my license plate was scanned, no agreement was in place to actually track me down and bill me. Same for using California plates in Ontario, Canada.\\n\\nToll collection authorities will only track down out-of-state plates that they expect to show up frequently. For example, New York is often visited by drivers from Ontario and Massachusetts, so if you're from those places and try to use NY tolled roads without an EZPass you will get nailed. Likewise, Ontario will track drivers from Quebec, Michigan, and NY for sure. But if your car is registered in California, then it's highly unlikely that they'll bother to track you down. It costs them money to do so, and California-plated cars are a rare sight in that region.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Interested in Beauty and the Beast, or Incredibles\\n\\nhttps://www.reddit.com/r/uvtrade/comments/72lsuf/offer_guardians_of_the_galaxy_2_diary_of_a_wimpy/', 'subreddit': 'uvtrade'}\n",
      "{'body': 'Fam we gon organize. #teamcro', 'subreddit': 'KendrickLamar'}\n",
      "{'body': 'I like Arriva as they have charge points and are cheaper ', 'subreddit': 'manchester'}\n",
      "{'body': 'Git ur ass over to r/tightywhities', 'subreddit': 'Bulges'}\n",
      "{'body': 'Yeah, pretty sure that was against the wind but he definitely shanked that one regardless', 'subreddit': 'CFB'}\n",
      "{'body': '*aboot\\nCanadians where right the whole time', 'subreddit': 'videos'}\n",
      "{'body': 'If you would like a battle run for this conflict post, you must post it in the appropriate [MODPOST] thread designated for collecting [CONFLICT] posts.\\n\\n#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/worldpowers) if you have any questions or concerns.*', 'subreddit': 'worldpowers'}\n",
      "{'body': 'Big_d_dray ', 'subreddit': 'Fireteams'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"He's more important to God.\", 'subreddit': 'exmormon'}\n",
      "{'body': 'unisom and b6. you HAVE to take them together for it to work. ask your doctor.', 'subreddit': 'pregnant'}\n",
      "{'body': \"Doesn't surprise me at all I wouldn't trust that lying scum to tell me the time . If his mouth is moving then rest assured it's spreading lies \", 'subreddit': 'joesymon'}\n",
      "{'body': \"143416868| &gt; United States Anonymous (ID: nN0OTd4c)\\n\\n&gt;&gt;143415951\\n\\nif you let me smoke weed and leave me alone i'll vote for you\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': \"I don't think we'll ever get We Are Legend. Sentido is pretty great as well. \", 'subreddit': 'EDM'}\n",
      "{'body': 'Chiefs ', 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': \"Aaaand....they're probably gonna remember it, come election time.\", 'subreddit': 'The_Donald'}\n",
      "{'body': \"Fuck sake, thank you for the answer bro. \\n\\nI'm £41 down for literally NOTHING :(\", 'subreddit': 'g2a'}\n",
      "{'body': \"I've been doing even 3rd washes and stocking those in my fridge and freezer. May or may not help but at least I'll have it if I run out. \\n\\nGlad you're feeling better too!!👏🏼🙂\", 'subreddit': 'PoppyTea'}\n",
      "{'body': '14, 32, and three randoms please. If the requested spots aren’t available, I’ll take randoms instead. Five total spots. ', 'subreddit': 'edc_raffle'}\n",
      "{'body': '[deleted]', 'subreddit': 'conspiracy'}\n",
      "{'body': 'Srs this is just insane. What is it about beauty gurus that makes people froth at the mouth? I just want to enjoy pretty makeup dammit. ', 'subreddit': 'BGCCircleJerk'}\n",
      "{'body': 'Thanks! That was interesting to read.', 'subreddit': 'trains'}\n",
      "{'body': 'As for the name? I doubt he was the first to pioneer it but I\\'ve always called them Wheeler handles or Wheeler knock off\\'s because he does it correctly while the majority fall short.\\n\\nI love the look of these handles in photo\\'s but the pinky swell is almost always too wide/fat for my tastes. Not a fan of having my index grip tighter than my pinky\\'s. I prefer an equal grip across the handle or slight tapering towards the rear.\\n\\nI understand the idea behind the design but it is actually counter intuitive for such a small knife. The flair is better suited on a chopper above 10\" in blade length. Think Kukri handles.\\n\\nIf my pinky doesn\\'t touch or nearly touch the inside of my palm when I grip it, it\\'s too fat and diminish the overall grip. I\\'m not a big guy but I have piano fingers and if the blade wont fit my hand it wont fit a 6\\' 5\" mans hand correctly either with there extra body fat.\\n\\nIf you want a great example of these in slight variations look too Nick Wheeler. He does it right. You\\'ll notice the pinky swell on most of his models are obviously smaller than the index grip unlike the one in the linked photo. And if they are not they are more dramatically shaped at the rear. Doesn\\'t look as pretty but I feel it\\'s more practical.\\n\\nHere\\'s a great [example](https://www.instagram.com/p/BXwy6dKFuew/?taken-by=nick_wheeler_knives_and_newfs). It may be hard to tell for some but that extra meat being removed will make a world of difference in the hand.\\n\\nQuick test for people at home. Hold your hand up. Is your pinky as long as your index? If no then the swell should not be thicker than your index unless your intention is to make a chopper. Equal grip across the boards makes for a more versatile grip but the blades shape truly dictates the knifes usage so plan accordingly.', 'subreddit': 'Bladesmith'}\n",
      "{'body': \"Yeah. His plan to replace Cersei with Margaery wouldn't work if he didn't know.\", 'subreddit': 'asoiaf'}\n",
      "{'body': 'Waiting on entry \\n', 'subreddit': 'nsfwrabbitrooms'}\n",
      "{'body': 'I agree. Shinji is somewhat of a coward but not nearly as cowardly as people make him out to be. ', 'subreddit': 'evangelion'}\n",
      "{'body': 'Jasmine masters*', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': \"Mond probably shouldn't pass tonight. It will only hurt A&amp;Ms offense. \", 'subreddit': 'CFB'}\n",
      "{'body': 'Wow they didnt know what they had at all.', 'subreddit': 'Guitar'}\n",
      "{'body': '[deleted]', 'subreddit': 'wholesomememes'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I would take out a loan for that sheep if they made her ', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'Poor guy', 'subreddit': 'holdmybeer'}\n",
      "{'body': \"I understand the impact of wireless vs wired. \\n\\nBut I'm taking wired out of the question, and asking... the difference between a $50 wireless adapter and $100 adapter.\\n\\nSorry if that wasn't clear.\", 'subreddit': 'buildapc'}\n",
      "{'body': \"It's really hard to know why Roger has such poor conversion rate relative to what you'd expect given his normal performance. It's hard to say he's choking, because he's extremely good on tie breaks. \\n\\nPerhaps he's overly conservative on breaks? I'm not sure, I feel like he's often too concerned with just landing the return in on breaking points and not risking it enough. Not always, of course, but often. \", 'subreddit': 'tennis'}\n",
      "{'body': \"That's just a taste of what FSU fans deal with all season long.  \", 'subreddit': 'CFB'}\n",
      "{'body': \"I really didn't expect to get in like this. I don't know what to do. \", 'subreddit': 'ColoradoRockies'}\n",
      "{'body': \"This!!! Like it's a competition. And omg, the hate Japril fans have been dishing out to Maggie since the season 13 finale is alot worse!\", 'subreddit': 'greysanatomy'}\n",
      "{'body': '2 spots', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"How'd you link to a picture? \", 'subreddit': 'hogwartswerewolvesA'}\n",
      "{'body': \"Not at all, we'll be better next year im sure \", 'subreddit': 'CFB'}\n",
      "{'body': 'Hey how did you resolve this issue? New account? Change payment methods?', 'subreddit': 'usenet'}\n",
      "{'body': 'Great ass', 'subreddit': 'gonewildchubby'}\n",
      "{'body': 'Haha', 'subreddit': 'FortNiteBR'}\n",
      "{'body': 'I’ve wondered- what does it mean to “keep your mind in hell and don’t despair”?', 'subreddit': 'OrthodoxChristianity'}\n",
      "{'body': 'How the fuck is that a sandwich ?', 'subreddit': 'food'}\n",
      "{'body': 'just don\\'t write the brand name, when it\\'s a Supreme tee you can just put \"graphic tee\" same with other apparel, if it\\'s a grey FOG hoodie you just put grey hoodie ', 'subreddit': 'FashionRepsBST'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I don't agree with them breaking up with you over it but still just be careful so you don't get hurt... might come across the wrong kind of person one day :( \\n\\nBut you did awesome. Bet little ol bitty bitch didn't know what hit her 😂\", 'subreddit': 'JUSTNOMIL'}\n",
      "{'body': '**VERY VERY BTA** dont you dare say its only bta', 'subreddit': 'GlobalOffensiveTrade'}\n",
      "{'body': \"Thanks!  I can't believe it either.  It's crazy how things turn out!\", 'subreddit': 'beyondthebump'}\n",
      "{'body': \"I don't think Melvin Manhoef ever gave a shit about weight. He was pretty muscular himself, trained to fight like a killer, and was confident as hell early in his career.\\n \\nThe guy KO'd Mark Hunt with pretty much 1 punch. https://www.youtube.com/watch?v=Cf9U0quIrB8\", 'subreddit': 'MMA'}\n",
      "{'body': 'Can you lend me a jar of love?', 'subreddit': 'fantasyfootball'}\n",
      "{'body': '/u/TOP_708', 'subreddit': 'TOP_20'}\n",
      "{'body': '&lt;3', 'subreddit': 'teenagers'}\n",
      "{'body': 'Spots 17 and 31 please.', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'What do I put on it to make it one?\\nI put “this is a poll” for now \\nDone', 'subreddit': 'CasualUK'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': \"I think that's Chinese you're hearing.\", 'subreddit': 'videos'}\n",
      "{'body': 'Yep, just did the same thing at the same time. Its getting too tight to get called on.', 'subreddit': 'BitcoinMarkets'}\n",
      "{'body': 'SemiAmusingBot says: I am now installed on a VPS. reply #3 test', 'subreddit': 'pythonforengineers'}\n",
      "{'body': 'E', 'subreddit': 'AskOuija'}\n",
      "{'body': \"LOL imagine a battle royale in destiny. I don't think bluehole has the guts to sue bungie :p\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"When trading use Warframe Market because if you don't you don't know if your getting a good or bad deal.\", 'subreddit': 'Warframe'}\n",
      "{'body': '100% the \"fighting for our freedom\" line as a recruitment tool is nothing but nonsense.   If it was all about the pay check or the education there are better and safer ways of getting those things.', 'subreddit': 'news'}\n",
      "{'body': 'You want too experience a different culture way of life want to see the world/your country from their point of view,Their history &amp; food Also what country do you want to study in?', 'subreddit': 'Advice'}\n",
      "{'body': '[deleted]', 'subreddit': 'funny'}\n",
      "{'body': 'This has been posted like 3 or 4 times in the last few days. ', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'I agree. But damn, someone made a huge error. We just moved a house across a street, remodeled it, and moved it back with less damage. (Moved out of a flood zone for FEMA rule work around )', 'subreddit': 'Wellthatsucks'}\n",
      "{'body': \"On buzzfeed in a week and your uncle's facebook in three.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'En resumen: que te hinchen los huevos a niveles de explosión', 'subreddit': 'argentina'}\n",
      "{'body': 'I read this with Rick\\'s voice from \"Rick and Morty\" and that was absolutely glorious lol.', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"so many African guys so obviously well off with their designer clothes and accessories hustled me for money handouts that i don't know what to think of it anymore, really perplexing\", 'subreddit': 'TumblrInAction'}\n",
      "{'body': 'one random please', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"It's free inhabitants all the way down.\", 'subreddit': 'videos'}\n",
      "{'body': '#FriendOfThePod', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': \"143417405| &gt; United States Anonymous (ID: w371oNMY)\\n\\n2008: Bob Barr and I think democratic downticket.\\n2012: Aleppo and I think republican downticket.\\n2016: Trump and all republican downticket.\\n\\nI'll vote democrat if it's a Jim Webb style of DINO or blue dog democrat. In other words I will never be voting democrat.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': 'Wasn\\'t saying it was that way for everyone, I specifically avoided saying \"all\" for that reason.  /shrug \\n', 'subreddit': 'gifs'}\n",
      "{'body': \"I think you guys are pretty solid honestly. You've really only struggled in our game, as far as I can tell.\", 'subreddit': 'CFB'}\n",
      "{'body': 'Which they have in multitudes of other call of duties its fun to go back to ww2 gunplay and no exo movements. ', 'subreddit': 'CallOfDutyWorldWarTwo'}\n",
      "{'body': \"Pilot who is scared of heights reporting in. Yes it's a thing, and I've got several friends who are also pilots and also don't like heights. For me at least, it's not heights themselves, it's what I'm on/in. On cliffs, tall bridges, buildings, ect. I loose my shit. Put me in an airplane, and I'm chill as fuck. I'm not sure why, but it might have something to do with being a bit of a control freak. If I'm in an airplane, in my mind, there's a whole lot less that can go wrong, because I've done my preflight, and followed all the checklists, and if I'm not PIC, then I know that whoever is flying knows what they're doing.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"I'm happy for you.  I was watching Stef Sanjati FFS video and hers was paid for by a gofundme campaign.  I think it was around $37,000 for forehead, upper lip, trachea shave, jaw and chin.  \", 'subreddit': 'asktransgender'}\n",
      "{'body': 'I think you misunderstood my comment. I agree with you though.', 'subreddit': 'australia'}\n",
      "{'body': 'FKM', 'subreddit': 'TrueFMK'}\n",
      "{'body': \"Wouldn't be surprised if Garza is actually hurting.  \", 'subreddit': 'MLS'}\n",
      "{'body': \"&gt;Definitely defective? The chip could be fine; the only way to know for sure is to test.\\n\\nIt was a fabrication issue(not a bug).  It's been replicated at level1techs as well.  So yes, it's definitely defective if you use compilers on Linux(which most people don't do in benchmarks).  Pre-week 25 has the compiler issue.  Post-25 doesn't.\\n\", 'subreddit': 'linuxhardware'}\n",
      "{'body': \"Got it. I don't have a particular attachment to either so I guess I'll be putting off rerolling until another event. Thanks for the explanation though.\", 'subreddit': 'grandorder'}\n",
      "{'body': 'Question from the woodwork: is it okay to come out?', 'subreddit': 'CFB'}\n",
      "{'body': 'Thanks for listening!', 'subreddit': 'JordanPeterson'}\n",
      "{'body': 'That concentration tho ', 'subreddit': 'TheStrokes'}\n",
      "{'body': \"The reasons for them not using it are vauge, we don't know exactly why, even in Gods of Mars they don't specify EXACTLY...since that's the freaking mantra of most black library books, they never want to be specific unless it leads to a plot which IS ambiguous. \\n\\nbut as for it being part of necrontyr tech.\\n\\n&gt;Kotov sighed and nodded as if Roboute had passed some kind of test.‘Very well, Mister Surcouf, I believe you may be correct. Perhaps some aspect of necrontyr technology does lie at the heart of the Breath of the Gods, and if that is the case, then it is doubly imperative we prevent Telok from leaving this world.’\\n‘Why?’ said Anders, ‘I mean, besides the obvious?’\\n‘Because if there is any truth to the old legends, then it is entirely possible that a vast shard of one of the ancient necrontyr gods lies entombed within the Noctis Labyrinthus.’\\nAnd suddenly it all made a twisted kind of sense to Roboute. He turned to Bielanna, who appeared to be studiously ignoring their conversation.\\n‘You knew, didn’t you?’ he said. ‘You said as much back in the cavern. What did you call it? “The infernal engine of the Yngir?” I’m going to assume that’s your word for the necrontyr gods.’\\nBielanna nodded slowly.\\n‘Now you see why we fought so hard to stop you,’ she said. ‘And why we now spill our blood to help you.’\", 'subreddit': 'whowouldwin'}\n",
      "{'body': 'He had it... but over slid the bag...', 'subreddit': 'letsgofish'}\n",
      "{'body': 'a puppy', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Your [**post**](https://www.reddit.com/r/twinpeaks/comments/73ig94/can_someone_translate_this_interview_with_monica/) has been automatically removed because every post title Must Begin With a **written** [bracketed tag] of what it has spoilers for. **Pick only one!** If your title does not contain one of the **exact tags from this list** it will be removed! The list of tags is as followed.\\n\\n* [No Spoilers] - (Your post has no spoilers and **won't generate spoilery comments**)\\n* [Original Run] -(Your post has spoilers related to anything prior to season 3 including the show, movie, and books)\\n* [All] -(Your post has spoilers for season 3 *and possibly all prior content*)\\n\\n\\nPlease [resubmit your content.](https://www.reddit.com/r/twinpeaks/submit?title=%5BReplace%20This%20With%20Proper%20Tag%5D&amp;selftext=true) with the proper tag.\\n\\n***\\n\\n[Submission Rules](https://www.reddit.com/r/twinpeaks/wiki/subredditrules#wiki_submission_rules) | [Spoiler Guide](https://www.reddit.com/r/twinpeaks/wiki/spoilerpolicy) | [FAQ](https://www.reddit.com/r/twinpeaks/wiki/faq)\\n\\n\\n\\n ***\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/twinpeaks) if you have any questions or concerns.*\", 'subreddit': 'twinpeaks'}\n",
      "{'body': 'Yes.\\n\\nhttps://alexandria-library.space/files/Ebooks/WorldTracker/', 'subreddit': 'opendirectories'}\n",
      "{'body': 'Cooking Mama', 'subreddit': 'AskReddit'}\n",
      "{'body': 'His humor has always benefitted from bombastic and spontaneous energy - in this he looked like he could barely contain himself from saying something ridiculous or breaking out in fits of laughter. Or madness. Or both. ', 'subreddit': 'gamegrumps'}\n",
      "{'body': 'Probably the latest ATH? 🤔', 'subreddit': 'waltonchain'}\n",
      "{'body': 'Mega big ass fries...you have been deemed an unfit monther. Thanks for choosing Carl’s JR. ', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'Would like to know if I messed up on making this map too horribly.', 'subreddit': 'mapmaking'}\n",
      "{'body': '[removed]', 'subreddit': 'television'}\n",
      "{'body': \"I'll take one! \", 'subreddit': 'edc_raffle'}\n",
      "{'body': \"Now I don't care. I actually prefer Doom\", 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'Depending on the size of the lens you\\'re using, you can sometimes get decent results by getting right up to a chain link fence and shooting \"through\" it. \\n\\nAs it stands, there\\'s no real subject here. The viewers\\' eyes just aren\\'t drawn to anything in particular. ', 'subreddit': 'photocritique'}\n",
      "{'body': 'Wooooo', 'subreddit': 'baseball'}\n",
      "{'body': 'Come on Truex I want you to win really badly.', 'subreddit': 'NASCAR'}\n",
      "{'body': 'Will have to disagree considering they aint. LOL people thumb down like total bots.  Both are offensive abilities and one is Doomfist primary weapon and allows him to take range started attack. The other is a long range attack for a melee. They both offensive and kill people and do damage but Doomfist attack only hits one target with damage whilst Firestrike goes through all targets.', 'subreddit': 'Overwatch'}\n",
      "{'body': 'Sad but true', 'subreddit': 'MGTOW'}\n",
      "{'body': \"I love this. When my mother was in the hospital after being diagnosed with non-Hodgkin's lymphoma, her (very healthy looking) doctor told her that she had beat the same cancer herself many years ago. It was an incredible ray of hope for my mom. No doubt the kids that this girl treats will feel as hopeful when they find out she beat cancer too.\", 'subreddit': 'UpliftingNews'}\n",
      "{'body': 'Yeah, but has he knocked out George Groves in front of 80,000 fans at Wembley Stadium? ', 'subreddit': 'MMA'}\n",
      "{'body': 'They like Hitler, hate Jews and deny the holocaust happened, you can determine where they fit in from that.', 'subreddit': 'worldnews'}\n",
      "{'body': \"No worries, I knew that, just having a bit of conversation. I wasn't at all offended.\", 'subreddit': 'soccer'}\n",
      "{'body': 'surely not when waiting for any switch game that takes forever lol.', 'subreddit': 'bindingofisaac'}\n",
      "{'body': 'Od crates. ', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"&gt;does Blackbeard already found out about poneglyphs?\\n\\nHe better have. He's already been established as someone who does his research on things of interest and the pieces that fit into his plan of becoming Pirate King. Also, if he didn't know before, his status as a Yonko, acquisition of a large crew and islands should've provided the opportunity for him to have discovered that at some point.\\n\\n&gt;and his whole crew gang up on her its likely he'll take her down\\n\\nIf Blackbeard still needs his entire crew to take down one weakened Yonko, he's weak, and I hope that isn't the case.\", 'subreddit': 'OnePiece'}\n",
      "{'body': \"Oh sorry, missed what you were trying to say. I'm honestly not sure then. That's super weird. Canadian is like 70 cents on the US dollar right now, yeah? \", 'subreddit': '3DS'}\n",
      "{'body': 'Well, getting naked would only work IF Kiyo sees you as Anchin.', 'subreddit': 'grandorder'}\n",
      "{'body': 'I like the whole aesthetic and mood of it. It rained pretty hard last night and i live in a city without led street lights, so the amber glow in the puddles looked pretty nice. I had to bike home though. That sucked. ', 'subreddit': 'CasualConversation'}\n",
      "{'body': 'What was that?\\nDid I hear a running game?', 'subreddit': 'CFB'}\n",
      "{'body': '\"Fuck Dee Gordon\" - Rio Ruiz', 'subreddit': 'Braves'}\n",
      "{'body': 'I played it a few times when I first started. Then I honestly forgot it was even part of the game.', 'subreddit': 'MLBTheShow'}\n",
      "{'body': 'I done a bamboozle.', 'subreddit': 'holdmyfries'}\n",
      "{'body': '[deleted]', 'subreddit': 'educationalgifs'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"What's spectrum?\\n\\nMore specifically where's spectrum. \\n\\nGFAQs is also yelling about DCs from people who I believe are in the UK.\", 'subreddit': 'Overwatch'}\n",
      "{'body': \"Hello, this is to let you know that your post is up and running.\\n\\nSince you are looking for a something in particular, please make sure that you [read all rules](https://www.reddit.com/r/sneakermarket/about/rules/) to ensure that your post isn't removed by a moderator.\\n\\nA few reminders:\\n\\n1. Sneakers only! You are only allowed to look for sneakers on this sub. Anything else (bots, services, other clothing items) will be removed **without warning**.\\n\\n2. If you're looking for something Deadstock/New, name the price you are willing to pay. Otherwise, your post will be removed by a moderator **without warning**.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/sneakermarket) if you have any questions or concerns.*\", 'subreddit': 'sneakermarket'}\n",
      "{'body': 'yeah, Davion Hall is not great, sadly, Lynch seems to be doing ok though', 'subreddit': 'CFB'}\n",
      "{'body': \"I don't even know what happened exactly.  Ugh paying 99 dollar deductible to replace is gonna hurt.\", 'subreddit': 'GalaxyNote8'}\n",
      "{'body': \"Sorry about my limitations. I'm doing the best I can!\", 'subreddit': 'Christianity'}\n",
      "{'body': 'And I lost an Arena game thinking Blade Furry with a LS weapon would AOE heal me.', 'subreddit': 'hearthstone'}\n",
      "{'body': '16,397', 'subreddit': 'antimatterindustries'}\n",
      "{'body': 'SemiAmusingBot says: I am now installed on a VPS. reply #3 test', 'subreddit': 'pythonforengineers'}\n",
      "{'body': 'Good job ', 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Max McCormick\\'s just updated his title on LinkedIn from \"Incoming NHL Regular\" to \"Incoming AHL Regular\"', 'subreddit': 'hockey'}\n",
      "{'body': 'Yeah. A non contact voltage tester would be easier. Check and see if the wires have voltage in them', 'subreddit': 'HomeImprovement'}\n",
      "{'body': 'Nah, Turner is an idiot but not an edgelord idiot.', 'subreddit': 'iamverybadass'}\n",
      "{'body': \"Well I'm trying to keep it discreet. \", 'subreddit': 'hacking'}\n",
      "{'body': \"Id's this from a a WikiHow on how to die from incurable disease?\", 'subreddit': 'meirl'}\n",
      "{'body': 'Idk what ephylone feels like but if its like meth it aint that bad, now if it burns like 2C-I then you have a reason to complain\\n\\nYou could always put a little lidocaine in your nose first', 'subreddit': 'researchchemicals'}\n",
      "{'body': '/r/bosozoku  is leaking again. ', 'subreddit': 'Shitty_Car_Mods'}\n",
      "{'body': '[deleted]', 'subreddit': 'ImGoingToHellForThis'}\n",
      "{'body': \"Yeah. In theory you could visit a half dozen countries scattered around the globe in a single day.\\n\\nWake up in New York, have breakfast in London, a business meeting in Japan, then lunch in San Francisco, then another meeting in Sydney, dinner in Shanghai and then go to bed in Rome.\\n\\nI dunno why you'd fly to another country for meals before moving on to another for the next meeting, but you could do it. \\n\\nI could see some rich guy trying to see just how many non-neighboring countries he could get through in the space of 24 hours, just for  fun.\", 'subreddit': 'spacex'}\n",
      "{'body': \"Who is downvoting these comments? We're just stating our results.\", 'subreddit': 'TheSilphRoad'}\n",
      "{'body': 'My thought was Djoos and Bowey would get more ice time if brooks and Carlson were split up and paired with one or the other. My fear is trotz is going to overplay Orpik/Carlson at the expense of ice time for the young guys. We’ll see maybe if Djoos and Bowery develop chemistry together that’ll work out idk', 'subreddit': 'caps'}\n",
      "{'body': \"To be fair, this single moment isn't most of the time. Just punch me right in the dick, man.\", 'subreddit': 'baseball'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': \"Agreed. He busted long runs juking Jabroni Pepperoni two years in a row. Everybody's a hater. He's not super fast, but he's damn shifty. \", 'subreddit': 'CFB'}\n",
      "{'body': 'You can also sacrifice allies to Satan mid-battle to boost his stats. Unfortunately, this deletes the ally from your roster, and - more importantly - lessens your arena score.', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'Or Rick and Morty if you really want to get intelligent.', 'subreddit': 'Android'}\n",
      "{'body': 'One Random Please', 'subreddit': 'edc_raffle'}\n",
      "{'body': '#SELLER', 'subreddit': 'DirtySnapchat'}\n",
      "{'body': 'Stop giving Killins the ball every play. Where the hell is Trequan??', 'subreddit': 'CFB'}\n",
      "{'body': 'i wiped my cookies for reddit-stream and reddit then went directly to it by changing the url from the actual game thread. that worked.', 'subreddit': 'nba'}\n",
      "{'body': 'So did you like repair all the stuff you broke?', 'subreddit': 'nosleep'}\n",
      "{'body': \"I hope there's a way to opt out.  I do not want to be bothered to need to remember another PIN and enter it every damn time.\", 'subreddit': 'NotMyJob'}\n",
      "{'body': '[deleted]', 'subreddit': 'ShouldIbuythisgame'}\n",
      "{'body': \"I'll take 2 spots please. Any of the following in this order: #11, #22, #32, #23. Random ok if all taken.\\n\\nThanks!\", 'subreddit': 'edc_raffle'}\n",
      "{'body': 'mods already stack, just with diminishing returns.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Wow, forgot the match was on this time', 'subreddit': 'OpTicGaming'}\n",
      "{'body': \"Meh, Danteh is more mechanically skilled than Esca is, and it's not like his gamesense is lacking. Individually I don't think you can really say Esca is significantly better, but at the same time if you replaced Esca with Danteh LH may not do as well because I gather Esca is a very important personality to have on the team. \\n\\nHard to judge without POV, obviously, or at least good stats over reasonable sample sizes. \\n\\n\", 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': '[deleted]', 'subreddit': 'totalwar'}\n",
      "{'body': 'You do not have enough karma, please message the moderators with proof of trade, please check the OP for the requirments.', 'subreddit': 'mechmarket'}\n",
      "{'body': \"How? All I said was that what you're probably classifying as a skilled PK (someone who doesn't use lights and zone much,) is someone that's probably losing a lot, because zone and lights attack are the best and kinda only good parts of her kit. Timesnap makes them better, sure, but idk why you're saying it's my crutch.\", 'subreddit': 'forhonor'}\n",
      "{'body': \"You have to be staring like directly at it basically.\\n\\nI only use them occasionally for the deafening effect. If they can't hear where you are in their building, they are less prepared when you come around the corner and beat dat ass.\\n\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': '[deleted]', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'How do access the monks toaster without them starting a war with you?', 'subreddit': 'Wasteland2'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I had just started the movie when I wrote this, and despite seeing Star Wars give or take 500x I didn't remember that part. \", 'subreddit': 'MawInstallation'}\n",
      "{'body': \"Ahh, I'll have to pass - not big on certs and I'm good on boosts right now\", 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Cheeky little cunt Danny. ', 'subreddit': 'formula1'}\n",
      "{'body': \"You're either doing him a favor, or cock blocking him. \\n\\n...Not sure I wanna know which\", 'subreddit': 'confession'}\n",
      "{'body': 'number 32 and one random please, if 32 is taken two randoms please', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"I think you're right, they [have a page here](https://support.google.com/youtube/answer/2853702?hl=en) saying to use h.264 but doesn't mention h.265 in the supported formats.\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"143413429| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413280\\nNo they don't. I wish they did lol.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': 'He actually had good intentions to go to Iraq. Alas, \"but Saddam is a bad guy\" is not a good excuse to overthrow him. For a politician, he\\'s weirdly naive on how complex international affairs is.', 'subreddit': 'history'}\n",
      "{'body': 'I’m not your friend, pal. ', 'subreddit': 'funny'}\n",
      "{'body': '----------&gt; /r/DHExchange', 'subreddit': 'DataHoarder'}\n",
      "{'body': \"So that went from : ' I was calling the Sens dumbasses. Calm your tits. ' \\n\\nTo : ' I was calling the Sens dumbasses. Calm your tits. Typical habs fan. ' \\n\\nNeat . \", 'subreddit': 'hockey'}\n",
      "{'body': 'I think so. ', 'subreddit': 'DotA2'}\n",
      "{'body': \"I love alpina and I've been hearing a lot about longines! I'll give em both a look! Thank you thank you!\", 'subreddit': 'Watches'}\n",
      "{'body': \"Nothing changed \\n\\n\\ngarbage teammates\\nflip a coin bro!\\nsearch up a pros rune and items and wing it zz\\nplay new champions and dont know what youre doing cause if you don't know youre doing the enemy wont\\nAlso don't go too ham you might get reported for toxicity!!!1\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"&gt; If you're a sponsor, would you rather have 100k watch for 1h or 200k for 30 minutes? (Made up numbers). To me, the clear answer is 200k, and I'm pretty confident they agree.\\n\\nThat's a made up number and didn't really portray the changes though. You're making an assumption that the total unique viewers are double for half the time. While it's true that total unique viewers might increase, there is nothing to support the idea that it'd double. On the other hand, it is already proven that the total number games (time) will be less than half of the previous year.\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': '\"For the next two weeks 100% of sales from these two @IsleSeatPodcast shirts will go to @UNIDOSxPR. \\n\\n#Isles \\n\\nhttp://www.NewYorkBootleg.com \"', 'subreddit': 'NewYorkIslanders'}\n",
      "{'body': '[removed]', 'subreddit': 'politics'}\n",
      "{'body': 'One random please', 'subreddit': 'edc_raffle'}\n",
      "{'body': '&gt;Elaborate, please. Show me where I said abandon your mortgage, kids and wife.\\n\\nYou stated to leave the premises, by virtue of American law, having left the premises is considered abandonment of the home and the children in the home. This is well proven through the amazingly broken family court system. \\n\\nIf you leave the home, you can pretty much write off ever seeing your children again unless you are independently wealthy and the wife/girlfriend does not have access to the accounts. \\n\\n&gt;I’ve about had my fill of Internet warriors who put words in people’s mouths, instead of owning their assumptions and having an adult conversion.\\n\\nI made no assumptions, you said to leave the premises, then you said you didn\\'t mean it, that it was \"figurative\". \\n\\nSay what you mean and you won\\'t have all of these \"internet warriors\" putting words in your mouth. In fact, if it happens so often, perhaps you should stop bitching about it, and start asking why it is that your statements are so often incorrectly read differently than you meant them. \\n\\n&gt;So, please, fucking show me how I minced words.\\n\\nOK.\\n\\nYou said. \\n\\n&gt;Woman hits you, you leave the premise one way or another.\\n\\n\\nThen you stated what that meant was:\\n\\n&gt; I didn’t mean, “leave right now and never look back”. More like, walk out of the room or building to let things simmer down a bit.\\n\\n\\nTo which I reminded you that in the American legal system, doing so is seen as willful abandonment and will count against you in court the majority of the time. \\n\\nSo first you said leave the premises, then said you didn\\'t mean to literally leave, just go to another room. Cause women can only hit you standing in one spot, they could never possibly follow you. It\\'s their one weakness right. \\n\\nAs an aside, you have made the mistake numerous times now, it is \"premises\", not \"premise\". THE MORE YOU KNOW 彡☆ ', 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'No worries. It got me out of bed at a decent time for once, and there were other posts giving the same info.', 'subreddit': 'Edmonton'}\n",
      "{'body': \"It kinda sucks though. Mine has been collecting dust for a little while by this point -- unless you are a Nintendo fan there isn't much available on it that one would want. \", 'subreddit': 'vita'}\n",
      "{'body': 'Seeing the patriots logos just makes my blood boil.', 'subreddit': 'MLS'}\n",
      "{'body': '[deleted]', 'subreddit': 'ClashRoyale'}\n",
      "{'body': 'What am i looking at here?', 'subreddit': 'whitepeoplegifs'}\n",
      "{'body': '[deleted]', 'subreddit': 'AlmostParkour'}\n",
      "{'body': 'Do I need this \"horizons\" thing(expansion pass? deluxe edition?) to encounter them?', 'subreddit': 'EliteDangerous'}\n",
      "{'body': 'Oh! fantastic.', 'subreddit': 'ApocalypseRising'}\n",
      "{'body': 'Nty', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Know your /r/politics rules 101', 'subreddit': 'politics'}\n",
      "{'body': \"afaik it's mainly to do with modding the Multiplayer. Mods could let you do some really fucked up things in MP so they made it bannable.\", 'subreddit': 'gaming'}\n",
      "{'body': 'Drones should be kept the same to ensure consistency. If skins are introduced they could be detected by defenders more easily or blend into the environment more.', 'subreddit': 'Rainbow6'}\n",
      "{'body': 'Well, I basically followed amazingribs.com recipe for wet curing a ham. Smoked this Duroc pork ham on my WSM at 325F for about 3 hours. Smoked with maple wood, taste delicious!', 'subreddit': 'BBQ'}\n",
      "{'body': '[deleted]', 'subreddit': 'productivity'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'I can see why he was so worried about uploading videos, this one was garbage', 'subreddit': 'JonTron'}\n",
      "{'body': '2 randoms please', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Whoosh', 'subreddit': 'CFB'}\n",
      "{'body': 'So 11:59 on October 1st?', 'subreddit': 'StarWarsBattlefront'}\n",
      "{'body': \"I'm not sure why, but when I loaded up my game suddenly everything in the Duna system had a good, solid connection to Kerbin and was able to transmit. Glad to have this sorted out before I start sending craft out to other planets :D\\n\\nEdit - The relay's orbital position... of course.\", 'subreddit': 'KerbalSpaceProgram'}\n",
      "{'body': 'one random please', 'subreddit': 'edc_raffle'}\n",
      "{'body': '[removed]', 'subreddit': 'Kikpals'}\n",
      "{'body': 'You should take better care of your phone then. Idk how people are having such a hard time with the getting your phone stolen part of this, it’s not like it’s making it more likely that you’ll get your phone stolen.', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'Best comment. ', 'subreddit': 'lularoe'}\n",
      "{'body': \"That's because paywave /is/ a credit card transaction, last I checked\", 'subreddit': 'newzealand'}\n",
      "{'body': '[removed]', 'subreddit': 'relationships'}\n",
      "{'body': '[removed]', 'subreddit': 'relationships'}\n",
      "{'body': 'Yeah, I do want to see how he rocks his socks!! ', 'subreddit': 'nba'}\n",
      "{'body': \"I'm trying to imagine running Goldschlager through a Brita. It'd be like panning for gold!\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'I see thanks for the info', 'subreddit': 'mobilelegends'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': 'With rumchata?', 'subreddit': 'AskReddit'}\n",
      "{'body': 'One random please!', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"And then proceeds to tell the rest of us how we ought to walk (drunken meanderings being explicitly prohibited), saying God told him to do that, too. And we'll be punished if we don't listen.\", 'subreddit': 'exmormon'}\n",
      "{'body': 'To be blunt, it\\'s *really* immature to just say \"I\\'m going to act however I want or find most convenient and everyone else should just deal with it.\" The rest of the world doesn\\'t revolve around you, and if you expect everyone else to change *for you* it\\'s not going to happen. If you\\'re acting clingy and insecure and that\\'s turning people off, no one cares that you \"can\\'t control it.\" ', 'subreddit': 'dating_advice'}\n",
      "{'body': 'I was about to say “Here in San Diego it definitely never died.” I can’t raid much since I work a lot, but managed to just walk in to a bunch of raids. Got one yesterday after seeing a group of about 14 people at a gym. ', 'subreddit': 'TheSilphRoad'}\n",
      "{'body': 'I like to think of it like scotch on the rocks. At the beginning, at its freshest, the flavors are bold and really pop, things are exciting and a great opener for the adventure your about to experience. As you get farther along and things soften and mellow, you notice the softer notes as they are allowed to come to prominence. And at the very end you are gently let back down as you throw back that last little cusp of flavor and the experience fades. All great breakfast cereal, like scotch, is a journey.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'That’s just a kick in the balls. (The Rangers jersey)', 'subreddit': 'canucks'}\n",
      "{'body': 'I\\'m super picky about screens, and I don\\'t think it really matters. There is an office supply store near me that still has 9.7 inch iPad Pros on display as well as the new iPad 2017, and I\\'m not kidding, I thought they mislabeled the demo units. I was so convinced they had swapped the labels on the demo units that I had to put the serial numbers in a search engine to verify that the iPad 2017 and the iPad Pro 9.7 were properly labeled.\\n\\nIf you know what to look for, it\\'s more clear, but it is hard to tell the units apart if you\\'re doing it for the first time.\\n\\nIncidentally, the screen on the iPad 2017 is not \"bad\". In fact, if you Google the Notebookcheck measurements of the screen, it\\'s actually quite accurate. (In terms of measured DeltaE within the sRGB gamut, both grey and color, it\\'s actually better than the measurements Notebookcheck got for the 10.5 iPad Pro -- though individual numbers aren\\'t everything and there will obviously be variation between panels.)\\n\\nTwo good things about the iPad 2017 that I\\'ve noticed: \\n\\n* There is less color variation severely off-angle than both generations of the iPad Pros. I think this is due to the difference in coatings, or perhaps the presence of the sensing layer for the Apple Pencil.\\n\\n* The anti-fingerprint oleophobic coating on the iPad 2017 is definitely more effective than on the Pros. I think they had to compromise there to support the Apple Pencil.\\n\\nThere is a little more brightness variation on the iPad 2017 in comparison to the Pros, and of course there is no support for an extended gamut or 120 Hz. But on the positive side, I find the iPad 2017 is easier to hold in the hand because of the bezel size and the slightly smaller screen is better for reading eBooks.\\n\\nI wouldn\\'t hesitate to buy the iPad 2017. I think it\\'s one of the best deals Apple has ever offered.', 'subreddit': 'ipad'}\n",
      "{'body': 'r/me_irl', 'subreddit': 'dankmemes'}\n",
      "{'body': \"I don't know, I think the xim4 is a pretty good one.  Once you get the mouse curve profile tweaked.  It's better than the controller for someone used to the kbm.\", 'subreddit': 'StarWarsBattlefront'}\n",
      "{'body': 'Work', 'subreddit': 'AskReddit'}\n",
      "{'body': 'They track and spy on you and all your activity. There was a canary clause taken down about 2 years ago indicating that they have been commandeered by the NSA or like.', 'subreddit': 'AdviceAnimals'}\n",
      "{'body': \"nonsense that's the fun part!\", 'subreddit': 'golf'}\n",
      "{'body': 'I just wanna see how we compare to Washington through transitive tbh', 'subreddit': 'CFB'}\n",
      "{'body': '\"He works too\"\\n\\n/news posts picture of trump golfing \\n\\n\"He was taking a break in between all his deals\"\\n\\nOr something. ', 'subreddit': 'politics'}\n",
      "{'body': 'Frugality and value are relative. Some people like to look for sales and to invest on higher quality products. If you feel this product does not fit your budget, then this product is not for you. There are cheaper alternatives out there. ', 'subreddit': 'frugalmalefashion'}\n",
      "{'body': \"What I'm thinking of is how natives and first nations in North America had lands and property taken by the government. As the government passed laws and executive actions to allow these seizures there is no 'legal' requirement for restitution, but there is obvious moral need to repay what was taken. Is there a name for such debts, where the debtor has escaped legal obligations, but there is a societal and moral expectation they repay the debt?\", 'subreddit': 'AskReddit'}\n",
      "{'body': '11564', 'subreddit': 'GreypoCounting'}\n",
      "{'body': \"Yep, that's what I'd suggest too.  I run Death, Curses, Reaper on my Necro, but Blood is a solid alternate to Reaper if you don't have HoT.  That in a condi damage build works nice for open world stuff.\\n\\nI've been able to solo most champions in open world (and HoT HP champs) with it just fine, usually staying near max health while my minions tank.  \", 'subreddit': 'Guildwars2'}\n",
      "{'body': 'Its a good thing D.Va doesnt have any other way of making Zarya use her shield.\\n\\nNone at all.', 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': 'How hard is it to take apart? My DMS Right is always activated so I was gonna see if I could unstick it.', 'subreddit': 'hoggit'}\n",
      "{'body': \"We're not the target audience.\", 'subreddit': 'pics'}\n",
      "{'body': 'I, too, was at Ribfest. A rib cooker singles out some young guy wearing a white \"Where\\'s the beef?\" T-shirt, standing off to the side. He says \"Hey buddy! That T-shirt ain\\'t going to stay white for long\" as he points to his immaculately clean white apron, confusing those watching the exchange. The young guy says \"oh, I won\\'t be *eating* any ribs\", and honestly his emphasis of the word *eating* creeped me out a bit, but whatever. So the cook huddles with associates and seem to come to some type of agreement and calls back over to the young guy \"Hey Buddy! Come here a minute!\" So he walks over and the cook turns on a loud white noise machine and we can\\'t hear what they\\'re saying. After a lot of emphatic hand waving, head nodding &amp; shaking, and some frantic scribbling and erasing on a white board, the young man walks over to the rib line (which is pretty long at this point since no one\\'s been serving ribs). Everyone in line does a triple-take as the man takes his place in line, and the whole line is now murmuring to each other trying to make sense of it all. Someone finally incredulously asks \"What could the cook *possibly* have said to you to convince you to eat after you already declined one single time?!?\" The man smiles and says \"Well you see, I\\'m a VEGAN and can\\'t eat meat, so the cook had offered to clean off the grill and make me some scrambled eggs instead.\" And one of the people in line says \"I didn\\'t know they had scrambled eggs! I want that\". Others joined in \"Me too!\" \"I\\'ll pay double!\" \"Triple!\" \"Money is no object!\" It was pandemonium. Another man stands in line with very puzzled look on his face. The line continues to swell as more people hear the rumor of scramble eggs and eventually all the Vegans at Ribfest make their way to the line. Finally, the puzzled man speaks up, \"Excuse me, I know this is none of my business, but do you mean vegetarian?\" The original white shirt guy gives his own puzzled look in return. So the puzzled man clarifies \"Well it\\'s just that you said you\\'re a vegan but you\\'re about to eat eggs, which are an animal product - plus I think there\\'s probably milk in it too if they\\'re doing it right\" Someone else shouts \"And vanilla extract!\" And the puzzled guy says \"Yeah, maybe. But that\\'s not an animal product. Anyways, I think maybe you meant vegetarian rather than vegan, since it sounds like you don\\'t eat meat rather than all animal products?\" White shirt: \"Vegan, vegetarian, what\\'s the difference?\" Puzzled guy: \"Um, well, I *just* explained the difference. You know what? Never mind; it\\'s none of my business anyway.\" Then someone pipes up \"Won\\'t the egg mixture just run through the grill?\" \"Maybe they have a skillet?\" someone replied. \\n\\nAfter all the eggs were handed out everyone held hands and ate their eggs (with their faces pressed into the paper plates since they didn\\'t have their hands free). It was the most beautiful thing I\\'ve ever witnessed; people of all genders coming together and eating scramble eggs with their faces. I shed a single tear and clapped, a slow non-sarcastic clap. My hands were free because I couldn\\'t eat the eggs - I\\'m gluten intolerant. u/manic_eye', 'subreddit': 'copypasta'}\n",
      "{'body': 'I mean, if I was nice and pulled them aside or messaged them anonymously to let them know maybe.\\n\\nI\\'m talking full on Spongebob \"bAriSta\" \"eXpReSsO\" shit. lol.', 'subreddit': 'AnimalsBeingDerps'}\n",
      "{'body': \"Oh my god, yes - when we do laundry, we tend to throw the clean clothes into a pile on the floor of my craft room. And then *I* separate it out, leave him a pile of his clothes, and it stays there for days until I literally dump it on his side of the bed so he has to deal with it before he goes to sleep. Don't be sorry, I'm working on my third glass of wine, because SOMEONE's gotta finish off that Carlo Rossi so we can have a change jar.\", 'subreddit': 'TrollXChromosomes'}\n",
      "{'body': \"Well an important distinction to make is that Puerto Rico isn't state. Second, around 80% speak only Spanish, which furthers the us vs them. And Third, it's hard to go to a place that's been leveled to the ground to help out. Houston still had some infrastructure. There is basically nothing in Puerto Rico \", 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'Ghostfacewilly1', 'subreddit': 'snapchat'}\n",
      "{'body': 'Have you seen the videos? Or of musical.ly in general?', 'subreddit': 'me_irl'}\n",
      "{'body': 'Oh, okay you’re actually not an asshole, just human. Sorry that this happened to you, in Steam I recommend you physically back up your saves because it’s a little bad too with it’s cloud saves especially if you mod games. ', 'subreddit': 'PS4'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Is there not an official rules set made for the anime yet? I'm really surprised.\", 'subreddit': 'rpg'}\n",
      "{'body': \"Search doesn't work most the time on mobile and most reddit users are mobile \", 'subreddit': 'DivinityOriginalSin'}\n",
      "{'body': 'The feathery creatures keep rushing past you in this tunnel, it is actually just a bit higher than you are tall and about 5 times as wide, it is damp and dirty. One of the creatures takes your hand and while pulling you talks.\\n\\n[\"Keep quiet already, or they will find us.\"](/white)', 'subreddit': 'RPGStuck_C4'}\n",
      "{'body': 'Learned *Logistics* on the Twelfth Moon, 370.', 'subreddit': 'awoiafrp'}\n",
      "{'body': \"Oh my goodness. This is pure adorableness. She looks so happy. I bet she'll remember this forever. You're wonderful.\", 'subreddit': 'gifs'}\n",
      "{'body': 'https://media.giphy.com/media/GcDtLf4RAdiRG/source.gif', 'subreddit': 'soccer'}\n",
      "{'body': 'idk my moms pretty cool for the most part. But every time I try to have a conversation with my dad about anything other than sports or politics it feels kinda awkward and I don’t interact with him a lot at home. We share very little in common, compared to me and my mom who actually have a decent amount in common (we like the same type of movies, we have TV shows we watch together, etc.)\\n\\nNot to say I don’t love my dad. He works hard for me and the rest of our family and he genuinely means well. He’s a bit strict and sometimes he goes overboard with it but for the most part my parents are cool.', 'subreddit': 'teenagers'}\n",
      "{'body': 'The Hell Express.', 'subreddit': 'politics'}\n",
      "{'body': 'Absolutely gorgeous', 'subreddit': 'MechanicalKeyboards'}\n",
      "{'body': '143414467| &gt; United States Anonymous (ID: eFvRVltr)\\n\\n&gt;&gt;143413729\\nThats just hurtful\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"GOOD FRICKIN' GOD.\", 'subreddit': 'Jessicamshannon'}\n",
      "{'body': 'We so bad. So bad... :*(', 'subreddit': 'CFB'}\n",
      "{'body': \"That's not exactly true\", 'subreddit': 'hiphopheads'}\n",
      "{'body': 'Get a scan gauge before shit gets weird.', 'subreddit': 'Justrolledintotheshop'}\n",
      "{'body': 'Ah, a micro fetish!', 'subreddit': 'furry_irl'}\n",
      "{'body': 'At least we all have access to alcohol', 'subreddit': 'CFB'}\n",
      "{'body': 'This person is very confused', 'subreddit': 'trashy'}\n",
      "{'body': 'two random spots', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Start planning then ', 'subreddit': 'me_irl'}\n",
      "{'body': 'On an updated note, Tennessee got shut out today.  ', 'subreddit': 'FloridaGators'}\n",
      "{'body': \"At first I thought he fucked up the kick. He's incredible.\", 'subreddit': 'soccer'}\n",
      "{'body': 'Like Saudi Arabia and the USA are now controlled by the rich and for the rich. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I bet they are. I hope you bought multiple pairs in different colors so you can feel amazing all week. ', 'subreddit': 'GoneWildSmiles'}\n",
      "{'body': '[deleted]', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'zoz', 'subreddit': 'Drama'}\n",
      "{'body': 'Please, stop making it into a joke and call it what it is:\\n\\npropaganda.', 'subreddit': 'politics'}\n",
      "{'body': 'You can get Frutopia at any Subway or movie theater in Canada.', 'subreddit': 'AskReddit'}\n",
      "{'body': '[Or](https://abimon.org/dr/busts/celeste/15.png#sprite) perhaps he suffered an unfortunate accident when you came into the room.', 'subreddit': 'DanganRoleplay'}\n",
      "{'body': 'Will women be allowed to get them up on two wheels?', 'subreddit': 'The_Donald'}\n",
      "{'body': '**[MIRROR: Someone in chat triggers a sfx at the perfect time.](https://livestreamfails.com/post/7082)**\\n\\n---\\nCredit to [twitch.tv/ramentard](https://www.twitch.tv/ramentard) for the content and [reddit.com/u/Baylix](https://reddit.com/user/baylix) for the clip. [[Streamable Alternative]](https://streamable.com/a9yz5) [[Vidme Alternative]](https://vid.me/7h1A5) ', 'subreddit': 'LivestreamFail'}\n",
      "{'body': \"&gt;No, you're playing word games. Either enslavement is the price of being saved, or the act of saving and the act of exploiting are separate. You can't have it both ways.\\n\\nThat's not how you've described the situation. If they offered him safety in exchange for becoming a slave, while he was floating through space, that would be a different issue. This would be the one case where the act of enslaving actually improves the situation.\\n\\n&gt;Here you are not comparing exploitation with slavery, you are comparing selling with slavery. Selling someone an apple is not exploitation.\\n\\nOK: You are starving and I sell you an apple for 100 000$. - I still improve your situation. \\n\\n&gt;Maintaining slavery requires constantly feeding, clothing and sheltering your slaves, so you are continuously improving their position. These acts are necessary to slavery, but slavery is not necessary to these acts.\\n\\nAh, but here's the rub! The only person who can judge the change in quality of the situation is the person themselves. If the upkeep produced an actual net improvement to the slave's situation, they would not need to be a slave - they would work voluntarily! The fact that a threat of violence is necessary to keep the slave working implies that maintaining slavery actually worsens their situation.\\n\\nFor slavery to be slavery (forced actions), a threat of force must be present. And this means that the slaver necessarily worsens the slave's situation to make them choose to work!\", 'subreddit': 'CapitalismVSocialism'}\n",
      "{'body': \"The Ello isn't going to blow your mind but it is one of the best included in a kit with very dependable coils. The only other kits that have a good tank are the Vaporesso mods as far as I know.\\n\\nThe Vtwo has a trash tank, kroma-a is meh, Kanger tanks are across the board bad, etc.\\n\\nEdit: the Ello is probably disappointing because of how good the original Pico tank was. The Mello. That doesn't mean the Ello is bad. \\n\", 'subreddit': 'electronic_cigarette'}\n",
      "{'body': 'Interesting that these days yellow cards are basically something you have to have a strategy for. ', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'My advice is not to think too much about the numbers; the entire war in this series is being fought by like twelve people. Assume the rest of the galaxy is having their own dramatic conflicts just offscreen.', 'subreddit': 'HFY'}\n",
      "{'body': \"I'd start Gillislee over Carson every week lol\", 'subreddit': 'fantasyfootball'}\n",
      "{'body': \"I would say their denim is the best bet. I've bought several pairs from them and they have all turned out well. \\n\\nThe tees on the other hand are pretty inconsistent. I have some that fit perfectly and others that fit like crap. They also do not hold up too well over time. They tend to really lose their shape in the wash compared to my Reigning Champ and 3Sixteen shirts, but that's a bit of unfair comparison. \", 'subreddit': 'frugalmalefashion'}\n",
      "{'body': \"Fake service dogs are a problem but it's usually obvious when it's not a service dog. A family member of mine had a terrible woman as a girlfriend who brought her dog to a hotel under the guise of being a service dog. It barked and ran around her. It was infuriating. From what you've said, I don't understand why she thought your dog was a fraud. \", 'subreddit': 'IDontWorkHereLady'}\n",
      "{'body': 'Yellow https://twitter.com/CityLinkMelb/status/914071763058696192', 'subreddit': 'melbourne'}\n",
      "{'body': \"A month into the school year and LO is settled in nicely to the routine! She's in a one year old room (she's 19 months) at the center that I work at.  Group size is small (3 teachers for a class of 6) and I love the her teachers. She's  helper at school but gets whiny if she's hungry and it's not snack time or lunch time yet.  So pretty reasonable! Everyone loves her.  Except for that one time that she covered herself in bird poop on the playground somehow.  I thought it was hilarious.\\n\\nThe worst part is having to pre-make food for her every single day.  Breakfast, AM Snack, Lunch, and PM Snack. So tiring! Luckily we're getting a routine down for that too.\\n\\nNot School related- My husband bought me a piece of wood so that I can make a height ruler chart to have on the wall in our apartment.  I want to make it (as opposed to just using a door jamb) so that when we get a house of our own we can bring it with us!  I'm excited :)\\n\\nTLDR: First month at school is great! Also, I'm excited to start a new project!\", 'subreddit': 'moderatelygranolamoms'}\n",
      "{'body': 'What material does one store HF in?', 'subreddit': 'oddlysatisfying'}\n",
      "{'body': 'And I am you and what I see is me.', 'subreddit': 'pinkfloyd'}\n",
      "{'body': '[removed]', 'subreddit': 'Gunpla'}\n",
      "{'body': \"At the zoo that I go to they have one of these but there are vents all along the floor. It's always cold and very cool to be in.\", 'subreddit': 'thalassophobia'}\n",
      "{'body': 'They should have a bunch of tasty food stands everywhere on sunday for when the anorexia comes into force. ', 'subreddit': 'worldnews'}\n",
      "{'body': 'We own every clip she ever made all the way back to when she actually stripped naked. We do not buy her foot fetish, not into it.', 'subreddit': 'popperpigs'}\n",
      "{'body': 'I\\'ve already had way better matches on VI than I have on GoT. I like that it\\'s not snowbally. I really expected it to be a map where whoever gets the first objective people call \"GG\". I kinda like where it is now ', 'subreddit': 'heroesofthestorm'}\n",
      "{'body': 'I’ve seen all their beers being sold at jacked up prices. Maybe it’s something they are doing to keep brewery sales up? I’m not really sure what’s going on.\\n\\nI saw the 18 watt couple months ago for like close to 20 bucks as well.', 'subreddit': 'njbeer'}\n",
      "{'body': 'Thanks got a bit excited and posted this one slightly, (OK very too), early thus the mad rash of edits afterwards. I had the idea and just had to post lest it suffered becoming another project prevarication, got so much to do at the moment but so little wilpower to knuckle under.\\n\\nEDIT Curses, one more edit before the sun sets or is it the night falls.', 'subreddit': 'NMSPortals'}\n",
      "{'body': 'It’s always sunny in Philadelphia- various episodes are my fav but one that sticks out the most is season 3 episode 9, it’s the first day man/night man episode. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I've said all along that we're going to experience a Tennessee post-Fulmer fall from grace.\", 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Itsy?', 'subreddit': 'neoliberal'}\n",
      "{'body': \"And I'm fine with that.\", 'subreddit': 'AskOldPeople'}\n",
      "{'body': 'Pm', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'Fishing and Football', 'subreddit': 'AskReddit'}\n",
      "{'body': \"The organization themselves is filing the suit. From the article it seems the leader, who is a lawyer,  is a racist piece of shit. Filing potentially frivolous law suits sounds like something on the lesser end of the bad shit this guy does. \\n\\nHere is an apt quote to showcase what type of man he is\\n\\n&gt; [Kill every goddamn Zionist in Israel! Goddamn little babies, goddamn old ladies! Blow up Zionist supermarkets!](https://www.splcenter.org/fighting-hate/extremist-files/individual/malik-zulu-shabazz)\\n\\n\\n\\n\\n\\nThat said, I have no idea if this lawsuit has merit or not since I'm not a lawyer but this guy being associated with it is horrible pr for trying to get people to support it. \", 'subreddit': 'news'}\n",
      "{'body': 'Rolling with Watson. Need a game before I trust Amari again and Crabtree is questionable. Too many question marks. Fairly confident in starting Carr ROS though.', 'subreddit': 'fantasyfootball'}\n",
      "{'body': '&gt;That doesn\\'t logic right. There may be 1000 types of Christianity but none of them are related to whether Christianity is \"correct,\" again, whatever that means. \\n\\nSure it does. If we take that Christianity is the correct religion ( instead of atheism or *insert a list of all other deistic ideology here*) then any given Christian has a 1 in 1000 shot at interpreting the dogma in the correct way as to not be a blasphemous hell bound sinner. \\n\\n&gt;What does it mean to be correct \\n\\nIn this case it means that of all available options, both past and present, you successfully chose the one that ends up being true. \\n\\n&gt;and furthermore, what does a sect of Christianity not practicing Christianity correctly mean to whether Christianity is correct? \\n\\nNothing. That\\'s why I said that if we accept the **premise** that Christianity is correct, it still only gives any single practioner a one in 1000 shot at being right. It does not bear on wether or not Christians are right, that number was too astronomical to calculate. ', 'subreddit': 'DebateReligion'}\n",
      "{'body': 'Send me a PM. :)', 'subreddit': 'FidgetSpinners'}\n",
      "{'body': \"Yeah, this is why I'm not too upset about it, it's still a beta. \", 'subreddit': 'webdev'}\n",
      "{'body': \"Which makes sense. It's the implication of being on a boat that makes it dangerous, not being on a boat itself. \", 'subreddit': 'TalesFromRetail'}\n",
      "{'body': \"Teacher as in English teacher? I'm guessing it must've been a struggle with the grind, props on where you made it. Any pointers for someone looking to make the same plunge?\", 'subreddit': 'Cyberpunk'}\n",
      "{'body': \"I mean going by the rules I could see that happening, but given the relationship that that Goku has with Zeno I don't think they'll beat him down for using that technique, even if it does draw energy from eliminated fighters. \", 'subreddit': 'dbz'}\n",
      "{'body': 'zle', 'subreddit': 'Drama'}\n",
      "{'body': 'Always and forever.', 'subreddit': 'casualiama'}\n",
      "{'body': \"How are you spending $15 by yourself at Five Guys? My boyfriend and I spend that between the two of us (maybe a dollar or two more but that's it).\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Yeah Nissan obviously isn’t stupid since they suffered so many problems with the Gen1s but then again when everyone else is going with active cooling it makes you wonder ', 'subreddit': 'teslamotors'}\n",
      "{'body': '[deleted]', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"Congrats on 90 days clean \\n\\nI know I am an impulsive person, but I'm fortunately also very paranoid and I only use when I know I have no engagements for the day. I'm quite secretive about my use... nobody knows and I have to balance these two sides of my life. So far so good. There is also excitement in that dichotomy of recklessness and caution. Who knows tho...\", 'subreddit': 'opiates'}\n",
      "{'body': '[deleted]', 'subreddit': 'UKPersonalFinance'}\n",
      "{'body': \"They're learning I say we pack up and ring Elon Musk for some tickets to Mars...\", 'subreddit': 'gifs'}\n",
      "{'body': 'Maybe it has to do with the fact that being a reservist sucks shit these days.\\n\\nI left a while ago, but comparing what we got to do, compared to what the militia got to do in the 1980\\'s, it is why no wonder why no one wants to stay in anymore.\\n\\nGuys I know that were in in the 80\\'s talk about thousands of reservists being on exercises in the summer and thousands on exercise in the winter. M113s, Grizzly, Cougar and Husky armored vehicles, multiple artillery batteries, Air Force helicopters for infantry air assaults, air defense batteries, thousands of mortar rounds, the fucking FN, New MLVW\\'s, Rambo tier amounts of M72\\'s, parade squares that fit 1500 soldiers and full support battalions were normal things. Sometimes Reg force was fully/partially involved, sometimes they weren\\'t.\\n\\nCompared to now. \\nMaybe 300 or 400 reservists will go one exercise if the Land Force Area can afford it that year. No M113\\'s, no LAV\\'s, Shit ass LSVW/Milverado/G-Wagon/MLVW vehicles. No artillery except the 4 forty year old field guns that the artillery platoons have in the whole province. Maybe the Air Force will be able to afford to send out 1 or 2 unarmed helicopters for a day or two. Maybe the medics will set up a tent if they can get enough members to go on exercise. The food will come from the base kitchen or will be MRE\\'s because the service battalions are incapable of getting enough people or equipment to provide those services. Any M72 training will be 35mm training insert on a reused tube. Your unit and the exerciser run out of training rounds so you literally say \"Bang Bang\" when \"shooting\". Soldiers get encouraged to stay home on range qualification days because their units can\\'t afford to qualify everyone on even the C7 let alone the machine guns.\\n\\nThat same parade square has crumbled so much that it could only parade two under strength platoons. The military can\\'t afford to fix it, and there is no point in fixing it really. The reserves totally depend on the Regular force as well.\\n\\n\\nThat and the older aged higher ranking NCO\\'s that are just riding it out to top up their pension.\\n\\n\\nReservists are part time. If you are going to attract high quality and talented people to the reserves, it has to be fun. Why would anyone earn very little money one night a week and do basically nothing with not much opportunity stay? The $1.25 beers in the mess only make up for doing your 3rd BFT of the month for so long. ', 'subreddit': 'canada'}\n",
      "{'body': \"People say this all the time, but one of my party plays with controller on PC, and I didn't know about it until he casually told me. We are both Gold III (*strange since we play together*), but I never felt he was carried, he's not the best nor the worst of the squad, he has a 1.2 K/D in Ranked which I'd say is good. Honestly I used to play on console, and I notice the increased accuracy of the mouse almost only on long range gunfights, which are quite rare in R6S\", 'subreddit': 'Rainbow6'}\n",
      "{'body': '*Phallus hadriani*\\n\\n*P. impudicus* and *P. hadriani* differ only in the color of the volva (white in the former, purplish in the latter).  ', 'subreddit': 'mycology'}\n",
      "{'body': \"I love Gerolsteiner for the same reasons.  It is the strongest seltzer I've had.  And I used to drink for the same reasons, too.  I was never one to drown my sorrows.  But I couldn't stop a good feeling from taking the party way too far.\\n\\nA good, hard-hitting seltzer really does the trick, though.\", 'subreddit': 'stopdrinking'}\n",
      "{'body': \"Cats are fun and magical when you can't smell their poop. FRESH STEP! \", 'subreddit': 'WTF'}\n",
      "{'body': '[these](https://imgur.com/a/YjZXx) what kind of adjustable elastic cuffs are these? Is there a specific name? How would I go about doing this myself to a pair of nylon wind pants?', 'subreddit': 'sewing'}\n",
      "{'body': 'The difference between the Silver photoshopper and the Diamond photoshopper. ', 'subreddit': 'Jaxmains'}\n",
      "{'body': 'Oh god if he does take pictures so we can laugh at his expense. ', 'subreddit': 'bmx'}\n",
      "{'body': \"I seem to remember having Bitmoji before other people. Your report very well may have had an effect, that seems to be the biggest use of Beta. We're the only people who have direct access to the devs to tell them our problems and, if they're small enough, get them fixed fairly quickly. All I'm saying is, though, is that if you have a Beta feature, maybe include it in the Beta release? I mean, sure they tested the multiple videos thing but it's not like they're going to get bug reports on it for as long as it's disabled so how do they know if the edits they're making are working for the people that complained?\", 'subreddit': 'androidapps'}\n",
      "{'body': ' You could just ask them out to a movie, restaurant, bar, or to do whatever else you like to do and let things happen more organically. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Good. \\n\\nVote for them. \\n\\nYou obviously hate the Greens. \\nVote for the Greens if you like. \\n\\nWhy don't you?\\n\", 'subreddit': 'newzealand'}\n",
      "{'body': 'No, it\\'s really not. Knowing your name does not mean you can be assessed for capacity. That patient is having a life-threatening respiratory decompensation and a psychiatrist is never going to tell you \"don\\'t intubate them until I get a chance to do my 30-minute competence assessment.\" That\\'s why none of the physicians in this thread thought a psych assessment was a reasonable suggestion, and it\\'s why you\\'re getting downvoted. You need to just accept that you didn\\'t understand this clinical scenario.', 'subreddit': 'medicine'}\n",
      "{'body': 'Terrible snap and hold. Not on the kicker.', 'subreddit': 'CFB'}\n",
      "{'body': \"To be honest, vanilla stalker is pretty shit compared to modded stalker, it's an inferior experience.\", 'subreddit': 'stalker'}\n",
      "{'body': '[removed]', 'subreddit': 'gaming'}\n",
      "{'body': 'Now all you have to do is figure out a way to connect paintball guns to them or something and have dog fights', 'subreddit': 'INEEEEDIT'}\n",
      "{'body': 'This literally just happened to me an hour ago. Her name came up under \\'friend recommendations\" and I saw her picture and it was of her and my ex.... looking happy. They just moved in together after only two months. I\\'ve been crying hysterically ever since.', 'subreddit': 'BreakUps'}\n",
      "{'body': '143417688| &gt; Australia Anonymous (ID: XYEoqXRK)\\n\\n&gt;&gt;143412250 (OP)\\nI voted NO on gay marriage. The whole campaign has been a fucking joke to the point that the media rarely reports on it anymore.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"I tried tempting him with my twsbi + diamine red dragon, but he won't give in yet! I think I'll try working on his mom first (she loves colour and I'll gift her a pen tomorrow), so we can fight him on two fronts xD \", 'subreddit': 'fountainpens'}\n",
      "{'body': 'Lockheed Martin has agreed to participate with El Salvador, and will committing the full arsenal of its resources as it would any other client. Of course this has been with the urging of the U.S State Department who is interested in increasing its positive diplomatic presence in the Caribbean. ', 'subreddit': 'worldpowers'}\n",
      "{'body': '[Original post](https://www.reddit.com/r/france/comments/73ig93/chamrousse_isère/) by /u/wisi_eu in /r/france\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#whitelist \"france\")\\n', 'subreddit': 'ImagesOfFrance'}\n",
      "{'body': 'It does matter for us in the sense that we need 1 more win to clinch HFA in the world series ', 'subreddit': 'Dodgers'}\n",
      "{'body': '[deleted]', 'subreddit': 'sex'}\n",
      "{'body': \"Burn crosses, but use responsibly-sourced wood that doesn't lead to deforestation?\", 'subreddit': 'Gamingcirclejerk'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I'm gonna start wearing heels soon... something I never wanted to do since I had a complex about being too tall.  But now that I'm thinner, it's stupid but I feel like a model when I wear heels\", 'subreddit': 'proED'}\n",
      "{'body': '[deleted]', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'Is this what is coming from the discontinuation of the sugar free syrups??', 'subreddit': 'starbucks'}\n",
      "{'body': \"Oh, thank you. Yeah, I was searching for a little while but couldn't find the OP.\", 'subreddit': 'zelda'}\n",
      "{'body': 'because making a joke about a regime which murdered 40-60 million people is  lulz funny.', 'subreddit': 'shreveport'}\n",
      "{'body': 'Every human is not an \"unborn clump of cells\" though.', 'subreddit': 'europe'}\n",
      "{'body': 'They look blue to me', 'subreddit': 'australia'}\n",
      "{'body': 'Bionic-Frog 283 Titan', 'subreddit': 'Fireteams'}\n",
      "{'body': 'I hope this goes viral and the winning name be like osama bin laden or something.', 'subreddit': 'AdviceAnimals'}\n",
      "{'body': \"My current shop head once got a muscle detached from his forearm and curled up into his elbow because someone dropped something they were hoisting up into the fly system. He didn't let go and the 500 lbs of weight on the other end of the rope won. He has a scar up his whole arm because he needed surgery to reattach the muscle.\", 'subreddit': 'techtheatre'}\n",
      "{'body': 'Hello /u/UR_JUST_TO_GREEDY\\n\\nYour post has been flagged as a repost and has been removed. In /r/giveaways, reposts are allowed once **48** hours pass.\\n\\n**To prevent reposts in the future, use the [Giveaways Repost Finder Tool](http://checkrepost.com/)**.\\n\\nHere is the original post: [Win a Luxury Workbag Collection or a $1000 Gift Card to Jennifer Hamley England (9/30){US UK},](http://reddit.com/r/giveaways/comments/73i211/win_a_luxury_workbag_collection_or_a_1000_gift/) by Derelictive. You can repost this in **1d 22h 44m 43s**. \\n\\nPlease note that the bot will never make a mistake about calculating 48h. But if a mistake was made please [message the mods](https://www.reddit.com/message/compose?to=/r/giveaways&amp;subject=/r/giveaways+Incorrect+Post+Removal&amp;message=/r/giveaways/comments/73ig6r/luxury_workbag_collection_giveaway_the_entire/%0A%0A&lt;&lt;Enter Reason Here&gt;&gt;)', 'subreddit': 'giveaways'}\n",
      "{'body': \"Many schools would only want SAT subject scores if you are sending the SAT, but in your case, your SAT scores are much better than your ACT scores so you should probably send those. As far as subject tests, that is a huge range of scores for the Math II you are getting. The Lit test is generally considered pretty hard, so even a 650 isn't that bad of a score. What books are you using for Math II? I could give you some tips on how I crammed the week before the test.\", 'subreddit': 'ApplyingToCollege'}\n",
      "{'body': 'Be interesting to see the stats for pc as well. ', 'subreddit': 'OWConsole'}\n",
      "{'body': 'Disgusting. Go fuck yourself.', 'subreddit': 'rickandmorty'}\n",
      "{'body': 'I identify with this on a deep, emotional level.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Oh we'd love to hear your mature opinion u/cannabis_detox\", 'subreddit': 'freefolk'}\n",
      "{'body': 'Spot 53 if open and one random (2 spots total)', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'zozzle', 'subreddit': 'Drama'}\n",
      "{'body': 'Or use a raspberry pi and have exactly the same set up / small sized box but with thousands of games from dozens of consoles instead of 20 games from 1 console, plus 4+ controllers of any console you want instead of 2 with a funky connector you can’t use with anything else.', 'subreddit': 'gaming'}\n",
      "{'body': \"Hey guys! Sorry I'm late to the game. \\n\\nI'm not sure if you remember playing at Tricky Falls in El Paso 2 years ago on the night that The Offspring was playing in town, but I had a blast at your show! There weren't many of us there, but the time you guys spent hanging out while playing and after was fucking awesome. I heard that the follow day your van was stolen with all of your equipment in San Antonio or something... Did you guys ever get your stuff back?\", 'subreddit': 'IAmA'}\n",
      "{'body': 'the bushido plan', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"Your fellow countrymen beg to differ; always taking credit for other's accomplishments in order to make themselves feel better/superior. That is until the connotations are bad like the guy above said.\", 'subreddit': 'mildlyinfuriating'}\n",
      "{'body': '1 random please', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Yeah I hate ashura the most... I\\'m also not allowed to play any sort of video games, tv or anything that involves fun on the 9th-12th day. I go to the mosque where they beat their chest (I like to be in spots with a lot of people so I don\\'t have to beat it hard and it looks like I just can\\'t with the guy in front of me being &lt;a feet away. I have to do zinjeel where I hit my back with the chains (been doing since 5th grade). At the end my uncle came up to me and told me that I needed to hit my back harder, he showed me a video of my fatass where I looked like a suicidal kid. I\\'ve gotta admit the worst part is sitting on the ground during the lectures in Arabic which I don\\'t understand yet I am supposed to attend earlier than regular. I\\'m 15 and I can assure you this entire \"commemoration\" is bullshit. If you don\\'t like my English, hit up my English teacher because I\\'m not writing a fucking MLA format essay for you guys.', 'subreddit': 'exmuslim'}\n",
      "{'body': 'Two please ', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"If you're interested, the Band plays a pregame concert with the halftime music of the week one hour before the game at Kimball Concert hall. Then they march down in front of the stadium. It's a pretty good time. \", 'subreddit': 'Huskers'}\n",
      "{'body': \"Lol isn't EG qualified for the exact same number of LANs?\", 'subreddit': 'DotA2'}\n",
      "{'body': \"It's sad that her mother is going to try to use this to influence her. \\n\\nHowever your ex might see through it this time and you were more than in the right to do this so don't worry about it.\", 'subreddit': 'relationships'}\n",
      "{'body': 'Schools AND parents. Ex gf was a goddamn neurotic mess from it all. Ridiculous.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I'm a simple man. I see Mughals, I upvote!\", 'subreddit': 'eu4'}\n",
      "{'body': '[deleted]', 'subreddit': 'NanatsunoTaizai'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Get higher baby. DONT EVER COME DOWN!!!', 'subreddit': 'Hulugans'}\n",
      "{'body': \"I'm gonna take a wild guess and assume that this game didn't go your way\", 'subreddit': 'ArenaHS'}\n",
      "{'body': 'This looks better', 'subreddit': 'muacirclejerk'}\n",
      "{'body': \"Um actually strimmer that's the personification of death masquerading a cop as played by Bobert\", 'subreddit': 'northernlion'}\n",
      "{'body': '[deleted]', 'subreddit': 'mildlyinfuriating'}\n",
      "{'body': \"Mazda already has a limited partnership with toyota. I wouldn't be surprised if they merged or cooperated much more closely.\", 'subreddit': 'cars'}\n",
      "{'body': \"Essentially what auto said. Everything is about the moment, there is nothing but the moment. Both past and future are concepts of mind. There is only change, every thing comes and goes. Now, what you do in the moment is what creates your life. What you set was an intention to do something, you knew what you had to do in the moment and you executed. What I'm saying is, there's no need to worry about something that hasn't come to pass, or get excited and create delusions of grandeur. Not necessarily that I say don't plan for the future or intend to direct your life a certain way. \", 'subreddit': 'Advice'}\n",
      "{'body': \"Maybe it's just me, but I'd give up a late first and cap filler. I trust Brad to get everything out of him there is. \", 'subreddit': 'bostonceltics'}\n",
      "{'body': \"What's that mean?\", 'subreddit': 'CFB'}\n",
      "{'body': 'You know that some women brew too, right?', 'subreddit': 'Homebrewing'}\n",
      "{'body': 'They paid Espn', 'subreddit': 'CFB'}\n",
      "{'body': \"Are you sure? Cause my momma's didn't produce 190 degree breast milk. \", 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Emailed. But in support of keeping daylight savings time. Light in the sky at 3:30am in summer is absolutely ridiculous.', 'subreddit': 'vancouver'}\n",
      "{'body': \"I think that's true through the history of mankind haha women have always been more self aware about appearance and conscious about how they appear to the world. There's definitely minimal effort on the male side, but women love analyze to the smallest detail \", 'subreddit': 'videos'}\n",
      "{'body': 'Best bet is to just download a release that comes with all the dlc, and transfer the files over to the legit steam installation folder.\\n\\nIdk uf it will actually work, im sure steam checks to see if you actually bought the dlc.\\n\\nGood luck.', 'subreddit': 'CrackSupport'}\n",
      "{'body': 'The new emotes + post match was confirmed in the touchdown tournament.', 'subreddit': 'ClashRoyale'}\n",
      "{'body': \"... it's not disrespecting the country. Nor is it disrespecting the flag. And it's certainly not disrespecting the military.\\n\\nWhy do people have such a hard fucking time understanding what's happening? \", 'subreddit': 'news'}\n",
      "{'body': '[deleted]', 'subreddit': 'androidapps'}\n",
      "{'body': 'You’ll be lucky to land a job at McDonald’s with a 3rd grade education LM🅱️O', 'subreddit': 'dankmemes'}\n",
      "{'body': \"I am a retired teacher and volunteer in kindergarten. I don't think ADHD is stigmatized, but that is based on my experience in education. \", 'subreddit': 'ADHD'}\n",
      "{'body': 'Ya. Dude really is A POS if you ask me', 'subreddit': 'pics'}\n",
      "{'body': \"This is an in game tip also so I don't know why people don't know it\", 'subreddit': 'FortNiteBR'}\n",
      "{'body': \"If the times stay the same, then I'd only be able to have Orgo with Tovar (unless Falzone decides he likes sophomores again)\", 'subreddit': 'jhu'}\n",
      "{'body': \"&gt; one or none\\n\\nI didn't forget\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"Knowing reddit some idiots will still think it's real\", 'subreddit': 'GlobalOffensive'}\n",
      "{'body': 'Thank you, I will check it out!', 'subreddit': 'productivity'}\n",
      "{'body': 'This sub now allows reposts from 2007?', 'subreddit': 'Jokes'}\n",
      "{'body': 'how to ruin an evening in 30 seconds', 'subreddit': 'WTF'}\n",
      "{'body': 'Because of how legendary their second fight was, a lot of people have forgotten just how good their first one was. It was my favourite kind of a fight: a technical barn-burned.', 'subreddit': 'MMA'}\n",
      "{'body': \"I'm sorry if this is a stupid question, I just got into LPOTL a few months ago, but what all do they do at their live shows? Thanks!\", 'subreddit': 'LPOTL'}\n",
      "{'body': \"Noob here. What's LTC's potential?\", 'subreddit': 'litecoin'}\n",
      "{'body': 'He was on the opposite side of the line, most likely not getting a call for that', 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '143415878| &gt; None Anonymous (ID: 5Pd916Ww)\\n\\n&gt;&gt;143412250 (OP)\\n2012: obama\\n2016: bernie then hillary\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'i would honestly get divorced , just because HE now  counts as Straight instead of lesbian doesnt mean you have to change your sexual orientation , its okay to still love him for who he is , but dont let him tell you what you have to like', 'subreddit': 'asktransgender'}\n",
      "{'body': 'Danke fuer den Lesestoff! Brauche gerade was zum bei der monotonen Arbeit nebenher machen.', 'subreddit': 'de'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': \"Why not?  What can't Ross do as a wr?  I'm not saying he's the next Antonio Brown, you can't say anyone will end up being a top 3 wr,  but he can be the next Emmanuel Sanders, or the next Greg Jennings.  What are all these flaws John Ross has at wr that makes him incapable of developing into a good wr?\", 'subreddit': 'bengals'}\n",
      "{'body': 'Let me elaborate, this unofficial app, released 4 weeks after the launch of Destiny 2, hit the mark on how the vault should be presented. It went FURTHER by adding sub categories, which just makes it look even cleaner. \\n\\nIf I wanted to find a specific helmet I have in my vault, and it’s located right in the middle, I’d have to skim over weapons first, then narrow my search through all of my armour to find one piece. If they had tabs for armour and weapons, I’d go to the armour tab to look. If they went further and added subcategories, I’d just have to look through that.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Well said! :D', 'subreddit': 'fivenightsatfreddys'}\n",
      "{'body': '[deleted]', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': 'Can you imagine how the left would respond if this man had been a Christian?\\n\\nAlso:\\n\\n&gt; Businessman Soruth Ali, 42, had been previously convicted of raping a girl in her school uniform.\\n\\nWhy was he free to commit this brutal assault, then?', 'subreddit': 'RightwingLGBT'}\n",
      "{'body': '[deleted]', 'subreddit': 'WritingPrompts'}\n",
      "{'body': \"^(Hi, I'm a bot for linking direct images of albums with only 1 image)\\n\\n**https://i.imgur.com/px1M2R7.gifv**\\n\\n^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dnqikk7) \", 'subreddit': 'PewdiepieSubmissions'}\n",
      "{'body': 'One random please ', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'I still replay Crysis 1 and Warhead to this day... But 2-3 are hardly worth another visit. Actually, 3 was a pretty good time. ', 'subreddit': 'Games'}\n",
      "{'body': 'I was running into issues with bad muscle memory on Listen to my Heart (along with the others) causing me to miss just a note or two. My FC was a fluke; I got it right after waking up and rolling over to put my phone on my pillow. Go figure!', 'subreddit': 'SchoolIdolFestival'}\n",
      "{'body': '/u/WaterGuy12\\n', 'subreddit': 'FireGuy12'}\n",
      "{'body': \"&gt; This was President Obama's response to the disaster in Haiti:\\n\\nSee above, he literally lead with a comparison of President Obama's relief efforts to President Trump's.\", 'subreddit': 'politics'}\n",
      "{'body': '/r/hmmm', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"Aren't right turns on red permitted in the US except for New York or whenever signs prohibit it? They might not be in the right lane for it, but it doesn't look like they inconvenienced anyone.\", 'subreddit': 'Roadcam'}\n",
      "{'body': 'If it was Heath Bell would he sprint back in?', 'subreddit': 'baseball'}\n",
      "{'body': 'Stop touching yourself. ', 'subreddit': 'exmormon'}\n",
      "{'body': 'Does it matter?', 'subreddit': 'firstworldanarchists'}\n",
      "{'body': 'How the fuck', 'subreddit': 'Warthunder'}\n",
      "{'body': \"So the bar is zero dollars? Answer the question. How much are the Clintons donating? They're worth hundreds of millions. \", 'subreddit': 'politics'}\n",
      "{'body': 'Rafis I hope you get well soon &lt;3', 'subreddit': 'osugame'}\n",
      "{'body': \"Is this like a novelty account where you pretend to live 100 years ago and spout racist epithets like they're candy? Cool. \", 'subreddit': 'Libertarian'}\n",
      "{'body': 'Where’s MySpace mountain?', 'subreddit': 'baseball'}\n",
      "{'body': 'Does that friskiness include a picture from behind? ;)', 'subreddit': 'gonewild'}\n",
      "{'body': '#DEAR MOTHER OF KEK DO I LOVE THIS PLACE!!!!!!!!!!!!!', 'subreddit': 'The_Donald'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Well I can see that now you say it! ', 'subreddit': 'food'}\n",
      "{'body': 'Two randoms please', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"&gt;I mean, would you pay attention to a pundit who is a young earth creationist?\\n\\nIf their area of expertise were sufficiently removed from the domain of creationism, I might. There have been many brilliant people who've believed in crazy and irrational things.\", 'subreddit': 'slatestarcodex'}\n",
      "{'body': 'Not really anything you can do until the car is above water again.  Selling it or refinancing will both require money from you.', 'subreddit': 'personalfinance'}\n",
      "{'body': \"I'm not sure anymore. I only get on long enough to farm up a forma BP and start building it anymore. \", 'subreddit': 'Warframe'}\n",
      "{'body': '[deleted]', 'subreddit': 'hockey'}\n",
      "{'body': \"Yeah, I think I'll be. If not, then around 7pm. \\n\\nA quick question, do you have summer, fall, and winter deerlings with HA?\", 'subreddit': 'pokemontrades'}\n",
      "{'body': 'Thanks for the chance! ', 'subreddit': 'jailbreak'}\n",
      "{'body': '*the door opens, and a radar tech walks onto the command deck*\\n\\n\"Captain.  We\\'ve found something on radar.  It appears to be a small, unmanned aerial vehicle\"\\n\\n\"Tag it\"\\n\\n*a crew fires a small adhesive RFID tag from a pneumatic launcher on a drone of their own.  It sticks to the other drone, and transmits its location to the Lemminkainen*', 'subreddit': 'TheBlankSlate'}\n",
      "{'body': '[dude that´s great man](https://i.ytimg.com/vi/SolmwnWnWW4/hqdefault.jpg)', 'subreddit': 'Drugs'}\n",
      "{'body': \"Arden key got fat. Don't hold your breath \", 'subreddit': 'CFB'}\n",
      "{'body': 'Look at you! ', 'subreddit': 'stalker'}\n",
      "{'body': 'One spot ', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Sprinting definitely feels faster with higher mobility. I notice it all the time when going from my Warlock with 2 mobility to my Hunter with 10 mobility. Sprinting feels fast as hell.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'spread was Georgia -10 but thanks I guess', 'subreddit': 'sportsbook'}\n",
      "{'body': \"Normally play thatcher as I only solo que and people generally don't pick him that often and on defence majority of the time I play smoke just because he is the most versatile operator in my opinion.\", 'subreddit': 'Rainbow6'}\n",
      "{'body': \"LIS had me enjoying Chloe as a character but irritated in some respects. BTS has made her my favourite character and justified plenty for why she is the way she is in LIS. \\n\\n\\nI'm looking forward and not looking forward to finding out how Rachel ends up with Frank. Chloe considers her a close connection and inevitably at some point, a lover, otherwise she wouldn't have flipped out when you find out the relationship of Rachel and Frank. \\n\\n\\nRachel's someone I think who enjoys playing with people to a degree and is almost certainly a liar, yet is the thing with Frank because of something that happens, or are the feelings genuine, and what possesses her to betray Chloe? Or was it all misinterpreted by Frank or mis-sold as something more serious than it really was. Hard to say. \\nSo many questions. Team Chloe all the way though. \\n\\n\\nHella looking forward to the next episode. \\n\\n\\n#hashtagNotmyDavid\", 'subreddit': 'lifeisstrange'}\n",
      "{'body': '[removed]', 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Maybe that will help wit me drinking problem. ', 'subreddit': 'explainlikeimfive'}\n",
      "{'body': '[deleted]', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'I feel like this is Prince_Kropotkin bait. ', 'subreddit': 'SubredditDrama'}\n",
      "{'body': 'This is awesome, which goodwill? ', 'subreddit': 'ThriftStoreHauls'}\n",
      "{'body': 'Remember when Obama started that fraudulent university named after himself, and have to give all that tuition money back when it was proven to be a fraud?\\n\\nRemember when Obama sold steaks through a Sharper Image, also using his game to brand these steaks?', 'subreddit': 'politics'}\n",
      "{'body': ' Straight up or on the line? ', 'subreddit': 'nba'}\n",
      "{'body': 'Elton John, Rolling Stones, Otis Redding, Neil Diamond, Neil Young, Al Green, Nina Simone, Dire Straits, Donny Hathaway, The Impressions, Iggy Pop, Rod Stewart, Earth, Wind &amp; Fire, ', 'subreddit': 'AskMen'}\n",
      "{'body': \"I hadn't spotted her new one, didn't she have /u/GoddessJ and /u/TheRealGJ too?  They're both gone as well.\", 'subreddit': 'nsfw'}\n",
      "{'body': 'did those today for the first time, it was pretty cool', 'subreddit': 'bodybuilding'}\n",
      "{'body': 'Those jeans look like shit. Gal however never ceases to amaze.', 'subreddit': 'GalGadot'}\n",
      "{'body': \"This isn't a new occurrence. Maybe you shouldn't complain about something changing if you didn't watch it enough to know if it changes or not \", 'subreddit': 'GhostAdventures'}\n",
      "{'body': \"That Republika Srpska will give up on Brcko, meaning that it won't even be contiguous? That if remainder of BiH (Federation of BiH + Brcko) decides to close the border with RS, then the western half of RS will live at mercy of Croatia?\\n\\nNow that'd be interesting to watch.\", 'subreddit': 'europe'}\n",
      "{'body': 'Got. DAAAAYUMM!', 'subreddit': 'worldnews'}\n",
      "{'body': 'Since rape is a legal crime, I think the law is pertinent here. ', 'subreddit': 'PurplePillDebate'}\n",
      "{'body': 'Gracias por la aclaración, no podría haberse dicho mejor. \\nAdmito mi total ignorancia y prejuicio en el tema, salte porque me pareció choto leer varios comentarios de ese calibre. ', 'subreddit': 'argentina'}\n",
      "{'body': '独自の文化が発達していておもしろいよね', 'subreddit': 'newsokur'}\n",
      "{'body': 'Eh, you guys did the right thing. Maybe just post more guys at the door?', 'subreddit': 'Frat'}\n",
      "{'body': 'Same with Four Horsemen and Mechanix', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I feel like everything but the guns separates it from cod', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'I drugged you.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'see ya in December ', 'subreddit': 'CFB'}\n",
      "{'body': 'Moisés Arias', 'subreddit': 'disney'}\n",
      "{'body': '[removed]', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': 'who?', 'subreddit': 'lakers'}\n",
      "{'body': 'Sounds like you gave this a lot of thought.', 'subreddit': 'videos'}\n",
      "{'body': 'According to some news articles, they are mentioning pay-by-the-hour prices. I’ll try the beta but if it’s true you have to pay by hour, especially at those prices, I probably won’t continue. \\n\\nEdit: Never mind. The official website states SHIELD is $7.99/mo. So I’ll go by that. ', 'subreddit': 'apple'}\n",
      "{'body': 'Fuck the Steelers!', 'subreddit': 'bengals'}\n",
      "{'body': \"143412442| &gt; United States Anonymous (ID: UwlHxiGn)\\n\\n&gt;&gt;143412250 (OP)\\n2012: Paul\\n2016: Trump\\n\\nTurned 18 in 2011 but I prob would've voted for Obama since McCain was worse.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': 'Exactly my thought. I had a professor in college who had four patents and 3 of them never made him a dime. He said one of them made him \"several hundred dollars a year\". ', 'subreddit': 'AskEngineers'}\n",
      "{'body': 'Just be honest with them. Expect their reactions to be negative. You said you met him a year ago - any grown man who pursues a minor is going to get a healthy amount of criticism for doing so.', 'subreddit': 'relationships'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'AP and Reuters twitter are both bare bones, but will give accurate, factually correct information. \\n\\nThe BBC radio service, NPR, Al Jazeera, &amp; PBS Frontline will give more in-depth reporting with context, but be aware that each of them will frame certain stories in ways that correspond with their political supporters (I.E. BBC may give biased Brexit coverage, NPR is very pro-establishment for the major 2 parties in the states, Al Jazeera on Israel/Palestine). But generally anything they cover in world news will be pretty in-depth and well covered, especially if you cross reference with other news agencies to get various viewpoints. \\n\\n', 'subreddit': 'AskReddit'}\n",
      "{'body': \"It's cool I was just trolling.\", 'subreddit': 'CatastrophicFailure'}\n",
      "{'body': 'They pop out so easy if you use two sewing needles at each corner of the pan diagonally - I did my Naked 2, 3, and Ultimate Basics that way. No heat/effort and took like 5 minutes.', 'subreddit': 'MakeupAddiction'}\n",
      "{'body': 'what do you mean?', 'subreddit': 'OpTicGaming'}\n",
      "{'body': \"Bots don't count.\", 'subreddit': 'EnoughTrumpSpam'}\n",
      "{'body': \"I don't think bills throw away comic aside is meant to be taken 100% seriously m8\", 'subreddit': 'Maher'}\n",
      "{'body': '#Player ratings in the local press  \\n[Ruhr Nachrichten](http://www.ruhrnachrichten.de/sport/bvb/) | [Westdeutsche Allgemeine Zeitung](https://www.waz.de/sport/fussball/bvb/bvb-star-aubameyang-verschiesst-elfmeter-klaeglich-note-5-id212097293.html)       \\n\\n-------------------------------------------------------  \\n\\nPlayer | RN Rating | WAZ Rating | RN Reader vote | Avg  \\n---|---|---|---|---  \\nBurki | 2 | 2.5 | 2.1 | 2.20   \\nAndrey | 2.5 | 2.5 | 2.2 | 2.40      \\nKagawa | 3 | 3 | 2.2 | 2.73  \\nSokratis | 2 | 3.5 | 2.7 | 2.73   \\nBartra | 2.5 | 3 | 3.0 | 2.83   \\nPiszczek | 3.5 | 3 | 3.1 | 3.20     \\nCastro | 3.5 | 3.5 | --- | 3.50   \\nWeigl | 4 | 4 | 3.4 | 3.80    \\nPulisic | 4.5 | 4 | 3.6 | 4.03    \\nDahoud | 4.5 | 4.5 | 3.8 | 4.27   \\nToljan | 4.5 | 4.5 | 3.9 | 4.30    \\nAuba | 5 | 4 | 5 | 4.67    \\n\\n-------------------------------------------------------  \\n\\nThought I\\'d post this, as the post match thread here has been unusually controversial. Accusations of sub biases notwithstanding, Burki, Andrey, Papa, Bartra, Kagawa who were all praised here and in the match thread, get good to passing marks. Weigl, Pulisic, Dahoud, Toljan, Auba who drew criticism here all get sub par marks from the German press as well as German readers at large. Pretty sure it isn\\'t \"American hipsters\" writing for German papers. Just saying.\\n', 'subreddit': 'borussiadortmund'}\n",
      "{'body': \"I didn't see this sign (or the shirt) but someone told me about it from my previous post. Enjoy!\", 'subreddit': 'exmormon'}\n",
      "{'body': 'Millwrong.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I have sometimes been using them on the same team, but not all the time.', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"That's my fear, except I see Cubs replaced with Nats just as easily. \", 'subreddit': 'WahoosTipi'}\n",
      "{'body': 'At least he had the decency to cover his pig butt cheeks.', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"If it says that, it's outdated. Contact the mods and they can cross it out. Or if it's not outdated, then they'll let you know, but as far as I know, Memu is dead and Remix OS still works (and can be installed even if it's not still updated). As for Win 10, can't comment on that, though it's hard to believe it Remix OS doesn't dual boot on Win 10.\", 'subreddit': 'grandorder'}\n",
      "{'body': \"whilst I respect your opinion I must disagree, Ghaul is simply a Cabal raised to fight someone else's fight then when the opportunity arises where he can finally reclaim his own destiny *heh* he kills the man that brought him up and engulfs himself in the power he doesn't understand \\n\\nalso he has a cool cape \", 'subreddit': 'destiny2'}\n",
      "{'body': 'Added a box score and they deleted again.  Too bad, would have been fun for the sub to spark a meme about a good D3 team. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Flair up bro', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"If I got $100 for every time I masturbated I'd have $1000 just today.\", 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'The gameplay is good. Every other part of it is miserable and buggy. ', 'subreddit': 'NBA2k'}\n",
      "{'body': 'Unless you happen to be trained, good at blocking kicks and fighting back... If you can you run', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"That's one reason why I don't want Bcash or 2x coin.\", 'subreddit': 'btc'}\n",
      "{'body': '[removed]', 'subreddit': 'politics'}\n",
      "{'body': '[deleted]', 'subreddit': 'The_Donald'}\n",
      "{'body': \"First off wtf i never mix up too and to\\n\\nSecond off yeah it seems logistically difficult to be there, vodafone doesn't transfer over to andorra so i wouldn't have data. I really want to bike from andorra la vella to the french border, and tally andorra off my list of countries I've visited, but its starting to look pretty tough\", 'subreddit': 'spain'}\n",
      "{'body': \"That does not answer the question, only deflects. Is the league's playoffs considered less competitive? \", 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Thanks for quick trade', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Jesus Christ what a bunch of insufferable twats', 'subreddit': 'videos'}\n",
      "{'body': \"And didn't use the train (which looks like the worst part polygon wise) and added LOD. and probably more.\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Much better team ', 'subreddit': 'CFB'}\n",
      "{'body': \"The text after the bit you've quoted says:\\n(Heads up: These codes can only be redeemed by non-Business Todoist users. Premium users will just receive an additional three months!) \\nSo the last bit suggests they'll still work?\", 'subreddit': 'todoist'}\n",
      "{'body': \"Discusting language! I don't want my fuzzy bunny milk money using words like *bosom*!\", 'subreddit': 'oldpeoplefacebook'}\n",
      "{'body': 'Ok so the paid is PVE fort defense with a campaign and the free one is PvP shooter with no building?', 'subreddit': 'PS4'}\n",
      "{'body': 'One random ', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"I'd a little more than a peak.\", 'subreddit': 'ass'}\n",
      "{'body': 'Son grasas. Ven guita y se desesperan.', 'subreddit': 'argentina'}\n",
      "{'body': '[deleted]', 'subreddit': 'amiugly'}\n",
      "{'body': 'I don’t remember any liberals making fun of her for her accent, but I do remember her being made fun of for plagiarizing Michelle Obama.', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': '[deleted]', 'subreddit': 'DIY_eJuice'}\n",
      "{'body': 'bad bot', 'subreddit': 'pebble'}\n",
      "{'body': 'Yup \\n\\nThe king is dead long live the king /queen', 'subreddit': 'TumblrInAction'}\n",
      "{'body': '[Link To Original Submission](http://reddit.com/73ifoe)', 'subreddit': 'MovieposterFans'}\n",
      "{'body': 'da noomerator +  \\nda denominattur =  \\ndanullment', 'subreddit': '90DayFiance'}\n",
      "{'body': '&gt;there is no universal \"motherhood\" that spans across species\\n\\nUhh...what? Animals have mothers and animals are mothers to their babies. Cows being mothers isn\\'t something hippies made up to try to anthropomorphise them... that\\'s the term everyone uses for a female that bears young. I get that you\\'re saying human motherhood is especially cool and important because we\\'re smart and all that. Fair enough. But motherhood *literally, factually* spans across species, and dairy absolutely is related to motherhood. The industry wouldn\\'t exist without it.', 'subreddit': 'TumblrInAction'}\n",
      "{'body': '[removed]', 'subreddit': 'gifs'}\n",
      "{'body': 'It’s most noticeable when fast scrolling ', 'subreddit': 'apple'}\n",
      "{'body': 'That\\'s awesome! When you were starting out, did it ever feel uncomfortable to be in charge of the more \"seasoned\" and experienced employees? What helped you to be a better leader?', 'subreddit': 'Chipotle'}\n",
      "{'body': 'Lorian and Lothric ', 'subreddit': 'darksouls3'}\n",
      "{'body': '[deleted]', 'subreddit': 'teenagers'}\n",
      "{'body': 'Any solos that are good redo for the stats?', 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': 'Arden Key blah blah shut uuuuuuuup', 'subreddit': 'CFB'}\n",
      "{'body': 'Just to be clear, are you a realtor or similar?', 'subreddit': 'canada'}\n",
      "{'body': 'Great to know! Thanks. ', 'subreddit': 'AlienBlue'}\n",
      "{'body': '###IM SO EMOTIONAL NOW. THE ONLY WAY THIS CAN GET BETTER IS IF WE KILL THE DODGERS TONIGHT AND DESTROY THE FACE OF EVIL TOMORROW.  ', 'subreddit': 'baseball'}\n",
      "{'body': '[deleted]', 'subreddit': 'gonewild'}\n",
      "{'body': '143417733| &gt; Canada Anonymous (ID: QG6wKQuQ)\\n\\n&gt;&gt;143415870\\n\\nIDK about that. I was skeptical about them but then a bunch of cucks got all huffy over the merger and left the Conservative parties and that made it all a lot more appealing in my eyes. Butt hurt cucks means there\\'s a good chance of potential there. Of course nothing\\'s certain in politics, there\\'s always a chance of backstabbing kikery but I\\'m hopeful. I really liked Brian and Callaway and the other 2 would be \"ok\". A Kike\\'d up business neo-liberal is still 1000x better than the out right SJW Commies we have right now.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'What will grow well depends in part on where you live - short season or long, coolish summers or blistering heat, etc. Probably want to factor that in.\\n\\nLate afternoon sun tends to be strong so shade lovers like ferns might not be the best bet. The upside of afternoon sun is that it will support a wide variety of colorful plants: annuals like fancy trailing geraniums and sun resistant coleus (Trailing Plum is pretty and dependable http://davesgarden.com/guides/pf/go/57816/) in hanging baskets. In pots, you could grow dwarf zinnias, dwarf to mid-height snapdragons, petunias in a zillion colors and patterns, and sun resistant impatience like SunPatiens (http://www.costafarms.com/plants/impatiens-sunpatiens) for color.\\n\\nSomething I tried for the first time this year that was super cute and unusual was dwarf eucomis or pineapple lilies. They grow from spring-planted bulbs in half day sub and look great from June into fall. Very unusual and whimsical. I grew this variety and will again:  http://www.leafari.com/eucomis-aloha-nani-hybrid.html\\n\\nYou have lots of options. Have fun!', 'subreddit': 'gardening'}\n",
      "{'body': '[deleted]', 'subreddit': 'Tinder'}\n",
      "{'body': '[removed]', 'subreddit': 'Accounting'}\n",
      "{'body': 'Plus he is the God.', 'subreddit': 'baseball'}\n",
      "{'body': \"Angst? Dissimulation? Virility? That's a well-trimmed luxurious beard, not gonna lie.\", 'subreddit': 'dragonage'}\n",
      "{'body': \"I also didn't realize it 3 cards or less, not 1, so I'm definitely showing signs of illiteracy \", 'subreddit': 'DuelLinks'}\n",
      "{'body': 'I meant to trade from strength to address weakness... though I do like how our D is progressing we could really use one more piece. Depending on this season we could be all in as early as next year ', 'subreddit': 'leafs'}\n",
      "{'body': 'Give directions, for sure. ', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'politics'}\n",
      "{'body': 'I feel so bad for Planet Dog :(', 'subreddit': 'DotA2'}\n",
      "{'body': 'Give directions, for sure. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I *rate* everything, but I've never rated a proper series less than a 5 since I don't seek out shows that I think I'll find shit (Except for Shitcom, but that was a 1 minute thing for the joke).\\n\\nMy score deviation is actually -.5, so apparently I'm a slightly harsh rater compared to average. For currently watching it's -1 because I've only rated Fate Apocrypha and NGNL, and I don't like the sound of tactical nukes every time something falls down.\", 'subreddit': 'anime'}\n",
      "{'body': \"**[Original Submission by /u/C0nguy](https://www.reddit.com/r/kindle/comments/736zvj/should_i_upgrade_or_wait_for_a_new_generation/)** into /r/kindle\\n\\n---\\n\\n# Subreddit Overview\\n* A community for: **8 years**\\n* # of subscribers: **33,917**\\n* # of mods: **6**\\n* Subscribers per mod: **5,652**\\n\\n# Popular Posts Summary\\n* Top domains: self.kindle **(100%)**, goodereader.com **(0%)**\\n* % NSFW: **0%**\\n* Average Score: **10**\\n\\n# Discussion Summary\\n* Average Comment Length: **~32** words per comment\\n* Flesch-Kincaid Reading Level: **4**\\n* Comments per post: **~10**\\n\\n# A sampling of top posts:\\n* Top all time: [Kindle 2 joystick breaks in half.  Amazon ignored my expired warranty and 2 day shipped me a new Kindle free of charge.  Oh, and they shipped me a Kindle 3.  Will do business with again, A++ (292 points by /u/jzzsxm)](https://www.reddit.com/r/kindle/comments/o4611/kindle_2_joystick_breaks_in_half_amazon_ignored/)\\n* Top this month: [The Kindle Oasis is no longer available for purchase (51 points by /u/ScubaSteve1219)](https://www.reddit.com/r/kindle/comments/70hgj4/the_kindle_oasis_is_no_longer_available_for/)\\n* Top this week: [Is there going to be a new Paperwhite at Kindle's 10 year anniversary this year? (31 points by /u/johnmountain)](https://www.reddit.com/r/kindle/comments/72ct4r/is_there_going_to_be_a_new_paperwhite_at_kindles/)\\n\\n## **[Subscribe at /r/kindle](/r/kindle)**   \\n\", 'subreddit': 'Serendipity'}\n",
      "{'body': \"You're in the know all right!\\n\\nThat's a useful general tip, good to know.\", 'subreddit': 'bloodborne'}\n",
      "{'body': 'I’m not very familiar with Audition, but you should have no trouble recording at -12dB without any effects in Audition.\\n\\nAre you hearing clipping with the recorded signal?\\n\\nIt sounds like something is wrong with Audition if normalizing to -3 isn’t actually bringing the peak level to -3. Like the meters are maybe calibrated to a different scale than DBFS. Again, I’m not familiar, but I can’t imagine what scale it would be calibrated to in this case.', 'subreddit': 'audioengineering'}\n",
      "{'body': \"Ah that sucks. I hope they'll fix that, then.\", 'subreddit': 'Guildwars2'}\n",
      "{'body': '80%? Where are you *getting* this from? It reduces your *two thousand* dodge bonus by four hundred at level 10. 1600 is still formidable.\\n\\nEDIT: Or, in terms of the percent formula I just saw in the FAQ out there, it reduces your +200% dodge bonus by 50%, and +150% is still incredible.', 'subreddit': 'EtrianOdyssey'}\n",
      "{'body': 'Looking for both. Srry', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Pennsylvania law:\\n\\n- Felony: Using legal documents (such as a real driver's license) to impersonate another individual. This is for example if you use an older brother's ID or someone who looks like you.\\n\\n- Misdemeanor: Possessing, purchasing, or using fake legal documents (such as a fake ID). This is most of us here.\", 'subreddit': 'fakeid'}\n",
      "{'body': '&gt; eating with your hands is completely unacceptable here.\\n\\nHow do you eat burger, fries, and pizza?\\n\\n', 'subreddit': 'AskReddit'}\n",
      "{'body': '[Hold my beer](https://steamdb.info/calculator/76561197966308041/?cc=us)\\n\\nA friend of mine on another site celebrated his 5000th game with Cuphead', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'There will be no undefeated Pac-12 teams. There never is. There’s a reason Pac-12 after dark exists. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Trebol and Fujitora', 'subreddit': 'OnePiece'}\n",
      "{'body': 'Ya imo my friends either loved or had no interest in monogatari. If you didn\\'t like the shaft aesthetics, long narration, and general stye of the show, you\\'ll prob not care for it ever. \\n\\nOnly thing I could suggest is give the kizumonogatari movies a try. You\\'re usually suppose to watch it after bakemonogatari but it\\'s better than not watching at all. Kizu is more \"newcomer\" friendly than the rest of the series due to less narration and more action then usual.  I\\'ve gotten a lot of people hooked to the show,who dropped bake at first, with the movies. ', 'subreddit': 'anime'}\n",
      "{'body': 'Smokin doints in Amish?', 'subreddit': 'trees'}\n",
      "{'body': \"New to this concept: is every song the same tempo, or do artists take advantage of different time signatures? Either way it seems odd that you'd listen to the same two seconds of music over and over. \", 'subreddit': 'vinyl'}\n",
      "{'body': 'Never a miscommunication. ', 'subreddit': 'baseball'}\n",
      "{'body': 'Dope af. ', 'subreddit': 'hiphopheads'}\n",
      "{'body': 'one random', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"Ah neat ! Mine have been like this my entire life though I'm not sure if mine is related to meds\", 'subreddit': 'schizophrenia'}\n",
      "{'body': 'Funny way to spell betamax', 'subreddit': 'PrequelMemes'}\n",
      "{'body': 'so you dont like the gov cuz she likes cats?  thats some pretty serious single issue voting.', 'subreddit': 'oregon'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"&gt;Youve got to be fucking kidding me. The is no comparison between the BBC and RT on this issue. http://www.bbc.com/news/blogs-the-papers-41209189 .\\n\\nI don't think that's the link you meant to include. It doesn't even mention the word Syria.\", 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': 'I try to give the founding fathers the same courtesy. They were imperfect men living in a different era. However, their accomplishments quite literally changed the world they lived in in the world that we live in as well.', 'subreddit': 'AskReddit'}\n",
      "{'body': '1/1 !', 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'ComedyCemetery'}\n",
      "{'body': \"Wow, that's too deep an explanation. I've understand it now. Thanks a lot for taking your time to help me!\", 'subreddit': 'learnmath'}\n",
      "{'body': '[If you want a tophat, then this might interest you, coming Halloween Day](http://steamcommunity.com/sharedfiles/filedetails/?id=949840128&amp;searchtext=)', 'subreddit': 'Warframe'}\n",
      "{'body': 'Warframe is a shitty game', 'subreddit': 'dankmemes'}\n",
      "{'body': \"Yeah, OP is not the only high level Widow I've seen do this.  Does the reload cancelling help at all?\", 'subreddit': 'Overwatch'}\n",
      "{'body': \"Don't worry, with your attitude you wouldn't last a week in the medical field.\", 'subreddit': 'battlefield_one'}\n",
      "{'body': 'Quote for them Jeremiah 10:1-4 (1)Hear the word that the Lord speaks to you, O house of Israel. (2) Thus says the Lord: “Learn not the way of the nations, nor be dismayed at the signs of the heavens because the nations are dismayed at them, (3) for the customs of the peoples are vanity. A tree from the forest is cut down and worked with an axe by the hands of a craftsman. (4) They decorate it with silver and gold; they fasten it with hammer and nails so that it cannot move.”', 'subreddit': 'atheism'}\n",
      "{'body': 'good shit sparty', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': \"&gt; i hate oversleeping becouse you sleep more , but you are tired all day .\\n\\nExactly. It's such a paradox!\", 'subreddit': 'anime'}\n",
      "{'body': 'you should probably reread the fight then', 'subreddit': 'hajimenoippo'}\n",
      "{'body': 'Tied the record for most pitchers used in a single season. ', 'subreddit': 'baseball'}\n",
      "{'body': \"which one? there's a boomstar sem making the bubbling bass drone thing and the higher one later is a simple ob6 patch\", 'subreddit': 'Techno'}\n",
      "{'body': 'Got [this one off Amazon](https://www.amazon.com/dp/B000XTH1GY/ref=cm_sw_r_cp_api_.-c0zb0B0H2GV) Had it for almost 3 years now and works great.', 'subreddit': 'teslamotors'}\n",
      "{'body': \"I've always wondered- doesn't that fragile ace by happy down banana look kind of sketchy? Haha\", 'subreddit': 'GlobalOffensive'}\n",
      "{'body': 'Yet my other posts are censored when asking simple questions. What is this sub afraid of?', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'Isaiah Thomas licking his lips', 'subreddit': 'nba'}\n",
      "{'body': 'Ida Maria - Devil\\n\\nThirty Seconds to Mars - Hurricane', 'subreddit': 'popheads'}\n",
      "{'body': \"It's *Nickelback*. \", 'subreddit': 'ProtectAndServe'}\n",
      "{'body': \"Yep, he has a nice lvl 1 cheese, it's ridiculous\", 'subreddit': 'Shen'}\n",
      "{'body': 'cash or do you add it on with the receipt? ', 'subreddit': 'UberEATS'}\n",
      "{'body': \"Yeah. Nah. That *is* the general consensus in Russia; post collapse the perhaps overzealous economic advise given by U.S. advisers was poor, and/or poorly understood, and/or poorly implemented, but mostly thought to be poor advise based on fundamentals that worked just fine in situation A, but not situation F(ucked) which is where Russia was. \\n\\nActually most of the population thinks Gorbachev sold out and/or was bribed by the west. I could be wrong but I think he's less popular than Stalin. Thereabouts anyway. \\n\\nNinja edit: oops, *after* the collapse I am talking about. The collapse happens all by itself (or was it Gorbachev?)\", 'subreddit': 'IAmA'}\n",
      "{'body': \"Amazing. It's just baffles me why/how they treat us so poorly. \", 'subreddit': 'raisedbynarcissists'}\n",
      "{'body': \"I don't agree. You're a piece of shit for even agreeing\", 'subreddit': 'funny'}\n",
      "{'body': 'That sounds very good.', 'subreddit': 'CasualConversation'}\n",
      "{'body': 'I suppose he won Juno over with his feminine side?', 'subreddit': 'fireemblem'}\n",
      "{'body': 'Zip? I’m local to 37211', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'I HATE TOM TOO BUT YOUR REASON GOES MORE DEEP THEN MINES...... AND THAT IS SAD', 'subreddit': 'StarVStheForcesofEvil'}\n",
      "{'body': \"Or the times you do get green it's either mustache man or a Merric breaking your rate\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'just the kinda daddy freedom sub is looking for', 'subreddit': 'facepalm'}\n",
      "{'body': \"Other distros just had weird effects where if i pressed any button then it would automatically ask me if i wanted to shut down the computer, or programs wouldn't work. Idk remember what distros specifically, I went through so many and I'm happy with ubuntu so I want to make it work.\\n\\nWhen I typed that in tge command prompt it asked for my password but when I type nothing shows up. Also I didn't even setup a password anyway\", 'subreddit': 'linux4noobs'}\n",
      "{'body': \"Fuck you're sexy\", 'subreddit': 'gonewild'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Yea steroids did a lot of that', 'subreddit': 'The_Donald'}\n",
      "{'body': '[Original post](https://www.reddit.com/r/NetherlandsPics/comments/73ig9u/youre_not_dutch_if_you_havent_lived_in_de_efteling/) by /u/winterbynes in /r/NetherlandsPics\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#whitelist \"netherlandspics\")\\n', 'subreddit': 'ImagesOfNetherlands'}\n",
      "{'body': 'Use sour cream, works great ', 'subreddit': 'food'}\n",
      "{'body': \"It's petty to kick children off a sports team because they don't agree with you politically. That doesn't make me a hypocrite. I don't like the coach because he seems like an asshole. It has nothing to do with his response to the kneeling, it's how an adult handled an interaction with children.\\n\\nIt's like seeing that parent who's WAY too into their kid's little league.\", 'subreddit': 'news'}\n",
      "{'body': '/u/apetvlad - Ghana (There yours)', 'subreddit': 'worldpowers'}\n",
      "{'body': \"Other than using pipes for fuel and waste I've only ever used the flux points, so I'm not sure if you can use cables. That being said, with the flux points you have to have a redstone flux access point (I believe is what it's called) as one of the blocks, with the flux plug attached to it. Might need a similar one for cables.\\n\\n\\n\\nAssuming you've built the reactor correctly, and have fuel and turned it on...as I may or may not have done once...\", 'subreddit': 'roosterteeth'}\n",
      "{'body': '143418846| &gt; France Anonymous (ID: vuuD9Obi)\\n\\n&gt;&gt;143412250 (OP)\\n2012 : Marine Le Pen\\n2017 : Marine Le Pen\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'The only confirmation they can get is from themselves.', 'subreddit': 'thatHappened'}\n",
      "{'body': 'A magnet walks into a bar and gets stuck.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'What a spellbindingly catty thing to say.', 'subreddit': 'funny'}\n",
      "{'body': 'The highest level was 31?\\nYou got 0 damage and 0 team balls!?', 'subreddit': 'pokemongo'}\n",
      "{'body': \"Well Corporal it doesn't feel any better being a 39 year old Lance Corporal  \\n\\nSome guys at work did make me a Senior Lance Corporal of the Marine Corps chevron so I got that going for me, which is nice. \", 'subreddit': 'USMC'}\n",
      "{'body': 'This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: https://www.reddit.com/r/Serendipity/comments/73igad/should_i_upgrade_or_wait_for_a_new_generation/', 'subreddit': 'kindle'}\n",
      "{'body': 'Good job!!!! Feels good dont it? :)', 'subreddit': 'stopdrinking'}\n",
      "{'body': 'Sorry, I didnt see your comment. But i want to trade for Jager :)\\n\\nPerharps, Im going to sleep and will be on in about 14-15h or maybe in 6-7h, if I will wake up earlier', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Prior to IP there were trade secrets that protected innovators from competition. Heck, many candy companies still have trade secrets today rather than patents. The idea isn't that IP protects competition, as competition would be fine without it. The issue is that IP provides other benefits besides rent seeking to innovators, advantages that aren't talked about.\\n\\nImagine modern medicine with trade secrets. There's no way for it to work. How would you provide studies if the methodology and composition couldn't be known by third parties? NDAs are a modest solution but everyone has a price. Patents means the mode of action of a medicine is out in the open so it can *immediately* begin benefiting future science.\\n\\nTrade secrets are also, well, secrets. It incentivizes industrial espionage which is usually damaging and illegal and harmful.\\n\\nHowever IP as it currently exists has flaws. Right now the incentive is not to get a patent and get back to inventing because it is more profitable to get a patent and use it as a bludgeon to sue for damages and prevent others from innovating. Inventing and progress must wait on the patent to expire which does more harm to innovation than the patent protects. \", 'subreddit': 'LibertarianDebates'}\n",
      "{'body': \"Agreed.  Our games have been super close with wiscy the last couple of years and I'm just tired of being teased by them.  Iowa also doesn't deserve to feel like they're good.\", 'subreddit': 'Huskers'}\n",
      "{'body': \"Thank you! It doesn't have to be traditional as long as it's good biscuits and gravy.\\n\\nI've had Bacon and Butter three times and I don't understand the love. I've had the biscuits and gravy on two separate occasions and it was bland gravy with stale biscuits.\\n\\nI will add your suggestions to my list. I'm going to have to spread it out or I will be keeling over ;)\", 'subreddit': 'Sacramento'}\n",
      "{'body': \"Ice isn't dead, only his content. \", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'i hope they add more in the future!', 'subreddit': 'ffxiv'}\n",
      "{'body': \"Nice : ) On Vanier? There's an STs of Ontario guy there and i'm moving to the same building lol \", 'subreddit': 'Guelph'}\n",
      "{'body': 'I\\'m not sure if comparing Battlefield and Call of Duty is a proper thing. Battlefield is not like Call of Duty at all. They might both be FPS, but that\\'s like trying to compare DOOM to Fallout. They\\'re 2 completely different games.  \\n \\nThis whole \"Is one better than the other?\" doesn\\'t make sense to me, one is massive battles with vehicles, classes, realistic sounds, teamwork, massive maps, and objectives. The other is tiny maps, fast gameplay, no recoil, and few players.  \\n \\nThese 2 games are nothing alike. ', 'subreddit': 'battlefield_one'}\n",
      "{'body': '#Safety Warning:\\n\\n**Sites below can potentially have multiple ads, and/or popups, and/or misleading download links.**\\n\\n* Always remember to never download anything from the websites posted here.\\n\\n* Use an **AdBlocker** - I recommend using uBlock Origin\\n\\n* Report any other suspicious links to the moderators and be sure to include a reason why.\\n\\n* Use these sites with your own discretion.\\n\\n*****\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/NHLStreams) if you have any questions or concerns.*', 'subreddit': 'NHLStreams'}\n",
      "{'body': 'Mark it with a sharpie', 'subreddit': 'AskReddit'}\n",
      "{'body': 'SS Tier = Sha Lin Smasher Tier', 'subreddit': 'Paladins'}\n",
      "{'body': \"This is true yes, also that book fucking rules (RAWs other stuff is good to, Robert Shea's other output isn't bad but waayyyy different)\", 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': \"He had his own TV show on RT - he literally accepted a paycheck from the Russian government's propaganda arm.\\n\", 'subreddit': 'evilbuildings'}\n",
      "{'body': 'I always feel like I move super lightning speed about 10 minutes after my hit. Then I go slow mo', 'subreddit': 'trees'}\n",
      "{'body': \"Exactly. We have players like Porzingis, who are in their 20s and people still feel like he needs to physically develop more. Gordon Hayward only developed the body he needed to absorb contact last year. No way we'll see 16 year olds in the NBA.\", 'subreddit': 'nba'}\n",
      "{'body': \"Hello, your title indicates that this post may contain content related to either server lag, desync, or some form of bug. If so - we would like to encourage you to report these issues directly to Bluehole using whichever of the following links is the most appropriate: \\n\\n[Server Lag Reports](http://forums.playbattlegrounds.com/topic/5435-server-lag-report-thread/)  \\n[Bug Reports](http://forums.playbattlegrounds.com/forum/10-bug-reports/)\\n\\n\\nBecause this subreddit is fan run there is unfortunately not much we can do here regarding either server issues or in-game bugs. The best way to bring them to the dev's attention is by directly reporting them using those links. Thanks!\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/PUBATTLEGROUNDS) if you have any questions or concerns.*\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': \"Great.  He helps people who don't know how to use google or don't know to just read Wizz's MI blog and this sub.  \\n\\nAnd this sub would become a much less useful place if polluted with marketing spam.\", 'subreddit': 'AmazonMerch'}\n",
      "{'body': '[removed]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"My last LTR was with an intelligent woman who is a prosecutor, so I can attest that everything you have written here is spot on- both her behaviour and mine. I found TRP after the relationship failed in this manner. \\n\\nFollowing that, doesn't that mean that some of the womens' quotes from the original article are actually fair? A miniscule sliver of men have red pill traits (learned or inherent). So when these women self identify as intelligent any man who can't stand up to their shit is going to be perceived as intellectually weaker, whether they are an astrophysicist or not. \\n\\nThis is just another shit test, and nothing a quick evisceration on a chess board wouldn't solve to quiet their lamentations. \", 'subreddit': 'TheRedPill'}\n",
      "{'body': '[link to a similar post 10 days ago](https://www.reddit.com/r/SquaredCircle/comments/71dun1/wwe_survey_sent_out_roh_tv14_inring_show_local/)\\n\\nI would really love to be able to get NJPW/RoH/WWE/etc all in one place for one price rather than multiple sites.', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'I really hope  that boys parents see this video...so his dad can send it to every scout in the major leagues ', 'subreddit': 'WTF'}\n",
      "{'body': 'I say we sacrifice White to the Lord of Light', 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'SanctionedSuicide'}\n",
      "{'body': \"No, it's probably portillo.\", 'subreddit': 'pics'}\n",
      "{'body': 'In a heartbeat but i doubt anyone bites for that \\n', 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'I think this is the correct course of action. Marincin has effectively played his way off our roster and needs to be removed. I don’t care how much of an analytical darling he is. ', 'subreddit': 'leafs'}\n",
      "{'body': '[removed]', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': 'What would you say is the perfect amount of almonds? ', 'subreddit': '2007scape'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Welcome to IOTA. +200 IOTA', 'subreddit': 'IOTAFaucet'}\n",
      "{'body': 'K &amp; R is an awesome book. But you should know some C first. \\n\\nMy favorite beginner C book is [C By Discovery by Foster &amp; Foster.](http://catalog.lib.utexas.edu/search/0?searchtype=o&amp;searcharg=63168068)', 'subreddit': 'learnprogramming'}\n",
      "{'body': \"That you think that proves it's not.\", 'subreddit': 'facepalm'}\n",
      "{'body': \"The caps are probably duds. 50mg and you feel nothing isn't normal.\", 'subreddit': 'CanadianMOMs'}\n",
      "{'body': \"Does anyone have any tips on using the work sharp? I have the ken onion edition. It seems like no matter what I do I cannot get the blade sharp enough to shave with. \\n\\nI'm starting to look at getting the wicked edge system but I figured I would look for some pointers before abandoning it completely. \", 'subreddit': 'knives'}\n",
      "{'body': \"It's a sad, sad world... Why didn't I buy a crate when I turned 18 and they were less than $200?\", 'subreddit': 'weekendgunnit'}\n",
      "{'body': 'Yes', 'subreddit': 'KenM'}\n",
      "{'body': 'At least there won’t be a MAC school going undefeated against a garbage schedule', 'subreddit': 'CFB'}\n",
      "{'body': \"The Internet. You use it every day but how often do you consider how powerful it really is? All things considered it hasn't been around for very long, yet our society world collapse pretty damn quickly if we ever lost it. \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'But the more we spam them, the faster he will get confirmed.', 'subreddit': 'Iota'}\n",
      "{'body': \"Neither. Fine as-is. Fort eats enough rounds to be worth the increase, but it isn't made to make you immortal. The 7.62x25mm would probably be able to knock through the slightly-less-than IIIa protection that fort provides, with a few rounds driven into a tight grouping IRL.\\n\\nIn terms of game mechanics, it's possible that you might've just had god-tier RNG and gotten 3 penetrating shots in a row (roughly .1% chance of happening) which would've killed him through the armor, plus one round that caught on the armor.\", 'subreddit': 'EscapefromTarkov'}\n",
      "{'body': 'So... fuck women, basically.\\r\\rOk.', 'subreddit': 'GCdebatesQT'}\n",
      "{'body': 'I feel like people would burn the flag for fun here in the UK and people would find it entertaining instead of offensive.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"He can't buy fakes he gets like 3m views a vid\", 'subreddit': 'FashionReps'}\n",
      "{'body': 'I think a lot of people don\\'t realize this about indigenous folks is that a lot of the councils are insanely corrupt.  Generally any money made on reserve usually stays among the council and their families.  I feel sorry for those people.  I used to live in North Western Ontario and man they got the short end of the stick from both the government and their own people who were supposed to be their \"leaders\".\\n\\nMany weren\\'t allowed to drink on the res so usually once a month when the cheques came in they\\'d drive to the nearest town that had an LCBO, Beer Store, and Walmart/major grocery store and just clear the places out.  Two things wouldn\\'t be uncommon to see at the local hotels/motels in town at the end of the month A. Pick up trucks full of food/supplies/etc and drunk/passed out natives everywhere.  \\n\\nNaturally they couldn\\'t get TOO drunk (many of the young ones would) otherwise they ran the risk of someone nosey person who lived in town (in small towns everyone knew everyone even if they lived on the res) to call the council leaders and rat on some poor drunk native kid.  \\n\\nAdd to the fact that they would jack up the prices on all the food/supplies on the res because a lot of times the council leaders were in league with either the airline that would fly the stuff in or the companies that would truck the stuff in and get a cut of the profit from that.  \\n\\nOverall they were just insanely corrupt and treated their own people like shit and then just told them it was the white man.', 'subreddit': 'worldnews'}\n",
      "{'body': 'Greed mode was already possible, Greedier is what was unbalanced. The movement was fine too imo, I got used to it after a few runs.\\n\\nThe bug fixes that made the game litterally unbeatable are what are really big here.', 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'This is literally how he campaigned. What is wrong with you?', 'subreddit': 'politics'}\n",
      "{'body': \"Ooh that's nice :) thank you \", 'subreddit': 'Smite'}\n",
      "{'body': \"do you buy the pink ones just in the store? I'm looking to over haul my brushes\", 'subreddit': 'MakeupAddiction'}\n",
      "{'body': 'When someone asks for help they probably already tried it by themselves. No need to go elitist even as a joke.', 'subreddit': 'darksouls3'}\n",
      "{'body': \"DAE think we should just brand users of subs we don't like with a universal flair so we know their subjective opinions are wrong by default? 😂\", 'subreddit': 'ComedyCemetery'}\n",
      "{'body': 'gotta maximize dat karma homie', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"Any two episodes. Each episode has its own 'score'. I think the term is 'music bed', which is different from 'cue', but I may be wrong on the terms. \", 'subreddit': 'KingOfTheHill'}\n",
      "{'body': ' roughly translates to \"I love to hockey\"', 'subreddit': 'hockey'}\n",
      "{'body': '[deleted]', 'subreddit': 'ProtectAndServe'}\n",
      "{'body': 'I have a very strong feeling against fusing limited units ever.\\n\\n', 'subreddit': 'FFBraveExvius'}\n",
      "{'body': \"In that case make sure you haven't just popped a hemorrhoid. \", 'subreddit': 'AskReddit'}\n",
      "{'body': \"Definitely not mono-fire. It's just weaker and you get a lot of mileage from a second faction (either shadow or justice).\", 'subreddit': 'EternalCardGame'}\n",
      "{'body': 'When virtue signaling and dating cross streams.', 'subreddit': 'Tinder'}\n",
      "{'body': \"Hope someone figures out which Democrats have ties with this union leader! This sounds like a setup to make President Trump look bad. They couldn't find much to criticize him for with the other hurricanes, the Russia narrative is falling apart, etc and they needed something to trash him for that people would care about (obviously nobody gives a shit about the NFL).\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'The environmental cost of earplugs adds up over time, both in terms of the actual item and the packaging.', 'subreddit': 'liberalgunowners'}\n",
      "{'body': \"Oakland Raiders fans love to watch their team lose games. They're Oakland Raiders fans, after all.\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Love the podcast', 'subreddit': 'digimon'}\n",
      "{'body': 'He just abuses tackles with power and then when they start bracing themselves for it he flies around them with speed and bend.', 'subreddit': 'CFB'}\n",
      "{'body': \"Goku was way beyond Frieza.  Goku held back because he wanted a good fight.  Frieza was at 120 million and Goku was at 150 million.  Goku since then got stronger so it makes sense he'd be above Cooler.  Although I would have liked a better fight too.\", 'subreddit': 'dbz'}\n",
      "{'body': \"I'm not on the Zarvox level of Elite RMT. I only make about $7 per day. Kappa\", 'subreddit': 'Eve'}\n",
      "{'body': \"143418344| &gt; United States Anonymous (ID: dO1vilZS)\\n\\n&gt;&gt;143417804\\nWhy have you gotten more liberal? I've gotten more conservative, but still voted hillary/obama/obama\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': \"Haha, you didn't buy mine. I listed mine for 850k and it sold a little while ago :)\", 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': 'Evil within 2, Gran Turismo, forza 7, ac origins, cod ww2, need for speed, battlefront, Wolfenstein, ni no kuni, Mario, Southpark... Probably more', 'subreddit': 'gameflysocial'}\n",
      "{'body': 'I wanna see dat\\n\\nMcCaw - Swaggy P - Iguodala - Casspi - Draymond lineup', 'subreddit': 'nba'}\n",
      "{'body': '[deleted]', 'subreddit': 'WebGames'}\n",
      "{'body': 'Consider the situation President Clinton inherited.  America at peace, prosperous, and an unchallenged colossus on the world stage.  What happened?  These three guys.', 'subreddit': 'The_Donald'}\n",
      "{'body': 'You do have a point, he is a \"public\" person and in his position/experience, probably he is used to all the flame/hate and controls himself in front of the camera. Maybe he curse us on his way home.\\n\\nBut do we need this? Should we keep doing \"our part\" and flame him, so he does his part and keep this mind game going? What I need to understand is what benefits we get from this.', 'subreddit': 'paragon'}\n",
      "{'body': '**PLEASE READ THIS MESSAGE IN ITS ENTIRETY BEFORE TAKING ACTION.**\\n\\nHi there, your post has been removed for one of the following reasons: \\n\\n* [Rule 3:](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_3-) Askreddit is for open-ended discussion questions. If you\\'ve posted a question that could be answered with just yes or no, it needs to explicitly ask for more discussion like asking \"What\\'s the story\" or \"Why or why not?\"  Also, questions with a single correct answer, that can be researched elsewhere or provide a limited scope for discussion (yes/no, DAE, polls etc.) are not appropriate.  \\n\\n* [Rule 1:](http://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_1-) You must post a clear and direct question in the title. \\n\\nIf you have any queries or concerns, please feel free to [contact the mods](http://www.reddit.com/message/compose?to=%2Fr%2FAskReddit&amp;subject=Yes/No+Related+Post+Review+Request&amp;message=My+post+was+removed,+automoderator+said+Rule+1+or+3,+please review%3a%0d%0a%0d%0ahttp%3A%2F%2Fwww.reddit.com/r/AskReddit/comments/73igbd/has_you_or_anyone_you_know_ever_try_human_meat/). Thank you.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*', 'subreddit': 'AskReddit'}\n",
      "{'body': 'This gif could go in so many places. Sure her falling is expected, but him kicking the fence is /r/unexpected and also /r/IdiotsFightingThings .', 'subreddit': 'instant_regret'}\n",
      "{'body': 'The full quote just confused me even more. That did nothing to clear up what he meant. It looks like several half thoughts thrown together. ', 'subreddit': 'worldnews'}\n",
      "{'body': 'Cheers G!', 'subreddit': 'JordanPeterson'}\n",
      "{'body': 'I have literally seen this repost 5 times this week', 'subreddit': 'hmmm'}\n",
      "{'body': 'Bizarre in the variation between cinemas as well. Vue on Fulham Broadway is £6.99 for any film any day of the week. ', 'subreddit': 'london'}\n",
      "{'body': \"I'm so sorry for your loss, it's always hard :(\", 'subreddit': 'corgi'}\n",
      "{'body': \"Wait ? The government does not need 50% of the seats in Parliament ? Are you sure ? We can have that over here in Germany, but that's a big exception to happen and you need 50% to pass any simple laws.\\n\\nAlso, thanks for answering.\", 'subreddit': 'europe'}\n",
      "{'body': 'It’s good to do both, I really like writing away from picture, especially at the beginning. But I still feel like you’ll learn more as a composer by trying to write through a scene than if you wrote the music first and tried to find a home for it. \\n\\nEdit: autocorrect!', 'subreddit': 'FilmIndustryLA'}\n",
      "{'body': 'Brilliant rebuttal.', 'subreddit': 'Roadcam'}\n",
      "{'body': 'Supporting Real Madrid has that depressing effect on people.', 'subreddit': 'blunderyears'}\n",
      "{'body': 'Farsi means persian. Just like Persian can refer to the language and the people, Farsi is the same way', 'subreddit': 'exmuslim'}\n",
      "{'body': \"If you're be the one to do this, at least have something to say about the shot \", 'subreddit': 'reddeadredemption'}\n",
      "{'body': 'refund me for all the garbage DLC content i bought at full price. ', 'subreddit': 'paydaytheheist'}\n",
      "{'body': 'Colo kanjuro can do it for free spirit and striker teams. RR Fukuro can be used convert qck orbs on fighter teams (works well with gear 4). Haloween Cora can convert qck into matching for free spirit and cerebral. Also the new RR Heracles can convert right column type slots into matching for shooters and striker.\\n\\nEdit: I forgot about the best one for slashers, RR Onigumo. Makes STR/DEX/QCK count as matching for driven and slashers, easy to socket and relatively low base CD.', 'subreddit': 'OnePieceTC'}\n",
      "{'body': 'Updoot for Callahan.', 'subreddit': 'keto'}\n",
      "{'body': 'I spent every other day in a guitar store \"just looking\" last year and eventually walked out with my dream guitar and my #1, a Fender Jaguar. It\\'s everything I\\'ve ever wanted in a guitar and more.\\n\\n\\nBut now that you mention it I *could* use a new Humbucker guitar...', 'subreddit': 'Guitar'}\n",
      "{'body': \"That's a perfectly logical strategy to get the most game for your buck but lowering their day1 price so that you buy the game earlier isn't doing the studio any favors.  They're better off selling at full price initially and slowly working their way down to your pricepoint.\", 'subreddit': 'Games'}\n",
      "{'body': \"I'm a simple man, I see Mississippi State, I root for them\", 'subreddit': 'CFB'}\n",
      "{'body': 'FWIW You\\'re actually what\\'s known as a \"reverse splitter,\" not a \"splitter\" because you have a high GPA/low LSAT. Cycles are hard to predict for splitters, potentially even more so for reverse splitters. ', 'subreddit': 'lawschooladmissions'}\n",
      "{'body': 'It\\'s a couple of different things:\\n\\nI could be mistaken here, but in the traditional fighting game sense:\\n\\n1. Tech = \"Throw Tech\\'ing\" or breaking and/or escaping a throw.  You\\'ll usually hear the word \"throw\" closely associated with the word tech to seperate it from the second definition:\\n\\n2. Tech = short for \"Technology\" or a method, strategy, or technique for dealing with a particular situation.  It\\'s slang and doesn\\'t really mean anything specifically but you\\'ll hear it used in a phrase such as:\\n\\n\"Did you see that new reality stone-Spiderman tech?!  You know, the \\'tech\\' where he can keep looping the freeze into web ball over and over again into massive combo damage.\"\\n\\nFinally, I\\'m not a Smash/Melee player but I think \"tech\" means something slightly different there where it has more to do with ones ability to do certain (difficult) techniques on the pad.  Usually it follows closely with the second definition above, and contextually it usually make sense, but at times it could mean something slightly different.\\n\\nTl;dr -- it\\'s generally the second definition unless the word \"throw\" is used closely to the word \"Tech\"', 'subreddit': 'mvci'}\n",
      "{'body': \"Obama was stuck between a rock and a hard place. Prisoners were there no clue if Iran would have even considered something else, and there's no clue if Iran actually helped profit the terrorist with money, plus that dug us in more debt \", 'subreddit': 'AskThe_Donald'}\n",
      "{'body': 'yeah even in the title he calls it the holidays lol, should be \"Happy Holidays\"', 'subreddit': 'ClashRoyale'}\n",
      "{'body': 'Rand Paul/Ben Carson 2024\\n\\n\"The Doctors will see you now!\"', 'subreddit': 'The_Donald'}\n",
      "{'body': 'r/keming ', 'subreddit': 'CrappyDesign'}\n",
      "{'body': 'I feel physically bad for that woman.', 'subreddit': 'PublicFreakout'}\n",
      "{'body': \"She was used to get a horrible, unfounded, pizzagate related shitpost to the front page (with all the shills that come with it,) to discredit the theory. \\n\\nIt was staged by the anti's. \\n\\nEdit: TMoR alert. Fuck off TMoR. \", 'subreddit': 'conspiracy'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': ';) ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Too many empty seats for a night game against a big opponent.', 'subreddit': 'CFB'}\n",
      "{'body': \"Can anyone recommend suitable alternatives to Google (Gmail, Google search engine, Chrome), Microsoft, Apple etc for someone that isn't a computer whiz?\\n\\n I've used Ubuntu in the past and I'm considering going back to that. One problem I had with it was sometimes not being able to get some programs to run on it. \", 'subreddit': 'Communalists'}\n",
      "{'body': 'Plus, Chancellor of the free world sounds better anyway.', 'subreddit': 'worldnews'}\n",
      "{'body': \"I'll make you feel better and tell you NE was my first guess, with IA the second.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"I don't want to risk turning into a pillar of salt \", 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'They can get on the public speaking tour and get paid by some non-profit so they can speak about how they persevered through self generated adversity.', 'subreddit': 'Conservative'}\n",
      "{'body': 'banana for tripping\\n\\nmost fruits really\\n\\nrolling? While peaking im not gonna eat most things, some oranges or some sort of juicy citrus are nice, cherries are bomb\\n\\non the come down i like to munch on chips after i take a little xan', 'subreddit': 'Drugs'}\n",
      "{'body': 'Oh my gosh, a megablep!', 'subreddit': 'Blep'}\n",
      "{'body': 'Anyone remember the game with LSU and Oregon State in the rain in 2004? Where Oregon State missed 3 extra points that day and ended up losing by one in OT?\\n\\n\\nMakes me LOL.', 'subreddit': 'CFB'}\n",
      "{'body': \"And what if you just summon the NPC?  I beat them solo my first playthrough, but on all subsequent playthroughs I summon Gael because I'm lazy and it makes the boss 10x easier.\", 'subreddit': 'darksouls3'}\n",
      "{'body': \"There's other ways of dealing with the pain.  The surgeries and the medications opiate based that follow will put you in serious risk of restarting the cycle again.  This is what happened to me after years clean so be careful.\", 'subreddit': 'OpiatesRecovery'}\n",
      "{'body': 'good point', 'subreddit': 'Christianity'}\n",
      "{'body': 'November 3 – 5 at the Portland Expo Center', 'subreddit': 'Portland'}\n",
      "{'body': \"You did say end-game. Old man can't read.\\n\\nRight. So. Bad mage with subjugated mages. He's gotta have some control over them beyond his threats, yes?\", 'subreddit': 'DMAcademy'}\n",
      "{'body': 'New York is brought up because it is the densest city in North America, and as an example that people are not “giving up privacy” for density. Reread the whole comment thread.', 'subreddit': 'vancouver'}\n",
      "{'body': 'You are absolutely right as at least where I live, etizolam is not a controlled substance and is not tested for. So they would not have a etizolam solution to run through the gc ms for reference versus another sample', 'subreddit': 'Etizolam'}\n",
      "{'body': '[deleted]', 'subreddit': 'keto'}\n",
      "{'body': 'So you are the ignorant jackass I thought you were?', 'subreddit': 'socialism'}\n",
      "{'body': 'They actually fixed that in the [sequel](https://www.youtube.com/watch?v=kL1QUmeEZQc)', 'subreddit': 'youtubehaiku'}\n",
      "{'body': 'On', 'subreddit': 'EarthPorn'}\n",
      "{'body': '[deleted]', 'subreddit': 'The_Donald'}\n",
      "{'body': 'show us samples', 'subreddit': 'wallstreetbets'}\n",
      "{'body': 'disagree, Soraka much more anti-fun.', 'subreddit': 'heroesofthestorm'}\n",
      "{'body': '143413934| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413660\\nYou are a traitor to the Democratic Party and a sad human being for wanting to watch the world burn. SJW is not bullshit.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': '[deleted]', 'subreddit': 'DrugStashes'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Nova launcher has so many gestures. It's so good.\", 'subreddit': 'GalaxyS8'}\n",
      "{'body': 'I detect sarcasm.', 'subreddit': 'detroitlions'}\n",
      "{'body': \"Thank you . Idk why more people don't know this.\", 'subreddit': 'runescape'}\n",
      "{'body': \"I would have figured that since it's a govt office they wouldn't care whether anyone showed up or not... In fact I would have expected them to be happy nobody was there.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"That's fucking ridiculous.\\n\\nSomeone said something like that during the Sens-Rags series and I couldn't stop laughing.\", 'subreddit': 'hockey'}\n",
      "{'body': \"You just wait. Whether she is or is not pregnant will reveal itself with time. Your friends are right, it's incredibly likely she's not pregnant, so try not to sweat it. It's not a bell you can unring anyway so no use freaking out over it. What's done is done.\\n\\nIf in 8 months or so she has a kid, you get a paternity test and go from there. Obviously if it's your kid you won't have a choice but to pay child support, etc. \\n\\nI recommend you go no contact until then. \", 'subreddit': 'relationships'}\n",
      "{'body': 'I said surrender cobra, not game over brother', 'subreddit': 'CFB'}\n",
      "{'body': 'where can I find this information then? nothing in the sub has the answer', 'subreddit': 'nintendo'}\n",
      "{'body': 'Isoprop all the way', 'subreddit': 'oddlysatisfying'}\n",
      "{'body': 'うんこ', 'subreddit': 'newsokur'}\n",
      "{'body': \"I didn't say he had the keys to all information, though I do think the Russians are feeding Wikileaks a good bit, but I do love the mechanical way he goes through people in a debate, the guy's a machine.\", 'subreddit': 'KotakuInAction'}\n",
      "{'body': \"No very different than the ceramic donuts that yocan makes. This is not a donut and somewhat raised , I have not used it yet , I been using the dual Quartz so far but I'll try it and write back . \", 'subreddit': 'Waxpen'}\n",
      "{'body': 'I\\'ve had nothing but problems with UCCI since the switch. They canceled my policy for nonpayment a week after cashing the premium check that we sent them. \\n\\nThey keep mailing me \"past due\" bills even though I have electronic payments set up and they\\'re being paid. But, the worst is that every time I call them their customer service reps are clueless. They take days to respond to anything that can\\'t be immediately solved over the phone. Same thing if you ask for a supervisor - 24-48 hour call back. \\n\\nI\\'m ready to give up and pay for private dental. ', 'subreddit': 'AirForce'}\n",
      "{'body': \"OK. this proves it whoever did this is a moron and can't read intel sites showing the real relations between these entities. Nice propaganda tho. Keep trying while going down\", 'subreddit': 'Eve'}\n",
      "{'body': 'Those are pretty good. I like his style.', 'subreddit': 'pics'}\n",
      "{'body': \"From Portland timberline and skibowl are the closest with meadows being the furthest, but not by too much i feel. \\n\\nAs for traffic, I usually go up on the weekends, because of my schedule, but theres not a lot of traffic when they're opening(except for some holidays) but leaving is always the worst. Mostly because of the traffic converging from both timberline and skibowl, all trying to get back to Portland. Some days can be a breeze and others horrendous but with government camp being decently close stopping for a quick bite to let some time pass is always a solid choice. \", 'subreddit': 'Portland'}\n",
      "{'body': 'Like the rangers game, this game was a little heated too. Bats also fucked up and admitted to it.', 'subreddit': 'sports'}\n",
      "{'body': \"Oh wow, didn't know. Uninstalling, its a shame though, its pretty cool :/\", 'subreddit': 'roblox'}\n",
      "{'body': \"Me and my buds were cracking up for a whole match of Cod BO2 when we were getting destroyed by a guy named 'Tactical Grandpa'. I'll never forget the name. \", 'subreddit': 'xboxone'}\n",
      "{'body': '0-3 3OT win...?', 'subreddit': 'CFB'}\n",
      "{'body': 'Hi there! Your post was removed because it uses the text box. Per [rule 1](/r/AskReddit/wiki/index#wiki_-rule_1-), use of the text box is prohibited. You can resubmit your post [here](/r/askreddit/submit?selftext=true&amp;title=HELP! Need help finding a drop ship t-shirt manufacturer. REDDIT BE MY SAVIOR!) without the textbox.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'science'}\n",
      "{'body': \"I mean Shangela's original elimination was distinctly racist, and separate from the ground with her whole power lesbian aesthetic.\", 'subreddit': 'SubredditSimulator'}\n",
      "{'body': 'two randoms please', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"Now you're asking the right questions. I dunno I reckon something like disaffection, alienation, neglect and multitude of other factors. What do you reckon?\", 'subreddit': 'news'}\n",
      "{'body': \"How exactly are these CL scams supposed to work where someone is selling a car that doesn't exist?  Are they thinking that an unsuspecting buyer is going to send them money before actually seeing the car/having the title in hand?  Who the hell would pay for a used Accord without going to look at at first?\", 'subreddit': 'cars'}\n",
      "{'body': 'wew.', 'subreddit': 'Eve'}\n",
      "{'body': \"He's just awful.\", 'subreddit': 'politics'}\n",
      "{'body': 'how did you take such gorgeous photo with the kit lens? all i get when using the kit lens is wash out and non sharp images. please teach me master ', 'subreddit': 'a6000'}\n",
      "{'body': 'Nooooooo', 'subreddit': '90DayFiance'}\n",
      "{'body': 'This sounds really cool! Hopefully this idea can be used at some point.', 'subreddit': 'fivenightsatfreddys'}\n",
      "{'body': \"one would assume they would conduct an investigation to determine if criminal activity occurred.  Were they to determine it had, a criminal case might ensue (the process would involve handing it over to the prosecutors office to make a determination).  If they determined it was civil, that would be the end of it.  Law enforcement doesn't pursue civil matters (outside of some minor exceptions like contempt or certain child support issues.\", 'subreddit': 'legaladvice'}\n",
      "{'body': \"Yeah, another post or two and you'll be challenging me to meet you behind the monkey bars \", 'subreddit': 'TrumpCriticizesTrump'}\n",
      "{'body': 'I unfortunately agree', 'subreddit': 'baseball'}\n",
      "{'body': \"Breanne at Stone Salon in Hoover is awesome! She's great at being chatty if you want, or cutting in silence if you'd rather not talk. \\n\\nEdited to add: she has a Facebook page if you wanna check out her work!\", 'subreddit': 'Birmingham'}\n",
      "{'body': \"Brad's dope pedagogy is so much better than Lonzo Ball's 6-0 scrimmage record. Fuck the Lakers.\", 'subreddit': 'bostonceltics'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'oh man', 'subreddit': 'premed'}\n",
      "{'body': '[deleted]', 'subreddit': 'hivaids'}\n",
      "{'body': 'Basically most RHONJ need to step up their tag line games.\\n\\nI do like Teresa\\'s this up coming season, but only because it\\'s a little more creative/ different compared to the others.\\n\\n\"The only life I envy is my own\" ... I\\'m sorry idk how to fix it... ', 'subreddit': 'BravoRealHousewives'}\n",
      "{'body': 'watch out for it! :) blue or red?', 'subreddit': 'phgonewild'}\n",
      "{'body': '\"If I told you the ache I had you\\'d ask why I had a cat with me.\"', 'subreddit': 'MyLittleHouseOfFun'}\n",
      "{'body': 'r/pareidolia ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"Being Human is a werewolf, a vampire and a ghost as roomates(it sorta has a similar situation to shameless where the UK version is better but the US version exists. And yes Phoebe Tonkins is on The Secret Circle.\\n\\nI really do think you should try Lucifer(its really funny) but I can't force you.\", 'subreddit': 'TeenWolf'}\n",
      "{'body': 'Lets say all these are like $200 a month I think.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"A quick look at their career stats will show that Brees is better in almost every category, by alot. idk where you got that they were similar.\\n\\nI'm not downvoting you btw.\", 'subreddit': 'nfl'}\n",
      "{'body': \"Basically, if you have an expression pedal and a pitchfork, you can create this cool detuned chorus sound by tapping the expression pedal verrrrrry slightly from when it's at dry signal - pretty neat when you don't have a chorus, even if it is a bit fiddly!\\nNow, two apologies: sorry the footage angles toward the pitchfork rather than the expression pedal when I'm tapping it, my brother is a goobus, and also, apologies for the bad guitar playing, its my brothers guitar and I'm normally a keys guy lol\", 'subreddit': 'guitarpedals'}\n",
      "{'body': \"2 hours eh? I'll try that instead next time \", 'subreddit': 'PoppyTea'}\n",
      "{'body': 'Heatwaffle 280 warlock', 'subreddit': 'Fireteams'}\n",
      "{'body': \"lmao. Only reddit is making it about race. Not like there aren't white people in Puerto rico. Its a red, conservative voting state. Get outta here with this weak shit bruh\", 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'Who among the fringe QBs do you folks like for next week? Just need a streamer for Brees during his bye next week. I would go ahead and roster the guy now.', 'subreddit': 'fantasyfootball'}\n",
      "{'body': '#SELLER', 'subreddit': 'DirtySnapChatPals'}\n",
      "{'body': 'I would love to see more!', 'subreddit': 'gonewild'}\n",
      "{'body': \"Nah, you're cool man. Play what works :) it's not been too oppressive since the nerfs anyway.\\n\\nDo you play any other games?\", 'subreddit': 'CasualConversation'}\n",
      "{'body': \"THIS IS SO WORTH IT. I felt really guilty hiring someone to clean my house like what, I'm too lazy to do it myself? WELL I AM. It is always the last thing on the list meaning it never gets done. For both of us! Would I rather we spent one of our precious days off together cleaning? Or banging?\\nThe woman and associates who clean my place make it look so good, that when they finished the first time I cried. Not a full on weep but it was so simple to pay a professional to do something I just hate. This is not worth the heartache to me. Dear OP, consider it.\", 'subreddit': 'TrollXChromosomes'}\n",
      "{'body': 'At least 1 ', 'subreddit': 'hockey'}\n",
      "{'body': \"&gt;Joe Arpaio\\n\\nLol I mean if you want to throw around falsified information as though it were factual, I wouldn't start out by bringing up Arpaio if you want ANYBODY to take you seriously\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'jameis pls', 'subreddit': 'nfl'}\n",
      "{'body': 'did u just call me a holocaust denier', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'ye fam', 'subreddit': 'IAmA'}\n",
      "{'body': 'No that would be mean', 'subreddit': 'osugame'}\n",
      "{'body': \"What i can't believe no one said Te Kā from Muana. Easily the most understandable and relatable villain out there.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Worth clicking though', 'subreddit': 'nfl'}\n",
      "{'body': \"There are a few instances in the show where the world does find out about their magic and it causes chaos, the world can't handle the truth about the magical world which is why it's kept a secret.... for the greater good\", 'subreddit': 'charmed'}\n",
      "{'body': 'Did he have to gain weight to look like that?', 'subreddit': 'movies'}\n",
      "{'body': 'Yeah I got some 4 cent skins lol', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'CATCH IT, BANANA! CATCH IT!', 'subreddit': 'CatTaps'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': '143413728| &gt; None Anonymous (ID: ycvOUfOn)\\n\\n&gt;&gt;143412250 (OP)\\nTrump. All other votes were unimportant as if waking from a nightmare.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"I don't want to hijack, and don't want to post a new thread, but on the note of 'making bloat easier', I'm working on Aprils Fool and it turns out The Wiz is actually super useful. Keeps you at a good angle to avoid most of his crap. Spectral tears so you can stay behind the rocks. Not a run winner, but for an item I laughed off at first, I'm finding more and more reasons to pick it up. \", 'subreddit': 'bindingofisaac'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Of course I would, why would you think I wouldn't.\", 'subreddit': 'Libertarian'}\n",
      "{'body': \"I wear hijab but I know the struggle. It was very difficult for me to take the step. \\n\\nIt wasn't that I thought hijab is oppressive and backwards. \\nIt wasn't that I denied that it's an obligation in Islam. \\nIt wasn't that I wanted to dress in a way that would arouse men. \\nIt wasn't that I didn't pray and had practically no personal relationship with Allah. \\nIt wasn't that I had big sins I wasn't ready to let go of yet and felt it would be hypocritical to wear hijab. \\nIt wasn't that I had no knowledge of Islam. \\nIt was simply that I was afraid...\\n\\nAfraid of people's reactions, afraid of being treated differently (I know how they think about Islam and Muslims), afraid of having less chances etc. I *hated* myself for apparently fearing people more than Allah, but it was just extremely difficult for me. I can't really explain how hard it was. It was a form of social anxiety/phobia and it took me years to finally be able to take the step. The tears I dropped in those years could probably fill a pool. The struggle was real, that's all I'm saying. But people don't see that. They just see your public sin and judge you based on that.\\n\\nThis was *my* personal struggle with hijab, but other girls can have different issues with hijab. Or they do wear hijab but have different struggles. Everybody sins, but they sin differently. I know a lot of people struggle with praying on time, porn addiction, masturbation, boyfriends/girlfriends, gambling, drinking, whatever. Some feel guilty about it and others don't. The only consistent thing is that everybody has their own struggles and we don't know who's more beloved by Allah.\", 'subreddit': 'Hijabis'}\n",
      "{'body': \"I mean why is your name in the history books? Gutenberg who? The printing press guy? Nobody would give a shit nowadays if your last name was Gutenberg. It's not even famous really. Isn't everybody's name in the history books if you look long enough? It's a meaningless statement to say your name is famous without more information.\\n\\nI also highly doubt that if your family is actually famous that it doesn't help you in life.\", 'subreddit': 'DebateAnarchism'}\n",
      "{'body': '\"Problem solving skills\" are improved when you waistband your dick in school so the girls can\\'t see your wee wee', 'subreddit': 'milliondollarextreme'}\n",
      "{'body': '[deleted]', 'subreddit': 'selfie'}\n",
      "{'body': 'https://imgur.com/a/DbbfT', 'subreddit': 'CFB'}\n",
      "{'body': \"I would go for Sorcery because of The Ultimate Hat most of the time, it sounds really cool to be in Huge Doggo form more often, 33s downtime on Ult sounds insane.\\n\\nIf the enemy is a CC Heavy Comp I would opt for Precision, with the 'Super Dangerous Game' rune and the Tenacity one, sounds good for surviving teamfights and waves of CC.\", 'subreddit': 'nasusmains'}\n",
      "{'body': '[removed]', 'subreddit': 'OldSchoolCool'}\n",
      "{'body': 'He does have pig legs. ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'Welcome to r\\\\/RandomActsOfBlowJob! \\n\\n[Search for others in LosAngeles!](https://reddit.com/r/RandomActsOfBlowJob/search?q=title%3ALosAngeles+%28+subreddit%3ARandomActsOfBlowjob+OR+subreddit%3ARandomActsOfMuffDive+%29&amp;sort=new&amp;t=all) ***New!!!*** Stay up to date with an [RSS feed!](https://reddit.com/r/RandomActsOfBlowJob/search.rss?q=title%3ALosAngeles+%28+subreddit%3ARandomActsOfBlowjob+OR+subreddit%3ARandomActsOfMuffDive+%29&amp;sort=new&amp;t=all)\\n\\n[sidebar](https://www.reddit.com/r/RandomActsOfBlowJob/about/sidebar) - [rules](https://www.reddit.com/r/RandomActsOfBlowJob/about/rules) - [message mods](https://www.reddit.com/message/compose?to=%2Fr%2FRandomActsOfBlowJob)\\n\\n---\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/RandomActsOfBlowJob) if you have any questions or concerns.*', 'subreddit': 'RandomActsOfBlowJob'}\n",
      "{'body': 'Seconded!', 'subreddit': 'mmgirls'}\n",
      "{'body': \"If you insert a print statement that shows low, high, and mid each time you recalculate them, I think you'll see some strange values.\\n\\nConsider this code:\\n\\n            low = values[mid] + 1;\\n\\nIf low=0 and high=4 (because there are 5 items in the array), this will set low equal to 1 + whatever **value** is in the middle of the array.  It could be 655, if the input was {0, 11, 655, 765, 888},.  You don't want low=656, you want low=3.\\n\\nSee the difference?\\n\\nThat mistake is in both your recalculation of low and of high. \\n\\nAnd then: once you reset either low or high, you can recalc mid based on these new values.  mid=(low+high)/2 no matter what, right?\\n\", 'subreddit': 'cs50'}\n",
      "{'body': \"I think it's been called a trace rifle.\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"This is some really dumb statistics.\\n\\nLet's not ignore Ossoff's race and all the other races we lost earlier in the year.\", 'subreddit': 'politics'}\n",
      "{'body': 'gg.  gg.', 'subreddit': 'photoshopbattles'}\n",
      "{'body': \"&gt; 'Leonardo da Vinci may have drawn nude Mona Lisa'\\n\\nThere really are no exceptions to Rule 34.\", 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'DotA2'}\n",
      "{'body': 'Crimson Hexphase? ', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"I would love something like this! It's a shame that more of the competitive scene isn't documented in video form.\", 'subreddit': 'yugioh'}\n",
      "{'body': \"Yeah... it'll amplify any non true damage, damage.  \\n\\nTrinity will be amplified. \", 'subreddit': 'FioraMains'}\n",
      "{'body': 'Secret volcano lairrrrrrrrrrrrr', 'subreddit': 'evilbuildings'}\n",
      "{'body': \"What's that\", 'subreddit': 'Rainbow6'}\n",
      "{'body': '[MovieposterFans](https://www.reddit.com/r/MovieposterFans/comments/73iga0/the_graduate_1967_1000_1500/) | [Link To Original Submission](http://reddit.com/73ifoe)', 'subreddit': 'NoSillySuffix'}\n",
      "{'body': 'One random please\\n', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"Sup y'all \", 'subreddit': 'CFBOffTopic'}\n",
      "{'body': '*a hush falls over the crowd*', 'subreddit': 'AskReddit'}\n",
      "{'body': '&gt;Time will tell with this one.\\n\\nI feel like they do it when it thematically makes sense. Zendikar was about discovery, Journey was about going to the realm of the gods, and Ixalan is about exploration, so it makes sense.', 'subreddit': 'magicTCG'}\n",
      "{'body': 'Hence **prediction**', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'what is the point of this', 'subreddit': 'Incels'}\n",
      "{'body': '\"So it\\'s safe to assume the short timeline of events that occured in the interval before the rewind would cease to exist?\"', 'subreddit': 'MyLittleHouseOfFun'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Right now you should catch up with the 7 deadly dragons. Get Key of \"insert dragon name here\" from each dragon and finish all story quest in /battleundere. Then do the first quest from a glowing LED dragon and pick up the drop. They would later release items for those who possess the 7 keys and the quest drop. \\n\\nIf you\\'re looking to farm xp and gold, /battleground is the best place. \\n\\nFor weapons, get the doomblade of destruction. It\\'s worth getting it considering the dmg and xp, rep, and gold boost it gives you. Probably the only weapon in game with these boosts combined in one weapon.\\n\\nIf you haven\\'t been on for 4-5 years, then I assume you missed finishing 13 lord of chaos. It\\'s an interesting storyline so I would recommend completing it.\\n\\nOther than that, aqw is packed with adventures and new items, so keep in track with the news, and patch notes. Also use your map to keep track of the storyline. It would help big time. Hope this helps.\\n', 'subreddit': 'AQW'}\n",
      "{'body': 'wow amazing here :www.gooodooo.me', 'subreddit': 'sneakerreps'}\n",
      "{'body': '2 random spots please!', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"My partner and I have the exact same situation! Except our rent is $1100. We'd love to move into a one bedroom but it's too expensive.\", 'subreddit': 'PersonalFinanceCanada'}\n",
      "{'body': \"He's a dataminer, he likely just datamined them out of the .dat file\", 'subreddit': 'Guildwars2'}\n",
      "{'body': '[deleted]', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'I may be rooting for Clemson tonight but you gobblers have an awesome intro.\\n\\n[Hit it!](https://www.youtube.com/watch?v=YxRxd8aNd6I)', 'subreddit': 'CFB'}\n",
      "{'body': \"Higher highs and lower lows. Although to be honest he hasn't really put a foot wrong in like 2 years. \", 'subreddit': 'Gunners'}\n",
      "{'body': \"That's cool, but most of your fellow veterans find the protest disrespectful.\", 'subreddit': 'news'}\n",
      "{'body': 'one random please\\n', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Oh shit', 'subreddit': 'me_irl'}\n",
      "{'body': 'Seriously. They were so set on penalizing FSU they even said a pass interference was against the offense that resulted in a 1st down... When FSU was on offense.... ', 'subreddit': 'CFB'}\n",
      "{'body': \"It hurts me that the only two Cordelia's I've pulled are -spd. She's still really good, but I just can't bring myself to invest in a firesweep or quad build when she has such a terrible IV.\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'I can confirm that one.  Just down Lake St. from my office.  Good Italian Beef sandwich as well.', 'subreddit': 'TheNewGeezers'}\n",
      "{'body': 'if you go back to when Rockstar released the screenshots, one of them was a gang. And the entire gang was ginger. He could have something to do with that. Maybe an Irish gang? Here is what I am referring to \\n\\nhttps://media.rockstargames.com/rockstargames-newsite/uploads/d0d67b9e853c2cebe6eb01a89cb524274fc949bc.jpg\\n\\n\\nEDIT: I also just realized that Arthur is the guy who is getting beat up by the big ginger...interesting', 'subreddit': 'reddeadredemption'}\n",
      "{'body': 'They are indeed! Tried them on a whim a few months ago, been hooked ever since. ', 'subreddit': 'Drumming'}\n",
      "{'body': 'r/unexpectedfuturama', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'I already spent all my stones on the banner, and finished the medal grinds expecting to get lr gohan 😢', 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': \"Sounds like a great idea. The MCU hasn't been compelling since Avengers 1.\", 'subreddit': 'imdbvg'}\n",
      "{'body': 'Also looks really close to Heroes and Generals, which is free2play on steam. https://www.youtube.com/watch?v=NqTynawPjug', 'subreddit': 'pcgaming'}\n",
      "{'body': '[removed]', 'subreddit': 'Animewallpaper'}\n",
      "{'body': '[removed]', 'subreddit': 'NASCAR'}\n",
      "{'body': 'If and only if they already have CTE which is degenerative. ', 'subreddit': 'news'}\n",
      "{'body': \"You may know me daughter Pearl. She's growing up fast. It seems like it was just yesterday I was teaching her how to breach. Me mammalian angel. OH- Anyway, uh, so she's going to be working here during her summer vacation. She's got a lot of fresh ideas to bring in some hungry customers!\", 'subreddit': 'spongebob'}\n",
      "{'body': 'You can multiply by (1 + csc(x))/(1 + csc(x)) and use the fact that tan^(2)(x) + 1 = csc^(2)(x).', 'subreddit': 'HomeworkHelp'}\n",
      "{'body': 'R I P', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'one sounds like the japanese version of the other, but he is not. ', 'subreddit': 'funny'}\n",
      "{'body': \"Don't be supportive at all of her and their relationship. She needs to see for herself how toxic it is and she can't lean on you for support anymore because she won't listen to your advice. Tell her you will have nothing to do with him and you don't want it to be like this, but he is affecting you as well and you're under no obligation to be friendly with him. Don't interact with him at all.\\n\\nThe worst thing you could do is be supportive of her relationship with him just to make her happy or keep the pace. She will keep getting hurt by him but she'll have you as a safety net. She needs to realize how bad it is without a safety net and then maybe she'll finally leave him.\", 'subreddit': 'relationships'}\n",
      "{'body': 'What is your scenario then?', 'subreddit': 'europe'}\n",
      "{'body': \"143413837| &gt; France Anonymous (ID: G0Z4ZOPh)\\n\\n&gt;&gt;143412250 (OP)\\nWhy didn't you vote for her in 2008?\\nEven though she won the popular vote back then.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': 'Å være is to be and et vær/været is a weather/the weather. without the e is a command so it’s “be so kind” or “be so good”. They are both idioms so they can’t really be translated directly ', 'subreddit': 'norsk'}\n",
      "{'body': 'FYI, the Caddy Limo is available in the first seeker league in a showcase event, I got mine yesterday.', 'subreddit': 'forza'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'gonewild'}\n",
      "{'body': 'Chips and Salsa(concept):\\nTabantha Wheat + Rock Salt + Any vegetable, flower, or herb. Salty wheat chips with a refreshing vegetable salsa.(Effect changes with ingredients)', 'subreddit': 'Breath_of_the_Wild'}\n",
      "{'body': '\"STAB THE SHARK! SLICE \\'EM \\'TWEEN THE EYES!!\"', 'subreddit': 'videos'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Oh, go beak some DS9 DVD's.\", 'subreddit': 'startrek'}\n",
      "{'body': \"Economic Prosperity is how the black community will achieve 'equal treatment'. Stop joining Gangs and Start sending in Applications. Until the community stands up it won't ever end. \", 'subreddit': 'news'}\n",
      "{'body': 'My VT coworker says GOBBLE everytime he passed my desk this week. Please Clemson. Help me.', 'subreddit': 'CFB'}\n",
      "{'body': 'That time when the Voyager cast was on *Jeopardy* in character. \\n\\nhttps://www.youtube.com/watch?v=7oncAGFBHzY', 'subreddit': 'startrek'}\n",
      "{'body': 'Go Clemson!', 'subreddit': 'CFB'}\n",
      "{'body': 'Yep. Espn streams have sucked lately on many levels ', 'subreddit': 'CFB'}\n",
      "{'body': \"That would be great if they're still available!\", 'subreddit': 'asianbeautyexchange'}\n",
      "{'body': 'That’s a fair way to go I would however not get the twin lascanon turret, the predator auto cannon is 2 d3 S7 at -1, but it does 3 damage flat.\\nTo me that’s superior then the twin lascanon.', 'subreddit': 'WarhammerCompetitive'}\n",
      "{'body': '[deleted]', 'subreddit': 'legaladvice'}\n",
      "{'body': 'Nty dude. Just keys. ', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'SuperDisk LS-120 baby! No one needs transfer speeds above parallel!', 'subreddit': 'blackmagicfuckery'}\n",
      "{'body': 'Except for fixing the secondary suites situation, at least.', 'subreddit': 'Calgary'}\n",
      "{'body': 'iamflapjak only a bachelors sorry', 'subreddit': 'Fireteams'}\n",
      "{'body': \"hmm, I was 12 when I first got started in /r/Ravenclaw :D\\n\\nNow I'm a tad older tho\\n\\nbtw oomps is definitely catfishing us\", 'subreddit': 'hogwartswerewolvesA'}\n",
      "{'body': 'It might be the blanket. My old cat would only ever sit on my lap if I was wearing a skirt or had a blanket over my lap. Made a warm hammock. ', 'subreddit': 'aww'}\n",
      "{'body': \"OMg I didn't even notice that it was you lol don't worry\", 'subreddit': 'whatstheword'}\n",
      "{'body': '\"Whale shaped\"', 'subreddit': 'ChoosingBeggars'}\n",
      "{'body': \"For the uninitiated, the other 4 elements are:\\n\\n- Emceeing\\n\\n- B-boying\\n\\n- DJ'ing\\n\\n- Graffiti\", 'subreddit': 'nba'}\n",
      "{'body': \"Perhaps a slight exaggeration. I put about 0.2-0.3 in my evo and have the temps around 2 and take 30sec+ draws which nearly clear the elb in one hit- 2 at most. So about half a g gets me more fucked then a J that big. \\n\\nI was always a billy smoker so J's/blunts/pipes never did much for me.\", 'subreddit': 'vaporents'}\n",
      "{'body': 'You can play ranked with your bud.', 'subreddit': 'GundamExVs'}\n",
      "{'body': 'But is it more addictive than TVTropes?', 'subreddit': 'WritingPrompts'}\n",
      "{'body': 'So so painful to watch.', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Oh I see.\\nThanks again :)', 'subreddit': 'ACTrade'}\n",
      "{'body': 'Weird how the title is not in the article. ', 'subreddit': 'Conservative'}\n",
      "{'body': 'One or both of my cats 100% would have pooped in that dirt ', 'subreddit': 'CatsAreAssholes'}\n",
      "{'body': \"He's got a sweet rack, then.\", 'subreddit': 'SwordOrSheath'}\n",
      "{'body': \"God I'm having flashbacks of middle school \", 'subreddit': 'AskReddit'}\n",
      "{'body': \"&gt; You are a moronic piece of garbage. \\n\\nyou're right.\", 'subreddit': 'microgrowery'}\n",
      "{'body': '[deleted]', 'subreddit': 'xboxone'}\n",
      "{'body': 'I\\'ll take good and boring over pathetic and more boring any day\\n\\nBut also our good years were when Les earned the \"Mad Hatter\" nickname.  We didn\\'t really get boring until \\'14. ', 'subreddit': 'CFB'}\n",
      "{'body': 'I always need strings.  I walk out with strings. ', 'subreddit': 'Guitar'}\n",
      "{'body': 'Well, thats pretty much what i did and i ended up dropping a whole semester and take 2 courses the following semester thinking things would get better but crazy stuff are back already and i cant afford any more time. Thing is family is extremely important to me so stuck between a rock and a hard place.', 'subreddit': 'UofT'}\n",
      "{'body': 'K/D matters in every situation, including public events.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"My counter argument to that is that we've faced two really bad offenses, specifically two really bad running teams, the cardinals and the browns who are bottom 10 in rushing. Heck, even the rams are 20th in rushing yards per game. If we face a high powered offense like the patriots or falcons, we will get exposed. Big time.\", 'subreddit': 'Colts'}\n",
      "{'body': 'is that good or bad?', 'subreddit': 'politics'}\n",
      "{'body': \"The upside is that if she ever decides to quit, she will still have her children in her life. You won't abandon her, which is more than many can say. She's a lucky woman, but doesn't even know it. I hope she'll come to her senses after not too much time has passed. \", 'subreddit': 'exjw'}\n",
      "{'body': 'turrible', 'subreddit': 'SWGalaxyOfHeroes'}\n",
      "{'body': ':(', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Is it gonorhea? That shits curable with one dose. Source: A busy had it, I got tested, results came back negative but they dosed me just to be sure. Did your test come back positive? Sure you haven't had sex since? Sounds like you need another round.\\n\\nEdit: This is purely based on experience. Waiting for a Dr to come by...   \", 'subreddit': 'medical'}\n",
      "{'body': 'And Niki Ashton is literally the definition of identity Politics.  Shes worse than Hillary Clinton in terms of identity Politics.  She is the most extreme SJW I have ever seen to run for office, anywhere.\\n\\nJust a recent example: \"Believe survivors\" whats that mean?  That means assume guilt on the accused, thats nonsense.  That\\'s completely irrational and disgusting.  Assume guilt on the accused even if they\\'re innocent, right?  Who cares if we ruin their lives. ???', 'subreddit': 'ndp'}\n",
      "{'body': 'I love my country everyday. I don\\'t need to be told when and where I should \"prove it\" \\n\\nEdit:\\nThis dude changed his comment completely.\\n\\nIt originally was \"why are you so afraid to love your country\" ', 'subreddit': 'news'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Any source of light causes them to illuminate a rainbow-like aura around them', 'subreddit': 'starbound'}\n",
      "{'body': 'good on you for buying the star wars prequels of type r civics', 'subreddit': 'RoastMyCar'}\n",
      "{'body': 'Hajimete no Gal has been the most enjoyable trash pile Ive seen in awhile', 'subreddit': 'TwoBestFriendsPlay'}\n",
      "{'body': 'Q1 take: You cant have facial hair unless you look like a man without the facial hair. If you use it to appear manlier you are just a liar. Shave your goddamn face ', 'subreddit': 'neoliberal'}\n",
      "{'body': 'Sorry.   Thought that was in the title.   Albuquerque/rio rancho.  Posted by phone. ', 'subreddit': 'videos'}\n",
      "{'body': \"Hey everyone! Since there are a lot of people talking about my window size, instead of replying individually to everyone I'll just say it here - it's just personal preference. I don't really enjoy playing with a big screen, and I feel that I don't perform as well when doing so. I kinda like being able to see my surroundings without really having to move my eyes too much (or at all), and am very used to the SQM size of this particular size. I also enjoy having a bigger chat, but that's just a nice bonus, I mainly do it for the screen's size. Too used to it by now, I guess! \\n\\nI apologize if it made the video uncomfortable to watch for some of you. Fullscreen should make it much better, but if you're on a phone, it really would suck, so, sorry bout that! \\n\\nI usually do my videos [like this](https://www.youtube.com/watch?v=Dcf5x3wpV3Q), the only reason I've been uploading my hunting videos with the entire screen is so you guys can see the Hunt Analyzer, Preys, stuff like that. But yeah, note taken, on future videos I'll work on a way to make it less painful to watch! :)\", 'subreddit': 'TibiaMMO'}\n",
      "{'body': 'I edited my comment with the salaries. Butch makes 4.1. Idk man. That would be a fucking home run hire for Tennessee. ', 'subreddit': 'LonghornNation'}\n",
      "{'body': 'Np thx', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'There\\'s a bit of a difference between \"didn\\'t happen\" and \"happened, but at a different event\"', 'subreddit': 'news'}\n",
      "{'body': 'One of my favorite alt movie posters. ', 'subreddit': 'MoviePosterPorn'}\n",
      "{'body': 'You can try carpooling, perhaps.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I just blocked him today. I don't follow him, but too many people that I do follow retweet his shit.\", 'subreddit': 'The_Donald'}\n",
      "{'body': '*One time at band camp...*', 'subreddit': 'nottheonion'}\n",
      "{'body': '[link](https://www.youtube.com/watch?v=Jdxa9o3poeE) to full video\\n\\n^lol ^if ^you ^saw ^the ^first ^post ^my ^b ', 'subreddit': 'LivestreamFail'}\n",
      "{'body': 'That means nothing. Tamir Rice got shot because he had a pallet gun and that was considered justified. If there is even the tiniest reason to be scared its a justified shooting which is a meaningless standard because in the end an innocent person just died. ', 'subreddit': 'nfl'}\n",
      "{'body': \"I've tried both PayPal and my regular debit card.\", 'subreddit': 'MoviePassClub'}\n",
      "{'body': '[deleted]', 'subreddit': 'sex'}\n",
      "{'body': \"The title is just so out of context. He is literally sending millions of dollars of relief and the media acts like he isn't doing anything. What do you want him to do, just magically make all of Puerto Ricos problems disappear? Sorry people but this is the real world where you can't sleep in your Minecraft bed to speed up the time. Theres only so much he can do, the rest is on the Puerto Ricans to actually rebuild themselves. Just look at all of his tweets about Puerto Rico from the past 24 hours. Does it seem like he doesn't care? Count the number of his Puerto Rico related tweets in the past week. I mean honestly it's like CNN and the rest of them are living in a parallel universe. He's going there on Tuesday. I don't have a problem with a human taking a golf break, especially after all the work he does. Come on people, it's a Saturday, he spent the entire week mucking through the tax code for US, The American people. Let him golf for an hour. \", 'subreddit': 'worldnews'}\n",
      "{'body': '[deleted]', 'subreddit': 'JusticeServed'}\n",
      "{'body': 'Mickey Gall was only 1 fight removed from being an amateur himself ', 'subreddit': 'MMA'}\n",
      "{'body': 'Thats my bad i used to drive past it all the time on my way home from work, sorry everyone', 'subreddit': 'Columbus'}\n",
      "{'body': \"No, it means you wont get good unscuffed vods, instead you'll be watching ice's normie vids with jumpcuts every 2 secs. \", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'We play in Neyland this year?', 'subreddit': 'CFB'}\n",
      "{'body': \"As long as they don't leave the base line it's legal\", 'subreddit': 'sports'}\n",
      "{'body': \"Thanks, this is why I'm not higher in arena. I definitely would've went Psamathe then Orion.\", 'subreddit': 'summonerswar'}\n",
      "{'body': '🤢🤢🤢', 'subreddit': 'shittyfoodporn'}\n",
      "{'body': 'A Turing test!', 'subreddit': 'Stellaris'}\n",
      "{'body': \"The 6 iv popplio sounds good, give me a few minutes to breed the exeggcute. Is there anything else (like Moon ultrabeasts/legendaries/anything of a higher value) you'd trade for the  Type: Null though?\", 'subreddit': 'pokemontrades'}\n",
      "{'body': \"Let's go Briscoe! \", 'subreddit': 'NASCAR'}\n",
      "{'body': '大人のふりかけの明太子だよ！', 'subreddit': 'noruue_'}\n",
      "{'body': 'I was living in North Moorhead 2-3 years ago, they were re-doing 15th between the river and 11th. I don\\'t think they did anything east of 11th though.\\n\\nAlthough I have noticed that the Fargo area has a really strange habit of re-doing the same damn roads every year. They\\'re \"fixing\" 19th North for the fourth time in the last five years.', 'subreddit': 'fargo'}\n",
      "{'body': 'It said it was a Christian school. Religious school = private.', 'subreddit': 'news'}\n",
      "{'body': \"Nope. They're going to deify him as they did Reagan. Supreme leader can do no wrong. Not worshiping him will be seen as disrespecting the troops. \", 'subreddit': 'politics'}\n",
      "{'body': '143413069| &gt; Canada Anonymous (ID: 8h4XA49X)\\n\\n&gt;&gt;143412829\\n&gt;&gt;143412925\\nyep thx bro\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Only because I don't think McCain will run again. But it'll still be a straight R ticket for me\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"No, I'm saying I don't watch him because I disagree with his reviews, why would I watch someone who doesn't share the same music tastes as me? That's all I'm saying, I think he's a great reviewer, just not for me and my tastes. I did word my original statement wrong, I shouldn't say he's not a legitimate source for reviews, I just don't think for me, personally, that he's a good source for reviews, not in general. \", 'subreddit': 'progmetal'}\n",
      "{'body': 'nikaigotskills 299 hunter/titan/warlock', 'subreddit': 'Fireteams'}\n",
      "{'body': \"Uhhhh......a $9.99 deck doesn't sound like something I'd see in a F2P store. That sounds like a DLC.\\n\\nA F2P store would be purely cosmetic and provide no in game advantages.\", 'subreddit': 'absolver'}\n",
      "{'body': 'Wow how did you get your hands on such a \"rare\" video?', 'subreddit': 'videos'}\n",
      "{'body': 'In one the HGCs where you could vote on a player and get packs based on how far they went in. People thought he would get far in the tournament, but he ended up dropping out really early resulting in all those people getting only 1 pack.', 'subreddit': 'hearthstone'}\n",
      "{'body': 'Danny Mills can suck a fat one, cunt of a man.', 'subreddit': 'MCFC'}\n",
      "{'body': \"I don't know what it is, but your comment had me rolling\", 'subreddit': 'EarthPorn'}\n",
      "{'body': \"It's the chords to the song.\", 'subreddit': 'gifs'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"That's not exactly true\", 'subreddit': 'hiphopheads'}\n",
      "{'body': 'Looks fucking sick. Like a real bullet hell. ', 'subreddit': 'RotMG'}\n",
      "{'body': 'royal yacht\\n\\nsalty dogs\\n\\nluxury bullseye flake\\n\\nno particular order', 'subreddit': 'PipeTobacco'}\n",
      "{'body': 'Miralo a este tan progre que era.', 'subreddit': 'argentina'}\n",
      "{'body': '[deleted]', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'You have successfully tipped muddd3d 200 iota($0.000124).\\n\\n[Deposit](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Deposit&amp;message=Deposit iota!) | [Withdraw](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Withdraw&amp;message=I want to withdraw my iota!\\nxxx iota \\naddress here) | [Balance](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Balance&amp;message=I want to check my balance!) | [Help](https://www.reddit.com/r/iotaTipBot/wiki/index) | [Donate](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Donate&amp;message=I want to support iotaTipBot!)\\n', 'subreddit': 'IOTAFaucet'}\n",
      "{'body': 'Can you say Fünf?', 'subreddit': 'miniSNESmods'}\n",
      "{'body': \"Basically the inverse of 'the social contract'.\\n\\n - Property rights are to be upheld.\\n\\n - No entity shall have authority over you on your own property.\", 'subreddit': 'Anarcho_Capitalism'}\n",
      "{'body': 'this is a good idea,, you can probably get a very good gradient of amount of effect&gt;how hardcore.', 'subreddit': 'science'}\n",
      "{'body': '[removed]', 'subreddit': 'sports'}\n",
      "{'body': \"It was more like $10 million dollars after the previous government infringed on his rights within the Charter of Rights and Freedoms (basically the Canadian constitution). Trudeau knew Khadr would win in court, and settled for paying $10 million instead of an amount multiple times more. \\n\\nNot saying I'm happy with Khadr getting $10 million, but this was more of a fuck up on the previous government since they blatantly violated his rights as a Canadian.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Touche', 'subreddit': 'CFB'}\n",
      "{'body': 'Oh. So they feel bad about punching him in the head to stick his tongue out, but not to springboard off him to make a long gap leaving him to die in a seemingly bottomless pit?', 'subreddit': 'nottheonion'}\n",
      "{'body': 'Subscribed! ', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'No more Pepe and fake cards', 'subreddit': 'MLBTheShow'}\n",
      "{'body': \"So....she refuses to meet with FEMA and to coordinate efforts for help and all we focus on this BS photo opp?  She has been so, so bad at helping out here and Trump did his dumb twitter thing and the issue is being lost here.\\n\\nYulin Cruz is being a dumb fuck and not coordinating help efforts like she should.  She won't meet with FEMA because she's too proud and her agenda of anti-USA is getting in the way of people getting help.\\n\\nDamnit\", 'subreddit': 'pics'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I mean I'm in favour of making it a criminal offence to knowingly have sex with someone who has HIV (unless you have it too) if that's more palatable to you.\", 'subreddit': 'unpopularopinion'}\n",
      "{'body': \"I appreciate your comment, thank you.\\n\\nWhile we may disagree on the solution to problems, I appreciate you being level-headed enough to see through the various headlines and actually view the source material. \\n Too often anymore people take the headline as fact and don't verify themselves.  \", 'subreddit': 'Pennsylvania'}\n",
      "{'body': \"A handful of them can be. If not, all of them. Trying to rack through my mind of each one's mechanics to think of anything majorly important. \", 'subreddit': 'ffxiv'}\n",
      "{'body': 'I pulled some strings for ya. ', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': 'I wouldnt Havr understood this had i not watched  a video about disrespect in the mlb', 'subreddit': 'sports'}\n",
      "{'body': 'I think 50 percent of the roster could be as “great” as Roman if they got the same amount of time and work put into them. Most wrestlers on the roster don’t get 25 minutes in a main event to do whatever they want too. I’m not going to credit others, it takes two to make a match, but he is put in positions that could be so much more watchable if he wasn’t just so god damn boring.\\n\\nAs for him being better than Cena, some people have short memories. I’ve never seen Roman have matches as good as Cena Vs Michaels, Cena Vs Rollins, he’s never done anything near most of the Cena Vs Punk matches, his three way with Rollins and Lesnar was better than the exact same match but with Reigns.\\n\\nAnd just like with Reigns, it took two people to make all of those matches.', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'Your submission has been automatically removed because it does not start with a continent tag inside brackets. The continent tags available are [NA], [SA], [EU], [ASIA], [AUS], [AFRICA] and [ME].\\n\\nPlease submit a new post that includes your current country or state after the continent tag, as in the examples below.\\n\\n* [EU] Norway DMG looking to play MM\\n\\n* [NA] California, team looking for 5th player\\n\\n* [ASIA] Korea, looking for mentor\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/RecruitCS) if you have any questions or concerns.*', 'subreddit': 'RecruitCS'}\n",
      "{'body': 'A lot of minorities feel unfairly represented.', 'subreddit': 'news'}\n",
      "{'body': \"Thank you.  I had an eye op 8 weeks ago and the resulting double vision has taken a while to go.  I've been depressed by how quickly I lost my 5k ability and have started c25k again.  If you can, I can!\", 'subreddit': 'C25K'}\n",
      "{'body': \"I believe JediMasterBen on a few reefing forums uses one. He was recently given TOTM on Reef2Reef if I'm not mistaken. He's is widely considered one of the top LED dudes of the hobby and helped bring forth quite a few of the now standards in reef LEDs. So if he still enjoys it now that he's had it a while then it's likely safe to say they're a pretty decent unit. \\n\\nEdit: turns out I was wrong, he uses a Coralvue system, my bad.\", 'subreddit': 'ReefTank'}\n",
      "{'body': \"Not from USA, but who is the HOA? If they're the homeowners can't anyone they want live in it and do the fuck they want with it (obviously with it being legal)?\", 'subreddit': 'bestoflegaladvice'}\n",
      "{'body': 'grasp', 'subreddit': 'nasusmains'}\n",
      "{'body': 'What do you think? Good idea to sell the only insurance available if btc goes boom?', 'subreddit': 'btc'}\n",
      "{'body': \"Yup, craziness. And wasn't that long ago really..\", 'subreddit': 'todayilearned'}\n",
      "{'body': 'My sac sack', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Your not the boss of me!', 'subreddit': 'gundeals'}\n",
      "{'body': \"Thought that I'd share my two femme fatales.\\n\\nKonahrik is sort of a mage/scholarly character, but with shouts.\\n\\nNightshade is a sort of stealthy assassin mage, but with style.\", 'subreddit': 'skyrim'}\n",
      "{'body': \"I've seen a recreation of the first model in real life; it's not as striking as on video but the effect is still pretty compelling and not as easily broken as you suggest.\", 'subreddit': 'videos'}\n",
      "{'body': 'Zai is incredible. Loving this Centaur pick so far', 'subreddit': 'OpTicGaming'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I hope you're not involved in any economic or scientific positions if your understanding of stats is that bad.\", 'subreddit': 'JordanPeterson'}\n",
      "{'body': \"Hey, I have your snorla ready. Let me know when you're ready to trade\", 'subreddit': 'pokemontrades'}\n",
      "{'body': 'Opening act just started.  You might be okay', 'subreddit': 'glassanimals'}\n",
      "{'body': 'I wonder if the airport cleaners do find valuable treasures: like all the drugs that smugglers dump in bathroom bins in a panic at the last minute? ', 'subreddit': 'funny'}\n",
      "{'body': \"My mom is a single and independent woman, I don't give a shit\", 'subreddit': 'pics'}\n",
      "{'body': \"The FYE by me has told me at least twice when I've asked that they will restock on Oct. 1st, but that seems strange because it's a Sunday. An employee at another FYE by my work said that they got them in on Thursday with some extras that weren't preorder and those all sold out the same day. She said to check back on Wednesday though because they get stock on Wednesday and Thursday but Wednesday is when they get the most stock.\\n\\nI'll check both tomorrow and Wednesday and cross my fingers!\", 'subreddit': 'funkopop'}\n",
      "{'body': 'Look, it\\'s simple.\\n\\n&gt;So he definitely applied the \"very fine people\" label to neo-Nazis, although it\\'s possible he\\'s too stupid to realize that they were neo-Nazis, if he thinks that all neo-Nazis are willing to tell you they\\'re neo-Nazis. \\n\\nWere they ***all*** neo Nazis? Can you say that with certainty? It\\'s a simple yes or no.', 'subreddit': 'worldnews'}\n",
      "{'body': 'Or she could be getting disability and only suing you for the difference in her full pay and disability.', 'subreddit': 'legaladviceofftopic'}\n",
      "{'body': 'I saw the picture and had a rush of memories, thank you for posting the name of it, I had completely forgotten. ', 'subreddit': 'DnD'}\n",
      "{'body': \"It's nice and quiet\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"Religion is a cancer that has held humanity back for thousands of years. Imagine what kind of incredible technology we would have today without idiots brainwashed by fairy tales sabotaging scientific progress.\\n\\nIf god exists, it clearly isn't interested in intervening in anything we do. So effectively, there is no god even if one exists.\", 'subreddit': 'MGTOW'}\n",
      "{'body': '[deleted]', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'Agreed. ', 'subreddit': 'leagueoflegends'}\n",
      "{'body': '[deleted]', 'subreddit': 'WredditCountryClub'}\n",
      "{'body': '[deleted]', 'subreddit': 'bonnaroo'}\n",
      "{'body': 'A friend is a huge fan of those, I also love how aggressive they look!', 'subreddit': 'askgaybros'}\n",
      "{'body': 'neaux', 'subreddit': 'CFB'}\n",
      "{'body': 'Woah spicy ', 'subreddit': 'baseball'}\n",
      "{'body': \"I know that this doesn't help you, but I am so relieved that it's not just me...\", 'subreddit': 'ShieldAndroidTV'}\n",
      "{'body': \"Hi jessicamshannon,\\n\\n\\n\\nYour submission has been removed because it violates:\\n\\n&gt;RULE #7: Off-topic\\n\\n\\n\\nLink goes to GTA screenshot\\n\\n\\n\\nYou can find all of our rules in the sidebar. Please look them over before contributing again.\\n\\nIf you have any questions or concerns, please [message the moderators](https://www\\\\.reddit\\\\.com/message/compose?to=%2Fr%2FCrimeScene&amp;subject=about my removed submission&amp;message=I'm writing to you about the following submission: https://www.reddit.com/r/CrimeScene/comments/6ryulq/soldiers_pov_during_operation_overlord_dday_june/ %0D%0DMy issue is...). Direct replies to official mod comments will be removed.\", 'subreddit': 'CrimeScene'}\n",
      "{'body': \"If you have the steam version, you have to buy it through steam. I'm not sure about the exact process but you can use your steam wallet.\", 'subreddit': 'blackdesertonline'}\n",
      "{'body': \"I haven't seen that in the stores here. I always gravitate towards the zero carb/calorie ones anyway but I've been known to dabble in their excellent Tea line.\", 'subreddit': 'AirForce'}\n",
      "{'body': '[removed]', 'subreddit': 'islam'}\n",
      "{'body': 'I wish that they would pay Baka Tsuki and print their translations and the translators for their time, perhaps to translate full time?', 'subreddit': 'HighschoolDxD'}\n",
      "{'body': 'Follow Your Heart says all of the plastic they use is recyclable ', 'subreddit': 'ZeroWaste'}\n",
      "{'body': '[deleted]', 'subreddit': 'Paramore'}\n",
      "{'body': 'All these streams be Texas A&amp;M Laggy.', 'subreddit': 'CFBStreams'}\n",
      "{'body': 'Nope, he says knees.', 'subreddit': 'shittyfoodporn'}\n",
      "{'body': 'The problem is that the EDA only starts to act after the block rate got very low for 12 hours, then it gives a fast block rate for a while, then the normal DDA kicks in, shoos the miners away, and brings the block rate to snail pace again -- over and over.  Each swing is unpredictable, and it may never stabilize. \\n\\nThe Bitcoin Cash devs are working on a better DAA that should ensure a stable block rate and hashpower, even if Bitcoin Core retains the old DAA.', 'subreddit': 'btc'}\n",
      "{'body': '[removed]', 'subreddit': 'tifu'}\n",
      "{'body': 'The lineup you wanted this season from your post history\\n\\nPG: CP3  \\nSG: Gordon  \\nSF: Harden  \\nPF: **Melo**  \\nC: Capela\\n\\nYeah, sorry about that. Thats sad as fuck', 'subreddit': 'nba'}\n",
      "{'body': \"That's just not true. Some features can be more recent innovations resulting from all sorts of different processes.  \\n\\n  This just happens to be a grammatical feature with an unbroken presence in the language since Old English. These are not unusual, so you shouldn't be surprised to see 'it's conserved from Old English' crop up quite a lot.  \\n\", 'subreddit': 'linguistics'}\n",
      "{'body': '*\"NiCE\"*', 'subreddit': 'ExpandDong'}\n",
      "{'body': 'Wow you’re gorgeous! You’re Hair! That eyeliner is so sharp it can stab. \\nI’m sorry about your mother’s passing. I promise it hurts a little less each day. Focus on the good memories :)', 'subreddit': 'FreeCompliments'}\n",
      "{'body': 'The side of her tit looks like gum stretching between a shoe and a sidewalk. ', 'subreddit': 'opieandanthony'}\n",
      "{'body': '143414817| &gt; United States Anonymous (ID: zR7qrd9W)\\n\\n&gt;&gt;143412442\\nThis was the right choice\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"1d20 /u/Razqn **Overall Success**: **1**\\n\\n(*1*)\\n*****\\n1d20 /u/Razqn **Secrecy**: **13**\\n\\n(13)\\n*****\\n\\n\\n\\n^(Hey there! I'm a bot that can roll dice if you mention me in your comments. Check out /r/rollme for more info.)\", 'subreddit': 'worldpowers'}\n",
      "{'body': '[deleted]', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'Okay, you talked me into it!', 'subreddit': 'delusionalcraigslist'}\n",
      "{'body': \"So why's this guy being downvoted so much?\", 'subreddit': 'roosterteeth'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Holy shit man, I don't know what the fuck is it with this song but it makes every female in the club lose their shit. I went yesterday with some mates to the club and it was all mellow and shit..... Until Bodak Yellow came on. Everyone went bat shit, two girls got into a fist fight, a group went full on rave, and one of my mates got slapped randomly.\\n\\nGood shit.\", 'subreddit': 'cringe'}\n",
      "{'body': \"Legally no. In terms of the mental capacity of a minor to consent, it's definitely not a yes or no question unless you think turning 18 is like having a switch turned on in your brain that suddenly allows you to make informed decisions about sex (it doesn't). \", 'subreddit': 'news'}\n",
      "{'body': 'Thanks for posting to /r/dirtykikpals, /u/anotherthingcoming! We encourage all of our users here to verify themselves, to possibly get more/better responses, as well as help us in dealing with sellers, scammers, etc. For information on how to verify, please check our sidebar or message us at [modmail](https://www.reddit.com/message/compose?to=%2Fr%2Fdirtykikpals).\\nAlso, be sure to familiarize yourself with the rules of this subreddit, as well as helpful tips [here](https://redd.it/6ojo0r).\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dirtykikpals) if you have any questions or concerns.*', 'subreddit': 'dirtykikpals'}\n",
      "{'body': 'Woooo', 'subreddit': 'baseball'}\n",
      "{'body': \"It's possible he called the wrong number. There are actually quite a few that are similar to the customer care line and the equipment activation line that some scammers must have set up.  It's also possible that the call center rep just didn't have a clue.\\n\\nThe person who responded first does have the proper answer, though.  \", 'subreddit': 'CharterSpectrum'}\n",
      "{'body': 'Woosh', 'subreddit': 'China'}\n",
      "{'body': 'No time to worry about the technicalities\\n', 'subreddit': 'Roast_Me'}\n",
      "{'body': 'The car in front slowing down to turn, how disruptive are they.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'My eyes have been opened forcefully like Podesta does with his pizza! \\n\\n\\n\\n\\nHow could i have been so blind?', 'subreddit': 'news'}\n",
      "{'body': \"Are you fucking retarded? If I drink a bottle of whiskey, immediately go for a drive and get into a crash that kills people that's absolutely 100% my fault. You make the choice to drink and then drive, just like the guy in OP's story chose to find a gun and act like an idiot with it. \\n\\n\\nYour logic is that the gun owner should be responsible by making it impossible for a drunk person to get a hold of, so how does that work when using your logic with drunk driving? If I use my friends car to drive while drunk without his permission, is that my friend's fault for 'letting me' use it? No, that's fucking stupid. \", 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'Tinder'}\n",
      "{'body': 'Sold Z97 Stinger motherboard to /u/Korrd', 'subreddit': 'hardwareswap'}\n",
      "{'body': \"It's something similar to high school prom (my school doesn't have a homecoming celebration either).\", 'subreddit': 'justneckbeardthings'}\n",
      "{'body': \"Little of Column A. Little of Column B. I'm glad workers receive protections but management needs to step it up or be replaced.\", 'subreddit': 'bayarea'}\n",
      "{'body': 'Greinke is stinky!', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': \"Hmm, legally allowed maybe. But you think their husbands will allow it? I'm not so sure.\", 'subreddit': 'pics'}\n",
      "{'body': '[deleted]', 'subreddit': 'albiononline'}\n",
      "{'body': 'Industrialization yes\\n\\nAgriculture no, there was this quack in the agriculture that did serious damage and was a favorite of Stalin, \"Trofim Lysenko\" was his name and he did enormous damage to the Soviet agriculture.', 'subreddit': 'catalonia'}\n",
      "{'body': 'Haha thanks!Very honoured to have so many questions ahahahaha.\\n\\nNEVER SOLOQ.The people there are stupid and will waste your time.They don\\'t even know the meta right now which is full of mages,and they keep insisting on playing mm which is just stupid.\\n\\nMany high elo players play in their teams (say 5 man)but personally I don\\'t.In fact I don\\'t really play with my squad but with other friends from other squads.But in my squad there is a whatsapp group and you can just ask for someone and then play tgt haha.\\n\\nI don\\'t plan I just play when I feel like playing.\\n\\nI got it one day before the season ended.Personally i don\\'t recommend gaming so much to 600 because the only good thing u get out of it is \"bragging rights\"/\"self-fulfilment\"but to be honest ..\\n-Nobody really cares how many stars u have \\n-You will never feel happy because there will always be someone higher than you.\\n\\nI had to spend ALOT of time(luckily it was holidays).I only played hard in the last month of the season.I climbed from 200 to 600 in one month lol.\\n\\n\\nAdvice ermmm....Don\\'t feel nervous if you meet Top squads.Because they are human and they make mistakes in game and can lose sometimes.They have high stars but stars aren\\'t really a good indicator of skill.ive seen 200 star legends beat a 5 man team all above 1k stars so yeah.\\n\\nTry to go for a trio q with your friends.One of the 3 people must play critical roles in the team like mages or assasin in case pubs can\\'t carry.(most of the time they can\\'t).If you get first pick,do choose what\\'s currently in the meta.When I played trio there was a time I won 30 straight rank games in a row so yea.\\n\\nDuo is a little more risky in my opinion as there will be 3 heavy people that u have to carry.\\n\\n5 man is hard since there are a lot of pros who are playing in 5 man teams so personally I stay away from it.But it can be great if your squad has a lot of experience together.\\n\\nAlways try to improve and ask yourself what you could have don\\'t better whenever you die.Maybe your Friend typed \"enemy missing in action\"and you thought meh they may be farming but then u get ganked.\\n\\nThis is a little weird but personally i feel if you play in the wee hours of the morning,that is the most optimal time to raise stars because there are many noobs at that time.It sounds funny but without it I wouldn\\'t be at 600 LOL.\\n\\nAnd lastl but not least,don\\'t be addicted to the game.I see many of my friends above 400 stars who play overnight until 7am and then have to go to work.You should know your priorities and not waste your life for a number that means shit.\\n\\nIn the next season I will retire as I feel that I\\'m too addicted and I have better things to focus on.Aft getting 600 stars I\\'m abit sick of ranked games haha.\\nI won\\'t go for it because there are a lot of things to sacrifice to gain it so yea\\n\\nHappy to ans your qns:)\\n\\n\\n', 'subreddit': 'mobilelegends'}\n",
      "{'body': 'Just started watching on ESPN3. Go Herd', 'subreddit': 'CFB'}\n",
      "{'body': 'I dont care if its a rapist a murder whomever it may be , this is no way to go, \\nNo one should feel their skin melting after being beaten near death, and the last thing i need is comments by edgy teens saying \"he deserves it he is a rapist!!!111 ', 'subreddit': 'watchpeopledie'}\n",
      "{'body': 'Bud Light is the worst beer on the market\\n\\nYes, that includes any and all 30-rack beer. Bud Light is honestly just so god damn bad', 'subreddit': 'baseball'}\n",
      "{'body': 'Women go their own way', 'subreddit': 'ShitPoliticsSays'}\n",
      "{'body': '[deleted]', 'subreddit': 'Zappa'}\n",
      "{'body': 'It plays a big role in linkffn(Harry Potter and the Enemy Within by Theowyn of HPG) and its sequel linkffn(Harry Potter and the Chained Souls by Theowyn of HPG)', 'subreddit': 'HPfanfiction'}\n",
      "{'body': \"I cringed when I read it but didn't want to say anything because people get really defensive and call me names when I try to correct things like that. D:\", 'subreddit': 'teenmom'}\n",
      "{'body': 'What’s on the menu for tonight? ;)', 'subreddit': 'AsiansGoneWild'}\n",
      "{'body': 'Twice!', 'subreddit': 'gaming'}\n",
      "{'body': 'It was a artist that redrew it in his own style he credited shadman', 'subreddit': 'Rainbow6'}\n",
      "{'body': 'In case anyone else happens to look back, I [did it](https://youtu.be/yCIiXZ0ayGM).', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"I got depressed in college as well. I was completely overwhelmed and realized way too late that I didn't have a lasting interest in what I was studying. I got counseling and medication (which I also hated) and struggled through. I got a job that I hated, played with my medications to make it through the day and tried to be a good person.\\n\\nAnd one day I was done. I didn't want to drug myself to get through life, to even get through a day. Life was more than that. So I quit my job and stopped taking my meds. Found something else I preferred to do and never looked back.\\n\\nAre you happy with your program that you are in? Are you happy with where you live and the direction your life is heading? Think hard about that. Especially if you can't even study on the weekends. I recall only doing that on weekends because I was so exhausted during the week.\\n\\nWeekends are supposed to rejuvenate you. Be a time to relax and catch up on 'you time'. Maybe you aren't loving yourself enough?\\n\\nI still suggest that you get up, shower, and get dressed on the weekends. Perhaps at least go for a walk to get out of the house. And definitely keep your therapy appointment. It always helps to talk to someone else.\\n\\nGood luck.\", 'subreddit': 'relationships'}\n",
      "{'body': 'Guys, come join us channel 3 on the OpTic Reddit Discord!!\\n\\nhttps://discordapp.com/invite/opticgamingreddit\\n\\nEdit: We have our own caster who takes questions xP\\n', 'subreddit': 'OpTicGaming'}\n",
      "{'body': '[deleted]', 'subreddit': 'Rainbow6'}\n",
      "{'body': 'I have 2 copies of this. I will need to sell one though.', 'subreddit': 'gratefuldead'}\n",
      "{'body': \"This isn't a report, Valve just sends everyone with over 100 points directly to Overwatch.\", 'subreddit': 'GlobalOffensive'}\n",
      "{'body': 'Yeh ur on my list of ppl to vote for', 'subreddit': 'NBA2KDesign'}\n",
      "{'body': \"No, because I don't know how. I think it would just come of as /r/fellowkids\", 'subreddit': 'OkCupid'}\n",
      "{'body': 'What the hell am I looking at here?', 'subreddit': 'gifs'}\n",
      "{'body': \"He saw how popular the crash Dankquan had was. It'll be in the script at some point.\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'Who the fuck are you?', 'subreddit': 'Animemes'}\n",
      "{'body': \"That people don't realize this is a choice. We don't have to accept it.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Oh I am living. ', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': 'https://youtu.be/Lpq53oR94JM', 'subreddit': 'twentyonepilots'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'They are both whatever. Henrich is better. He can 6 star. Those two can only 4 ', 'subreddit': 'Battlejack'}\n",
      "{'body': 'But that series was way too close, HR kinda fucked up that hg push.', 'subreddit': 'DotA2'}\n",
      "{'body': \"I'll definitely bring that up when I go in again, last time I took it to someone they said that one of the engine's cylinders was misfiring but changing the spark plug seems like it fixed that at least\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"Now that is a name I haven't heard in a long time. Beardown🐻⬇️\", 'subreddit': 'nba'}\n",
      "{'body': \"I can attest that this deck is quite good at achieving total lock, but I still have two big grievances with it. First is that without White it can have a hard time making up large point differences in a timed environment. Second is that, as the deck is presented here, there isn't any forced movement from the Purple cards. I know the deck is straining to include all the answers it needs, but without movement effects a Villain farm looks at this deck, shrugs its shoulders, and does what it was going to do anyway (with the one exception of Aloe &amp; Lotus).\", 'subreddit': 'MLPCCG'}\n",
      "{'body': 'If you think Gatsby was not a Chad you\\'re fucking delusional. It\\'s clearly explained in the books how he fucked many women while at Lake.Superior after he ran away from college. Hell it is even stated in the book that he \"took [Daisy Buchanan]\" sexually. What the fucking story shows is that all women are basically fucking cucking their husband. Daisy cheated on her husband cos she thought Gatsby was better but then went back to her husband when she realized he was better. Fucking women. All of them. ', 'subreddit': 'Incels'}\n",
      "{'body': '[removed]', 'subreddit': 'Sat'}\n",
      "{'body': 'Dilemma: I\\'m a female and not fantasizing about \"Chad\", in fact many \"Chad\" qualities (even physical) are not appealing to me.\\n\\nSo, am I not a female? Is Chad a lie? Please, save me.', 'subreddit': 'IncelTears'}\n",
      "{'body': 'Related question: is it better to do dual channel at 2933, or quad channel at 2133?', 'subreddit': 'buildapc'}\n",
      "{'body': \"Let's hope that Weber flattens McCormick\", 'subreddit': 'Habs'}\n",
      "{'body': \"When there's no electricity. \", 'subreddit': 'The_Donald'}\n",
      "{'body': 'Interested in the salty banner for your calculated one?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'The only one you could probably run non-standard (mostly referring to tankless runs) for is Halatali. ', 'subreddit': 'ffxiv'}\n",
      "{'body': \"&gt; Audrey’s most famous moment is without question her dance in the Double R diner.\\n\\nAudrey's most famous moment was tied the cherry stem.  No contest.\", 'subreddit': 'twinpeaks'}\n",
      "{'body': '[deleted]', 'subreddit': 'eagles'}\n",
      "{'body': \"&gt; He was driving me home, and it started raining, and he pushed the brakes, to the floor, as you do with anti-lock brakes. \\n\\nNot to argue but aren't you supposed to brake normally, feel the antilock engage and then press to the floor?  Seems like he didn't quite do the right thing regardless.\", 'subreddit': 'bestoflegaladvice'}\n",
      "{'body': \"I haven't seen all the line outs but are the Pumas throwing down the middle?\", 'subreddit': 'rugbyunion'}\n",
      "{'body': '[Definition of \"Sea-star\"](http://dictionary.com/browse/Sea-star)', 'subreddit': 'GooglePorn'}\n",
      "{'body': 'Geez. 2:45... What mode was that? ', 'subreddit': 'Warthunder'}\n",
      "{'body': '[deleted]', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': 'Transgenders. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Very true. He is indeed a good boy. ', 'subreddit': 'aww'}\n",
      "{'body': \"I definitely wouldn't say I hate him  but this makes me think of his hollywoodreporter profile. In the part where his castmates describe him, Patrick said he thought Ryan was famous because production was always talking to him more than everyone else. And he was featured prominently in a lot of the promotion for the season.\\n\\nSo I think it's more of a backlash because people feel like he's transparently being fed to them as the face of the season. And maybe there's an impression that he's aware of his archetype and he's playing it up to get screen time and become a big character. \\n\\nI'm not saying that's true, but that's my guess as to what people don't like about him.\", 'subreddit': 'survivor'}\n",
      "{'body': 'Great pic!', 'subreddit': 'BABYMETAL'}\n",
      "{'body': '143417265| &gt; United States Anonymous (ID: nN0OTd4c)\\n\\n&gt;&gt;143417220\\n\\nWEED\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Keep doing this. Zombie build is the strongest one for hero league in my opinion and I have a huge win rate with it. Alarak is one of the best heroes for solo lane and you only need to ban Sonya and don't face with Gazlowe main. Show of force is a good talent for braxis.\", 'subreddit': 'heroesofthestorm'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'He wasn’t brought back, the gates of hell were broken and the living were aloud to walk with the dead. Freeza just came to the living world because he could. ', 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': 'Swim to freedom', 'subreddit': 'worldnews'}\n",
      "{'body': '[removed]', 'subreddit': 'malefashionadvice'}\n",
      "{'body': \"Ga'head babe, talk at me.\", 'subreddit': 'investing'}\n",
      "{'body': \"Well... Some people here eat dogs, and I've heard stories.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'yes', 'subreddit': 'teenagers'}\n",
      "{'body': \"I couldn't agree more LOL. He just gives it his all to the team. And if he never got injured, things could've been a lot different in the play offs.\", 'subreddit': 'leafs'}\n",
      "{'body': \"Because you don't sign up for football, run in a pretty standard way by the coach?\", 'subreddit': 'news'}\n",
      "{'body': 'Can we summon the ghost of Patrick Swayze to hash this all out? ', 'subreddit': 'likeus'}\n",
      "{'body': '[deleted]', 'subreddit': 'worldnews'}\n",
      "{'body': 'Do you even know what a walled ecosystem is?\\n\\nOr are you just repeating it because you think it makes you looks smart?', 'subreddit': 'Amd'}\n",
      "{'body': '[deleted]', 'subreddit': 'MMA'}\n",
      "{'body': \"I definitely see a ton of red flags. Your mom is abusive, for sure. She also sounds super unstable, and it must be exhausting having to deal with her constant up and down fluctuations.\\n\\nYour best bet is to avoid her when possible and prepare for the day when you can move out to go to college and get some distance. In the meantime, don't trust her with ANYTHING. Keep your valuables locked up when possible. Don't let her have access to your money.  She's already threatening to take away things you own and apparently feels justified in this. Don't doubt that she'll make good on those threats one of these days.\", 'subreddit': 'raisedbynarcissists'}\n",
      "{'body': \"I agree 100%. Truth be told, I wasn't a fan of Susperia (sp.). Scream is like #100 for me.\", 'subreddit': 'horror'}\n",
      "{'body': 'Yikes', 'subreddit': 'neoliberal'}\n",
      "{'body': \"I'd actually been working under that assumption since the first Vecna fight until this week.\", 'subreddit': 'criticalrole'}\n",
      "{'body': \"If you haven't already, see a doctor. Obviously, your hand is critical to your occupation. It's also much easier to sustain another injury when you have to compensate for an existing one. You may have a fracture. Either way, a doctor should help get you on the fastest path to recovery.\", 'subreddit': 'tifu'}\n",
      "{'body': 'He is a shitbag. I despise him worse than Don Lemon. At least Lemon is openly a hater. Cernavich is a ambulance chaser of the quick blog views. ', 'subreddit': 'The_Donald'}\n",
      "{'body': \"I'm pretty sure there is a physical deluxe version as well.\", 'subreddit': 'MonsterHunter'}\n",
      "{'body': 'I grant you official permission to eat Turkey on our Thanksgiving as well as long as you grant us the same priviledge on yours. ', 'subreddit': 'hockey'}\n",
      "{'body': 'this photo is terribly sad and funny but really sad ', 'subreddit': 'funny'}\n",
      "{'body': \"Please note that having a few narc traits doesn't make you one. The same way everyone has probably had a symptoms of depression now and then. but that doesn't make you depressed.  \\nRegardless, your not a bad person for expressing yourself in a constructive and appropriate manner. \", 'subreddit': 'raisedbynarcissists'}\n",
      "{'body': 'Does anyone know where I could possibly watch this movie? I know it is permanently in the vault for good reason, but ever since I was kid I wanted to see this movie.', 'subreddit': 'disney'}\n",
      "{'body': 'Can put the secondary in the holster but the primary dissapear when using the secondary.\\nAbsolutely.\\nUnplayable.', 'subreddit': 'Rainbow6'}\n",
      "{'body': \"I mean it's hard to know since I don't know who you are or what your preferences are, but yes if you want a serious, dedicated, anarchist communist group with intersectional politics and lots of experience in organising then sign up to their emails and see what you think from there. Also there's https://www.facebook.com/WorkersSolidarityMovement if you're on Facebook. And a Twitter, etc.\", 'subreddit': 'Anarchism'}\n",
      "{'body': 'I did fart, for the record', 'subreddit': 'conspiracy'}\n",
      "{'body': 'Politics ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Evolution convinced you that evolution is false? Why not just admit that you don't know why evolution is false, but you just wish it were?\\n\\nThose threads also got some highlights from here. But there are still plenty of snarky comments to smarter posts on those threads, from people like stcordova. And I still see quite a few posts that aren't very high, but are correct.\\n\\nIt's not demonstrably false. It's you selecting only favorable facts to support your case, without looking at the whole picture.\\n\\nHow many non-creationists are allowed to post to /r/creation? I rest my case.\", 'subreddit': 'DebateEvolution'}\n",
      "{'body': 'Somebody with better skills than me... Please r/reallifedoodles this.', 'subreddit': 'IdiotsFightingThings'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'all hail gold', 'subreddit': 'me_irl'}\n",
      "{'body': 'i wouldnt go for a high pop clan, find a small clan and build it up. You will make better clanmates and you will find there are always people to do things. I was in a 55 player clan and it was dead, joined a 26 player clan and it was full of assholes that wanted to be carried through everything, joined a 5 player clan and i have been playing every night and they are always up for raids, nightfalls ect. Small clans are the way to go in my book.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Sack incoming. ', 'subreddit': 'CFB'}\n",
      "{'body': \"Bless her.... I hope she's the kind of person who just never takes her laptop out in public. \", 'subreddit': 'oldpeoplefacebook'}\n",
      "{'body': \"That's what freaked me out so much when upper management thought it would be okay to just let it die. I can imagine that if this resident was able to somehow get better and came back, she would be pretty pissed to know we allowed her pet to die. \", 'subreddit': 'legaladvice'}\n",
      "{'body': 'He pop him with a 30-30???? Awesome....', 'subreddit': 'watchpeopledie'}\n",
      "{'body': \"I make soap and lotion and I've thought how cool it'd be to do sonething similar to this with lotion. Almost like a little milkshake kiosk. You could pick different oils and butters and fragrances. And mix and package right there.\\n\\nAnyone interested in DIY make up and nail polish TKB Trading is highly recommended.  They have kits too.  Great company, reasonable prices. \\n\\nhttps://tkbtrading.com/\", 'subreddit': 'educationalgifs'}\n",
      "{'body': \"Yeah. So far both shorts have premiered at a convention and released on a monday. It's a pretty obvious schedule they've got going. There's not much to speculate on.\", 'subreddit': 'RWBY'}\n",
      "{'body': \"What do you get when you cross a chicken with the Terminator?  I'll be bock, bock, bock.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"he did and tried to stream with it and couldn't get it to work\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'Because it IS hard.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"It's a possibility, guess we'll find out\", 'subreddit': 'CFB'}\n",
      "{'body': 'Yeah....  no.  Just like everything else, there are some people who like porn and there are some people that don’t.  Do you really think *porn* of all things is what Dan would lie to us about liking?  If he liked porn, he wouldn’t be uncomfortable saying it on YouTube. ', 'subreddit': 'nerdcubed'}\n",
      "{'body': 'they are', 'subreddit': 'GlobalOffensive'}\n",
      "{'body': \"I actually had some parents refuse the IM vitamin K but later agree to let the baby have it orally. It's the same medication from the same vial and they were okay with the baby receiving via the less effective and, arguably, more miserable route. I still don't understand their objection. \", 'subreddit': 'thatHappened'}\n",
      "{'body': 'The Simpsons, just let it be over please', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Just looking for advice to know what to do.', 'subreddit': 'dontstarve'}\n",
      "{'body': 'It was an okay episode, but the reasoning behind it was strong.  Jon knew there was no way Cersei would ever agree to any alliance unless she saw a wight for herself.  It had to be done.\\n\\nAs for the episode itself, I think it did suck due to the heavy use of \"redshirts\".  Most of the deaths were from random people that we didn\\'t even know.  They were just death fodder.', 'subreddit': 'freefolk'}\n",
      "{'body': 'I saw something about that. I think he article said Apple knows the issue and is pushing out an update. My new issue is battery life with IOS11 but thats another issue altogether.\\n', 'subreddit': 'iphone'}\n",
      "{'body': \"We didn't have a QB coach until January of this year. No, I'm not joking.\", 'subreddit': 'CFB'}\n",
      "{'body': 'Technically a sequel considering Star Wars took place a long time ago.', 'subreddit': 'PrequelMemes'}\n",
      "{'body': 'Both sound great but the 92 has insane headroom. It can handle all types of extreme redlining without sounding bad.\\n\\nI think the 92 sounds better overall (I never redline anything anyway) but I might  be biased, since  I absolutely adore  everything about the 92. I would be interested to hear an official statement about if they sound identical at some low level though.', 'subreddit': 'DJs'}\n",
      "{'body': \"Why is it bad that a tv show is make you think for a second and ask yourself a question? It's not mindless tv. That means it's at least somewhat decent, and better than most of the shows out there. I'm sick of explaining this to people. It's a Duplass Brothers production. They have their own style, it's very indie. Don't watch it if you don't like it. \", 'subreddit': 'Room104'}\n",
      "{'body': 'Gordon just successfully beat the throw but overslid the bag to miss #60.', 'subreddit': 'baseball'}\n",
      "{'body': '\"I\\'m trying to eat my breakfast and you\\'re just talking *at* me about a ball. Like, do you have any idea how dull, and irksome you are? What is the point in spending your life being such a pissant?\"', 'subreddit': 'CampHalfBloodRP'}\n",
      "{'body': '[deleted]', 'subreddit': 'Overwatch'}\n",
      "{'body': 'Cmon man. Don’t you know that girls are vulnerable and guys are predators when drunk?', 'subreddit': 'Wellthatsucks'}\n",
      "{'body': 'Is Gears of War 2 the original case? If so, can you provide pictures?', 'subreddit': 'GameSale'}\n",
      "{'body': 'Profile elite is the loudest hub on the market.  Profile hubs have almost always been the loudest, back in the day when my friends and I would road trip to skateparks we would hear people air out and be like \"that dude has a profile hub\".', 'subreddit': 'bmx'}\n",
      "{'body': '143414191| &gt; None Anonymous (ID: K4S0eFk8)\\n\\nslide thread\\n\\nsage\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Tracklist:\\n\\n01. Jason Ross feat. Lauren Ray – I Will Be There [ANJUNABEATS] w/ Seven Lions &amp; Xilent – The Fall [OWS\\n02. Jason Ross – Cairo [ANJUNABEATS] w/ Seven Lions feat. KARRA – Silent Skies [SEEKING BLUE]\\n03. Seven Lions – Steps Of Deep Slumber [SEEKING BLUE] w/ Jason Ross feat. Lauren Ray – Me Tonight [ANJ\\n04. Wrechiski &amp; Jason Ross – Atlas [ANJUNABEATS]\\n05. Seven Lions – Cusp [WHO’S AFRAID OF 138]\\n06. Seven Lions feat. Skyler Stonestreet – Freesol [SEEKING BLUE]\\n07. Jason Ross – Valor (Seven Lions Edit) [ANJUNABEATS]\\n08. Jason Ross – Mirror Image [ANJUNABEATS]\\n09. Seven Lions &amp; Jason Ross feat. Jonathan Mendelsohn – Ocean\\n10. Excision – The Paradox (Seven Lions &amp; Dimibo Remix) [ROTTUN]\\n11. Seven Lions &amp; Jason Ross – ID\\n12. Seven Lions feat. Ellie Goulding – Don’t Leave [CASABLANCA]\\n13. Jason Ross – Coaster [ANJUNABEATS] w/ Ilan Bluestone &amp; Jason Ross – Amun [ANJUNABEATS] w/ Seven Lio\\n14. Seven Lions feat. Vök – Creation [CASABLANCA]\\n15. Seven Lions feat. Lights – Falling Away (Festival Mix) [CASABLANCA]\\n16. Seven Lions &amp; Jason Ross feat. Paul Meany – Higher Love [ANJUNABEATS]\\n17. Dirty South feat. ANIMA! – I Swear (Jason Ross Remix) [ANJUNABEATS] w/ Seven Lions &amp; Echos – Cold S\\n18. Seven Lions feat. Kerli – Worlds Apart (ABGT250 Outro Edit) [CASABLANCA]\\ufeff', 'subreddit': 'AboveandBeyond'}\n",
      "{'body': 'Hey pm me girl ', 'subreddit': 'nycmeetups'}\n",
      "{'body': '\" forcible suppression of opposition\"  Check', 'subreddit': 'CanadaPolitics'}\n",
      "{'body': 'Just a quick check on Vivid Seats which is the ticket page espn uses. Those tickets are still way too expensive. Seats around that price are going for $30 per.', 'subreddit': 'KansasCityChiefs'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"...that's why they move their productions to China \", 'subreddit': 'canada'}\n",
      "{'body': 'At this point it’s when not if', 'subreddit': 'ockytop'}\n",
      "{'body': \"That's what the balls are for dude.\", 'subreddit': 'OkCupid'}\n",
      "{'body': \"Yes, I'm having it too. thank you for letting me know I'm not alone\", 'subreddit': 'AgirlAdrift'}\n",
      "{'body': \"Who the fuck is going to watch Hangin' with Mr.Cooper? \", 'subreddit': 'television'}\n",
      "{'body': 'Houston Chronicle articles are frequently behind a metered paywall. This link may let you view the article if you have reached your limit, though you may have to wait a few hours for it to show up in the cache:\\n\\n* [Google Cache](https://www.google.com/#q=site:http://www.houstonchronicle.com/news/houston-texas/houston/article/Harvey-laid-bare-lack-of-resources-training-at-12243556.php?cmpid=reddit-premium)\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/houston) if you have any questions or concerns.*', 'subreddit': 'houston'}\n",
      "{'body': \"Hey, this isn't a liberal thing, This is a sovereign person thing. They tend to be conservative/ libertarian people railing against any taxation and a bunch of other weird stuff.\", 'subreddit': 'videos'}\n",
      "{'body': \"Thanks! Yeah, figured that would happen....I honestly can't tell if they're trolling or not sometimes. It's alright, consider it complimentary entertainment.\", 'subreddit': 'goodyearwelt'}\n",
      "{'body': \"Thanks for this. However, I'm still going to be skeptical until the orgs in question validate receipt \", 'subreddit': 'politics'}\n",
      "{'body': 'If you join us right now, together we can turn the tiiiiiiiiiiiide', 'subreddit': 'AskReddit'}\n",
      "{'body': 'He \"literally\" was, though. Answering in more words than I do doesn\\'t make you right. ', 'subreddit': 'JusticeServed'}\n",
      "{'body': \"wait, it doesn't proc off of the cards left in your deck? Two Princes (one in deck, one in hand) SHOULD work right? It has to go off the cards that are actually in your deck and not the ones that you start out with because it is invalidated by Elise packs yet still works when you have other 2 drops in your hand (say discover a 2 mana card off of lotus illusionist or whatever). \", 'subreddit': 'hearthstone'}\n",
      "{'body': '[deleted]', 'subreddit': 'conspiracy'}\n",
      "{'body': \"It's definitely tricky, as an SD you certainly are a target for gold-digging SBs, lol.  I would never want my SD to feel taken advantage of though, so I'm very conscious of what he spends. I would not ask him to buy me something, but I do appreciate when he has offered.  He's my first SD, and I was really uncomfortable with the money aspect of things in general, but I've gotten a little better about it.  :)\", 'subreddit': 'sugarlifestyleforum'}\n",
      "{'body': \"I like how this post isn't flaired\", 'subreddit': 'teenagers'}\n",
      "{'body': 'If you pay above average, you’ll get people who will stay with the company longer.  I’ve seen countless stats that show money is the biggest driving force for just about everything when it comes to motivation etc.  $50k plus bonus is a good place to start. ', 'subreddit': 'msp'}\n",
      "{'body': \"Nope, just super bored. The landscape loses all of it's color during the winter, and the cold weather drives me insane. Not to mention that damned snow.\", 'subreddit': 'depression'}\n",
      "{'body': \"I don't know that I'll ever reach my goal weight. It seems so far away..... (I have to lose about 25lbs) \\n\\nBut if/when I do get there I think I could be satisfied with how I look in any type of clothes. :)\", 'subreddit': 'proED'}\n",
      "{'body': '[Related piece.](http://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/)', 'subreddit': 'neoliberal'}\n",
      "{'body': 'It was my tenth month after being hired and felt like literally two months wages had just been taken away prior to going on holiday because of this.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"There is a 0% chance he coaches against WV week 1 next year. I just hope to god they fire him within in the next 2 days. I'm guessing Brady Hoke would take over until we hire someone \", 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'Kawaiiikitten'}\n",
      "{'body': 'I propose that Puerto Rico no longer has to be taxed since the money that Uncle Sam got from them will not be used when they need it.', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': \"IIRC they're heavily armored. So that's something at least\", 'subreddit': 'gtaonline'}\n",
      "{'body': \"Indeed it is, and you failed to notice that this is one of the oldest, most copied text on here. It's a classic.\", 'subreddit': 'pics'}\n",
      "{'body': 'THICC', 'subreddit': 'CFB'}\n",
      "{'body': \"You're getting down voted because you guys are looking impressive, but i understand that halftime sinking feeling when you're at the game. Staring at the field and just feeling it. Don't worry, I think you guys are competitive and are gonna fight for this win.\", 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'politics'}\n",
      "{'body': '[deleted]', 'subreddit': 'PS4'}\n",
      "{'body': 'You are the fetish I have been looking for', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I think the word you\\'re looking for is \\'ratio\\'. Anyhow, even if it was 1:100000000, are you saying that one is \"insignificant\"?\\n\\nSee, I can do fallacies too. 😉', 'subreddit': 'pics'}\n",
      "{'body': 'already decorated my desk and desktop a bit. probably gonna bring in some new decorations as usual and compete against my rival around the corner.', 'subreddit': 'halloween'}\n",
      "{'body': \"I have no idea so I'm asking. Wouldn't mixing that with a pregnancy test be a bad idea? There must be *some* sort of chemical that reacts with urine to indicate pregnancy right? How safe is that to put in your drink?\", 'subreddit': 'GifRecipes'}\n",
      "{'body': \"That's really my biggest complaint about OP's first point. Less popular teams won't get any significant bigger viewership because they still will be less popular.\\n\\nIf I can sit down and enjoy 3h of games of a team I like I'll do that.\\n\\nBut whether it is 3h of games or 1h for a single game I won't sit down to watch 2 teams I don't care about playing.\\n\\nEven in a talented region like Korea I don't watch teams like BBQ Olivers play Afreeca Freecs because I don't care about either of those teams so I definitely won't watch the bottom NA teams play.\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'I think she just might be banned from MTV. \\n\\nI guess anyone with 200+ followers on Instagram is considered a celebrity now 🙄', 'subreddit': 'BigBrother'}\n",
      "{'body': \"What's the shortest song for grinding tokens?\", 'subreddit': 'TheaterDays'}\n",
      "{'body': 'This is amazing ', 'subreddit': 'funny'}\n",
      "{'body': \"Estie c'est profond.\", 'subreddit': 'Quebec'}\n",
      "{'body': 'The key designers could have benefitted from some design improvement as well...such as placing picture instructions of how to use the key on the reverse side.  Currently it has pictures of all the family brand logos...which is really worse than being blank if a guest is standing there getting frustrated because then they associate your brand logos with that emotion.', 'subreddit': 'TalesFromTheFrontDesk'}\n",
      "{'body': \"I'm a currycel. \\nFoids who are attractive and can get chad won't go for traditional arranged marriage, average and below foids who go for arranged marriage are mostly gold diggers looking for betabux to cheat on.\\nArranged marriage works differently now, foids do have a choice, she can keep rejecting proposals which her parents find, might be forced to settle if she hits her late 30s.\\nMarriage is cope altogether, every married wimen i know cheats.\\n\\n\\nDowry is impossible now, might even be falsely sent to jail. Fuck feminism. \\n\\n\\nI wish escorts were legal and easy to find in india, being escortcel is way better than becoming a betacuck.\\n\\n\\nYou can get a hot curry if you can afford to bring her into your country through marriage because you're white.\", 'subreddit': 'Incels'}\n",
      "{'body': 'Mine is \"Thanks for calling X, this is Qari, can I get your name and company please?\"\\n\\n\"uhh yes my problem is X.\"', 'subreddit': 'NotMyJob'}\n",
      "{'body': 'Hey, but Doom is coming this holiday', 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'Close enough.', 'subreddit': 'RWBY'}\n",
      "{'body': 'Brb gonne glue some money on the back of my phone just in case', 'subreddit': 'LifeProTips'}\n",
      "{'body': \"Most of what the government does is the extremely boring shit needed to continue to supply basic services. Governments maintain roads, regulate labels on ketchup so you know what you're buying, and do science to figure out the limit on the number of ducks you can kill at once so there's enough ducks.\\n\\nA very small amount of the stuff the government does is SCARY AS ALL FUCKSTICKS.\", 'subreddit': 'news'}\n",
      "{'body': 'You must hold up your hand a lot when watching The Simpsons then. 🤔', 'subreddit': 'anime'}\n",
      "{'body': \"Garza looking like he needs to come out now. He looks like he's having a lot of trouble.\", 'subreddit': 'MLS'}\n",
      "{'body': 'This content brought to you from \"Spain Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#off site feed \"Spain Pool\")\\n', 'subreddit': 'ImagesOfSpain'}\n",
      "{'body': 'How did you see him?', 'subreddit': 'titanfall'}\n",
      "{'body': '143417408| &gt; Switzerland Anonymous (ID: /BaaYiQg)\\n\\n&gt;&gt;143416975\\nIronically you got to be a retard if you choose to live in a city.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Contingency and necessity bit is pretty incoherent, and is really only a product of trying to understand the Universe before modern physics.\\n\\nThe whole \"we need God to create the Universe... well then what created God?  God doesn\\'t need a creator!  Then why does the Universe need God to create it?\"', 'subreddit': 'DebateReligion'}\n",
      "{'body': \"SAR no water add peach infusion and white tea. It's not that big if a pain but I guess feel silly ordering it. So I just make them for myself before a ten. \", 'subreddit': 'starbucks'}\n",
      "{'body': 'I’d love to cum all over your perfect tits and have you suck me dry ', 'subreddit': 'wifesharing'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Wait for coffee lake before picking a CPU. Intel 7700K might go down. Ryzen 5 1600 is a good pick too.', 'subreddit': 'buildapc'}\n",
      "{'body': 'I have a 760 for 60$ shipped', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'Look at their shoes:\\n-Too flashy and they won\\'t tip generously unless they are shitfaced\\n-really expensive fine leather and you\\'ve hit the jackpot\\n-heavily worn work boots and they might tip you well one night but they don\\'t make good recurring customers\\n\\nTeam up:\\n-ask customers \"who exactly is your favorite?\" And then tell your coworker, the coworker might return the favor (but do not expect them to team up, many will not)\\n-tip the bouncers to send generous customers to you\\n-tip the bartenders to send generous customers to you\\n\\nBe a contradiction:\\n-be funny (memorize lots of jokes) but claim that you don\\'t know any jokes\\n-be aggressive but act shy\\n-act drunk but be sober (don\\'t ever get drunk on the job actually)\\n-claim you don\\'t know anything but have intelligent conversations\\n-claim to be boring but be super entertaining\\n', 'subreddit': 'stripper'}\n",
      "{'body': \"Suddenly I'm reminded of Red Dwarf.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Or just get a real one for $50 at a flea market...', 'subreddit': 'gaming'}\n",
      "{'body': 'Maybe you should read the OP again.', 'subreddit': 'relationship_advice'}\n",
      "{'body': 'Butthurt China topi spotted lol', 'subreddit': 'MapPorn'}\n",
      "{'body': \"You don't need roasting, being a Rams fan is bad enough already\", 'subreddit': 'RoastMe'}\n",
      "{'body': 'User name checks out', 'subreddit': 'DIY'}\n",
      "{'body': 'Hey at least the women up top serve a purpose. ', 'subreddit': 'funny'}\n",
      "{'body': 'What about luckyduck', 'subreddit': 'DarkNetMarkets'}\n",
      "{'body': 'I have stabilized the video for you: https://gfycat.com/bluetartdavidstiger \\n\\nIt took 95 seconds to process and 15 seconds to upload.\\n___\\n[^^how ^^to ^^use](https://www.reddit.com/r/stabbot/comments/72irce/how_to_use_stabbot/) ^^| [^^programmer](https://www.reddit.com/message/compose/?to=wotanii) ^^| [^^source ^^code](https://gitlab.com/wotanii/stabbot) ^^| ^^/r/ImageStabilization/ ^^| ^^for ^^cropped ^^results, ^^use ^^\\\\/u/stabbot_crop', 'subreddit': 'Damnthatsinteresting'}\n",
      "{'body': 'What does that mean?', 'subreddit': 'nattyorjuice'}\n",
      "{'body': 'It seems like artists have been using this as an easy was to generate excitement in the UK for their album. ', 'subreddit': 'grime'}\n",
      "{'body': 'I think you’re too upright. A magazine once noted the belt buckle should be angled toward ball. A more athletic stance, with more knee bend allows the arms and core to turn, and come back with more explosive contact. The tips above all sound good to me as well. ', 'subreddit': 'golf'}\n",
      "{'body': \"'I watch Driller Killer.' The class is shocked at my overwhelming intelligence and taste in 'artistic' movies. '...how? I can't understand its sheer nuance and artistry. Laser Mission is so much more accessible.' 'Well...OOP IS THE PUNCH.' One student laughs, and I turn to see who the fellow artist is. Its none other than Rob 'Alpacapatrol' Robert.\", 'subreddit': 'northernlion'}\n",
      "{'body': '[removed]', 'subreddit': 'WatchItForThePlot'}\n",
      "{'body': 'L-lewd :o', 'subreddit': 'AceAttorney'}\n",
      "{'body': \"I don't know why you were down vote so much. You aren't wrong.\", 'subreddit': 'niceguys'}\n",
      "{'body': 'I appreciate the honest thirst in this comment.', 'subreddit': 'The100'}\n",
      "{'body': 'Chelsea reminds me of Michael Cera a bit in this picture and now I really really want to see Michael Cera in a wig gangbanged by a bunch of muscular, strong military men  \\nNo homo, of course.', 'subreddit': '4chan'}\n",
      "{'body': 'Got the same thing. I love this generation’s version of Space Gray.', 'subreddit': 'iphone'}\n",
      "{'body': 'Me, you and one other person at least. I finally got them 6 hours ago.', 'subreddit': 'GreasyMoney'}\n",
      "{'body': 'Too much water 7.8/10', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'Oh, I am sorry I was unaware that you personally knew these two young men.', 'subreddit': 'news'}\n",
      "{'body': \"They didn't nerf or buff anything. They changed a formula that unfairly punished Pokémon with unbalanced stats.\", 'subreddit': 'TheSilphRoad'}\n",
      "{'body': 'We fucking NEED a bot that comments thisemojipasta every single time it scans a word of this in it.', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': \"Sucks when grandma isn't streaming. :(\", 'subreddit': 'CFBStreams'}\n",
      "{'body': \"Bro. It's Tina. She cray. Gtfo. \", 'subreddit': 'nosleep'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Heinz only. Pickles are great. At least you’re 1 for 2. ;)', 'subreddit': 'PenmanshipPorn'}\n",
      "{'body': \"But you'd be wrong. \", 'subreddit': 'news'}\n",
      "{'body': 'Only conservatives are allowed to be offended these days. ', 'subreddit': 'news'}\n",
      "{'body': 'Then Lee or White.', 'subreddit': 'Fantasy_Football'}\n",
      "{'body': 'I disagree with it, unless they start charging families PER child.  I don\\'t mind tax money to go to paying for schools, but Utah is in a unique situation where our local culture encourages oversize families yet everyone picks up the tab equally.  I\\'m all for 2 kids included with your taxes, beyond that $1000 a year (or whatever a reasonable price is).  Until then NO NO NO.\\n\\nI don\\'t have kids if that was no clear already, and like I said, I don\\'t mind paying taxes, but the system is currently woefully unfair and is not able to deal with said \"culture\".  Maybe the \"CULTure\" that encourages the ravenous growth could chip in so these bonds would not be needed.', 'subreddit': 'SaltLakeCity'}\n",
      "{'body': '[deleted]', 'subreddit': 'RandomActsOfBlowJob'}\n",
      "{'body': \"I meant public in the sense that it wasn't just between a group of close friends but between a group of people from across the country who didn't necessarily know each other outside of the group or organisation it was affiliated with. Seems as though anyone could have been a member of the group.\\n\\nWhy does hanging a banner on a bridge make it worse that you have those views? It might make you a bit more annoying than if you keep it quiet but I'm not sure that matters much in the context of this discussion.\", 'subreddit': 'ukpolitics'}\n",
      "{'body': \"You can't just run stage 4 on LR Goku event to get dupes?\", 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': 'rip lsu', 'subreddit': 'CFB'}\n",
      "{'body': 'Not gonna lie, kinda looks like Winnie the Pooh with bigger ears', 'subreddit': 'funny'}\n",
      "{'body': \"Hail Mary's?\", 'subreddit': 'religion'}\n",
      "{'body': \"Good points. As bad as they handled Oliver and Laurel's relationship, it could have been much worse.\", 'subreddit': 'arrow'}\n",
      "{'body': '[deleted]', 'subreddit': 'ffxiv'}\n",
      "{'body': '[+Llim](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoblhz/):\\n\\nWhat are giraffes even trying to do?', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '[deleted]', 'subreddit': 'personalfinance'}\n",
      "{'body': \"&gt; So in your mind, it would be more humane to simply eradicate the species in what tantamounts to genocide, than to simply continue sheering them as we've done for millennia.\\n&gt; For somebody pretending to love animals, you sure seem to want them dead.\\n\\nI will never not be surprised by how stupid the mental gymanstics people like you will come up with! Ha!\\n\\nNext up wanting to ban bullfighting is genocide to Spanish Bullfighting Bulls, wanting to ban pugs is genocide to pugs, wanting to ban puppy mills is also genocide too!\\n\\nYay I can play this\", 'subreddit': 'todayilearned'}\n",
      "{'body': \"I just said that I liked her. \\n\\nIt's easier if you have a crush on someone you already know, though.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Central air?\\n\\nHello Azrael.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I bet it will leak in a few weeks about how Trump didn\\'t know the United States was responsible for Puerto Rico. And *then* his supporters will defend that revelation as \"SO ALPHA.\"', 'subreddit': 'worldnews'}\n",
      "{'body': 'Nothing has been confirmed about celebrity big brother outside of the fact that it’s happening. Everything going around about potential houseguests has come from completely unreliable sources. ', 'subreddit': 'BigBrother'}\n",
      "{'body': 'I\\'m just going to assume you\\'re trying to \"troll\", so have a nice day.', 'subreddit': 'Rainbow6'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': 'What are the minimum requirements to complete the 7th tier battle gear wise?', 'subreddit': 'SWGalaxyOfHeroes'}\n",
      "{'body': \"That same story is true of a whole lot of Americans too. Hey, I'm poor, I live on benefits but my rent is $3 a month and I can cheat like crazy on my Section 8 housing! \", 'subreddit': 'The_Donald'}\n",
      "{'body': 'Cat.', 'subreddit': 'CatsStandingUp'}\n",
      "{'body': \"There's no real way to compare them.  I'd assume that Cell is significantly stronger when Ganos first transforms, but given enough time, Ganos could surpass him assuming that his strength would keep increasing. (hence why Roshi took him out ASAP)\", 'subreddit': 'dbz'}\n",
      "{'body': \"Well everything is entirely dependent on your situation. I just got done reading through this thread and a 3.9 GPA is extremely strong for a community college. Most CC's(Community College's) will have you take a placement tests and that will determine which class levels to take. In an ideal world you would have a good chunk of change saved up for life after the corps and use that to pay for CC Courses and you would crash at your parents place. Once your done with your Core Classes and begin your transfer to a University and start using your GI bill. If you apply for FAFSA you can also opt to do a work study which gives you a chance to work &lt;20 hours per week at the University as a side job. Being a MCWIS you should be able to land a life guard job no problem. \\n\\n\\nPaying for Community College with the GI Bill can seem like a waste if you can pay for it out of pocket. But if you dont have a choice then it can be a lifesaver.\\n\\nNot to get too personal but the more info you give about your situation the better we can help you out brother. For example if you dont have dependents then you will have a greater degree in freedom, what state your from/ plan to go to school in, or if Grad school is something your looking into for the future?\", 'subreddit': 'USMC'}\n",
      "{'body': \"Yeah that'd be the easier way to explain it. Didn't think to say it like that lol\", 'subreddit': 'nfl'}\n",
      "{'body': \"No problem. I would ask AFPC if it had to be signed by your commander. If they say yes, then you could talk to the IG or legal, I have no idea whom, but there should be a way around it. Your commander doesn't have to approve it, hence the approval/disapproval format of the memo. You just need his signature stating whether he does or not. \", 'subreddit': 'AirForce'}\n",
      "{'body': '143413367| &gt; United States Anonymous (ID: msELbNeP)\\n\\n&gt;&gt;143412250 (OP)\\nTrump\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'well he applied and will be working in Chicago proper so idk 😶', 'subreddit': 'television'}\n",
      "{'body': 'All 6 gear slots have to be classified. To unlock the 6th talent. Sorry to burst your bubble', 'subreddit': 'The_Division'}\n",
      "{'body': 'Alright, cool. And thank you for doing this for us poor IB kids! :D ', 'subreddit': 'IBO'}\n",
      "{'body': 'Great taste!  The General Lee is always awesome when done right!', 'subreddit': 'ATBGE'}\n",
      "{'body': 'They surely got way more money from SCR than what they invested into retexturing. They need to hire more people to work on this.', 'subreddit': 'starcraft'}\n",
      "{'body': \"When your low is 80 and your high is 100, opening windows at night may not be useful. If you have a two story house, the upper area will be warmer at least 10 degrees. Running fans while you're in the room will enhance cooling; you will want to turn the fans off when the room is empty, because it's wasting electricity. \\nWhich part of Texas are you in? I mainly lived in DFW, before I escaped. \", 'subreddit': 'HomeImprovement'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"That doesn't follow. The construction workers could work a little faster but they're still restricted by their materials.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Other than Watergate, Nixon was actually a pretty good president. ', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'NeckbeardNests'}\n",
      "{'body': \"Hi Guys!\\n\\nSorry to disappoint, but I'm not KD.\", 'subreddit': 'nba'}\n",
      "{'body': 'DELET THIS', 'subreddit': '4chan'}\n",
      "{'body': 'So yeah. Late last year I was travelling through Japan with some friends of mine (I tend to goto Japan for a few weeks every year) \\n\\nOn this occasion I met this lovely 38 year old woman named Hiroko at a cocktail bar in Kyoto. I noticed her looking my way a few times but I couldn\\'t tell if it was good or bad because, well I\\'m a bigger guy and in Japan it\\'s already a big disadvantage lol \\n\\nWith the power of a little liquid courage I managed to walk past her as I went to the to the bar and as I walked back I managed to make a little joke about the drink she had (wish I did in English, that could\\'ve been a waste of time lol) thankfully she saw the humor to it and asked where I am from \\n\\nSo we sat and spoke for a bit, found out she actually visited my city with her now ex husband a few years ago. So we already had some common ground. In the middle of chatting causally we were making cheeky jabs at each other too so I could tell there was something there. Her girlfriends arrived and so we exchanged Line IDs and went our seperate ways.\\n\\nNow I didnt think this would go anywhere, because I was going to Osaka the next day. But we spoke for a couple days on Line before she asked me straight up if I wanted to spend some time with her on the weekend. Of course I jumped at the opportunity and booked us an Airbnb.\\n\\nShe caught the train in from Kyoto and she was wearing this cute little outfit that and a little bow tie lol but she made it look cute. We headed back the Airbnb where wow it had an incredible view of Osaka Castle. \\n\\nWe sat down on the couch to rest our feet and she didnt waste much time. She told me she was excited because she had never been with a \"western man\" before (whether thats true or not, i dont give a shit) and went straight to unzipping my pants and reaching into grab my cock. With a smile on her face she looked and me and said \"very big\" (now for record, she\\'s clear fan servicing. Im not overly big in that area at all. Probably just abit over average in length, pretty girthy though)\\n\\nShe told me stand up and when i did she ripped my pants down and got to her knees infront of me and started service my cock. Her gorgeous dark eyes staring up at me while she tried her best to deep throat my cock was mind blowing. Not only this, she was very attentive to my sensitive spots and would torture me with them and giggle at the sight of me shaking. \\n\\nAfter I came I was hard again a few minutes later and ready to feel her little Japanese pussy. She layed out on the bed and I got the tip of my cock slightly in before she pushed me back and told me no! No..no? I couldnt understand, she then smirked and stood up and whispered in my earth \"you must wait\" she got dressed and told me to get my clothes on too because she wanted to have dinner.\\n\\nI stood there with my throbbing cock, confused and aroused haha this woman was teasing the fuck out of me and I loved it. So we went back out for a couple of hours and had dinner before heading back to the airbnb. The whole time all I could think about we fucking her brains out, I think this is exactly what she wanted.\\n\\nWe finally get back. She has a shower straight away and tells me to have one after her. After mine, i come out and see her on the bed wearing thigh highs and nothing else. They were pinching in her skin ever so much that it caused a slight dimple in the skin, so hot!\\n\\nShe sits down on the bed and gestures me over and stops me when im leaning down to kiss her. She puts her hand on my head and pushes me down so i am level with her pussy and demands me to eat it and you bet I did. Her pussy tasted incredible and I could tell from sliding a finger into her pussy that it was gonna be tough when my cock was going to enter.\\n\\nAfter willingly complying to her demands and eating her pussy she told me that i had waited long enough and now i was allowed to enter her. We started with her on her back and her holding her legs back. It took abit of work and some lube, but that moment of penetration where her eyes shut tight, mouth gasped, head back and hands gripped the blanket beneath her is an image and feeling i\\'ll never forget.\\n\\nI had to work her tight pussy slowly to begin with though and as she became more used to my girth she started to push against me and as my cock pushed a little further in each time she would let out a sigh. When we finally worked it in and we could build up a good rhythm it was amazing. This gorgeous Japanese woman taking my cock and shaking and one put she was moaning so hard i could see a vessel form on her forehead before she finally gasped for air and let go of the blankets.\\n\\nAfter seeing her let go i decided to lean down and scoop her up. Her legs dangling over my arms and i held onto her cute little ass. She put her arms around my neck and held on while I pounded fuck out of her pussy but she could only handle this for a couple of minutes because it was beginning to hurt.\\n\\nSo I placed her back onto the bed on all fours and teased her pussy and clit from behind with my cock before slowling re-entering her incredible pussy. With her thrust in she would push back. Her ass was the tight little thing but enough meat for me to grab a handful with each hand and pound away her. \\n\\nThere was a moment I looked up and realised how cool everything was about this too as i looked out the window and saw osaka castle lit up and i stopped for a moment, still inside her and told her to look too. She shared the same enthusiasm of how cool this experience was. We went back to push each others sexual limits and fucking like animals for the rest of night.\\n\\nWe lazed around in bed together the next morning before we had a couple more rounds of sex and then check out. We got brunch. Went to some cool bamboo forest and shrines. We said our goodbyes and that was that. We still keep in touch but we are both busy. Hopefully when I\\'m in Japan next we\\'ll be able to hook up again :)', 'subreddit': 'AsiansGoneWild'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnocfl4/):\\n\\nI'm not sure, but I'm glad they're trying to do it. \\n\\nFun fact: I got to feed giraffes once and I cried\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"Wasn't there a thread with the opposite happening? The poster's dog kept attacking some other guy's dog, and that owner kicked the attacking dog and the poster was indignant that someone would dare to try to prevent his dog from attacking another one, because it was just playing or whatnot.\\n  \\nAnyway, the reality is that while dogs are great, other owners aren't. They often let bad behavior go on and sometimes even encourage it.\\n\\n \", 'subreddit': 'StLouis'}\n",
      "{'body': \"I mean it's just as nice during the day as well.\", 'subreddit': 'Scotland'}\n",
      "{'body': 'Mercy!!!', 'subreddit': 'BBWGW'}\n",
      "{'body': 'That is an immoral act and should be censored to protect the children.', 'subreddit': 'gaming'}\n",
      "{'body': 'At least they should have made it an option to rub two sticks together to make a fire.', 'subreddit': 'stalker'}\n",
      "{'body': 'Another scene that really pays homage to it is when the super battle droids take Fives from the republic shuttle in season 6. Such a great scene.', 'subreddit': 'StarWars'}\n",
      "{'body': 'I just said the same thing about this sub', 'subreddit': 'thingsforants'}\n",
      "{'body': '#**SportsHD**\\n\\n\\n📽**HD** [Mariners at Angels | Home | English | ad 1 | 📲 Mobile: Yes](http://sportshd.me/mlb/492512/h)\\n\\n\\n📽**HD** [Mariners at Angels | Away | English | ad 1 | 📲 Mobile: Yes](http://sportshd.me/mlb/492512/a)\\n\\n\\niOS Uncertain', 'subreddit': 'MLBStreams'}\n",
      "{'body': \"Man, that's rough. \\n\\nI cringe that they're drinking while on medication. That's extremely risky behavior with antidepressants because you're giving two types of mood changing drugs. I'm on antidepressants and am now abstaining from alcohol. I don't know anything about stroke meds but I can't see that being a greenlight on consuming ethynol, which is the scientific name of alcohol.\\n\\nI would bring it up very delicately and with a heaping amount of support. An unexpected job change after 30 years is a very stressful thing to happen. It's right up there with school, marriage, and funerals. Being depressed is hard and requires a lot of unrequited support.  Try doing activities or planned days where you spend most of the day together. \\n\\nEven with the best intentions they aren't going to change unless they want to. \\n\\nA local clinic or health center could have listings for a support group for family of alcoholics anonymous. \", 'subreddit': 'relationships'}\n",
      "{'body': 'This content brought to you from \"Spain Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#off site feed \"Spain Pool\")\\n', 'subreddit': 'ImagesOfSpain'}\n",
      "{'body': \"Well, yeah that's how it's normally done lmao. There are certain people known to have really good combinations of gut flora and no health issues and they are paid handsomely for their poop. It's all been extensively studied for health risks and all that before they let it loose on the public\", 'subreddit': 'todayilearned'}\n",
      "{'body': 'i payed 25 for mines and depends on what crates they are', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'The CE of Shirou, Rin, and Sakura at their full potential is a good sight to see', 'subreddit': 'grandorder'}\n",
      "{'body': \"Ahhh because it's Solo, ie doesn't need to be attached to anything else or online. Thank you. \", 'subreddit': 'NBA2k'}\n",
      "{'body': \"You seem to be a bright guy and I don't mean to insult your intelligence, but have you verified game cache through Steam on the off chance a spider file went missing? Long shot, but...\", 'subreddit': 'skyrimmods'}\n",
      "{'body': 'Thank you. Good that at least some people know this.', 'subreddit': 'runescape'}\n",
      "{'body': \"Please I really need to find this.\\n\\nI'm looking for a doujin where There's a guy that tells his friend he's going to confess to a girl. Then the guy is alone in a coffee shop and the girl is the barista. He orders a coffee and tries to make small talk, from time to time the girl goes to the kitchen were he can't see her.\\n\\nWe see that the girl goes out from the back exit and has sex with the guy's friend. She knows that the guy wants to confess to her and she wants to be with him. The friend comes inside her so he uses an special cork-like thing to close her pussy so the semen doesn't come out, he promises to remove it when the guy confesses.\\n\\nAs time passes, the guy doesn't confess, the girl keeps coming out and the friend fucks her ass.\\n\\nIn the end she can't take it anymore and she leaves the guy hanging asking the friend to remove the cork and fuck her.\\n\\nThe doujin ends with the friend lamenting that the guy never confessed, and that he would definitely enjoy her while the girl is filled with semen on all holes and discarded on the ground.\\n\\nThe doujin also had a mini 2 pages story at the beginning where the girl has sex with the friend in a fitting room at a clothing story. \\n\\n\", 'subreddit': 'doujinshi'}\n",
      "{'body': 'I literally finished it yesterday. I really liked it', 'subreddit': 'teenagers'}\n",
      "{'body': 'Nty dude. Not a fan of Helios. ', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"LED, resistor(s), momentary switch, recharge port, soundboard, speaker, wire.\\n\\nCheck out the custom saber shop they'll have everything you need, but they only carry plecter labs soundboards, NEC is the other main soundboard brand and they have their own site. \\n\\nWhat kind of budget are you playing with?\\n\", 'subreddit': 'lightsabers'}\n",
      "{'body': \"Makes sense. Whatever works! It looks like a freaking beast and I'm hopeful I'll give it a shot someday - I'll just lust after you alls for now :)\", 'subreddit': 'vaporents'}\n",
      "{'body': 'When we switched email systems at work one of my employees emailed me asking for instructions on how to send an email. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Then maybe they need better advertisers?', 'subreddit': 'news'}\n",
      "{'body': 'What can i do about that?', 'subreddit': 'Amd'}\n",
      "{'body': 'A brain filled only with Reddit comments and left wing media headlines.', 'subreddit': 'milliondollarextreme'}\n",
      "{'body': 'Brock Lesnar.\\n\\nBoom', 'subreddit': 'todayilearned'}\n",
      "{'body': 'Do you wanna see spiders? Man get some 1p if your getting RCs ', 'subreddit': 'Psychonaut'}\n",
      "{'body': 'Thank you.', 'subreddit': 'bloodborne'}\n",
      "{'body': 'i also kinda agree , but mostly becouse i played like 4 hours of the vn , and it felt like not that intersting . So if HF is the Good part , and you needed to play like 50 hours to get there , you want to hide from yourself that you wasted that much time , so you go all the way in the  fate fan rute  and watch and play everything fate (that i actually admire like its so vaste that knowing all the stuff is like a really hard achivment ).\\n\\nBut yeah im talking as a person that doesnt really like battle royales , nor likes sabers disgns , so im totaly not the target XD ', 'subreddit': 'anime'}\n",
      "{'body': 'Looks like the kind of place people get murdered in in movies.', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'crash vs ratchet?\\n\\nwhich one?\\n\\ni have no idea what the objective is on either of them.', 'subreddit': 'PS4'}\n",
      "{'body': 'Money in the back of their phone?', 'subreddit': 'UnethicalLifeProTips'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'I think you should post to r/sex. Also, I am in the same situation as you so if you find any good advice let me know!', 'subreddit': 'relationships'}\n",
      "{'body': 'You have proof? ', 'subreddit': 'trashy'}\n",
      "{'body': \"That's absolutely true :/\", 'subreddit': 'fountainpens'}\n",
      "{'body': '[deleted]', 'subreddit': 'teenagers'}\n",
      "{'body': \"And the best part?  Ken Penders apparently intended to use the crossover to launch his own series, *The Lost Ones*, which bombed and has yet to get past its first issue.  \\n\\nI can't quite remember the exact details, but it involved people getting superpowers from the atomic bombing of Hiroshima.  \", 'subreddit': 'SonicTheHedgehog'}\n",
      "{'body': '[deleted]', 'subreddit': 'TeenMomOGandTeenMom2'}\n",
      "{'body': \"It's always top news. The fact that republicans freaked out doesn't mean anything, because then a republican does something, and the democrats lose their shit. \\nThings haven't changed; that's how the news operates. \", 'subreddit': 'cringepics'}\n",
      "{'body': '25 lb ≈ 11 kg\\n\\n^metric ^units ^bot ^| ^[feedback](https://www.reddit.com/r/metric_units/comments/73edn2/constructive_feedback_thread/) ^| ^[source](https://github.com/cannawen/metric_units_reddit_bot) ^| ^[hacktoberfest](https://www.reddit.com/r/metric_units/comments/73ef7e/contribute_to_metric_units/) ^| ^[block](https://www.reddit.com/message/compose?to=metric_units&amp;subject=stop&amp;message=If%20you%20would%20like%20to%20stop%20seeing%20this%20bot%27s%20comments%2C%20please%20send%20this%20private%20message%20with%20the%20subject%20%27stop%27.%20If%20you%20are%20a%20moderator%2C%20please%20go%20to%20https%3A%2F%2Fwww.reddit.com%2Fr%2FproED%2Fabout%2Fbanned%2F) ^| ^v0.11.2', 'subreddit': 'proED'}\n",
      "{'body': \"Two wrongs don't make a right. And it's worse when it's done deliberately. It's not so much the act itself but the reason behind it. People that get paid millions of dollars a year to play a sport that's only played in the US, supported by tax payers, and are examples of how far liberty and freedom can get you if you work hard and work well with others, and then protest against that country doesn't really offend me, but it is really contradictory and hypocritical. All those guys could instead declare that all their pay goes to support poor communities. Instead they just want to feel special without actually sacrificing anything. \", 'subreddit': 'UnpopularOpinions'}\n",
      "{'body': \"Its his own fault.  He said yesterday when he started the stream that he had woken up at 9AM that morning and hadn't done shit and how he had broken phones and stuff (use that time he didn't do shit/went to whataburger with Greek and go get your phones fixed maybe?) so thats on him for not doing shit about it.  Teradek trash?  Why hasn't he gone to Irvine again and seen what was up with his setup?  He just keeps making excuse after excuse for scuffed content/not streaming and you normies just eat that shit up.  I call a spade a spade and this dude is a fucking victim-card playing idiot.\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"I didn't realize I had this for free after already owning the UC4 digital deluxe version \", 'subreddit': 'consoledeals'}\n",
      "{'body': \"That's very helpful thank you. There's a lot of peer pressure to get this game, but I'm on the fence because I'm neither into golf or RPGs. I haven't read proper answers about the RPG side of things until now. And I do realize that a lot of good stories and gameplay hide behind themes and subject matter I'm not a fan off like sports, but this game looks really charming and delightful. Now the only thing keeping me from buying it is the need to finish Thimbleweed Park and Shovel Knight.\", 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'This is more than just lines.', 'subreddit': 'MLS'}\n",
      "{'body': 'Your offense only put up 31. Most of your points came from giveaways by Indiana on their side of the field. Only ~~two~~3 of your point scoring drives were longer than 50 yards. \\n\\nEdit: it was 3 scoring drives of 50+. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Go get em!!!', 'subreddit': 'CFB'}\n",
      "{'body': 'JRR Tolkien. Gandalf, Aragorn, Frodo, Bilbo Baggins, Gollum...', 'subreddit': 'books'}\n",
      "{'body': 'Thanks for the answer. Makes sense to me. I\\'m in engineering, so my use of terminology is definitely different from how scientific fields use it, and there\\'s differences even among fields of science. I was thinking that \\'data\\' is almost like a unit of measurement. How much data supports a proposition/theory is often a measure of its likely accuracy. So in this way I wish it were standardized so we could use data vocabulary to communicate how much confidence we have. Saying \"we have datum supporting it\" should be higher confidence than \"we have data supporting it\". But obviously context would make this apparent most of the time. Anyway, good answer.', 'subreddit': 'askscience'}\n",
      "{'body': 'Main saucey ingredients\\n\\nhttps://imgur.com/a/Bsgob\\n\\nhttps://np.reddit.com/r/MakeupAddiction/comments/73geqz/first_post_subculture_apocalypse/\\n\\nSrs, This was quite.. a look \\n', 'subreddit': 'muacirclejerk'}\n",
      "{'body': 'I got a lot of good advice when I asked a few months ago.  [Link](https://www.reddit.com/r/Lightroom/comments/5laag8/transferring_to_a_new_laptop_nows_the_time_to_do/)', 'subreddit': 'Lightroom'}\n",
      "{'body': 'A reference to the new dropship troop in bh I suppose', 'subreddit': 'ClashOfClans'}\n",
      "{'body': \"Not updated as of yet. GOG's really slow to greenlight updates though; it could quite possibly be several weeks.\", 'subreddit': 'Games'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': 'Hahahahaaha', 'subreddit': 'nba'}\n",
      "{'body': 'Thanks', 'subreddit': 'summonerswar'}\n",
      "{'body': \"Ditto on the wall/sun comment. Even your latitude will have limited sun in winterPlant your tallest vegetables nearest the wall to give the shorter ones more rays. As for what you plant, plant what you'll eat! Getting excited about eating the stuff is what gives you the energy to maintain it. \", 'subreddit': 'gardening'}\n",
      "{'body': 'There are multiple ways it could have been made, but let\\'s even pretend it was something as \"advanced\" as silk screening. The way people keep whining about this you\\'d think they thought silk screening process was this crazy, futuristic technology. Try visiting one of these shops: it isn\\'t. And even what the good ones do can be done on the cheap if wanted -- this shirt is block white letters. It\\'s like being mystified over someone drawing a poster with a sharpie.\\n\\nI\\'m not sure why it\\'s beyond belief that some person who owns a shirt making business took the time to make this because he thought it\\'d get someone\\'s attention. It was probably something the creator thought he or she could do with their skillset. And so what? I know you don\\'t actually care about this and are deflecting from the real issue, but seeing this argument for the 100th time is mind-numbing.', 'subreddit': 'politics'}\n",
      "{'body': \"&gt; Dennis told The Sumter Item that people think the church which was founded by Scotch-Irish settlers in 1759 is haunted, but he doesn’t know why.\\n\\nHe doesn't know why people think that the **Salem Black River** Presbyterian Church is haunted?\", 'subreddit': 'Christianity'}\n",
      "{'body': \"Yea it isn't the best for durability but it works!\", 'subreddit': 'Breath_of_the_Wild'}\n",
      "{'body': 'https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/', 'subreddit': 'shittyfoodporn'}\n",
      "{'body': 'He was running like a damn good back last night. A little bit of flashes of Helu.', 'subreddit': 'Huskers'}\n",
      "{'body': 'First avenue has room for improvement....', 'subreddit': 'pittsburgh'}\n",
      "{'body': '143415817| &gt; United States Anonymous (ID: nN0OTd4c)\\n\\n&gt;&gt;143412250 (OP)\\n\\n2008: Primary - Ron Paul | General - Nobody\\n2012: Primary - Ron Paul | General - Nobody\\n2016: Primary - Bernie Sanders | General - Hillary Clinton\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Up until this post I had thought the X and the 8 were the exact same width screen', 'subreddit': 'apple'}\n",
      "{'body': 'Three spots, 42, 35 and 16', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'True to some extent.  But for default costumes, it might be better off to blend them now if you only have 1-2 of them (will depend how many costumes you already own).  If you have 3+, probably better off to keep them', 'subreddit': 'marvelheroes'}\n",
      "{'body': \"This is the top comment, so I'm going to apologize here because I just want views.  This was a breakdown I did of this a while back  It still works.\\n\\nhttps://www.reddit.com/r/nba/comments/6x7t89/a_rant_on_russell_westbrook/\", 'subreddit': 'nba'}\n",
      "{'body': '292 hunter, psn: chemicologist', 'subreddit': 'Fireteams'}\n",
      "{'body': 'All good things take time, my son...', 'subreddit': 'Gamingcirclejerk'}\n",
      "{'body': '[+AxezCore](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoku7m/):\\n\\nDid you know swans can be gay?', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'jesus that price on the tt... y dont i have keys', 'subreddit': 'GlobalOffensiveTrade'}\n",
      "{'body': \"For you kids who don't remember, Grant Roberts smoked some weed in the Shea Stadium parking lot in 2002 or so. Also, the Mets had a press conference at Shea Stadium for Mike Piazza to address those gay rumors.\", 'subreddit': 'NewYorkMets'}\n",
      "{'body': 'I hope to be there by 1:30-2', 'subreddit': 'KansasCityChiefs'}\n",
      "{'body': 'Romans bugger off,\\nSaxons and Danes arrive then stab each other until only one Saxon kingdom remains, Decides to call this England, Gets a fancy hat.\\n\\nFrench vikings decide fancy hat would look better on them, Become kings of England.\\n\\nLast welsh kingdoms decide now would be a great time to attack (I mean what could go wrong)\\nOnly border with Scotland left on island, England and Scotland then proceed to try kill each other for the next few century\\'s.\\n\\nEngland runs out of people to wear fancy hat so Scottish king gets both. Procced to have civil war, Get Totally not dictator for life \"Lord protector Cromwell\" he bans fun, When he dies we decide kings aren\\'t that bad and some dutch guy comes to wear the fancy hat.\\nHas no children so we import Germans.\\n\\nTry to take over planet starting with America, America escapes.\\nTry to take over the planet again this time much more successfully.\\nFrench midget annexes Europe, we ruin his day, Twice.\\n\\nSteal everybody\\'s stuff for a century without much bother, Germans unite finally and decide world would be better with them in charge (I mean what could go wrong) we get colony as spoils.\\nGermans decide again they should rule world, we laugh, the french laugh, the Russians laugh and then the Germans kick all our arses. America saves the day but say \"no empire any more\"\\n\\nWe lose the empire really quickly, then go broke.\\nJoin this new European trading thingy get rich again. then a few decades later leave again', 'subreddit': 'europe'}\n",
      "{'body': 'HOLD THREE, THE SASKATCHEWAN SPINNING NERVE HOLD\\n\\n\"Fucking Ridiculous\" Cried Tony Ferguson.', 'subreddit': 'MMA'}\n",
      "{'body': 'Sent invite. ', 'subreddit': 'Fireteams'}\n",
      "{'body': 'I grabbed the red rye today. Gotta say it’s pretty enjoyable. Might have to grab some more before they sell out!', 'subreddit': 'njbeer'}\n",
      "{'body': '1.  782 (TransUnion on Credit Karma), 776 (Equifax on Credit Karma)\\n\\n2. Chase Sapphire Reserve (1/17), JetBlue (no annual fee)\\n\\n3. Currently w/ 15k AA miles; 100k UR; 10k Jetblue\\n\\n5. Flying out of BOS, but LA starting in December\\n\\n6. Tokyo, Hong Kong, SE Asia\\n', 'subreddit': 'churning'}\n",
      "{'body': \"Did you order on Amazon?  When were you given a shipping estimate and all that?  My estimated delivery date is monday but I'm past the estimated ship date now :(\", 'subreddit': 'Amd'}\n",
      "{'body': 'that spider is terrifying', 'subreddit': 'natureismetal'}\n",
      "{'body': 'The interior of that vehicle is no more', 'subreddit': 'funny'}\n",
      "{'body': \"Who's 17? He looks a lot like Allan Houston\", 'subreddit': 'NYKnicks'}\n",
      "{'body': 'Climate change is a phenomenon that is studied by science. And the science on it is settled. Denying it is denying science. The only political aspect of it is how we should go about alleviating the effects of it.', 'subreddit': 'atheism'}\n",
      "{'body': '&gt; Xbox One\\n\\nY u do dis to me', 'subreddit': 'TexasRangers'}\n",
      "{'body': 'I saw a documentary called CROPSY on Netflix awhile ago and it really creeped me out. Not sure if it’s still there. Beware the Slenderman is also really unnerving. ', 'subreddit': 'horror'}\n",
      "{'body': 'See, this is why they all need to be hooked into this central Ai. So it can keep them and any infiltrators apart. Nothing could ever go wrong and no this is not a sinister plot to become an even more autocratic ruler. Now have the damn ship installed.', 'subreddit': 'Stellaris'}\n",
      "{'body': 'Like? Nope. More like LOVE!', 'subreddit': 'gonewild'}\n",
      "{'body': \"At that point I like to just do the homework in class so at least my time isn't a complete loss\", 'subreddit': 'EngineeringStudents'}\n",
      "{'body': 'Nah', 'subreddit': 'nba'}\n",
      "{'body': \"If you're focused on Beermoney by referrals, use RobinHood instead.  Their referral program is amazing. You and your referral each get a full share of stock like apple, ford, bank of america, sirius XM.  Also, no fees to cash out or trade.  No DRIP or partial shares though.\", 'subreddit': 'beermoney'}\n",
      "{'body': 'In my head canon it still happens before the actual best ending', 'subreddit': 'lifeisstrange'}\n",
      "{'body': 'Yeah I keep hearing that rumor I would like to see evidence but he does look much older.', 'subreddit': 'Boxing'}\n",
      "{'body': 'Yay! Small battles!', 'subreddit': 'vegan'}\n",
      "{'body': 'Hopefully this will help reduce the number of “we’ve reached” screenshot posts of memes in other subs. ', 'subreddit': 'freefolk'}\n",
      "{'body': 'Him and his adDicktions', 'subreddit': 'canucks'}\n",
      "{'body': \"That's silly and irrational \", 'subreddit': 'WatchItForThePlot'}\n",
      "{'body': \"Hey folks,\\n\\nPS4 on the west coast with a mic. 2 wins so far and own a house near retail row.\\n\\nLongwaywest is my tag. Feel free to add me if you want to play. \\n\\nLet's go bag some definitely not chicken dinners.\", 'subreddit': 'FortNiteBR'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol72o/):\\n\\nI did! I think its lovely ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"Oh god, see to be triggered is to be offended and filled with hate about something someone says. The only thing I'm doing is making fun of your retarded point of view. Also, good job ignoring the Asian thing which proved you unequivocally wrong. I'm done mocking you now, it's getting boring. Bye faggot! \\n\\nSorry to use another ad hominem attack! :(\", 'subreddit': 'opieandanthony'}\n",
      "{'body': 'Your comment has been removed because:\\n\\nGendered slurs are strictly scrutinized; please see our [gendered slurs policy guide.](/r/askwomen/w/genderedslurs)  \\nIf you edit your comment, let us know and it may be reinstated. \\n\\n\\n\\n**[Have questions about this moderator action? CLICK HERE!](http://www.reddit.com/message/compose/?to=/r/AskWomen&amp;subject=Why+was+this+removed?&amp;message=\\\\[My+comment\\\\]\\\\(https://www.reddit.com/r/AskWomen/comments/73i7xi/what_are_your_thoughts_on_the_idea_behind_male/dnqhbc1/\\\\)+was+removed+and+I+do+not+understand+the+reason+given+by+the+mod+who+acted upon+it.)**    \\n\\n\\n[AskWomen rules](/r/askwomen/w/rules) | [AskWomen FAQ](/r/askwomen/w/index)  \\n[reddit rules](http://www.reddit.com/rules/) | [reddiquette](http://www.reddit.com/wiki/reddiquette)', 'subreddit': 'AskWomen'}\n",
      "{'body': 'No because you are selling them for a loss', 'subreddit': 'personalfinance'}\n",
      "{'body': '[deleted]', 'subreddit': 'startrek'}\n",
      "{'body': 'This posting aroused my interest, alas there is no details. Gonegirl27 I want all the juicy details of your past debauchery ... please, please, please do share.\\n\\nJust kidding !!! ... I am more interested in the rock n roll. I know you are a big fan of the [Grateful Dead](https://www.youtube.com/watch?v=yOTWLpSUBM8) which other bands inspired you ?', 'subreddit': 'exjw'}\n",
      "{'body': 'Your post has been removed because it appears to be about a survey or poll, which belongs in /r/samplesize.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cars) if you have any questions or concerns.*', 'subreddit': 'cars'}\n",
      "{'body': 'Rowlet plushie fer sure yoo', 'subreddit': 'PokeMoonSun'}\n",
      "{'body': 'Always so dramatic.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Dabo looks like he'll change your oil right quick\\n\\nFuente looks like he'll be the +1 your rebellious daughter brings home at her second Thanksgiving Break home from college\", 'subreddit': 'CFB'}\n",
      "{'body': \"&gt;it ended up being added to the pool    \\n    \\nWow they were! I never noticed that before but I clearly see Brave Lyn listed under colorless. So I have hope for another Sacred Sparrow 2.  \\n      \\n&gt;And it never hurts to have dancers for arena assult    \\n    \\nFor sure since they're viable even at level 1. Chain Challenge missions too. Is stopping me from merging my 4x Olivia together at 4 stars.\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'A spot', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'Yeah good thing you choke In the FA cup', 'subreddit': 'nfl'}\n",
      "{'body': 'The video is over five minutes, so here is the mandatory brief description. Curtis explains that swapping far-left messages with far-right messages isn’t going to de-politicize comics or result in better stories. Instead it will only exacerbate the problem and result in more shitty, heavily politicized comics.', 'subreddit': 'KotakuInAction'}\n",
      "{'body': '[deleted]', 'subreddit': 'hockey'}\n",
      "{'body': \"It's ok Emmitt we all make mistakes and post being a Gaylord on the internet for sympathy points.\", 'subreddit': 'PS4'}\n",
      "{'body': 'Dammit', 'subreddit': 'nutrition'}\n",
      "{'body': 'For fucks sake this is like watching Wales Women trying to attack. Or just Wales, really.', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Goose bumps 👍', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': \"[If you want to watch in, there's a fairly recent miniseries about it.](http://www.imdb.com/title/tt2372220/)\", 'subreddit': 'history'}\n",
      "{'body': 'Growth rates have crashed in the western world but places like Ethiopia it has surged.', 'subreddit': 'HumansBeingBros'}\n",
      "{'body': '[deleted]', 'subreddit': 'AdviceAnimals'}\n",
      "{'body': 'i like the name a lot', 'subreddit': 'Cloak_Coin'}\n",
      "{'body': 'WHERE!?', 'subreddit': 'darksouls'}\n",
      "{'body': '143413305| &gt; Canada Anonymous (ID: 8h4XA49X)\\n\\n&gt;&gt;143413258\\nk sorry lol\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'It\\'s 2am here and I literally went \"OMG this is GENIUS\" in front of my computer', 'subreddit': 'magicTCG'}\n",
      "{'body': 'If Nantes could somehow finish first I would probably nut for 3 days straight.', 'subreddit': 'soccer'}\n",
      "{'body': '[removed]', 'subreddit': 'Tennessee'}\n",
      "{'body': 'Well as Moira would say: there could be no other end ', 'subreddit': 'ManyATrueNerd'}\n",
      "{'body': 'they changed it you cant get the gamma from the quest anymore ', 'subreddit': 'TarkovTrading'}\n",
      "{'body': \"Spectre has fallen and isn't nearly as desirable as it was. I value the like octane at a lab and honestly I value the halo around 2 slips right now. I'd probably take 20xx for both \", 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'did he eat it all?', 'subreddit': 'LivestreamFail'}\n",
      "{'body': '&gt; Rhopalogaster transversarium\\n\\n[NEAT!!!](http://mushroomobserver.org/observer/index_observation?q=9fXu)', 'subreddit': 'mycology'}\n",
      "{'body': 'This content brought to you from \"Argentina Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#off site feed \"Argentina Pool\")\\n', 'subreddit': 'ImagesOfArgentina'}\n",
      "{'body': 'The stock pickups are nice. They can be split into single coils and sound good both as singles and humbuckers. Plus with pickups, the higher the output, the less they clean up.', 'subreddit': 'Guitar'}\n",
      "{'body': \"As in a single 11x draw? Because if it were single pulls then that's incredible luck.\", 'subreddit': 'FFRecordKeeper'}\n",
      "{'body': \"Looking for a winner of Hey Arnold cast signing that doesn't want to attend or has an extra spot. I am willing to pay you handsomely for the trouble ! \", 'subreddit': 'NYCC'}\n",
      "{'body': \"Why don't you try it? There is a guide in my answer: https://blog.brave.com/loading-chrome-extensions-in-brave/\", 'subreddit': 'TREZOR'}\n",
      "{'body': 'Not at all. I enjoy being an Engineer and think playing games for a living would actually ruin gaming as a hobby for me. ', 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'true, my bad', 'subreddit': 'Warframe'}\n",
      "{'body': 'How appropriate that this was posted on conference weekend &lt;3', 'subreddit': 'AskReddit'}\n",
      "{'body': \" Should have definitley, but it's so common out here I considered it pointless. \", 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"Can you not please I'm in public I can't cry rn \", 'subreddit': 'nba'}\n",
      "{'body': 'Not in IB but in a similar program so I can relate\\n\\nMy friend just graduated IB and she says her class of 60 people in grade 9 turned into 15 people in grade 12, so good job for making it this far.\\n\\nThe only advice I have motivation-wise is to go somewhere where lots of people are studying and working. I like to go to a big library in my city and just sit there studying with everyone else until very late in the evening. I never get work done at home but I know lots of people like me find it easier to go out and study.', 'subreddit': 'getdisciplined'}\n",
      "{'body': 'That Cut-throat bitch! (and about 15 other juicy successful-series repeating character roles).', 'subreddit': 'psych'}\n",
      "{'body': 'bruh lmao ima have to try this', 'subreddit': 'Rainbow6'}\n",
      "{'body': \"Til how fucking toxic online dating is and why no one on Reddit has success.\\n\\nOh every profile is the exact same. Not like I haven't heard the same meme in every comment thread for years now.\\n\\nPeople afraid to open up and give shitty one word answers? Naaaah not on Reddit full of confident as fuck people all the time.\\n\\nPictures that don't really show what a person looks Like!? Jesus Christ not on my Photoshop free Reddit. \\n\\nHave some patients, first dates are nerve racking So will the first day or two of messages. People definitely aren't uploading their worst photos, who would have thought, every person you meet isn't some insanely original human when all they have to go off of is a paragraph? Say it ain't so. \\n\\nI get the joke, but y'all take it a little too far.\", 'subreddit': 'videos'}\n",
      "{'body': 'As a borderline Aspergers-type person, I have to say, the thing that keeps Aspies from being religious is a quality that EVERYONE should have more of, or at least learn it as a skill that they are able to turn on at will. ', 'subreddit': 'atheism'}\n",
      "{'body': '[deleted]', 'subreddit': 'anime'}\n",
      "{'body': \"Ikr. Like, damn, I'm gonna be SOOO pissed when I get killed because some dude had a 2 second quicker cooldown on his Star card..\\nAnd holy crap, I'm really gonna notice it when I get killed by a grenade from 5.5metres instead of 5metres...\", 'subreddit': 'StarWarsBattlefront'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Way better to have octogenarians running things, who still think the world can be run the same as if it's the same as it was when they were young- leaving a mess that they conveniently are too dead to have to deal with.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"[+Will_dance_for_bells](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokaxw/):\\n\\nThere's a drive through safari park here in the UK where I live. You can buy a box of food and the giraffes come and stick their heads in the car to get some eats.  It's so much fun. \\n\\nCongratulations on the book! Think my daughter will love it.\\n\\nEDIT: For those of you that asked it's West Midlands Safari Park in Kidderminster, UK. I haven't been for years but at one point went every birthday for a five year stretch. \\nThere used to be a monkey section too but they had to close it due to the monkeys royally screwing up cars.  \\n \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Academics are easy as long as you study. The nervousness will eventually start to go away. Good luck. ', 'subreddit': 'ProtectAndServe'}\n",
      "{'body': \"The sens are so irrelevant yet we all hate them \\n\\nThat's how you know they are trash. \", 'subreddit': 'Habs'}\n",
      "{'body': 'Day and a half', 'subreddit': 'AskReddit'}\n",
      "{'body': '#SELLER', 'subreddit': 'DirtySnapChatPals'}\n",
      "{'body': 'This is really high quality stuff.  From the shrieks of terror to the guy pointing at and yelling about his upside down sign this really has it all.  Shame the cops gave into the mob though.  Not a good idea.', 'subreddit': 'PublicFreakout'}\n",
      "{'body': 'What do you mean Marvel vs Capcom?\\n\\n...all I see is Ryu...', 'subreddit': 'ActionFigures'}\n",
      "{'body': \"I have had phone calls from Eastons in PA, MA, OH, and TX. We're located in Talbot County, Maryland and VERY OFTEN the SO will get phone calls from Talbot County, Georgia.\\n\\nAlso, I will trade you an Easton MD patch for an Easton MA patch... pm me if interested.\", 'subreddit': 'ProtectAndServe'}\n",
      "{'body': '[removed]', 'subreddit': 'GlobalOffensive'}\n",
      "{'body': \"I see. \\n\\nSFV is just too trash for me to go back. If they actually do SuperSFV maybeeeeee\\n\\nEveryone knew what Capcom was doing w/ SFV, and the casuals(well, most of them) didn't buy it. We're seeing Capcom double down on stupidity and ESPORTS greed w/ MVCI.\\n\\nI was soooooo done with the game when i realized alex was in the same tier(trash) he was in SFIII. All those alex avatars and alex hype and he can't be played. I'm not even a juri fan but it's a tragedy what they did to her - and a good example of some of the idiocy of capcom\\n\\nI hate hate hate hate hate hate how mika's v-skill is useless and like u said v-system is unbalanced. It's like Ultras but wayyyy worse.\\n\\nI'm not a real technical guy but someone put it well like u said about the v-system but also the defensive options in this game are so fucking low. \", 'subreddit': 'Kappa'}\n",
      "{'body': \"Most likely. He can check if he goes through transaction details. Not going to bother emailing PayPal, don't really know why that would be of help. \", 'subreddit': 'signupsforpay'}\n",
      "{'body': '[deleted]', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'MXPlayer, VLC or Kodi', 'subreddit': 'software'}\n",
      "{'body': 'Thats going to be the interesting part with the new patcher, not having to load 30+ almost daily its hopefully becoming more and more playable.\\n\\nWhat i personally wonder is chris statement on not wanting crashes every 5-10minutes for evocati, as i still think to recall that was exactly how 2.0 ptu was like.', 'subreddit': 'starcitizen'}\n",
      "{'body': 'kuh lick bait', 'subreddit': 'movies'}\n",
      "{'body': \"Perfect. You got a mic? And not a dick? Or you're a dick but funny? \", 'subreddit': 'Fireteams'}\n",
      "{'body': 'If questions are frequent, you can redirect inquiries to a FAQ rather than banning...', 'subreddit': 'megaIOTA'}\n",
      "{'body': 'Thanks mom.', 'subreddit': 'arrow'}\n",
      "{'body': \"**Off-Topic Discussion**: All top-level comments must be a story or poem. Reply here for other comments.\\n\\n#####Reminder for Writers and Readers:\\n* Prompts are meant to inspire new writing.  Responses don't have to fulfill every detail.\\n\\n* Please remember to [be civil](https://www.reddit.com/r/WritingPrompts/wiki/rules#wiki_rule_10.3A_be_civil) in any feedback.\\n\\n---\\n\\n[](#icon-help)[^(What Is This?)](https://www.reddit.com/r/WritingPrompts/wiki/off_topic)\\n[](#icon-information)[^(First Time Here?)](https://www.reddit.com/r/WritingPrompts/wiki/user_guide)\\n[](#icon-exclamation)[^(Special Announcements)](https://www.reddit.com/r/WritingPrompts/wiki/announcements)\\n[](#icon-comments)[^(Click For Our Chatrooms)](https://www.reddit.com/r/WritingPrompts/wiki/chat)\\n\", 'subreddit': 'WritingPrompts'}\n",
      "{'body': \"I'm in the same boat as you. I really like a lot of things about the game, but the content and complexity are seriously lacking. I feel like everything it's lacking could be added later. Crafting is really the only thing that needs serious rework. \", 'subreddit': 'MMORPG'}\n",
      "{'body': '[deleted]', 'subreddit': 'baseball'}\n",
      "{'body': '[deleted]', 'subreddit': 'CFB'}\n",
      "{'body': \"R10 per week? What a lucky little person you were! I'd get R2 when my dad had some extra cash to spare. Then I couldn't even afford a packet of lays and would need to settle for nik naks or fritos 😒\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Is that warning label basically saying \"stop now\"? Seems kinda blunt haha. ', 'subreddit': 'cigars'}\n",
      "{'body': 'We had them when I lived in Costa Rica. They would climb up on our porch to eat our potted plants, run around on roofs and in the gutters scaring people. I had a couple climb in my windows and I would have to shoo them out. they bite. The locals call them tree chickens.', 'subreddit': 'funny'}\n",
      "{'body': 'Mocha Uson', 'subreddit': 'Philippines'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': \"I don't really trust the save feature on the app. So I'm replying to you\", 'subreddit': 'ImGoingToHellForThis'}\n",
      "{'body': 'if king takes - Nb6\\n\\nif queen takes -  Ne5 dxe5 Ne5\\n\\nif king to e4 - Nf6\\n\\nif king to c2 - Ne3', 'subreddit': 'chess'}\n",
      "{'body': 'Yea i know what you mean, but if they did do something stupid like that regarding your pay, it would be lawsuit time. ', 'subreddit': 'PostmatesCouriers'}\n",
      "{'body': 'As a foreign it seemed as all the news were pro Hillary', 'subreddit': 'worldnews'}\n",
      "{'body': 'There is a whole system in place that forces farmers out of business unless they use big pharma services. It’s not like you are going to survive by producing small scale organic when a neighboring farmer gets all the financial benefits by producing allot through the use of chemicals. Teaching farmers is not going to do anything, it is the consumers who need to be thought.', 'subreddit': 'Monsanto'}\n",
      "{'body': \"143413258| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413069\\npls don't assume my gender.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol6ic/):\\n\\nI might have to visit where you live then. . . \\n\\nThank you! I hope she does :)', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I'm in interested in it, but it's not cheap fosho. Is it a textbook? \", 'subreddit': 'twinpeaks'}\n",
      "{'body': 'If you click the link, there is a video of him saying it.', 'subreddit': 'worldnews'}\n",
      "{'body': 'one was yellow stairs and the other was by white van. none of them were near me.', 'subreddit': 'Rainbow6'}\n",
      "{'body': 'Can count the dozens of times ENFP and INTJ get back together, and can count the dozens of times they never get back together, and I can count the dozens of times where they get back together only to break up again.\\n\\nI am not exaggerating.', 'subreddit': 'ENFP'}\n",
      "{'body': 'Include bendy and have the title be a joke ', 'subreddit': 'BendyAndTheInkMachine'}\n",
      "{'body': \"I never understood that until now. The game and DLC are the greatest I've ever played in my life \", 'subreddit': 'shockwaveporn'}\n",
      "{'body': '[deleted]', 'subreddit': 'Volkswagen'}\n",
      "{'body': 'Depends on the crate really.\\n\\nWhat do you have?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'It populates to your account automatically for some reason without a download. So it’s there, but for some reason doesn’t need to download. Was wondering about this for the longest time.', 'subreddit': 'forza'}\n",
      "{'body': 'WARNING: \\n\\nLower the volume! Sound effects are loud and may cause jumpscare. I do not intend to jumpscare you. Sorry about that.\\n\\n\\nABOUT:\\n\\n\\nTank and Camo are having a serious money problems. They have no choice but to rob a bank. Will they succeed?', 'subreddit': 'Diepio'}\n",
      "{'body': 'screw FCC, its all about Miami FC ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': '[deleted]', 'subreddit': 'comedynecromancy'}\n",
      "{'body': 'One random spot please', 'subreddit': 'edc_raffle'}\n",
      "{'body': '\"What are Saturdays for?\"\\n\\n\"The boys!\"', 'subreddit': 'hockey'}\n",
      "{'body': \"Saved for later. You're super smokin hot baby\", 'subreddit': 'gonewild'}\n",
      "{'body': \"&gt; With favorites ready, guard your side—\\n\\nGuys, it's been confirmed, all of our favorites are coming! That means literally everyone gets a spot! All well over 1K characters!\", 'subreddit': 'fireemblem'}\n",
      "{'body': \"no, but it also doesn't make it the wrong way either.\\n\\n\\n\\nif I can shoot the 3pt at 40% success rate with good release, what the he'll does my attribute number really matter? like who cares if it was 10, if I can shoot at 40%  it doesn't matter if that number is 19, 50, 200, 5656.\", 'subreddit': 'NBA2k'}\n",
      "{'body': \"TGT. I got legend two times, but only because usually I get to rank 5 and then stop playing on ladder because I prefer arena. I'm confident that I could reach legend almost every month if I would like to\", 'subreddit': 'hearthstone'}\n",
      "{'body': 'Yeah and all muslims are terrorists. The pot calling the kettle black. Come on..', 'subreddit': 'AskAnAmerican'}\n",
      "{'body': '0.5 PPR standard\\n\\nMartavis Bryant @ BAL\\n\\nWill Fuller vs. TEN\\n\\nJason Witten vs. LAR\\n\\nDerrick Henry vs. HOU', 'subreddit': 'fantasyfootball'}\n",
      "{'body': '&gt;The RES spam tracker.\\n\\nIs it possible to learn this power?', 'subreddit': 'politics'}\n",
      "{'body': \"What's the song playing when he tests out the speakers? \", 'subreddit': 'apple'}\n",
      "{'body': 'I had no idea.  ', 'subreddit': 'Fallout'}\n",
      "{'body': 'https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/', 'subreddit': 'Prematurecelebration'}\n",
      "{'body': 'What do you mean, man, aghs is super good on Bloodseeker in almost all cases. ', 'subreddit': 'DotA2'}\n",
      "{'body': 'that was a misclick i meant to reply that to the tread lol', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Would you be interested in a straight trade for the Eventide TimeFactor? ', 'subreddit': 'letstradepedals'}\n",
      "{'body': \"No real explanation aside that he seems to play jump rope with his little sister a lot and ride his bike up and down a mountain everyday. He's got a lot of energy which also puts more spring in his step. \\n\\nAs to whether it's realistic to jump that high, I don't know. But he does at least seem to have strong legs due to cycling and always being physically active since middle school.\", 'subreddit': 'haikyuu'}\n",
      "{'body': 'I always try and pick up 1 cheap foil for my zada EDH deck.', 'subreddit': 'magicTCG'}\n",
      "{'body': 'If healers straight up out-healed the DPS, it would be a game of which support dies first. Overwatch would never work if healers did more HPS than damage did DPS.', 'subreddit': 'Overwatch'}\n",
      "{'body': 'That ship has sailed my friend. When the FDA’s swat teams started swarming buildings, weapons hot, to bust the unpasteurized milk syndicates we were already lost. ', 'subreddit': 'Libertarian'}\n",
      "{'body': 'Isn\\'t this the same question as \"Smart people, do you get frustrated at people who are slow to understand how things work?\" -- in which case the answer is, as always, \"it depends\".  I\\'m reasonably mechanically inclined.  Would I get frustrated at someone who saw the inside of a transmission and didn\\'t immediately understand it? No, of course not.  Would I get frustrated at someone who didn\\'t understand an Archimedes\\' screw after 2 hours of detailed explanation? Yes. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Safe on paper yes. For coin splitting ask around.', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'The next time you fill up your water bottle from the tap, receive something from usps, or use countless other services you can thank our mixed economy. http://www.investopedia.com/ask/answers/031815/united-states-considered-market-economy-or-mixed-economy.asp. \\n  \\n  \\nNo market type is flawless but ours has made the USA an absolute powerhouse. If not for our market we would very likely be behind China and Russia in many respects', 'subreddit': 'worldnews'}\n",
      "{'body': 'Well then... you provide the costume and let me know what you want :O', 'subreddit': 'csgo'}\n",
      "{'body': 'Good to know for the future! Haha', 'subreddit': 'badwomensanatomy'}\n",
      "{'body': \"Don't you still have to pay for the Nazi tattoo to begin with?\", 'subreddit': 'UnethicalLifeProTips'}\n",
      "{'body': \"He started acting very strange, hiding from us and seemed to be absolutely terrified of something. He wasn't eating, drinking, or going to the bathroom. When we took him in they noticed he was displaying unusual rapid eye movements and ran an MRI which displayed swelling. I've also been told some animals display this by placing their head against an object but we didn't experience that. Basically things went really wrong really fast. \", 'subreddit': 'aww'}\n",
      "{'body': \"I have four reasons why I cant use pams in this pack. \\n\\n* It's in literally every pack in the modded world. I use mostly mods that go unused or underrated.\\n* Recent pams changes made the game insanely hard.\\n* Pams is in a very transitional stage right now for balance and recipes. If I had it I would have to push updates like every 3 days.\\n* Pams adds too much for this type of pack. I'm trying to reach out to people that have not played mods before and having over 200 food items is a tad overwhelming.\", 'subreddit': 'Minecraft'}\n",
      "{'body': 'Your submission has been removed for the following reason(s):\\n\\n\\n* Rule 6 - No Fireteam, Friend Request, or Clan threads. Please use /r/Fireteams for generic requests and clan posts, /r/DestinySherpa for help teaching/learning the raids, /r/CrucibleSherpa for teaching/learning the Crucible, or the Team Up Tuesday Megathread instead.\\n\\n\\n\\n---\\n\\nFor more information, see [our detailed rules page.](http://www.reddit.com/r/destinythegame/wiki/rules)', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"It's on regular fox here.\", 'subreddit': 'CFB'}\n",
      "{'body': 'I wanna fuck her tits, but she need to stfu ', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'This season ash/IQ and ela/jager but I usually play what the team needs', 'subreddit': 'Rainbow6'}\n",
      "{'body': '[deleted]', 'subreddit': 'Inktober'}\n",
      "{'body': 'So weird hack at Best buy. If you buy the GoPro6 and three \"camrea\" accessories. You get 10% off each acsessorie. So Karma drone, Grip, and SD card count as three accessories. But you have to find them individually. Lol ', 'subreddit': 'gopro'}\n",
      "{'body': \"Did not excpect to see wongraven on this thread thats awesome, ive Been inte alot of at The gates, dissection,  Skitarg latley and i dont know if you can call skitarg deathmetal but they're awesome \", 'subreddit': 'CasualConversation'}\n",
      "{'body': '&gt;[**Tyler Joseph talking about song Blasphemy [1:40]**](http://youtu.be/Lpq53oR94JM)\\n\\n&gt; [*^JohnyBoy*](https://www.youtube.com/channel/UCkwxnLofimoJRF0DvGeErQw) ^in ^Music\\n\\n&gt;*^8,554 ^views ^since ^Jun ^2017*\\n\\n[^bot ^info](/r/youtubefactsbot/wiki/index)', 'subreddit': 'twentyonepilots'}\n",
      "{'body': \"She's hot AF\", 'subreddit': 'The_Donald'}\n",
      "{'body': \"Well what are they going to do? Wait until it's a good time ? They don't have a lot of options.\\n\\nI understand their decision and in many ways it's great life experience that is irreplaceable. But let's not pretend like they were only thinking of other people when they joined.\", 'subreddit': 'news'}\n",
      "{'body': 'Err dungeon. Thanks for the correction. I meant that. Any idea when that second one will occur?', 'subreddit': 'FFRecordKeeper'}\n",
      "{'body': 'Please refer to [this thread](https://www.reddit.com/r/gopro/comments/4wac7w/lets_talk_about_sd_cards_again/) for information and discussion regarding SD cards for GoPro\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/gopro) if you have any questions or concerns.*', 'subreddit': 'gopro'}\n",
      "{'body': \"To be fair though typology among other things appears as early as Genesis. The difference is in whoever is claiming to interpret said prophecy. If Matthew does not have any authority, then you pretty much can throw out almost of all Christianity. \\n\\nSecondly who knows maybe you could reinterpret it, but the reason they would get laughed at is because there would be reasons to go against that belief. People also did interpret Daniel, and Revelation for thinking the end times were in the 70s-80s because of the cold war. And actually that belief was a big reason the modern version of the rapture is still professed today, so they were not laughed at.\\n\\nIt's very important to look at the context behind the interpretations.\", 'subreddit': 'Christianity'}\n",
      "{'body': \"I think it wouldn't even boot without a gpu.\", 'subreddit': 'Amd'}\n",
      "{'body': 'Officially licensed by Capcom', 'subreddit': 'Kappa'}\n",
      "{'body': '2 randoms please ', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"When we talk the middle east, sadly we can't hold everybody to the western standard. So we have to look at it in context of brutal leaders who often imprison or kill  opposition. \\n\\n\\nYet you kinda further compounded the point of my post: you happily embrace things that are fully supportive of your view, and stop caring, or ignore everything you dislike. \\n\\n\\nI would seriously suggest recognizing that your side in anything may not be perfect, and that you need to take on challenging views otherwise you risk falling into the same trap that the FSA fell into where they became so full of hatred and willing blindness to the evil that claimed to support their revolution, that they allowed themselves to be so distanced from reality that none saw how the evil had long usurped their cause. \\n\\n\\nSo my question to you:\\n\\nIs it justified for you to be lied to, and to completely believe their lies, then stop caring about this the moment you find out the person who committed this murder was on the side that you support, and to disregard how people exploited your anger for their own gain? \", 'subreddit': 'syriancivilwar'}\n",
      "{'body': 'Thanks, me too.', 'subreddit': 'me_irl'}\n",
      "{'body': \"The problem isn't your personality, it's your husband. He doesn't pull his weight, complains when you do it, complains when you ask him to do it, and is an asshole when you point out the problem. And he's found a way to blame this all on you.\", 'subreddit': 'relationships'}\n",
      "{'body': 'It was made by PDP (Then known as Pelican) to interact with Rock Band. Every song made from Rock Band 1 through 2 plus all the DLC contains Stage Kit commands to go along with the song. Rock Band 3 actually did not support the stage kit at launch (due to it\\'s low sales), but a later patch added it back in, although I think all RB3 DLC and disc songs just had a sort of \"generic\" stage kit support based off BPM or something.\\n\\nThe reason it was 360 only is the stage kit re-uses the rumble information channel for a standard controller to send the stage kit controls for what it does. Since the PS3 did not support rumble at all at the time, it never got the stage kit.', 'subreddit': 'Rockband'}\n",
      "{'body': 'Unless your teacher said that before 1995, he was mistaken.  On 23 March 1995, a federal-level fatwa was issued saying cigarettes are haram.', 'subreddit': 'malaysia'}\n",
      "{'body': '**HD**|\\n[**AWAY MultiBitrate**](http://www.streams4sport.com/index.php?/topic/331-mariners-vs-angels-away/) | \\nMobile Compatible : Yes 📱 | Ad Overlays: 0 |\\n\\n**HD**|\\n[**HOME MultiBitrate**](http://www.streams4sport.com/index.php?/topic/330-mariners-vs-angels-home/) | \\nMobile Compatible : Yes 📱 | Ad Overlays: 0 |\\n\\nUpvote if you like .', 'subreddit': 'MLBStreams'}\n",
      "{'body': 'Bamboo House in Hollywood. Get there before 2am if you want to have a seat or table', 'subreddit': 'LosAngeles'}\n",
      "{'body': 'Ditto!! R1 is the truth', 'subreddit': 'FIFA'}\n",
      "{'body': 'I was thinking specialization within a class, not switching between classes entirely.', 'subreddit': 'warcraftlore'}\n",
      "{'body': '[Enemy Intel: The Power Of Play-Action (3:28)](http://www.philadelphiaeagles.com/videos/videos/Enemy-Intel-The-Power-Of-Play-Action/0787dc83-4832-45e2-a527-e38aa57ffa75)\\n\\nThe Chargers boast one of the best pass rush duos in the entire NFL. Greg Cosell shows us what makes them so good, and John Clark joins Mike Quick to explain how the Eagles can negate their effectiveness.', 'subreddit': 'eagles'}\n",
      "{'body': '[+AgaveNeomexicana](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokl00/):\\n\\nGet to those really delicious leaves just up there a little higher . . .  ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'He’s talking about being killed mentally .The pain from knowing he is just like his dad is killing him mentally. Edit: not sure if that made sense', 'subreddit': 'Slipknot'}\n",
      "{'body': '143412560| &gt; United States Anonymous (ID: xfBAecFs)\\n\\nI voted for DRUMPF and I hate myself every day for it.\\nIT WAS HER TURN\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Given that race is still the primary social divide in the world, we shouldn't point out when people have beliefs that our unusual within their group?\", 'subreddit': 'samharris'}\n",
      "{'body': 'this dude is in every single thread complaining about the sub', 'subreddit': 'deathgrips'}\n",
      "{'body': 'Playing as American on Argonne. Run into a squadmate capping B from the other direction and mine (or his) character goes,  \"Hey, good to see ya!\".', 'subreddit': 'battlefield_one'}\n",
      "{'body': 'Hehe. Thank you. ', 'subreddit': 'bisexual'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"This has been happening to me as well especially with Eagles and Phoenixes, and it's starting to get rather frustrating. I've been playing as Teclis.\", 'subreddit': 'totalwar'}\n",
      "{'body': 'Do you know which one?', 'subreddit': 'BigBrother'}\n",
      "{'body': 'We have a support group, if you need it', 'subreddit': 'CFB'}\n",
      "{'body': '[Found a dotard tweet!  Straight from the golf course.](https://twitter.com/AndrewOnSeeAIR/status/914227871010447360)', 'subreddit': 'The_Dotard'}\n",
      "{'body': '[removed]', 'subreddit': 'politics'}\n",
      "{'body': \"Great job. The deck looks sweet. I love Deeproot Waters and Shaper's Sanctuary as sideboard tech. So much resilience. I'm going to keep them in mind as the Modern UG brews develop.  \", 'subreddit': 'FishMTG'}\n",
      "{'body': \"so it is legit? BTW doesn't thunder road own john wick? since that is canon. So thunder road and starbreeze would be making it even if john isn't in it i am assuming.\", 'subreddit': 'paydaytheheist'}\n",
      "{'body': \"You want to go to a unit on a Defense Support of Civil Authorities (DSCA) tasking but if you're not already in one it would be hard to get reassigned to one in time to help with the hurricanes. \", 'subreddit': 'army'}\n",
      "{'body': \"He's allowed to protest however the heck he wants, so long as he doesn't break any laws (which he hasn't), but people are also allowed to call him out for being a fool who can't even articulate what he's actually protesting. Considering he wore cop-as-pig socks and made some ridiculous statements admiring Che Guevara, it's pretty clear he has absolutely no freaking idea what he's talking about. Some would say its a publicity play to go out as a martyr for social injustice when he wasn't good enough to justify his massive contract or hold on to an NFL job. He found a way to stay relevant. People forget that pretty much everyone in the SF locker room hated this dude before he ever did anything political, even going back to their 2013 SB run. Now he thinks he's some kind of MLK-style civil rights intellectual? Puh-leeze. \", 'subreddit': 'politics'}\n",
      "{'body': 'Its easy, just setup ports 80-10080 as open!! Security is for 🅱ussies', 'subreddit': 'techsupport'}\n",
      "{'body': \"I don't think I want to see your goggle history. \", 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': 'he gave up bc he’s a failure ', 'subreddit': 'teenagers'}\n",
      "{'body': 'Yeah, those two senators are reserved for 20 fucked up farmers in Wyoming.', 'subreddit': 'AdviceAnimals'}\n",
      "{'body': \"Durant can't lead a team by himself \", 'subreddit': 'nba'}\n",
      "{'body': \"&gt; I'm pretty hungover right now so I'll update this later.\\n\\nSo, how's the saving the booze money going? XD\", 'subreddit': 'entp'}\n",
      "{'body': \"When I feel like that with deads I switch it up for the day. I pull conventional but if I feel off then I switch to sumo or stiff leg. Or I'll do low weight high reps super slow to focus 100% on form.\", 'subreddit': 'bodybuilding'}\n",
      "{'body': 'Especially during festivities and holidays ', 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol7tj/):\\n\\nThis is a good answer, that's what I would be doing if I were a giraffe. \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'If I have an FLB Benedia at Lv150, which weapon should I drop? (For the f2p grid, specifically)', 'subreddit': 'Granblue_en'}\n",
      "{'body': 'fucking preach my nigga...', 'subreddit': 'HotlineMiami'}\n",
      "{'body': '[removed]', 'subreddit': 'nbastreams'}\n",
      "{'body': 'But even if you do a degree that has weight. You can\\'t guarantee everyone who goes on the course will pass, nor can you guarantee they will actually get a job.\\n\\nBesides. \"Pointless\" degree is very subjective.', 'subreddit': 'ukpolitics'}\n",
      "{'body': \"Spotted the no-nothing, never been outside of his own bubble, coastal city dwelling, liberal. \\n\\nKiddo, you don't understand logistics. \", 'subreddit': 'politics'}\n",
      "{'body': '[deleted]', 'subreddit': 'buildapcsales'}\n",
      "{'body': \"Shit man, I'm not even sure about almost 1300 games I'd want to try\", 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'First of all, Brigham Young NEVER said that.  Second of all, every Prophet and Apostle I know of has already stated how to gain a testimony.........by the Spirit.', 'subreddit': 'latterdaysaints'}\n",
      "{'body': '\"Oh 2.147m that\\'s not bad....wait a minute\"', 'subreddit': 'runescape'}\n",
      "{'body': \"Bias or something else? Cos I don't feel like they've been biased.\", 'subreddit': 'rugbyunion'}\n",
      "{'body': '☑ mucho\\n\\n☑ trabajo', 'subreddit': 'MLS'}\n",
      "{'body': \"#2 is the only one that looks like it belongs on a runway to me,  so that's my vote. \", 'subreddit': 'TaylorSwift'}\n",
      "{'body': 'When rebuilding StoreFront servers you must ensure to use the same Hostbase URL, same store path and the same SRID in order to avoid this particular issue with Windows Receivers.\\n\\nhttps://discussions.citrix.com/topic/357226-receiver-failover-between-storefront-server-groups/?p=1856858', 'subreddit': 'Citrix'}\n",
      "{'body': 'I do my best, lol. ', 'subreddit': 'IAmA'}\n",
      "{'body': 'The amount of pressure needed to create a star in a universe in which is, in the \"big freeze\" scenario, is trying to grow faster apart would be too high for anything to get done. Regardless of that, in the \"big freeze\" scenario, that are many dwarf stars that remain for - theoretically - trillions of years. We could build a Dyson sphere around that.', 'subreddit': 'Futurology'}\n",
      "{'body': '\"I thought you\\'d never ask! &lt;3\"', 'subreddit': 'worldnews'}\n",
      "{'body': 'Je suis enrhubé. Arrive bô à dormir.     ', 'subreddit': 'france'}\n",
      "{'body': 'Thank you heaps for the trade &lt;:\\n\\nTrade summary for rules:\\n\\n- Me: WIN2011 Suicune, Gedatzu -&gt; me\\n- You: SUM2013 Palkia + XY torchic, obtained by you', 'subreddit': 'pokemontrades'}\n",
      "{'body': '1. Nightcap \\n2. Quiet Nights or Pembroke\\n3. 1Q (had to throw an aro in there)', 'subreddit': 'PipeTobacco'}\n",
      "{'body': 'So is this the Binging with Babish, but for TV drinks instead of food? Nice!', 'subreddit': 'GifRecipes'}\n",
      "{'body': 'Guzan is a god', 'subreddit': 'MLS'}\n",
      "{'body': '[deleted]', 'subreddit': 'maryland'}\n",
      "{'body': '[deleted]', 'subreddit': 'Fishing'}\n",
      "{'body': \"Yeah he's one of the best (in terms of usernames and in shitposts)\", 'subreddit': 'NCAAFBseries'}\n",
      "{'body': \"Don't think so. Can't see any connection. Also that site is awful and not reliable.\", 'subreddit': 'CollegeBasketball'}\n",
      "{'body': \"I haven't actually seen any, I just want to. \\n\\nExcept a death count at the beginning of a Naked hunt against one.\", 'subreddit': 'MonsterHunter'}\n",
      "{'body': 'Breadman gave Murray a yeast infection. Murray better get used to it.', 'subreddit': 'hockey'}\n",
      "{'body': \"Depends on how patient I am willing to be. Usually I'm pretty lax about it and can explain. There are a few rarer times where frustration will set in pretty easily, generally if I'm already agitated by something or other.\", 'subreddit': 'AskReddit'}\n",
      "{'body': '# **[PC]** Paypal {Friends and Family}, Prices are in USD. \\n\\n# I also accept most cryptos (5% discount). \\n\\n#**[H]** 16 Accelerator Crates **[W]** $1 each! ($15 for all)!!!\\n\\n~~**[H]** Jager 619 RS **[W]** $7~~\\n\\n**[H]** Apex  **[W]** $2\\n\\n**[H]** Breakout Type-S/Dominus GT/Octane ZSR/Forest Green Breakout**[W]** $1 each\\n\\n**[H] Purple Thermal (Sweeper)  [W] $2** \\n\\n[H] Sky Blue Thermal/Crimson Unicorn Horn/Black Falco [W] $1 each\\n\\n[H] Turbine/Zomba/Looper/Roulette/Kalos/Photon/ [W] $0.75 each \\n\\n[H] **RLCS** Decals (Type-S, Dominus GT, Breakout)/ Mount Champ[W] $0.75 each\\n\\nIf you have any questions/requests feel free to add me on Steam!\\n\\n[**Steam :) :)**](http://steamcommunity.com/profiles/76561198184805093/)**|**\\n[**myrep :) :)**](https://www.reddit.com/r/RocketLeagueExchange/comments/52k9le/meta_successful_tradereputation_thread/da9vudm/)**|**\\n[**myrep2 :) :)**](https://www.reddit.com/r/RocketLeagueExchange/comments/5yzsc4/meta_successful_tradereputation_thread_20/dhcjm85/)\\n[**myrep3 :) :)**](https://www.reddit.com/r/RocketLeagueExchange/comments/6ywnpi/meta_successful_tradereputation_post_30/dmsopus/)\\n\\n', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'I\\'m a seamstress, do mostly costuming and reenactment garb. Being an empath in this line of work is incredibly helpful because most people know what they want, but have zero idea how to put words to the vision in their heads. Just through conversation  (in person, of course!), I can get a solid feel for my clients\\' costuming desires. They seem to think it\\'s magic that I \"just get them.\" :)  Oddly enough, I\\'m certain my empath abilities also translate into an ability to make things that fit perfectly the first time they try them on.\\n\\n(Just a note: I did *not* go to school for fashion design or tailoring. I\\'m self-taught.)\\n\\nAnyway, the point here is that your coworker doesn\\'t necessarily need to go into typical \"healer\" type work. If she enjoys working with her hands and being creative, there are jobs that involve working with the public *and* creative activities. Interior design, home remodeling, custom sewing, commissioned art... heck, even designer cakes. And, if her parents are dead-set on her going to college, these are all things that she can get an Associate or Bachelor degree in.\\n\\n(Re: student debt... tell her she can start her higher education at a community college for her \"basic\" courses and then transfer to a 4-year school. Saves a ton of money! I wish it had been an option 25 years ago!)', 'subreddit': 'Empaths'}\n",
      "{'body': \"Oh sorry eh but it does get a tad annoying ya know?  I know we're buds and all eh but sometimes it's really nice when someone asks if we're Canadian eh!  \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Indigenous rights, unless the indigenous people are white. ', 'subreddit': 'SocialJusticeInAction'}\n",
      "{'body': \"I don't think anyone can rock that\", 'subreddit': 'G59'}\n",
      "{'body': '143414041| &gt; None Anonymous (ID: pkH2Dye0)\\n\\n&gt;&gt;143412250 (OP)\\nWhat OP was really asking is:\\n\\nHow far up my asshole do you think the government can reach?\\n\\n&gt;2008: Deep\\n&gt;2012: Really Deep\\n&gt;2016: Deepest (lungs and heart deep)\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"How about not have the car instantly spin out whenmy tires touch the apron at like New Hampshire. Its cool for the 1.5 and platw tracks but it's game breaking for me at the smaller, flatter tracks when I'm trying to gain spots\", 'subreddit': '704nascarheat'}\n",
      "{'body': \"I'm not gonna give out details of where this happened, but a friend of mine would go see a movie and he would leave the outside door cracked ever so slightly. He smoked, so he was always taking a step out, propping it with something, smoking, and returning. He was caught once and they asked that he go out the front to smoke but he kept doing it.\\n\\nHe's in jail, but that's unrelated to my point.\", 'subreddit': 'UnethicalLifeProTips'}\n",
      "{'body': 'Baaaahahahaha.  $0-$100k....really nailing down that sugar daddy status.  5\\'2\" and an angry internet troll...someone will be lucky.', 'subreddit': 'casualiama'}\n",
      "{'body': \"Many of the local tool libraries have presses available for loan. I like the one at the Phinney Neighborhood Assoc. best, it's got an electric scratter and two baskets.\", 'subreddit': 'SeattleWA'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '*a new cracked game', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Before and after of your ass from being spanked would be hot', 'subreddit': 'gonewild'}\n",
      "{'body': 'Y siendo monotributista.', 'subreddit': 'argentina'}\n",
      "{'body': 'how much was your guitar?', 'subreddit': 'IAmA'}\n",
      "{'body': 'cant get off the plane...', 'subreddit': 'aviation'}\n",
      "{'body': 'Its the 6th pillar or Islam.', 'subreddit': 'exmuslim'}\n",
      "{'body': 'S!!!!!!', 'subreddit': 'CFL'}\n",
      "{'body': 'The demand for the 70 mats is still there because of how good they are for spiritbonding. Molybdenum Ore is one of those items.\\n\\nEven some of the lower SB stuff like Kopranickel Sand is good because people leveling their crafters need so much of it.', 'subreddit': 'ffxiv'}\n",
      "{'body': 'Here is the post for archival purposes:  \\n\\n**Author**:  _uacdeepfield_ \\n\\n **Content**:  \\n\\n &gt;Why can we see this?', 'subreddit': 'BitcoinAll'}\n",
      "{'body': '2) Mechanics is basically reaction time similar to gun skill.  All that you listed is relevant with one additional important note.  In Dota you acquire two resources: gold and experience.  You get these resources in three ways, killing creeps, killing enemy heroes, and killing towers.  In the early game killing creeps is the most important way to get these resources. The mid lane is as close at it gets to a fair fight (its a neutral lane not closer to either teams base and historically was played as a 1v1 contest) so there is a lot of contesting over last hits (also known as CS).  So people that find success in the mid lane tend to have really good reactions to win these 1v1s.  Also a lot of mid heroes are \"flashy\" with fast movement based abilities.  So you usually want your most mechanically talented player located there.\\n\\n3) The offlaner is probably the most vulnerable (its sometimes called the hard lane) since you are usually playing 1 vs 2 or 3 heroes.  But mid and the mid tower are often a very important early advantage and the focus of a lot of action early on in the game.  Since you can teleport to towers and they give vision the mid tower (with its neutral central location) is a very important objective to open up space on the opponents side of the map.\\n\\nRecently the \"new meta\" has been the position 4 player sitting mid, turning mid into a duel lane vs its historical 1v1.  The nice thing about Dota is that there aren\\'t really fixed positions for how you set up your lanes.  Every team will send 1 hero to each lane, but what you do with your two supports depends on draft and strategy.  You can go aggro and send them all to the offlane, you can have 1 in the safe lane and 1 roam, etc.  \\n\\n4) Depends on the team, a lot of times coaches in Dota are picked up right before a LAN since they are often pro players that didn\\'t qualify for the LAN.  For instance Pajkatt was the coach of DC during their TI6 2nd place run (Misery\\'s team) since his team wasn\\'t able to qualify.', 'subreddit': 'OpTicGaming'}\n",
      "{'body': 'i think this is a solid coin\\n', 'subreddit': 'Cloak_Coin'}\n",
      "{'body': 'I saw Wazzu go victory from the gun last night, we may need to look into that.', 'subreddit': 'CFB'}\n",
      "{'body': \"Now we just need to find someone who isn't banned from that subreddit.\", 'subreddit': 'thatHappened'}\n",
      "{'body': '[deleted]', 'subreddit': 'MoviePassClub'}\n",
      "{'body': \"No offense need meant but that sounds like they're really not interested, hence the one word answers \", 'subreddit': 'videos'}\n",
      "{'body': '王力宏', 'subreddit': 'languagelearning'}\n",
      "{'body': 'You mean your monthly is 70. Anyways 110 at union station is probably the cheapest. ', 'subreddit': 'LosAngeles'}\n",
      "{'body': '⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆ ❗ **NOTICE** ❗ ⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆\\n\\n* Please consult the [List of Scammers](https://www.reddit.com/r/SmiteTrades/wiki/scammer_list) before trading.\\n\\n* If someone contacts you via PMs and they have not responded to your thread be aware that the user may already be banned from the subreddit for being a scammer.\\n\\n* Consider taking screenshots of all parts of your trade to use as evidence if the need arises and [report](https://www.reddit.com/message/compose?to=%2Fr%2FSmiteTrades) anything untoward to the moderators.\\n\\n* *Never* share your Smite or Reddit password with anyone.\\n\\n----------------------------------------------------\\n^^^^^^^Post ^^^^^^^made ^^^^^^^by ^^^^^^^/u/Ronnenn\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/SmiteTrades) if you have any questions or concerns.*', 'subreddit': 'SmiteTrades'}\n",
      "{'body': 'E', 'subreddit': 'Rainbow6'}\n",
      "{'body': \"hahahahaha oh yeah? \\nhttps://en.wikipedia.org/wiki/Demographic_history_of_New_York_City\\nas recently as 1960 more than 90% nohhispanic white. you're being overrun\", 'subreddit': 'politics'}\n",
      "{'body': 'I picked up \"three green, everything else is optional,\" look at the indicators and touch the gear handle when turning final and again on short final.  If I\\'m in a fixed gear airplane I\\'ll still say it and look out the window at the gear.', 'subreddit': 'flying'}\n",
      "{'body': 'I dont know about them', 'subreddit': 'Android'}\n",
      "{'body': '[+MarkoWolf](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnona1q/):\\n\\nHow much have you spent, out of pocket, from start to finish?', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Not upvoted for ending a sentence with a preposition. FeelsBadMan', 'subreddit': 'smashbros'}\n",
      "{'body': 'Nice place for an e-sport', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Also, internet tubes should be shut down to defeat ISIS recruitment.', 'subreddit': 'politics'}\n",
      "{'body': 'Not really. The only difference is the battery gets slightly worn down over time. If you have a used and new watch side by side, you couldn’t tell the difference. ', 'subreddit': 'AppleWatch'}\n",
      "{'body': \"good deal, it's gone. there was probably only one.\", 'subreddit': 'buildapcsales'}\n",
      "{'body': 'Would you be willing to share your decklist?', 'subreddit': 'EDH'}\n",
      "{'body': \"It's only a coincidence that all these things exist in this one person, and similar weirdness is in nearly all SJWs.\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'He has said a lot of completely hateful shit, the people who support him are more concerned about \"the blacks\" kneeling.', 'subreddit': 'neoliberal'}\n",
      "{'body': 'Leaked halloween skin', 'subreddit': 'Paladins'}\n",
      "{'body': '\\n\\n\\n[***Harry Potter and the Chained Souls***](http://www\\\\.fanfiction\\\\.net/s/3490702/1/) by [*Theowyn of HPG*](https://www\\\\.fanfiction\\\\.net/u/633246/Theowyn\\\\-of\\\\-HPG)\\n\\n\\n\\n\\n\\n\\n&gt; Harry must discover how Voldemort cheated death\\\\. He faces Death Eaters, shadowy Ministry officials &amp; suspicions that threaten to tear his own allies apart\\\\. But the answers lie in the mind where victory can only be won by freeing the chained souls\\\\. SEQUEL\\n\\n^(*Site*: [fanfiction.net][140280130791632:site] **|** *Category*: Harry Potter **|** *Rated*: Fiction  T **|** *Chapters*: 31 **|** *Words*: 231,287 **|** *Reviews*: 760 **|** *Favs*: 929 **|** *Follows*: 221 **|** *Updated*: 7/12/2007 **|** *Published*: 4/16/2007 **|** *Status*: Complete **|** *id*: 3490702  **|** *Language*: English **|** *Genre*: Angst **|** *Characters*: Harry P., Severus S. **|** *Download*: [EPUB][140280130791632:epub] or [MOBI][140280130791632:mobi])\\n[140280130791632:site]: http://www.fanfiction.net/\\n[140280130791632:epub]: http://www.ff2ebook.com/old/ffn-bot/index.php?id=3490702&amp;source=ff&amp;filetype=epub\\n[140280130791632:mobi]: http://www.ff2ebook.com/old/ffn-bot/index.php?id=3490702&amp;source=ff&amp;filetype=mobi\\n\\n\\n---\\n\\n\\n\\n\\n[***Harry Potter and the Enemy Within***](http://www\\\\.fanfiction\\\\.net/s/3417954/1/) by [*Theowyn of HPG*](https://www\\\\.fanfiction\\\\.net/u/633246/Theowyn\\\\-of\\\\-HPG)\\n\\n\\n\\n\\n\\n\\n&gt; In his sixth year at Hogwarts, Harry\\'s mental link to Voldemort is stronger than ever\\\\.  Can Snape teach him to control the nightmarish visions? And is their connection the key to ending Voldemort\\'s reign?\\n\\n^(*Site*: [fanfiction.net][140280130674248:site] **|** *Category*: Harry Potter **|** *Rated*: Fiction  T **|** *Chapters*: 19 **|** *Words*: 173,220 **|** *Reviews*: 442 **|** *Favs*: 1,197 **|** *Follows*: 238 **|** *Updated*: 3/27/2007 **|** *Published*: 2/28/2007 **|** *Status*: Complete **|** *id*: 3417954  **|** *Language*: English **|** *Genre*: Angst **|** *Characters*: Harry P., Severus S. **|** *Download*: [EPUB][140280130674248:epub] or [MOBI][140280130674248:mobi])\\n[140280130674248:site]: http://www.fanfiction.net/\\n[140280130674248:epub]: http://www.ff2ebook.com/old/ffn-bot/index.php?id=3417954&amp;source=ff&amp;filetype=epub\\n[140280130674248:mobi]: http://www.ff2ebook.com/old/ffn-bot/index.php?id=3417954&amp;source=ff&amp;filetype=mobi\\n\\n\\n---\\n\\n**FanfictionBot**^(1.4.0) **|** \\\\[[Usage][1]\\\\] | \\\\[[Changelog][2]\\\\] | \\\\[[Issues][3]\\\\] | \\\\[[GitHub][4]\\\\] | \\\\[[Contact][5]\\\\]\\n[1]: https://github.com/tusing/reddit-ffn-bot/wiki/Usage       \"How to use the bot\"\\n[2]: https://github.com/tusing/reddit-ffn-bot/wiki/Changelog   \"What changed until now\"\\n[3]: https://github.com/tusing/reddit-ffn-bot/issues/          \"Bugs? Suggestions? Enter them here!\"\\n[4]: https://github.com/tusing/reddit-ffn-bot/                 \"Fork me on GitHub\"\\n[5]: https://www.reddit.com/message/compose?to=tusing          \"The maintainer\"\\n\\n^^^^^^^^^^^^^^^^^ffnbot!ignore\\n\\n^(*New in this version: Slim recommendations using* ffnbot!slim! *Thread recommendations using* linksub(thread_id)^)!', 'subreddit': 'HPfanfiction'}\n",
      "{'body': \"He's pretty good on the Late Show.\", 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'Floor 117 is to test how many confusion immunities you have..', 'subreddit': 'FantasyWarTactics'}\n",
      "{'body': \"I was just telling him that it exists. I'm not telling him to post it here.\", 'subreddit': 'FIFA'}\n",
      "{'body': 'Anything [here](https://www.reddit.com/r/indiegameswap/comments/73fbxx/h_keys_from_the_telltales_and_capcom_hb_paypal_w/) for infested planet + DLC? ', 'subreddit': 'indiegameswap'}\n",
      "{'body': 'That’s true, but unfortunately State interactions are only about half/maybe slightly under half of what the president does. A president guided the nation in making new laws even if he can’t create him. But what he focuses on is sully the focus of Congress', 'subreddit': 'technology'}\n",
      "{'body': 'so much throws\\n', 'subreddit': 'DotA2'}\n",
      "{'body': 'The Master Race is forgiving, but only if you submit to true glory. \\n\\nThose who fail to submit will be branded as peasants, and appropriately so.', 'subreddit': 'FortNiteBR'}\n",
      "{'body': \"He's Mason Verger in the Hannibal TV show?\\n\\nOr is this the one with Anthony Hopkins?  \", 'subreddit': 'movies'}\n",
      "{'body': \"I'm currently recovering from a limb salvage at the [Center for the Intrepid](https://www.bamc.amedd.army.mil/departments/rehabilitation-medicine/cfi/) -- it's the US Army's biggest research, development, and rehabilitation center for injured veterans, serving all branches of the military. \\n\\nI don't know if they have a specific point of contact set up, but I do know that they occasionally post surveys and stuff for people who are going through rehabilitation there. It might be worth it to find a contact link on the site I linked to, and seeing if anyone there can help answer questions, or is willing to spread your survey around. \", 'subreddit': 'Prosthetics'}\n",
      "{'body': \"No you definitely missed the point. The frontflip is not dumb, doing it on wet ground is. \\n\\nI don't know how many times you need this to be said\", 'subreddit': 'holdmybeer'}\n",
      "{'body': \"It's preseason \", 'subreddit': 'OttawaSenators'}\n",
      "{'body': 'Rule #1 ladies and gentlemen. ', 'subreddit': 'Tinder'}\n",
      "{'body': \"Ian and then Tycho and Dogmeat in Junktown. Katja in the boneyard too but you're usually pretty tough by the time you get her.\", 'subreddit': 'Fallout'}\n",
      "{'body': \"Literally all of that is one big load of crap. Can you dispel my suspicion that you're just trolling?\", 'subreddit': 'canada'}\n",
      "{'body': \"I mean this with NO offense to McCann, he's a far different player with a different skillset...but this game is showing how important Yamil is to our team as well. \\n\\nHis pace and workrate are essential for Josef and Tito to get going up top. I hope we sign him permanently!\", 'subreddit': 'AtlantaUnited'}\n",
      "{'body': '[deleted]', 'subreddit': 'CFB'}\n",
      "{'body': \"I'm actually an associate myself! I might be the bossman in a few years. If I ever pay off my student loans. \", 'subreddit': 'reactiongifs'}\n",
      "{'body': \"This is a bit inconsistent with Thanos Imperative considering Robbie made it safe to Hala and was in Rich's and Star-Lord's funeral.  \\n\\nThis preview implies he disappeared sometime before they launched their assault against the Cancerverse creatures and that Rich taking the full Nova force partially fucked him up.\\n\\nBut seeing how much of a minor player Robbie was in Thanos Imperative I really don't mind that much if all of that was changed, especially considering where I think Duggan is going with Robbie.\\n\\nThat said Duggan mentioned this issue is going to be heartbreaking, and I think we're also suppose to find out who Talonar is in this issue (if not we will know before October ends), coupled with the fact that ot might be the reunion between Rich and Peter, yeah I'm preparing for a tearjerker (and going by his Deadpool run Duggan can do depressing scenes).\\n\\n\", 'subreddit': 'comicbooks'}\n",
      "{'body': '(2, 5)?', 'subreddit': 'cheatatmathhomework'}\n",
      "{'body': 'More please! Where is part 3, 4 &amp; 5!', 'subreddit': 'nsfw'}\n",
      "{'body': 'Best championship ever man.  Congrats either way.', 'subreddit': 'Hulugans'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoptrj/):\\n\\nSo far about less than 1,000. The editor was paid for by my parents who are awaiting me to pay them back every penny. So including what I owe my parents I'll be at about 5,000. I could've paid them back before, but they don't want me to deplete my savings. \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Awww. 🙂 People united by hate are sweet. 😙 Just like when George Lincoln Rockwell met the Nation of Islam, this moment warms my heart ❤', 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'apex?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'It’s honestly my favourite part of the song', 'subreddit': 'hiphopheads'}\n",
      "{'body': 'Have you spoken to a social worker?', 'subreddit': 'singapore'}\n",
      "{'body': 'I doubt whether it would be powerful enough to emulate anything after the SNES / Sega Megadrive era. Raspberry pi can struggle slightly at times with N64 and most playstation roms will be too big to fit. ', 'subreddit': 'emulation'}\n",
      "{'body': 'thanks for the help everyone 6mb speeds not as bad as the tmobile subreddit had me thinking and for such a low price!', 'subreddit': 'Sprint'}\n",
      "{'body': 'Well Kelly and Ed *did* live in NYC during the time period of the show... ', 'subreddit': 'TheOrville'}\n",
      "{'body': 'Probably Rea Lemon bc the capitalizes L indicates the start of a new word.', 'subreddit': 'thatHappened'}\n",
      "{'body': 'Restricted 18+', 'subreddit': 'DotA2'}\n",
      "{'body': 'Hit me up if you see me, khakis and baseball style young the giant tee.', 'subreddit': 'Paramore'}\n",
      "{'body': \"Aside from the included ones: Turtles in Time, DKC 2, and one of the Super Star Wars series off the top of my head...\\n\\nEdit: and since there are a few FX chip games with the Star Fox series and Yoshi's Island, why not Stunt Race FX?\", 'subreddit': 'miniSNES'}\n",
      "{'body': \"This was from WAY back in the day. During Christmas time our mall had a Santa/christmas elf that was making balloons. Boys were getting swords, girls flowers. Well 7 year old me wanted a sword instead and he wouldn't make me one. So i started playing with it took the flower apart and made a crappy sword. My parents got me a balloon book/kit for Christmas and the rest was history. Also I ride a unicycle. Added am album below\\n\\nhttps://imgur.com/gallery/21Djb\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"The only games I'm really worried about there are Arizona and Minnesota.\", 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'Depends. They are probably still paying for Miles.', 'subreddit': 'CFB'}\n",
      "{'body': \"No problems bud. Some other good places for this stuff is r/depression and to some extent r/anxiety \\nThere's a couple others that I can't remember the name of. There are people to talk to, and you're not alone. &lt;3\", 'subreddit': 'HorriblyDepressing'}\n",
      "{'body': \"agl hits between 400 and 500 on a double vegeta lead. teq only hits just below 300 but that's on a godtenks/goku lead.\", 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': 'I actually think that I could probably drive. I can put pressure directly on it but any sideways pressure makes me feel like passing out. ', 'subreddit': 'FocusST'}\n",
      "{'body': \"And she's flattered because OP obviously has a crush on her.\", 'subreddit': 'funny'}\n",
      "{'body': \"He's reading 2 pages of bulletpoints?  I think you are too generous.  It wouldn't surprise me if he's being briefed in the form of daily puppet shows.\", 'subreddit': 'politics'}\n",
      "{'body': 'Bald cunts ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"&gt; So you're cool with harassment as long as it's by your tribe? \\n\\nWhat specifically in Dragon's previous proceeding comment lead you to believe that was their stance?\", 'subreddit': 'GGdiscussion'}\n",
      "{'body': \"Same here. In fact, any Murakami novel I've attempted - or, er - dragged myself through (the first couple of chapters of). He tries too hard to find something 'deep' for his characters to say. And this whole 'ooOOOooOH m y s t e r i o u s   m a g ic' thing really falls flat because of it, imo. \", 'subreddit': 'books'}\n",
      "{'body': 'hazzymancer, 305 warlock.  bad enough dude', 'subreddit': 'Fireteams'}\n",
      "{'body': 'Well that one looks interesting ', 'subreddit': 'booksuggestions'}\n",
      "{'body': 'Checking out those lines. Just a correction--Aimee isn\\'t from Dixie. She\\'s from a fantasy world. Even then, she\\'s has a Brooklyn accent. It doesn\\'t really come out that much, cause her subtitles normally just drop \"g\"s from \"ing\" and turns you/your to ya/yer. She also uses a lot of Yiddish, which is much more common in New York. \\n\\nGranted, I\\'ve never been to New York, so I was mostly just listening to and miming speakers and stereotypical New Yorker characters. If anybody\\'s got some Brooklynisms, slam them in my inbox. \\n\\nI guess I could have her target lines back saying that she doesn\\'t know where this \"America\" city is, but later Marionette calls her a Westerner, which means Aimee can\\'t have already corrected her. I don\\'t mind finding a way to make this dialogue work, but I think we\\'ll need to collaborate a bit. \\n\\nThank you though--that\\'s a heavenly host of lines. I need to learn more about Miraculous Ladybug so I can return the favor. ', 'subreddit': 'spnati'}\n",
      "{'body': \"143417870| &gt; United States Anonymous (ID: Cr/oee8K)\\n\\n&gt;2012\\n&gt;Obama\\n&gt;2016\\n&gt;Trump\\nI'll leave to your imagination what happened in between.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': '[removed]', 'subreddit': 'television'}\n",
      "{'body': 'Me too -- sad to one of the little people. ', 'subreddit': 'politics'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Thank you for the response. It’s a warehouse supervisor for a certain department if that makes any difference ', 'subreddit': 'publix'}\n",
      "{'body': \"That's a nice heavy cylindrical transformer. \", 'subreddit': 'ExpectationVsReality'}\n",
      "{'body': '9 PCC, 2 ODC, 1 AC', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'just your typical r/all user', 'subreddit': 'vegancirclejerk'}\n",
      "{'body': \"The job market actually is that awful outside of hot beds. Sure, the pay doesn't suck if you get a job, but good luck getting a job. Especially if your school doesn't have a significant relationship with chemical companies.\", 'subreddit': 'chemistry'}\n",
      "{'body': \"**/r/CrackWatch**\\n\\nIf you're looking for the latest news on cracks for your favorite games, want to post game related topics or discuss your issues with certain games? Well if so, this is the place to be!\\n\\n*****\\n^(Bot created by /u\\u200b /el_loke - )^[Feedback](http://www.reddit.com/message/compose/?to=el_loke&amp;subject=TrendingCommenterBot%20Feedback)\", 'subreddit': 'TrendingReddits'}\n",
      "{'body': '[removed]', 'subreddit': 'videos'}\n",
      "{'body': 'I always preferred Rocky Flop ', 'subreddit': 'CFB'}\n",
      "{'body': 'Two episodes.\\n\\n**Two**. ', 'subreddit': 'startrek'}\n",
      "{'body': 'Oh they do, curious. I read that Terminator was also basically gnome-terminal.', 'subreddit': 'archlinux'}\n",
      "{'body': \"pm'd\", 'subreddit': 'hardwareswap'}\n",
      "{'body': '[deleted]', 'subreddit': 'waltonchain'}\n",
      "{'body': \"thanks now i don't need to write this myself. Exactly my point also. Fuck jacob manipulative\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"you get credits. woah didn't see that coming.\", 'subreddit': 'PhantomForces'}\n",
      "{'body': 'Yes, but does it hang like sleeve of wizard?', 'subreddit': 'news'}\n",
      "{'body': 'give you a target in a bh world, which it does.', 'subreddit': '2007scape'}\n",
      "{'body': '[deleted]', 'subreddit': 'investing'}\n",
      "{'body': \"I don't see what's wrong with making mythic flex too. \\n\\nBeing locked to one raid ID is pointless and serves no purpose other than to stop people pugging it.\\n\\nBeing realm specific is the same. There's no such thing as realm competition any more, so there's no point using that as an excuse anymore.\\n\\nThe only thing mythic only raid rules do is limit the number of people who can do it and the only reason people defend it is because they don't want to lose their epeen when puggers start clearing it faster than guilds do. Just like when heroics where meant to be in pugable or even raids in general if you go back far enough.  \", 'subreddit': 'wow'}\n",
      "{'body': \"Your yellow marker light bulb isn't blinking at the correct speed, tire pressure too low, and no cups in cup holders. That will need impounded \", 'subreddit': 'HaggardGarage'}\n",
      "{'body': 'At least this is something you can change.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"On one hand, we've got SKSE and CBBE for SSE.\\n\\nOn the other hand, there's the Creation Club.\\n\\nThis month has been full of ups and downs.\", 'subreddit': 'skyrimmods'}\n",
      "{'body': 'godammit, if only we had Mark Craig, a battle of gods.', 'subreddit': 'Cricket'}\n",
      "{'body': 'Pineapples are good.', 'subreddit': 'CFB'}\n",
      "{'body': 'It copies the stats of a 6/6 trickster/kitchen sink for 2 brains', 'subreddit': 'PvZHeroes'}\n",
      "{'body': 'Primus is eternal ', 'subreddit': 'indieheads'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Good point it's late I'm not paying enough attention apparently. OP can you confirm where your getting the clock jnformation? If it from the sticks themselves or bios/software?\", 'subreddit': 'pcmasterrace'}\n",
      "{'body': \"Ugh my dad flew bombers in the Gulf war and is in disbelief that I'm on the side of the kneelers after even saying it's not even their cause that I necessarily support but their freedom to do so. He literally doesn't understand the true cause that he fought for\", 'subreddit': 'news'}\n",
      "{'body': 'Been banned for years', 'subreddit': 'bodybuilding'}\n",
      "{'body': \"Hmmmm. If you think the pension clause is so destructive, why do you need to ask? It's almost like you have only a very cursory understanding of the topic.\", 'subreddit': 'chicago'}\n",
      "{'body': '[deleted]', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'But then gives one up!', 'subreddit': 'collegehockey'}\n",
      "{'body': 'Sorry, the bot told me it deleted the post already. I sent it to the other subreddit.', 'subreddit': 'toronto'}\n",
      "{'body': \"Let's see, I'm going to guess 25?\", 'subreddit': 'kratom'}\n",
      "{'body': 'They hired Qualcomm engineers for their OS and camera stack...', 'subreddit': 'Android'}\n",
      "{'body': 'All fire but why didn\\'t Nitty say\\n\\n\"This baby\\'ll come out slow if I heard he gettin bread like a incest birth\"', 'subreddit': 'rapbattles'}\n",
      "{'body': 'Unrelated. You should go post in redacted, there always looking for retards.', 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': 'I like how the person filming her goes just a little slower at her butt', 'subreddit': 'WrestleWithThePlot'}\n",
      "{'body': 'Ser Emmon Costayne +4\\n\\nLord Addam Costayne\\n\\nSer Ryswin Costayne', 'subreddit': 'IronThronePowers'}\n",
      "{'body': 'http://lmgtfy.com/?q=donk', 'subreddit': 'Shitty_Car_Mods'}\n",
      "{'body': 'There is no place. If your wife is allowed so is the dog. ', 'subreddit': 'service_dogs'}\n",
      "{'body': 'owenwilsonwow.wav', 'subreddit': 'gaming'}\n",
      "{'body': \"[+entropizer](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnor3j8/):\\n\\nThank you for answering this. I didn't expect it, but I really appreciate the honesty.\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Brexit. \\n\\nNeed to tap into jingoist nostalgia to soothe the current anxiety.\\n', 'subreddit': 'movies'}\n",
      "{'body': 'OOOOOOH SHIT! HERE WE GO!', 'subreddit': 'CFB'}\n",
      "{'body': 'Oh sorry man, thought you were saying bike was titanium or something. I’ll stop joking around around now. ', 'subreddit': 'cycling'}\n",
      "{'body': \"I was also overweight going into pregnancy, and I lost weight both pregnancies during the first half. I finally stabilized at the same weight as my last visit at my 20 week appointment. I think I followed the same trend last time, but I still only gained a net 7 pounds when my 7 pound baby was delivered. But she was 7 pounds 5 ounces at 38 weeks and perfectly healthy. She always measured on track, and my doctor never said anything negative. I also went on to gain all the baby weight from stress and comfort eating postpartum. :/ Really, if your baby is growing well and your doctor isn't worried, especially if you have fat stored up, weight gain isn't strictly necessary. You and baby both will be fueled off your stored fat (which is the purpose of it). You just really want to make sure you're getting enough calcium (baby will leach it straight out of your bones if you're not getting enough from your diet). And keep up on your prenatal vitamin.\\n\\nSo my appetite is pretty small. I just can't eat much at a sitting and loading up at meals isn't an option for me. Figure out healthy, reasonably energy dense foods you can snack on through the day like nuts and cheese, fruits and veggies. I like to nibble on beef jerky sometimes or peanut butter. I also started drinking milk sometimes, mostly because I think it's tasty but I only seem to when I'm pregnant.\", 'subreddit': 'BabyBumps'}\n",
      "{'body': \"I can speculate, but still can't be sure, nonetheless though I'm thinking *real*.\", 'subreddit': 'creepyPMs'}\n",
      "{'body': 'City’s sub actually has more subscribers than their average attendance.', 'subreddit': 'reddevils'}\n",
      "{'body': 'Last 10-11 games of that 2009 season were legendary.', 'subreddit': 'nfl'}\n",
      "{'body': 'Geico.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"All of us gets drained by negative social interaction and energized by positive social interaction regardless if you're an introvert or extrovert. It's not about social stimulation, it's about whether you thrive off your energy more from any form of external stimuli whether it's going to interesting places or being in charge of people (extrovert) or if you thrive off your energy from any form of internal stimuli such as being inside your head all the time (introvert).\", 'subreddit': 'mbti'}\n",
      "{'body': 'as does yours', 'subreddit': 'bostonceltics'}\n",
      "{'body': \"It's amazing how TA3 gets so much subjective praise.\\n\\nBut on topic RobinF's defensive stats are better than Indigo due to her physical bulk, Indigo requires lots of work to make work in dealing with the more advanced players with heavy merged BLyn's and Reinhardts, which are really common personally. Compared to RobinF who with a few merges can counter +10 opposition running double goads and hone.\\n\\nIndigo requires bowbreaker (or lots of spurs) to kill a BLyn that runs Mulagir. Ignoring defensive battles, at neutral atk, a simple +6 atk from any source (Ex. Hone/Ally Support) is enough for Indigo to kill a BLyn running Brave Bow, this of course changes dramatically if they run fortify as well. \\n\\nThe simplest way for Indigo to deal with Blyn is bow breaker, which comes at an expense for some in the form of WoM or GTB.\\n\\nMeta Reinhardt's running QP/DB/DB3/Moonbow with 1 hone and goad can still kill a unmerged RES Indigo not running buffs, requiring +3~5 res to handle hone x1goad/x2 goads, which can be simply Fort Res1 Seal+Ally Support+HP/Res Seal on Indigo.\\n\\ntl;dr unmerged Indigo will help against unmerged opposition like a prince charming. But against more invested teams, not so much without severe buff work, you'll probably sacrifice something to fit him to do his job when he needs to pull the weight of his intended roll. \\n\\nDef not S+-Rank unless your just saying that because he has dance, then at that point you can argue another color+dance.\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"I know you what you're saying, But you create a false dichotomy that we either gotta be in a lawless land or a police state. You'd make a better argument if you just paraphrased Milton Friedman critique on the war of drugs and link using drugs to over eating since a significant portion of Americans do actually overeat while we don't actively drink HCl.\", 'subreddit': 'Drugs'}\n",
      "{'body': '[deleted]', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'The above submission has been **removed** because:\\n\\n* It does not appear to contain a tl;dr or any sort of short summary. Please edit your post to add a **\\\\*\\\\*bolded**** tl;dr. Refer to FAQ: [What is TL;DR? Why do I need it?](http://www.reddit.com/r/relationships/wiki/index#wiki_what_is_tl.3Bdr.3F_why_do_i_need_it.3F) for some pointers.  If you feel you are receiving this message in error, please [contact the moderators](http://www.reddit.com/message/compose?to=%2Fr%2Frelationships&amp;subject=Submission+removed+for+no+TLDR&amp;message=My+post+can+be+found+at:+https://www.reddit.com/r/relationships/comments/73hehs/me_20_m_and_tinder_girl_20_f_after_first_date_is/) and include your problem.  \\n\\nYou must make suitable edits to ensure that the submission complies to **all the rules** listed in the sidebar, and in the [wiki](/r/relationships/wiki/index#wiki_about_.2Fr.2Frelationships) (relevant for mobile users).  When you are done, please repost your submission.  \\n\\n---\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/relationships) if you have any questions or concerns.*', 'subreddit': 'relationships'}\n",
      "{'body': '143416975| &gt; Canada Anonymous (ID: rW+pZFeX)\\n\\n&gt;&gt;143412250 (OP)\\n&gt;rura\\nThe only people who voted for Trump are rural and suburban retards.\\n\\nCity people all voted for Hillary.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Could be just friends and see how you're doing.  Could be testing for spark.  Could be that she's got a room booked next to the coffee shop.  Give us an update afterwards\", 'subreddit': 'adultery'}\n",
      "{'body': 'What an epic show. [This](https://i.imgur.com/ZvqT4iS.png) was just too epic. Also, it was awesome to have an extended episode for the season finale. ', 'subreddit': 'anime'}\n",
      "{'body': \"Sorry, was out later than expected! If you're still up let me know, otherwise we can always try tomorrow!\", 'subreddit': 'SVExchange'}\n",
      "{'body': 'Being all about Jesus.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'First of all, Phoinex o hara ', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': \"It sounds a lot more like the October 3rd FNN will be announcing no more major content updates from Jay's wording in the /r/Titanfall post.\", 'subreddit': 'xboxone'}\n",
      "{'body': '*The shelving contorts and begins bringing a specific shelf to the top. Once it arrives at your eye level, the shelf slides to you, stopping inches from the top of the podium.*\\n\\n*The shelf contains a number of small paphlets, and two larger sized hardback volumes.*', 'subreddit': 'WayfarersPub'}\n",
      "{'body': 'Also, don\\'t blame yourself so much for letting it \"get\" this way.  Ultimately the majority of the responsibility (it sounds like) should fall on him for not being willing to take a fair share of household labor. \\n\\nBut yes, let us both be a warning to have these conversations early in the relationship!!', 'subreddit': 'TrollXChromosomes'}\n",
      "{'body': \"We're gonna' need some of that tomorrow.\", 'subreddit': 'nfl'}\n",
      "{'body': \"Get yer ass to upvotin' in here. That's what you do\", 'subreddit': 'ColoradoRockies'}\n",
      "{'body': '[deleted]', 'subreddit': 'PetiteGoneWild'}\n",
      "{'body': \"Maybe firefox is just a turd.  I installed that modify headers add on into chrome and now it's playing.  woot!\\n\", 'subreddit': 'jayhawks'}\n",
      "{'body': '&gt; on mobile.\\n\\nSo why would it be a risky click? Is your personal cell phone screen broadcast to the entire company or something?', 'subreddit': 'AskReddit'}\n",
      "{'body': 'It says in the article that there is a good 4 weeks to complete. I ended up isolating myself from all of their services, it would be an on off of you and your side of the room.', 'subreddit': 'SubredditSimulator_SS'}\n",
      "{'body': \"Anyone know anything about the 90's starters? I've love to learn more.\\n\\nAlso, was this an alternate back in the day? I know the pooh patches date it but I am not sure what I actually have.\", 'subreddit': 'hockeyjerseys'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnor9aa/):\\n\\nI always try to be honest. Unfortunately some people on here think I'm a lying trust fund baby :( \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"The EDA isn't broken, just not designed well so it doesn't handle the adjustments as gracefully as it should. The next update (which will be a hardfork) will adjust it.\", 'subreddit': 'btc'}\n",
      "{'body': 'Ok Captain Critic, \\n\\nIn the meantime, Elon Musk has already started fixing the problem. \\n\\nhttp://fortune.com/2017/09/28/tesla-battery-puerto-rico-power/\\n\\nMaybe we should all invest in Tesla stock, in addition to donating to charity. ', 'subreddit': 'conspiracy'}\n",
      "{'body': \"you can't spot a speed ling from a regular one?? it's got wings. only issue with speedlings i've seen complained about is counting them.\\n\\nthey like start to shimmer with wings.\", 'subreddit': 'starcraft'}\n",
      "{'body': \"Yeah, didn't plan well.\", 'subreddit': 'fireemblem'}\n",
      "{'body': 'its like using a condom vs. no condom. ', 'subreddit': 'baseball'}\n",
      "{'body': \"Could be - I can't remember. When I thought of thunder and lightning I thought of Jacobs and Bradshaw though. I watched football during the Tiki years, but not nearly as heavily as I watched during the Bradshaw years.\", 'subreddit': 'NYGiants'}\n",
      "{'body': 'Okay lets upvote this to share this info!', 'subreddit': 'ptcgo'}\n",
      "{'body': 'mxfushi fam', 'subreddit': 'DesignerReps'}\n",
      "{'body': \"I always assumed he would crash the dogfight over D'Qar, have an emotional encounter with Leia, and make off with the map to Ahch-To.\", 'subreddit': 'StarWarsLeaks'}\n",
      "{'body': 'Deepest lore', 'subreddit': 'anime'}\n",
      "{'body': \"Not really, if he doesn't take care of it then, you just have a crappy school system. No offence.\", 'subreddit': 'Advice'}\n",
      "{'body': 'Wtf.   we are getting pushed around by a sun belt team.  ', 'subreddit': 'CFB'}\n",
      "{'body': 'Google is too difficult for you, there is homepage, there is GitHub repo - and your parents are related.', 'subreddit': 'btc'}\n",
      "{'body': 'I know it started but maybe not the main event. Usually this goes on for a little so I would keep checking ESPN 3', 'subreddit': 'jayhawks'}\n",
      "{'body': 'Funny thing is the guy next to me is a co-worker who actually was racing me. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I feel like after you watch Wonder Woman, that dim light of hope you're holding onto will grow. Great film.\", 'subreddit': 'arrow'}\n",
      "{'body': '196pts by late June too', 'subreddit': 'Habs'}\n",
      "{'body': 'Meh. Fuck em', 'subreddit': 'MLS'}\n",
      "{'body': \"**/r/UnethicalLifeProTips**\\n\\nAn Unethical Life Pro Tip (or ULPT) is a tip that improves your life in a meaningful way, perhaps at the expense of others and/or with questionable legality. Due to their nature, do not actually follow any of these tips–they're just for fun. Share your best tips you've picked up throughout your life, and learn from others!\\n\\n*****\\n^(Bot created by /u\\u200b /el_loke - )^[Feedback](http://www.reddit.com/message/compose/?to=el_loke&amp;subject=TrendingCommenterBot%20Feedback)\", 'subreddit': 'TrendingReddits'}\n",
      "{'body': \"ahh if only your presence would have a permanent effect lol! Sadly it started happening again tonight, especially during fractals (with only a temperature of 82) so i have no idea what's going on now anymore. So weird that it worked great the entire day, until tonight... \", 'subreddit': 'Guildwars2'}\n",
      "{'body': 'John the Baptist?', 'subreddit': 'atheism'}\n",
      "{'body': 'I never discouraged the US helping. Although, administering aid is a two way street.', 'subreddit': 'nfl'}\n",
      "{'body': 'Awesome thanks :)', 'subreddit': 'BABYMETAL'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Two separate people have emailed me this article and asked about Brock... people who I wouldn't expect to even care.  I feel like I'm missing something.  \", 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'Dont @ him', 'subreddit': 'nba'}\n",
      "{'body': 'Like fuck would I give my phone to a \"friend\" like OP!', 'subreddit': 'UnethicalLifeProTips'}\n",
      "{'body': \"I mean, I firmly believe if you expect me to put my mouth near your genitals, common courtesy dictates you'll reciprocate. \\n\\nBut it's up to you. \", 'subreddit': 'relationships'}\n",
      "{'body': 'It was incidental hand contact. If you look at the replay, Ricketts hands never moved at all or intentionally did anything', 'subreddit': 'MLS'}\n",
      "{'body': \"i have both. i give the edge to the pg1 as i need cushion in the forefoot area. They're also a bit cheaper\", 'subreddit': 'BasketballTips'}\n",
      "{'body': \"I'll give you the face is cute but her jorts are scraping her nipples.  \", 'subreddit': 'barstoolsports'}\n",
      "{'body': 'Love my OD-2r. I use it more as low gain filter for nailing late 80s U2 tones or for stacking with other overdrives. ', 'subreddit': 'guitarpedals'}\n",
      "{'body': 'You’ll almost always see a reverse variation on a gender/age/job based question.\\n\\n“Men of Reddit...”\\n\\nFollowed by\\n\\n“Women of Reddit...”', 'subreddit': 'AskReddit'}\n",
      "{'body': \"See it like this, if you touch the ground outside the boundary you get tagged, and you can't take catches if you are tagged, only way to untag is to touch the ground inside the boundary again. As long as your last touch with the ground (before taking the catch) is inside meaning you are untagged, your catch is legal.\", 'subreddit': 'Cricket'}\n",
      "{'body': 'Yeah and then when she runs up on Armin it’s like oh shit this mission is not going to go well', 'subreddit': 'ShingekiNoKyojin'}\n",
      "{'body': '[deleted]', 'subreddit': 'politics'}\n",
      "{'body': \"Didn't realise it. \", 'subreddit': 'popping'}\n",
      "{'body': \"Ball boys are not getting as much of a workout as they did when Auburn didn't have nets behind their goal posts.\", 'subreddit': 'CFB'}\n",
      "{'body': \"I have no idea what I'm feeling from it.. but I do feel more calm yet more energetic at the same time so I guess I can attribute it to the kratom since I'm just taking it alongside my normal daily stack.\", 'subreddit': 'Stims'}\n",
      "{'body': 'Wait a minute, this seems familiar ', 'subreddit': 'worldnews'}\n",
      "{'body': 'Finally, a quality submission that I feel like I as a normal person should be able to do.', 'subreddit': 'food'}\n",
      "{'body': \"Sure but you're forgetting Nigel is a single person working on his free time and without a budget.\\n\\nI would love to see what he could come up with if he was to redo the whole official app with a proper budget and a small team to help.\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Mine just said complete for a couple days and it arrived. ', 'subreddit': 'fakeid'}\n",
      "{'body': 'Flash pan camera, but close ;)', 'subreddit': 'guns'}\n",
      "{'body': '143418575| &gt; United States Anonymous (ID: 7vmrTfB3)\\n\\n&gt;&gt;143418344\\nFucking boomers...\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"That's fair enough\", 'subreddit': 'funny'}\n",
      "{'body': '[deleted]', 'subreddit': 'DotA2'}\n",
      "{'body': 'Uh they went to the doctor to get the baby extracted', 'subreddit': 'thatHappened'}\n",
      "{'body': 'Something something *\"all feel gay whe Johnny comes marching home.\"*', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I remember those. I was like 7 though when i used them. ', 'subreddit': 'CasualConversation'}\n",
      "{'body': \"Mobil is an oil company, it's most likely a subtle commentary on the Tyranny of America outlined in Drag Queen. \\n\\nBasically saying the government is profiting from war (with oil) if anything he is wearing it ironically. \", 'subreddit': 'TheStrokes'}\n",
      "{'body': 'Cosquillas en el ano. ', 'subreddit': 'vzla'}\n",
      "{'body': '\"No need for formalities when you\\'re in the wrong place at the wrong time.\"\\n\\n*Joyce has her pistol out. Since Sturge is behind cover, she\\'s trying to move to where he is.*', 'subreddit': 'RvBRP'}\n",
      "{'body': 'You should start /r/JunkieProTips.', 'subreddit': 'LifeProTips'}\n",
      "{'body': \"That's great to hear. I also miss it quite a lot.\", 'subreddit': 'firefox'}\n",
      "{'body': 'This game is shit,  \\nThe servers need improvement,  \\n\"A system error occurred  \\nduring event movement.\"\\n\\n\\\\- Anonymous', 'subreddit': 'ffxiv'}\n",
      "{'body': 'This.  I got it on sale with the throttle for 30$.  Amazing value for that money.  Got it to play Elite: Dangerous.  Funny story, my friend dropped 300$ on a high end HOTAS n throttle but in the end it was so heavy there was no way he could whip the stick around fast enough or pull/push the throttle fast enough to be effective in combat.  At 30$ I could swing the thing all over the place without worrying about breaking it.', 'subreddit': 'gamingsuggestions'}\n",
      "{'body': \"[+Fetus-P](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnot0o8/):\\n\\nI think people are overusing trust fund, but you seem to be blessed with a nicer upraising than a good chunk of people. That is why they are being like that. \\n\\nNot to many people can get their folks to give them 4k without wanting to be paid back.\\n\\nI wouldn't be offended by it, I'd be proud that my family did well and are supportive\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Navy Federal: \"Good morning Lance Corporal!\"\\n\\nWeirds me out.  No one ever addresses me by rank.  It\\'s always just \"Hey radio!  Get over here!\"\\n\\nEdit: drunken typo', 'subreddit': 'USMC'}\n",
      "{'body': 'I reinstalled the os and set a static ip but how do i restart the service?', 'subreddit': 'raspberry_pi'}\n",
      "{'body': 'I am suspicious too. Lay low until she presents you with an actual baby and then ask for a paternity test. ', 'subreddit': 'relationships'}\n",
      "{'body': \"I just beat him. I ended up using the blue bouncy ball, the pink shotgun, the smoke grenade (for the dash) and the super 1. \\nThe blue bouncer murders his first form and with the smoke dash, you can dash through him as he charges so you don't need to worry about the ducks. \\n\\nSwitch to the shotgun for the second part. That murders the dogs. The key hear is to kill this part right AFTER the second train so you don't have to worry about that when his third form appears. \\n\\nFor the third you get under him and use the blue lob shot. You can dash through his green horse shoe attacks if you happen to be in front of him. \\n\\nFor the last form use the blue lobber. Also use your super attack right as the teacups are descending. Stay on the teacup and shoot. Once he spawns those green guys drop to the middle and kill a couple and hop back up to the teacups and let the train kill the other two. \", 'subreddit': 'xboxone'}\n",
      "{'body': 'For some reason I read the heading as Ferris Buellers Day Off. ', 'subreddit': 'The_Donald'}\n",
      "{'body': 'This is really pretty. You did a wonderful job. Keep posting!', 'subreddit': 'crafts'}\n",
      "{'body': 'I’ve seen Corelle plates in different patterns, sizes and quantities at stores like Walmart, Target and even our local grocery store (Publix). \\nI’ve also had great luck with stores like CB2 and Crate &amp; Barrel (they sell them individually). Not sure where you’re located, but I also liked Fishes Eddy &amp; Anthropologie  for individual plates as well. If you like turquoise, now should be a good time to get them since it’s more of a summer color and will probably be on sale. ', 'subreddit': 'Frugal'}\n",
      "{'body': 'I know someone who has had an iPhone 7 for less than a year. The glass was extremely scratched and even slightly cracked, and the matte black coating was rubbing off. ', 'subreddit': 'LifeProTips'}\n",
      "{'body': \"I am sure there is a conspiracy here. I just can't locate it.\", 'subreddit': 'TumblrInAction'}\n",
      "{'body': \"You are assuming a lot.  The fact is I have played organized football, as well as other sports.  I've also been a coach for the past few decades.  These boys didn't go out drinking the night before the game, they didn't celebrate in the endzone, they didn't get into a fight...  They wanted to bring attention to an issue even though they were warned.  To me, they believed in what they did and were willing to suffer the consequences even if those consequences were because their coach felt his beliefs were more important than theirs.  As a coach, I'm not a huge advocate of those kneeling during the anthem, but I also respect their right to do it.  The right to a peaceful protest is what makes this country great.  It's small and petty to abuse your power as a coach to punish players for something like this.  \", 'subreddit': 'news'}\n",
      "{'body': 'i just want to play lag-free, crash-free......', 'subreddit': 'bravefrontier'}\n",
      "{'body': 'Wouldn’t it be better to transfer SPG to Marriott for 1:3 rate? \\n\\nBut thank you! I’ll just go with these two then :) ', 'subreddit': 'churning'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Definitely should have shown more of Houli when I was trying to explain Aussie Rules to locals in the Middle East haha. \\n\\nBut it's great to see. This game knows no bounds for who they want to participate, and that is fantastic. \", 'subreddit': 'AFL'}\n",
      "{'body': 'it’s too cold\\n\\nwhy was i downvoted', 'subreddit': 'teenagers'}\n",
      "{'body': \"Yep. You guys get to play us in a tiny arena that isn't on our campus!\", 'subreddit': 'CFB'}\n",
      "{'body': 'Uncontrolled Substance is dope too, Inspectah Deck tends to fly under the radar but he’s got some of the dopest rhymes out of the group.', 'subreddit': 'hiphopheads'}\n",
      "{'body': \"Je mate du College Football jusqu'au bout de la nuit.\", 'subreddit': 'france'}\n",
      "{'body': \"Thanks! I briefly looked into accordion tables earlier, and also noticed that it's possible to insert them in Dreamweaver. Is it a good idea to do that, then modify the code or should I start with some other sample code?\\n\\nAlso - an issue I had when playing around with accordion tables was that I couldn't collapse all panels simultaneously, and was forced to have exactly one expanded at all times. Can this behaviour be changed so that they all start collapsed and only expand/collapse when clicked?\\n\\nWould it be reasonable to put an accordion table inside another? Cause it seems to me like I'd have to to achieve two levels as intended. While we're at it, would it be possible to dynamically add panels to both levels based on the imported data (mentioned in point 3 in the OP)?\\n\\nLastly; how flexible are accordion tables when it comes to styling? I'm a little picky when it comes to design.\\n\\nDamn, that turned out a wall of questions. Don't feel obligated to answer them all!\", 'subreddit': 'web_design'}\n",
      "{'body': 'From the same guy too lol', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'So... [John Cena, Paladin of the Cenation?](https://www.youtube.com/watch?v=ui9egS_0PQ8)', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'It is, yes', 'subreddit': 'killthosewhodisagree'}\n",
      "{'body': 'It weighs 2 pounds is made of electronic grade aluminum. It screws apart and is a tubular cylinder. It was in a \"mixed lot\" box of items from an industrial auction.', 'subreddit': 'whatisthisthing'}\n",
      "{'body': 'I feel like everything in this photo was planned out.  ', 'subreddit': 'Justrolledintotheshop'}\n",
      "{'body': 'That’s fine. You’re not president, though ', 'subreddit': 'pics'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'it needs to be someone with a KWS', 'subreddit': 'EliteDangerous'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnow70m/):\\n\\nNot trust fund, but I was blessed with a nice upraising. I'm glad they're supportive of me :)\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"He's A POS apparently according to his shoulder patch. :)\", 'subreddit': 'pics'}\n",
      "{'body': \"I wouldn't hold out much hope this game.\", 'subreddit': 'CFB'}\n",
      "{'body': \"huh.  didn't know that.  thanks\", 'subreddit': 'CFB'}\n",
      "{'body': \"Now that's a sexy pic\", 'subreddit': 'gonewild'}\n",
      "{'body': 'Santa', 'subreddit': 'MapPorn'}\n",
      "{'body': 'I could see this being useful in the city, or at the very least a mall.', 'subreddit': 'EngineeringStudents'}\n",
      "{'body': 'She might ‘be the night!’, but she is also an elaborate bin bag.', 'subreddit': 'OldSchoolCool'}\n",
      "{'body': 'Right I find trafficking children to be the most heinous crime to be committed  and yet the CIA is involved. WOO HOO!!!!! ', 'subreddit': 'news'}\n",
      "{'body': \"Eh I wouldn't, I don't think either Big Ben or Dak are matchup-proof and would keep both and play the best matchup. I'd only do it if you're really weak in one position and are looking for a lotto ticket in that position\", 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'Classic Halo 3', 'subreddit': 'PrequelMemes'}\n",
      "{'body': \"While I agree, we've gotta start somewhere lol. It's clear that Vince isn't going to change the main product, so until he goes, what can we hope for? \", 'subreddit': 'SquaredCircle'}\n",
      "{'body': \"Are you internet explorer?\\nCause you're kinda slow.\", 'subreddit': 'Rainbow6'}\n",
      "{'body': 'To play baseball also', 'subreddit': 'pics'}\n",
      "{'body': 'You are asking the wrong question. \\n\\nYou should ask what places to avoid. It’s a MUCH shorter list.\\n\\nBut in all honesty it depends on what type of Asian cuisine you’re in the mood for. It’s all different and it’s all DELICIOUS!', 'subreddit': 'Atlanta'}\n",
      "{'body': 'Tens of thousands of dollars ', 'subreddit': 'lego'}\n",
      "{'body': \"&gt; when the section WO says they're not leaving because the sky is blue, it's a valid order.\\n\\nNo argument from me at all, but those dinosaurs are (hopefully) quickly being extinguished as new blood (slowly) comes into the CF. There are some &lt; 27 year old WO's starting to show up and maybe they'll remember where they came from because the old fucks certainly don't.\", 'subreddit': 'CanadianForces'}\n",
      "{'body': \"I'll admit I had to Google 'netorar' and yes indeed you're right and you're welcome. \\n\", 'subreddit': 'gonewildaudio'}\n",
      "{'body': 'Do you have a horse emblem, with emblem buffs Lyn  outclasses Cordelia.\\n\\nHone can give +6 attack(I think +Attack Cordelia will have the same attack as a neutral buffed Lyn) and speed, Goad Cavalry which is situational but it has saved me in a lot of places like TT and Arena gives +4 to attack and Speed.\\n\\nSo essentially, she is Cordelia with more movement and speed. ', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I googled because your question made me wonder. It looks like Michigan doesn't allow early voting. It is the same here in Alabama. \", 'subreddit': 'AmericanHorrorStory'}\n",
      "{'body': \"Vietnam and Korea were to stop the spread of communism. That's preemptive war. You can't get much more trigger happy than that. And the bombing campaigns in Vietnam, Cambodia and Laos could definitely be described as carpet bombing. The amount of UXO there is incredible, as well as ongoing effects from agent orange and the near indiscriminate use of napalm. That war was trigger happy and far more unjustified than anything Obama or Clinton did. Same with BUsh Jr and both of his major wars that are still ongoing.\\n\\nObama showed restraint against Syria, refusing to bomb without congressional approval which he did not get, whereas Trump did exactly that seemingly on a whim. Obamas use of drones, however, is unconscionable. I just don't see where you are drawing this distinction apart from the fact that the two administrations you mention are Democrats, and the others are Republican. I'm still calling partisanship. I'm not American, I'm not a democrat or a republican, but I can spot hypocrisy when I see it.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"I love roasting kids with my katana. Here's my setup\\n\\nLoadout Name: Ninjitsu\\n\\nKatana (Black Sky), Ghost + Recon, Tracker + Momentum (if I had epic katana I would trade momentum for Tac Resist), Hardwired + Dead Silence.\\n\\nKillstreaks: UAV Purple, Care Package Purple, CUAV Purple\\n\\nRig: Warfighter Overdrive Ping (Oni), or Skinnyboob Rushdown Rewind (Black Sky)\\n\\n\", 'subreddit': 'Infinitewarfare'}\n",
      "{'body': '[deleted]', 'subreddit': 'jobs'}\n",
      "{'body': '[deleted]', 'subreddit': 'pcmasterrace'}\n",
      "{'body': '143413280| &gt; United Kingdom Anonymous (ID: a9Bg/T2P)\\n\\n&gt;&gt;143412250 (OP)\\nOf course the shill voted for obongo and killary, they pay your wages\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"By refusing to send more help, people will die. Which is akin to killing them. \\n\\nThe mayor of San Juan's word, not mine.\\n\\nNo need to yell, douchebag.\", 'subreddit': 'preppers'}\n",
      "{'body': \"It'sa mee @dankfiber! That’s so sweet of you, thank you so much! It was really cool to see everyone’s different variations and how the colours evolve in the brioche and garter fade sections. Cant wait to see your Hedgehog shawls here &amp; on Insta when you’re finished! 😍\", 'subreddit': 'knitting'}\n",
      "{'body': \"kinda reminds me of the basement from that 70's show\", 'subreddit': 'trees'}\n",
      "{'body': \"Nah I mean I learn better when I have something in my hands. Once something breaks and I fix it once, I'll always remember how to fix it again. I didn't know much about computer parts until a few weeks ago I upgraded my PC, something stopped working, and now I know everything xD \", 'subreddit': 'Dirtbikes'}\n",
      "{'body': 'Are you the most downvoted person on this sub lmao', 'subreddit': 'Monstercat'}\n",
      "{'body': 'Just wanted to point out that his first name is Jong Un, most Korean first names have two parts.', 'subreddit': 'SketchDaily'}\n",
      "{'body': 'Leeeeeroy Jeeeeenkins... \\n\\nJeeenkins... \\n\\nJenkins... \\n\\nJen...\\n\\nJ...', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"The purpose of birth control is to be able to cum inside the pussy. If she is on the pill and she is regular with taking it, you can cum when you want. My wife was on the pill during our dating years and after before we started trying to have kids and we had no issues whatsoever. I can't believe how uninformed people are these days. \", 'subreddit': 'sex'}\n",
      "{'body': 'I’m really interested if this is a viable option for adult video websites as a supplement or alternative to ads https://tubeace.com/will-mining-cryptocurrency-porn-tube-sites-way-future/', 'subreddit': 'Monero'}\n",
      "{'body': '[deleted]', 'subreddit': 'cats'}\n",
      "{'body': \"I'm sure it is.\", 'subreddit': 'Music'}\n",
      "{'body': \"Just when I think I've heard it all some new piece of shit emerges to surprise me again with a new low.  \", 'subreddit': 'news'}\n",
      "{'body': 'I worked at a churrasco place for 6 years!', 'subreddit': 'hiphopheads'}\n",
      "{'body': 'My parents took me  to my first real restaurant. I thought I was living the high life when I got a Shirley temple drink.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'ye bro', 'subreddit': 'comicbooks'}\n",
      "{'body': 'Dunno what are those, but nofap generally not about hate against females. It is protest as saying \"no\" to porn. ', 'subreddit': 'NoFap'}\n",
      "{'body': 'No, but I have a Rowdy Tellez prospect base.', 'subreddit': 'baseballcards'}\n",
      "{'body': 'It looks good, but I see what you mean by not quite right. It could be that the tip of the nail doesn’t extend far enough.', 'subreddit': 'HelloInternet'}\n",
      "{'body': 'Aww thanks', 'subreddit': 'gonewildcurvy'}\n",
      "{'body': 'I would have forced a return.  People like this need to try a little bit.  Sometimes when you force the return, and the person realizes they need to spend their inept life packaging it and driving it to a dropoff point, it suddenly starts working correctly again.\\n\\nNow they get a free machine that you even paid to give to them. So what was the alleged \"problem\"?  (Needs new belt?  New light bulb?  Bent needle?  wTF else could possibly go wrong with one?)', 'subreddit': 'Flipping'}\n",
      "{'body': 'What’s the budget?', 'subreddit': 'gaming'}\n",
      "{'body': 'If 90% sold the calls that they owned at the same time, it does nothing to the price of the stock but it does have an impact on the price of that call option itself. ', 'subreddit': 'options'}\n",
      "{'body': \"For certain you're going to want to buy Intel then, assuming you're playing at 1080p with at least a GTX 1080. But wait to see how to 8600K fares, might not be enough to merit spending $100 more.\", 'subreddit': 'Amd'}\n",
      "{'body': 'It would be nice to have a male manakete.', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"Presumably it doesn't go bad, hang onto it until you want to buy something from there? There's no reason to rush to spend your money, even if it's only valid at target.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Translation: my wife won\\'t stop complaining about this, so I promised that I\\'d \"do something\" to shut her up.\\n\\nWhat a hypocritical loser.  If he feels sooo strongly about this, and Jesus is telling him to step up, then let him change *his* name.  I would pretend that I never got the letter if it were me.  And if confronted, say that it\\'s your policy to recycle junk mail without reading it.  ', 'subreddit': 'relationships'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I honestly believe Guy tweaked the system for this season and it's not working out too well. Seems like they are trying to be a little more offense minded and it's burning them on the defensive.\", 'subreddit': 'hockey'}\n",
      "{'body': 'Depends on what you want... The most common is an M4 model since parts for it are readily available. You might want to check the sidebar. It will help you decide on what brands you might want to go with.', 'subreddit': 'airsoft'}\n",
      "{'body': \"No, it's all local to the phone.\", 'subreddit': 'AppleWatch'}\n",
      "{'body': 'Thank you very much!', 'subreddit': 'SVExchange'}\n",
      "{'body': \"As I said before I am not hating the players, it's their money they can do whatever they want, I dislike the fact that Niantic has decreased the value of reaching Level 40 because it's easy to farm XP by buying raid passes.\\nHardcore Free2players do not use money on the game so it's not an option to start using money, that would go against their terms.\", 'subreddit': 'pokemongo'}\n",
      "{'body': 'Same! ', 'subreddit': 'BigBrother'}\n",
      "{'body': '[deleted]', 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Oh you motherfucker, well played!', 'subreddit': 'gaming'}\n",
      "{'body': 'everyone knows that. even you ', 'subreddit': 'PS4'}\n",
      "{'body': \"I think what he's saying is that people tend to weigh offense more than defense for MVP. This is historically what the voters do (usually). \\n\\nAlthough with Trout it seems they're looking more at sabermetrics so who knows? \", 'subreddit': 'baseball'}\n",
      "{'body': 'Looks like GMK Sky Dolch, or something super similar', 'subreddit': 'MechanicalKeyboards'}\n",
      "{'body': 'I, too, spent a great deal of time trying to figure out what the Roadhouse characters had to do with the main story. I thought they had to tie into Audrey because of the \"Billy\" and \"Tina\" connections. Alas, we are left with no resolution on this, just like we are left with no resolution on Audrey whatsoever.\\n\\nI think your instincts are good, but that you are being too specific and literal with these scenes. Someone already posted the Lynch quote saying that the Roadhouse vignettes were never meant to be anything other than \"slice-of-life\" moments in the town of Twin Peaks, with no connection to the larger story. What I do think they connect to are the themes of awakening and duality. The show gives us many clues that we are watching two worlds, and that awakening is the key to crossing from one to the other.\\n\\nIn my opinion, the Roadhouse is a junction point between worlds. Anyone seen in the Roadhouse is not awakened in the beginning of TP, and awakens by the end.\\n\\n* Audrey is the most obvious\\n* James was in an accident and is \"quiet\" now\\n* Shelly is trapped in the same self-destructive cycle of being with terrible men\\n\\nThe random bar conversations are part of this same routine. Whether it\\'s drugs or insanity or infidelity, the people are all trapped in some kind of cage. When they awaken, they will be gone from the Roadhouse forever, just like Audrey.\\n\\nRichard was probably trapped by his own evil heart, hence his appearance at the Roadhouse. But unlike the others, he didn\\'t get a chance to awaken. He just got killed.', 'subreddit': 'twinpeaks'}\n",
      "{'body': '[deleted]', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': \"[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnodjog/):\\n\\nI've often thought that SciFi is at its core a sociological genre - put normal human beings in outlandish situations and examine what the outcome of that may be.\\n\\nDo you agree with that and what are some of the main themes you adress in this book?\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'They won the restaurant wars! ', 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"Le problème c'est le nombre de gens qui se cassent justement parce que d'une il y a relativement très peu de postes de permanents, et de deux parce que le salaire est minable comparé à ce qui ce fait à l'étranger ou dans le privé.\", 'subreddit': 'france'}\n",
      "{'body': 'Sick lol', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': \"Who's the last hab to have a 6 point game lol\", 'subreddit': 'Habs'}\n",
      "{'body': \"Yeah this was the biggest plot hole to me. Instead of just giving up when they figure it out he needs to plan Phase 2 where he either A) convinces them they're wrong, or B) tortures them in other ways once they figure it out. I feel like Vicki will have a better grasp of this. \", 'subreddit': 'TheGoodPlace'}\n",
      "{'body': 'garfield_irl', 'subreddit': 'hmmm'}\n",
      "{'body': \"Not really no, my guys are union and just got their pension slashed by 70%, although the pension is through the union so perhaps it's different \", 'subreddit': 'personalfinance'}\n",
      "{'body': '[removed]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Question from a noob, Im having trouble getting some hard stuck on bird poop that didn't come off after 2 soap sprays and rinses with a power washer and hand wash with microfiber mit.  I used the mit and got most of it but there is a white haze area around it still that i cannot for the life of me get off. I dont want to scratch the paint trying to get it off.\\n\\nAny ideas would be greatly appreciated. \", 'subreddit': 'FocusST'}\n",
      "{'body': \"&gt;he became a multiversal abstract \\n\\nThis is not true. He spread across the universe but that's not multiversal at all. Source?\\n\\n&gt;Here is a time ring.\\n\\nJust as you said in this same post, a wiki link isn't proof of anything. Go get me actual proof.\", 'subreddit': 'whowouldwin'}\n",
      "{'body': \"Plot twist: there was a Mini P.E.K.K.A. with 239 HP underneath. Playing The Log allowed me to kill the Mpekka and cycle to my Arrows, saving us the game!\\n\\nDon't quit, kids!\", 'subreddit': 'ClashRoyale'}\n",
      "{'body': \"Take it to a vape shop. Garuntee they'll have something that can fit it. \", 'subreddit': 'knives'}\n",
      "{'body': \"Goku wasn't really brought back either yet he's in the category.\", 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': 'Clean!', 'subreddit': 'WWII'}\n",
      "{'body': 'Fuck off.', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'funkoswap'}\n",
      "{'body': 'The NLCS perhaps?', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': '[deleted]', 'subreddit': 'supremeclothing'}\n",
      "{'body': 'People are looking at me funny cause I was laughing so hard😂 that is one of the greatest things I’ve ever seen', 'subreddit': 'MCFC'}\n",
      "{'body': '23', 'subreddit': 'RandomKindness'}\n",
      "{'body': 'America looking like Syria.', 'subreddit': 'politics'}\n",
      "{'body': 'lolwut? ', 'subreddit': 'The_Donald'}\n",
      "{'body': 'Cue the patented Auburn third quarter slump. ', 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '*Slow Clap*', 'subreddit': 'StarWarsBattlefront'}\n",
      "{'body': 'As the name suggests, should be a place to farm something... some sort of special token?', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Garbage bags on the seats and steering column. Carpet comes out and dries much easier.', 'subreddit': 'Jeep'}\n",
      "{'body': 'Cool! Sent a pm. ', 'subreddit': 'asianbeautyexchange'}\n",
      "{'body': 'If your party has a healer (especially a healer who casts *aid* every morning), drop *armor of agathys*. If your party has a character with good Charisma skill modifiers, drop *disguise self*.', 'subreddit': 'DnD'}\n",
      "{'body': \"That's a sweet deal right there. I almost bought one in way worse condition for $100 a few months back, ungraded too.\", 'subreddit': 'pkmntcgcollections'}\n",
      "{'body': 'Thats exactly what someone who hates people like them would say.', 'subreddit': 'GlobalOffensive'}\n",
      "{'body': \"That's unlucky. Cults suck, huh?\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Good Lord they just shifted the entire Old World down...\\n\\n[Risk world map](http://static4.businessinsider.com/image/51e6d49b6bb3f7a42d000008-926-563/screen%20shot%202013-07-17%20at%201.29.36%20pm.png)\\n\\n[Real world map](http://geology.com/world/world-map.gif)', 'subreddit': 'MapsWithoutNZ'}\n",
      "{'body': 'You have thoughtfully tried to explain another perspective, shit like that will get you downvoted and kicked out of the circle jerk.\\n\\n', 'subreddit': 'nfl'}\n",
      "{'body': 'aaaaaaaaaaaaaahhh freaking out!! I made him get me tampons while he was out! He was texting asking what kind the same time I was trying to take a pic trying to process what I was seeing!', 'subreddit': 'TFABLinePorn'}\n",
      "{'body': \"Finally defeated Lyrith for the first time.  All that work... for a water samurai.  At least I don't need to fuse one for the fusion.\", 'subreddit': 'summonerswar'}\n",
      "{'body': '143418691| &gt; Japan Anonymous (ID: bwkKKbpn)\\n\\n&gt;&gt;143412250 (OP)\\n2005 Meme Tadoshi\\n2009 Dikku Ratino\\n2013 Keku Potato\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnodwfm/):\\n\\nOhhhh, I like your view on that. I think that it could be, for certain novels, but I wouldn't put the whole genre in there. This is because there are books, like mine, where the main characters aren't human and therefore it creates a different balance of things. However, I could see the validity of your point in novels that do feature humans in outlandish situations. \\nMy main themes that are addressed within this book are ambition, coming of age, courage, discovery, and loss. \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'this, but unironically', 'subreddit': 'neoliberal'}\n",
      "{'body': \"Thanks! I guess we would call that a camp bed. I've always wondered what you meant by cot. I know I could have googled it so thanks for the reply. \", 'subreddit': 'politics'}\n",
      "{'body': 'You are correct. You are biased ;)', 'subreddit': 'politics'}\n",
      "{'body': \"The Dark Tower series has been fantastic so far for me. The Gunslinger (first one in the series) was a great intro to King's writing I thought. \", 'subreddit': 'books'}\n",
      "{'body': \"Fuck that, I make semi-near that in my job and I love what I do. No fucking way I could lay down that long for that. You'd need to comp me like 200k for that. \", 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"Someone get a bucket of water for Marlo's thristy derrière. \", 'subreddit': 'BravoRealHousewives'}\n",
      "{'body': 'Is your letter K gone as well?', 'subreddit': 'hiphopheads'}\n",
      "{'body': 'Roll Tide, fuck Tennessee, fuck Auburn\\n\\nGeorgia, you are alright people', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'TrumpCriticizesTrump'}\n",
      "{'body': 'Bogus news.\\n\\n #fakenews', 'subreddit': 'lakers'}\n",
      "{'body': 'This guy jokes. ', 'subreddit': 'Jokes'}\n",
      "{'body': 'because its a 4-5* egg......pretty sure its less than a 5% chance for a nat 5', 'subreddit': 'MSLGame'}\n",
      "{'body': '[deleted]', 'subreddit': 'MLBTheShow'}\n",
      "{'body': \"What's his name!?!?!?\", 'subreddit': 'aww'}\n",
      "{'body': 'This was unnecessary bot. ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"This post has been removed by AutoModerator.\\n\\nYour account must be at least one month old *and* you must have at least 50 comment karma in order to create a self post.\\n\\nIf you're new to the sport and want to ask a question then check out /r/SoccerNoobs.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/soccer) if you have any questions or concerns.*\", 'subreddit': 'soccer'}\n",
      "{'body': 'Sorry for your loss, happy for your memories.', 'subreddit': 'blackcats'}\n",
      "{'body': '[deleted]', 'subreddit': 'The_Donald'}\n",
      "{'body': 'Praise be. ', 'subreddit': 'penguins'}\n",
      "{'body': \"What's the plane behind the 737? Is that a Dreamliner?\", 'subreddit': 'AviationPorn'}\n",
      "{'body': '[deleted]', 'subreddit': 'weekendgunnit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '[deleted]', 'subreddit': 'goldenretrievers'}\n",
      "{'body': 'This is a weird one for sure', 'subreddit': 'nintendo'}\n",
      "{'body': 'Can confirm. Am fan.', 'subreddit': 'television'}\n",
      "{'body': 'Wow beautiful wig, what a fag', 'subreddit': 'howardstern'}\n",
      "{'body': 'Mularkey!', 'subreddit': 'startrek'}\n",
      "{'body': 'Looks normal to me, cheers. 🍻 ', 'subreddit': 'graphic_design'}\n",
      "{'body': \"Thanks for posting . I've  sent the link to others.\", 'subreddit': 'WayOfTheBern'}\n",
      "{'body': 'Everyone wearing pajamas outside of the house!', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I expected a Photoshop, but I think this one is legit, guys!', 'subreddit': 'FloridaGators'}\n",
      "{'body': 'Yeah, sorry about that.', 'subreddit': 'CarAV'}\n",
      "{'body': '22, about 4-6 months.', 'subreddit': 'askgaybros'}\n",
      "{'body': \"Try using [[Liberty]] or JailProtect from Julio's repo. \", 'subreddit': 'jailbreak'}\n",
      "{'body': \"I had a similar problem where I would turn it on and audio wouldn't come out or it would be flashing the menu screen while ear deafening audio would play. It turned out that it was because of the HDMI switch that I was using, even though every other device I had connected to it worked fine the snes mini just doesn't\", 'subreddit': 'SNESmini'}\n",
      "{'body': \"Yep, Just got my copy refunded. Definitely disappointing that it doesn't live up to the old forza games.  \", 'subreddit': 'forza'}\n",
      "{'body': 'Dude, CoD1 and UO are still active. :P', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Noice.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I'm not seeing it. \", 'subreddit': 'buildapcsales'}\n",
      "{'body': 'D-Daddy...', 'subreddit': '2007scape'}\n",
      "{'body': 'https://www.reddit.com/r/traps/comments/57fwi6/i_go_on_dates_with_guys_and_dont_tell_them_im/', 'subreddit': 'traps'}\n",
      "{'body': 'Okay np', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"HEY SIR THAT'S MY VOICE\", 'subreddit': 'teenagers'}\n",
      "{'body': 'Aghs BS is pretty common in pro games tho. ', 'subreddit': 'DotA2'}\n",
      "{'body': 'yeah it was about [Q2 2015](http://darrellx.com/buildapc/in-depth-seagate-western-digitals-harddrive-price-fixing/) when on a deal you could snag a TB at less per TB than 2011.  Still pisses me off. ', 'subreddit': 'buildapc'}\n",
      "{'body': 'One random spot please!', 'subreddit': 'edc_raffle'}\n",
      "{'body': \"or german, or portuguese, or dutch, or ~~romans~~ italian, or russian, or from the Balkans, or ...\\n\\nYou know what, just tell us where you come from, that'd make it easier.\", 'subreddit': 'europe'}\n",
      "{'body': 'yes exactly! Most fish once you get them in a good tank will change color, typically become more vibrant\\n\\nBetas are one of the few where the color change can be so drastic ', 'subreddit': 'Aquariums'}\n",
      "{'body': \"I've read that part too. And it doesn't change my mind about corruption that I've stated before.\", 'subreddit': 'worldnews'}\n",
      "{'body': 'Won Super Bowls as a coach, AND a player. Still no love. Also, first Hispanic head coach to win a Super Bowl.  ', 'subreddit': 'nfl'}\n",
      "{'body': 'kinosaurus6', 'subreddit': 'Fireteams'}\n",
      "{'body': 'Seems to be spurious: https://en.wikiquote.org/wiki/Aristotle#Disputed\\n\\nI agree with the sentiment, though.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"To me it's kind of a costume I put on 5 or 6 times a year. It's fun and women love it. If I had to wear every day I'd be in the wrong career.\", 'subreddit': 'malefashionadvice'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"If your bench is big enough to stash him and you have someone worth dropping, why not?\\n\\nWho's available on the WW/FA that you're thinking about? \", 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'John Malkovich seems to know what he’s talking about.  ', 'subreddit': 'The_Donald'}\n",
      "{'body': 'they added favele back years ago', 'subreddit': 'MW2'}\n",
      "{'body': \"I'm a 6'8 SF Playmaker/Shot Creator and I'm shooting close to 47% at 81 ovr.\\n\\nI basically just do pick n roll and can usually beat my guy to rim 1-on-1\", 'subreddit': 'NBA2k'}\n",
      "{'body': 'Yes!', 'subreddit': 'The_Keepers'}\n",
      "{'body': \"It doesn't start shooting until January next year.....\", 'subreddit': 'marvelstudios'}\n",
      "{'body': 'It depends on where you go.  Go somewhere that makes the most economical sense such as An in-state school (if your not in like California), or any non-for-profit schools.\\n\\nAs a musician or producer, you will naturally not be making a whole lot to pay off a huge amount of debt at a reasonable rate unless you would make it big.  And let’s face it, few make it big.  So if you college would cost you $200,000 for 4 Year’s, not worth it.  If college would cost you $50,000 potentially!\\n\\nA degree would provide a good structured environment and fellow peers that would allow you to collaborate and build connections with.  At the end of the program, you would know your stuff, and proof to back it up, while without a degree you may have not had experience with one thing, but a lot in another.\\n\\nFor me, I’m going to school for a job that pays alright that I enjoy, but not my favorite hobby, while also getting a music minor.\\n\\n', 'subreddit': 'WeAreTheMusicMakers'}\n",
      "{'body': \"i use both the greens and the whites. if you cook them for a couple of hours, they'll be soft enough. i know that's anathema to some, so maybe one of these days you should try both with and without the greens and see if it makes a difference to you. lots of less-informed people use leek greens all the time, and leek greens (which i realize are not the same thing as green onions) are commonly used in certain Chinese dishes.\\n\\nand like others have said, rinse rinse rinse all the dirt out!\", 'subreddit': 'AskCulinary'}\n",
      "{'body': 'That back seam is fucking great man.', 'subreddit': 'streetwear'}\n",
      "{'body': \"Traps are gay and that's okay. \", 'subreddit': 'settlethisforme'}\n",
      "{'body': '\"I do not admit that a great wrong has been done to the Red Indians of America, or the black people of Australia by the fact that a stronger race, a higher grade race has come in and taken its place.\"\\n\\nHe was a fucko by nearly any modern standard.  That said, he did have some redeeming qualities. I think the main problem with most Churchill films is that they fail to show both sides of the man.', 'subreddit': 'movies'}\n",
      "{'body': 'The only reason I have two landing pads instead of one is to have ships visit and make the base seem more alive.', 'subreddit': 'NoMansSkyTheGame'}\n",
      "{'body': 'I think the only need he needs is his turrets. Not their damage or anything but only their fire rate. If you lower that you could make his turrets much more balanced. It’s just the torvald Barik combination that has become the meta, it’s hard to balance wrecker, caut and bulldozer in a team.', 'subreddit': 'Paladins'}\n",
      "{'body': 'It honestly never occurred to me that people saw the two entities as separate.', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'I am very sorry for your pain.  \\nWas your husband 17 when you married??', 'subreddit': 'relationship_advice'}\n",
      "{'body': '[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoeidp/):\\n\\nThanks for the reply! \\n\\nI think that even in books that feature primarily or solely non-human characters the main scope would still fall under sociology, since it is impossible to be completely divorced from our own humanity. It would then be a view of humanity through an alien lens, but still a human view by necessity. Because if it were to be fully alien then there would be nothing for a human reader to identify themselves with. \\n\\nIn the seminal \"The Dance of the Changer and Three\" by Terry Carr the only human character is the narrator who tries to explain this fully alien story to a human audience and coming to the conclusion that the story must be taken as is, because they are alien any human interpretation of this story that is so important to this alien culture must always fall flat because we cannot ever completely understand their psychology. \\n\\nThe Dance of the Changer and Three teaches us that whatever we do, we will always be locked inside our own skulls, viewing the universe through human eyes. \\n\\nI  wish you the best and hope your book is a great success. :)', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"Jemmye claims to be smart because she's not a good athlete and out of shape. She's a good social player, but is also weak mentally. \", 'subreddit': 'MtvChallenge'}\n",
      "{'body': \"I'd drive it..maybe change out those headlights.. \", 'subreddit': 'heep'}\n",
      "{'body': \"Honestly as much as I wish he wasn't the qb I do respect him for trying. Chicago is by no means a dream team to play on and he knew that coming in here. It was a shot in the dark and I thank him for giving me something to cheer for I guess. Would love to keep him as a back up, I think he makes a decent one.\", 'subreddit': 'CHIBears'}\n",
      "{'body': '143412306| &gt; None Anonymous (ID: 0SeYs2Tv)\\n\\n&gt;&gt;143412250 (OP)\\nneck yourself\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'No, I am not vegan. That\\'s a different argument in my opinion. Rights and treatment of livestock are, of course, also an important thing to be critical of. However, the fact that PETA runs \"adoption centers\" for companion pets and kills almost 90% of them is fucking atrocious.', 'subreddit': 'todayilearned'}\n",
      "{'body': '[deleted]', 'subreddit': 'changemyview'}\n",
      "{'body': 'I\\'m sure this \"Skeptic conference\" was very informative and not just a auditorium full of people who continue to make the YouTube comment section a cesspool. ', 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': \"There are some pretty good LGA775 Motherboard out there from reputable recyclers.  This pc has a Asus p5g41t-m lx in it.  It is the second one I have bought on eBay.  The other is in a PC with a Q6600 I made for the kids to do homework.  Can't beat the value of these Core 2.\", 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'My my you are amazing at this...', 'subreddit': 'secretsubgonewild'}\n",
      "{'body': \"Monroe wasn't photographed for Playboy. The publishing rights to the photographs weren't owned by Monroe. They had been taken previous to Playboy's publication, and were owned by a third party.\", 'subreddit': 'todayilearned'}\n",
      "{'body': 'Lowes works fine for me.  I even had to do a return at Lowes and had to \"swipe\" my phone for that and the return processed successfully.', 'subreddit': 'SamsungPay'}\n",
      "{'body': \"You wouldn't jam a king in there? It takes 2 seconds and you don't have to stop compressions.\", 'subreddit': 'ems'}\n",
      "{'body': 'Best moment of the episode. \\n\\nI was never really a fan of the comedic styling in some of the older/no-longer-around videos, and for a while after Greg left it seemed pretty evident that Jirard may have been struggling to find his new comedic voice. But with this scene, this one side-skit, this could not have been done better. It may not be to some peoples taste, but to me, the immediate sight gag of Brett and the long drawn out \"where is this going\" Tarantino-esque vibe, followed by the perfect timing of the punchline. Goddamn. I wasn\\'t laughing, but this mother fucker right here could have given the Cheshire Cat a fuckin\\' inferiority complex. Grinning ear to ear.\\n\\nThis shit is my jam.', 'subreddit': 'TheCompletionist'}\n",
      "{'body': 'YES! More Kuzma playing time!!', 'subreddit': 'lakers'}\n",
      "{'body': 'You could argue that Nazi ideology poses a threat', 'subreddit': 'Maher'}\n",
      "{'body': 'Damn, they shit should be better monitored and controlled.', 'subreddit': 'Parenting'}\n",
      "{'body': 'Walmart.com\\nThey deliver for free and you can take what you do not want right back to the store!', 'subreddit': 'halloween'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Fuck Bethany', 'subreddit': 'ComedyCemetery'}\n",
      "{'body': 'Thank you both for sharing your experiences! It’s helping me think of questions to ask my doctor. I feel like I can never think of any until after all is said and done. \\n\\nI’m sorry to hear about your failed induction :(', 'subreddit': 'BabyBumps'}\n",
      "{'body': \"Yeah that was a completely isolated yellow card incident, shouldn't have anything to do with repeated infringements.\", 'subreddit': 'rugbyunion'}\n",
      "{'body': '[removed]', 'subreddit': 'The_Donald'}\n",
      "{'body': 'I low key believe that they purposefully lower the green orb rate during certain banners.  I sniped exclusively green (and even bought 100+ orbs) on the Hero Fest banner a while back, and in every summon, never more than 2 green orbs appeared at once, when they bothered to show up at all.\\n\\n/tinfoil hat', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'Veignn', 'subreddit': 'Fireteams'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnof35w/):\\n\\nThat sounds interesting- I'll have to give it a read! \\n\\nThank you so much for your support!\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Did anything change with your settings? Double check all the boxes and addresses. I’ve had the window immediately close when I have errors in my command/batch file', 'subreddit': 'vertcoin'}\n",
      "{'body': 'Damn, Humans of New York is really getting stretched for ideas.', 'subreddit': 'funny'}\n",
      "{'body': 'Mahalo broseph ranch it up ', 'subreddit': 'Chargers'}\n",
      "{'body': \"So what you're saying is...\\n\\n...we're gonna need a bigger bolt\", 'subreddit': 'Skookum'}\n",
      "{'body': 'love you bb', 'subreddit': 'teenagers'}\n",
      "{'body': 'We ran out a couple of weeks ago ):', 'subreddit': 'starbucks'}\n",
      "{'body': \"I wanted to join the military. I wanted to see the world, and to prove that there was more to me than everybody thought.\\n\\nOne bout of self-harming later, that dream was dead. I'm now a engineer who spends everyday trying to prove that there is more to me than everybody thinks.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Ctrl-alt-NOPE', 'subreddit': 'manga'}\n",
      "{'body': 'I work for Child Services. We had a parent whose kids were detained because we received a report that both parents were dealing drugs from the home shared with the kids and using regularly without anyone else there to watch the children. When investigated, this parent admitted to all of the allegations as well as to shooting up in front of the kids. After the intial hearing, this parent posted on Facebook that they were \"lockin n loadin. comin for that fat bitch and mofo lawyer who think they can take my kids.\"\\n\\nThere have been other vague threats and anger at court, but usually more ambiguous than that.', 'subreddit': 'Ask_Lawyers'}\n",
      "{'body': 'yes he has had my money for 5 days and i was upset that he never sent and he went back on the deal we made. and i got banned for being upset about this', 'subreddit': 'Etizolam'}\n",
      "{'body': 'Priceless', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Mod the colours yourself, it's pretty simple.\\n\\nNow, about the borders, IIRC the old border was the modern border, while the current one is more accurate.\", 'subreddit': 'Kaiserreich'}\n",
      "{'body': 'Osc 1 and 2 are turned on and so is Noise.', 'subreddit': 'edmproduction'}\n",
      "{'body': \"Yes, when it's time. Not before it's ready. Not because Reagan or Trump or whoever the president is wanted to make US corporations more profitable for political reasons.\", 'subreddit': 'Futurology'}\n",
      "{'body': '[deleted]', 'subreddit': 'StockMarket'}\n",
      "{'body': \"I have no problem with people using it.  My issue is I've never played with it a n any game and now it's messing up my aim lol\", 'subreddit': 'FortNiteBR'}\n",
      "{'body': 'Gipsy Proud. e misto tare ms de raspuns... faza e ca cel mai trol nu mai am unde. Decy mai bine ca mama sa fie mai frumos lucru pe care o sa aiba s classe.', 'subreddit': 'romania_ss'}\n",
      "{'body': 'Cat.', 'subreddit': 'CatsStandingUp'}\n",
      "{'body': 'Gradle tasks. Or Maven if you happen to use it.', 'subreddit': 'java'}\n",
      "{'body': \"I still don't understand why this man ever reached any sense of fame at all - seems like a massive douche, terrible journalist and just straight up unlikeable. The only thing good about Piers Morgan is his Twitter account when Arsenal loses.\", 'subreddit': 'WatchPeopleDieInside'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'You want to do that on a basketball discussion board? ', 'subreddit': 'bostonceltics'}\n",
      "{'body': 'Very cool guide. ', 'subreddit': 'pokemon'}\n",
      "{'body': 'That\\'s so strange! On my mobile it says \"13 images\" and the wheel is missing. Thank you ', 'subreddit': 'interestingasfuck'}\n",
      "{'body': \"Race relations are fucked in this country and it only got worse with Trump being elected. I'd rather a group of celebrities kneel for 2 minutes every week than have citizens tear up their city with riots.\", 'subreddit': 'Redskins'}\n",
      "{'body': \"You're right there isn't...no wonder I lost the link at the home page.\", 'subreddit': 'grandorder'}\n",
      "{'body': \"I like to diversify my investments so check out another great site outside of the bitconnect zone. FYI I also invest with Bitconnect both of these are great platforms.\\nI wanted to give you another site that has been paying me and I was able to withdraw my earnings on a daily basis and the transactions are quick too! The company is called BitPetite and the concept is great! Below are few details of what they have going on and I also received a newsletter from them that they will be expanding their investment options so, I think this site is not a scam but like all HYIP's, withdraw your monies on a daily basis. So far, I have earned over $100 and have only been in it for 2 weeks. I usually test the waters first before I invest a bit more and with that said, I will be investing into LiteCoin. 2 investment options: 147% after 6 weeks and 180% after 9 weeks. Interest: 3.97% on working days and 1.00% on weekends. Minimum deposit: 0.005 BTC. Refund of the principal along with interest payments. Withdrawal minimum: 0.005 BTC or $10. Payment methods: Bitcoin or USD equivalent. Affiliate program: 10%-5%-2%. Here's the link to join this opportunity and like I always say, I am not a financial adviser thus, you are investing into these things at your own risk. But as for me, what is life without taking some small risk? Invest small and see what you think but I honestly can tell you it's worked for me thus far. Here's the link:https://bitpetite.com/?aff=stonesour73\", 'subreddit': 'Bitconnect'}\n",
      "{'body': 'Why is this a post', 'subreddit': 'funny'}\n",
      "{'body': \"If this has always been a thing, I'm amazed the map's been available for almost a year and this was just now noticed. \", 'subreddit': 'GlobalOffensive'}\n",
      "{'body': \"You'd think an old world symbol at a sacred site would be a bad thing.\", 'subreddit': 'canada'}\n",
      "{'body': \"Keyword is *might* clearly it's context dependent. Like I wouldn't buy someone a beer for randomly punching someone in a Che T-Shirt. But if one was being sufficiently dickish and got punched I might buy them a beer when they got out of jail. I've had friends go to jail for things that, while illegal, were emotionally understandable. A cold beer when they get out is just being nice. \", 'subreddit': 'samharris'}\n",
      "{'body': \"I don't think so. Doesn't the game have black cinema bars when you warp in? Then they disappear when you get control of your ship back?\", 'subreddit': 'NoMansSkyTheGame'}\n",
      "{'body': '\"Cake.\"', 'subreddit': 'CampHalfBloodRP'}\n",
      "{'body': \"Because, as of right now while religion is a protected class, ideology isn't. That's why people are allowed to discriminate against neo-Nazis, but aren't allowed to discriminate against Muslims. \", 'subreddit': 'news'}\n",
      "{'body': 'How about a Chinese painting instead? \\n\\nhttp://www.pheilcia.com/products/keycaps/35/', 'subreddit': 'MechanicalKeyboards'}\n",
      "{'body': \"When he rolled for neckbeards' charisma stats, he rolled a one.\", 'subreddit': 'justneckbeardthings'}\n",
      "{'body': \"That sucks. I came across some 4'10 woman on AIU u/ pipcleaner. You'd appear very tall to her\", 'subreddit': 'amiugly'}\n",
      "{'body': '143417872| &gt; None Anonymous (ID: o0cqoZbW)\\n\\n&gt;&gt;143417408\\nbut cities are centers of diversity and culture you racist bigoted misogynistic trans phobic piece of shit\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Mind linking the cable you’re using? It might just be the inline repeater that’s preventing you from having the same issue. ', 'subreddit': 'pcmasterrace'}\n",
      "{'body': \"It's just logical though, if dozens of devs that all code differently have come and gone, of course the code is going to be all over the place.\", 'subreddit': 'tf2'}\n",
      "{'body': '&gt; a 99% white town in america\\n\\nhahahahaha. there is no such thing. give me the name. \\n\\nbtw my country has less than 5000 muslims out of 5 million people. you probably have more than that in your supposedly white town lmao. ', 'subreddit': 'milliondollarextreme'}\n",
      "{'body': 'Solving the discrete log problem would break RSA?', 'subreddit': 'crypto'}\n",
      "{'body': 'This is Solo Q man. The point is that the scaling should be done on an overall team basis. And the difference was way more than 10ip overall as you said :)', 'subreddit': 'albiononline'}\n",
      "{'body': '#[We know what you are doing, OP.](https://www.reddit.com/r/UnethicalLifeProTips/comments/73igbu/ulpt_persuade_some_redditors_to_keep_cash_between/)', 'subreddit': 'LifeProTips'}\n",
      "{'body': \"Interesting.\\n\\nStill, unless you're overclocking or run the highest end i7, pk3 is more than enough thermal conductivity for 15 watts. The hard limit is really the metal and fan involved. \", 'subreddit': 'thinkpad'}\n",
      "{'body': \"lol yes actually. He was gassed from carrying. We got killed whenever he was on the bench so he played more minutes than Harden while continuing to have more useage than him. There was one particularly disastrous 2 minute spell at the end of a 3rd quarter with a 10 point turnaround. Beginning of the 4th quarter he was straight back in early. It was probably the game you've referenced actually.\", 'subreddit': 'nba'}\n",
      "{'body': 'I was the whm, he was holding the sign for me haha', 'subreddit': 'ffxiv'}\n",
      "{'body': \"[Then why can't I use all of the warfare abilities with a wand equipped?](https://i.imgur.com/nkJjZCi.png)\\n\\nedit: Downvoted for posting proof? OK then, lol.\", 'subreddit': 'DivinityOriginalSin'}\n",
      "{'body': \"It doesn't, but people think we honor our nation and troops by playing the anthem at every single sporting event. I think it's odd.\", 'subreddit': 'nba'}\n",
      "{'body': 'We kinda did that last week. There were a lot of quick passes, some with a moving pocket, and a lot of play action on the deeper passes to help slow down the pass rush. We made them respect the running game and slow down the pass rush. So in a way we did do that and that’s why our offense was more successful', 'subreddit': 'Texans'}\n",
      "{'body': 'red onions and red peppers are a must. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I don't think it was the killing, I think it was the chase. It was the ferreting out of anything or anyone he was after. He was still a detective at heart. An amoral, vicious detective, but still.\", 'subreddit': 'AskReddit'}\n",
      "{'body': '4?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Thanks for posting, **/u/Bestielingerie**! Your submission has been automatically removed because the title includes tags that are not used by this subreddit. Please see the tags we use in the sidebar.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/FetishSelling) if you have any questions or concerns.*', 'subreddit': 'FetishSelling'}\n",
      "{'body': \"I'm 100% sure it's fake but with some decent guesses. But guess =/= insider info.\", 'subreddit': 'starwarsspeculation'}\n",
      "{'body': '&gt; oing locums work for a few years in between jobs too. So yes, you can work locums in surgery. Sounded like it paid pretty decently but hard t\\n\\nthanks. I would really like to work locums for a few years', 'subreddit': 'Residency'}\n",
      "{'body': 'He’s a great vendor, he will sort it out for you. Very reliable and it’s a great ID.', 'subreddit': 'fakeid'}\n",
      "{'body': 'Dammit. I completly forgot that you were using a submod!', 'subreddit': 'CK2GameOfthrones'}\n",
      "{'body': 'r/totallynototters', 'subreddit': 'Eyebleach'}\n",
      "{'body': 'Hair. ', 'subreddit': 'blunderyears'}\n",
      "{'body': \"[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnofp3n/):\\n\\nYou definitely should, it's a great short story!\\n\\nI looked, but it doesn't seem to be available online in a full version. But I'm sure you can find a book of short stories that has it for very little money online. It's been reprinted a lot. \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Cite a source that says what you are saying. Cite a source that says this was the case in any of the last 5 2k games. Or honestly you have nothing to speak on. ', 'subreddit': 'NBA2k'}\n",
      "{'body': 'NFL.com video: Enemy Intel: The Power Of Play-Action [HD](https://phieagles.akamaized.net//PHI/videos/dct/video_audio/2017/09-September/GP17WK04-at-Chargers-Feature-B1B2-Enemy-Intel-and-Breakdown-OFF-5000k.mp4) [SD](https://phieagles.akamaized.net//PHI/videos/dct/video_audio/2017/09-September/GP17WK04-at-Chargers-Feature-B1B2-Enemy-Intel-and-Breakdown-OFF-500k.mp4)\\n\\n', 'subreddit': 'eagles'}\n",
      "{'body': '[deleted]', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'This content brought to you from \"Taiwan Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#off site feed \"Taiwan Pool\")\\n', 'subreddit': 'ImagesOfTaiwan'}\n",
      "{'body': \"People keep turning a blind eye to the death toll, as in not even the local newspaper has dared to give a real number yet.    \\n  \\nI'm sorry you're so angry and bitter. Maybe you should go to that subreddit yourself.\", 'subreddit': 'worldnews'}\n",
      "{'body': 'I just turned on the TV; with all these fucking lines, I keep thinking Devonta Freeman is about to pick up the ball and start running with it\\n\\nEdit: this field is a piece of shit', 'subreddit': 'AtlantaUnited'}\n",
      "{'body': \"Or you know, EMU might have a good DL... Bad day may have played into it but with the level of recruits that Kentucky gets compared to EMU there's no excuse for the line \", 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'God I loved me some Jamal Lewis  ', 'subreddit': 'nfl'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'No, currently overseas now its fucking joke', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Love your nipple ring', 'subreddit': 'wifesharing'}\n",
      "{'body': 'Ah ok. That makes sense. ', 'subreddit': 'RTLSDR'}\n",
      "{'body': 'Vroom vroom ', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'This is so good.  Watching this woman expose her female nature for the whole world to see.  \\n\\nA TO THE MOTHERFUCKING WALT', 'subreddit': 'MGTOW'}\n",
      "{'body': \"Hey sorry, was away for a bit. Yeah sure I'll take it!\", 'subreddit': 'GameSale'}\n",
      "{'body': \"Best show on TV. Problem is I've watched them all multiple times.\", 'subreddit': 'aviation'}\n",
      "{'body': 'But he really does have a swine ass!', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'Hahha you can use it under artistic license. ', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': 'Lets bet 1 btc?', 'subreddit': 'btc'}\n",
      "{'body': 'It was always fucking stupid', 'subreddit': 'nfl'}\n",
      "{'body': 'I was that retarded', 'subreddit': 'stalker'}\n",
      "{'body': 'Yeah, but what do you do with the extra pedal?', 'subreddit': 'science'}\n",
      "{'body': 'The ultimate jefi', 'subreddit': 'gifs'}\n",
      "{'body': 'All I got was downvoted. Not really sure why.', 'subreddit': 'pokemongo'}\n",
      "{'body': 'It’s a natural response.  I like it, a lot.  This is going to be inexcusable for a lot of people, it’s definitely going to turn off people who are on the fence about Trump.  Remember how bad George W looked after Katrina?  And he didn’t even say anything that stupid about it. \\n\\nWhat’s not cool is walking around like a smug asshole about it.  Keep that shit to yourself.  That hurts the cause and perpetuates the stereotype of the smug liberal.  ', 'subreddit': 'politics'}\n",
      "{'body': 'I do a similar thing with my Titan. I run the boots that lets me insta draw my trials SMG   ', 'subreddit': 'destiny2'}\n",
      "{'body': 'After my watch lasting 6 hours yesterday I did some digging around and turned off WiFi and location services and more than doubled my battery life.', 'subreddit': 'AndroidWear'}\n",
      "{'body': \"Sometimes I use a pomodoro timer just to get started. After 25 minutes I'm so laser focused and in the zone that I don't even think about distracting myself from the task.\", 'subreddit': 'getdisciplined'}\n",
      "{'body': 'ʎɐpɥʇɹıq ʎddɐɥ', 'subreddit': 'cancer'}\n",
      "{'body': '[deleted]', 'subreddit': 'CFB'}\n",
      "{'body': 'I don’t get the South America part\\n\\nedit TIL South Americans have a reputation for being rude', 'subreddit': 'Jokes'}\n",
      "{'body': \"&gt;so I was wondering if anyone had any idea how much it would roughly cost to wire 12 work stations, cat 6 or 7.\\n\\n[Depends on how long you want it.]\\n(https://www.amazon.com/slp/shielded-twisted-pair-wire/zjqwn87ea454csk)\\n\\n\\n&gt; I'm debating on a switch. A router. Very very basic, but I want it to look nice and last the company a long time.\\n\\n[Hmmmm... not that professionaly looking, but the combination is very nice.\\nDont't mind the gaming marketing]\\n(https://www.amazon.com/dp/B06WGS5RH3/ref=twister_B075ZTJZJM?_encoding=UTF8&amp;psc=1)\\n\\nI have no idea how big the company will be.\\nBut this you can look at the suggestions that i've mentioned above.\", 'subreddit': 'techsupport'}\n",
      "{'body': 'What about my statements!? Did you give them a chance!? DID YOU!?', 'subreddit': 'raimimemes'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnofrui/):\\n\\nAwesome, I'll put it on my to read list!\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Head of the epa doesn't believe in climate change.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Cause Kurt Angle KNOWS', 'subreddit': 'MMA'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': \"143414125| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413837\\nI was too young but I probably would have voted for Obama. His campaign spoke to me as a young person in ways that Bernie's couldn't in 2016. Obama was just a special candidate which is probably why I supported him over Hillary. I would have voted for her had she won the nomination.\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': \"I don't understand why you'd play a game like EVE and then just join the largest most powerful alliance in the game in your first month.\\n\\nSo it's generally hard to respect their line members off the bat.\", 'subreddit': 'Eve'}\n",
      "{'body': \"I didn't even have to use /s\", 'subreddit': 'detroitlions'}\n",
      "{'body': 'I own a copy of one of the 111 limited editions of 1Q84 by Haruki Murakami!\\n\\nhttp://www.thecurvedhouse.com/portfolio/limited-edition-of-1q84-by-haruki-murakami/', 'subreddit': 'rarebooks'}\n",
      "{'body': \"Sounds like:\\n\\nWhat are Saturdays for?\\n\\nBoys\\n\\nI dunno I don't understand twitter memes or whatever this is.\", 'subreddit': 'hockey'}\n",
      "{'body': 'I don’t discount advanced stats, but in the situation where Rudy Gay has such a small sample size, it’s not 100% accurate of what he’d produced if he was healthy. Also these specific stats don’t factor in defense, which is where Rudy Gay is a plus compared to Pau (purely based off the eye test)  I’m aware of the injury, which is why I was questioning how you though Pau was better prior to the injury. ', 'subreddit': 'nba'}\n",
      "{'body': \"Shi'ar\\n\\nBrood\\n\\nSentinels \\n\\nHellfire club is a threat cause of money and reach, the inner members aren't always mutants\\n\\nSinister, would still have all his non mutant stuff\\n\\nJuggernaut isn't a mutant he's magic\\n\\nPhoenix Force\\n\\nAll the stuff from limbo\\n\\nThe Phalanx\", 'subreddit': 'AskScienceFiction'}\n",
      "{'body': '[deleted]', 'subreddit': 'LifeProTips'}\n",
      "{'body': \"That's so sad :( I can't even imagine what the poor kid was going through.\", 'subreddit': 'marchingband'}\n",
      "{'body': \"Have you considered a speedrun, you've got a natural talent\", 'subreddit': 'xboxone'}\n",
      "{'body': 'Supertightwizard, Titan 285', 'subreddit': 'Fireteams'}\n",
      "{'body': '[deleted]', 'subreddit': 'PickUpTorrents'}\n",
      "{'body': 'So they say. Why not stuff stores with them on launch?', 'subreddit': 'gaming'}\n",
      "{'body': 'I find that alcohol free cider here is closer to mulled apple juice. Quite often it has cinnamon, cloves and orange peel as part of the spice mix', 'subreddit': 'mead'}\n",
      "{'body': 'I need to beef up my team before I do more Squad Battles after this challenge.', 'subreddit': 'FIFA'}\n",
      "{'body': 'Geographic location would have helped but you got it. We googled it and milk snake looks right. We are in MAssachusetts, USA. Thanks!', 'subreddit': 'snakes'}\n",
      "{'body': 'what the fuck did i just watch?', 'subreddit': 'MMORPG'}\n",
      "{'body': 'Was not expecting that. ', 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': 'Stop staring at my screen!', 'subreddit': 'gaming'}\n",
      "{'body': 'In continuation of this:\\n\\nAnyone without tscc = an enemy of tscc\\n\\nAs we know the natural man is an enemy to god.', 'subreddit': 'exmormon'}\n",
      "{'body': 'Young Dolph needs this car', 'subreddit': 'cars'}\n",
      "{'body': 'Colorizebot', 'subreddit': 'colorizebot'}\n",
      "{'body': \"Ive had to move in with my mum again for a few months and I have about 3mb (with about 0.2mb upload) here I want to die. I mean I can just about play pubg on it if no one else is using the internet but god it's painful to use.\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Major bummer, Join a category league :)', 'subreddit': 'fantasybball'}\n",
      "{'body': 'ASSASSins', 'subreddit': 'todayilearned'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Now I feel bad realizing how seriously he was hurt.  Hope he is ok now.  😯', 'subreddit': 'holdmybeer'}\n",
      "{'body': \"Kept out so well that they're not even in the game\", 'subreddit': 'civ'}\n",
      "{'body': 'A classic household item.', 'subreddit': 'engrish'}\n",
      "{'body': 'That sucks. Cops can\\'t just stop and frisk you on the sidewalk for no reason. I know racial profiling happens and there are some real asshole cops out there. I\\'ve heard stories of people getting stopped and patted down because they \"fit the description\" of someone who robbed a house.\\n\\nBut I\\'ve never heard of that happening in the city of Atlanta, at least not near the downtown areas. \"Walking while black\" isn\\'t suspicious in neighborhoods that are 50-100% black.', 'subreddit': 'Atlanta'}\n",
      "{'body': 'I would be happy with $30', 'subreddit': 'gaming'}\n",
      "{'body': \"That's a great idea, actually. \", 'subreddit': 'videos'}\n",
      "{'body': 'The fairness doctrine relied on the government having the right to set rules for a licence combined with the pragmatic practice of granting monopolies on broadcast frequencies with such a licence. \\n\\nThere are no broadcast frequencies on the internet. \\n\\nNobody needs a licence to run a website. \\n\\nSo it would fail at the first constitutional challenge. ', 'subreddit': 'politics'}\n",
      "{'body': \"Yeah, you guys just don't understand it for the same reason you don't understand quantum physics. \", 'subreddit': 'iamverysmart'}\n",
      "{'body': 'A driving job then? Just trying to find the connection between a speeding warning and you being let go...', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Fair enough. I just prefer sitting in front of my tv on the couch to my computer.', 'subreddit': 'GWNerdy'}\n",
      "{'body': \"Being a gay man, I enjoy reminding straight men that you are what you eat. Who's the pussy now?\", 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Spot 28 or one random please ', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'I personally though revelations had some of the best enemy design lol. Drifted from that classic zombie horror but dipped more into body horror, I think it had some of the scariest enemies of the series, the \"mayday mayday\" buzzsaw guy yelling \"please help me I\\'m still human\" while simultaneously trying to kill you really stuck with me. Let me try and find a link.\\n\\nEdit: [Found it](https://youtu.be/mVCjtl2jeyg) listen closely to what it says, honestly pretty terrifying since it makes you think he has just enough humanity to cry for help, or that he died screaming for it. ', 'subreddit': 'residentevil'}\n",
      "{'body': 'If you know something about them and the issues the care about, that helps a little bit. For example, my mom was spreading the word about the dangers of the TPP before the campaign started. So I would just ask her things like, \"Do know if any of the candidates have taken a position yet against the TPP...??? She\\'d have to answer Donald Trump. But then she\\'d say Rex Tillerson is a billionaire, blah, blah, blah so Trump can\\'t be trusted to keep his word on that. \\n\\nBut you\\'re asking them to redefine who they are. They\\'ve labelled themselves as liberals, and that comes with all sorts of positive associations for them that proves to themselves that they are a good person. If they let that go, they are now a bad person according to their ideology. So they need to not only see the hypocrisy and lies and failures of progressivism, but they need pointing out, for example, that African American working poor will fare better with less illegal aliens taking jobs for low pay, etc. Or that the welfare state incentivizes fathers not living with their children, perpetuating a cycle of poverty and delinquency. Otherwise, they feel like they are abandoning all of their victims of the evil white man. \\n\\nI have to say, though, I haven\\'t red pilled a single one. Just sown targeted doubts. ', 'subreddit': 'The_Donald'}\n",
      "{'body': \"To be fair, the idea isn't mein.\", 'subreddit': 'ImGoingToHellForThis'}\n",
      "{'body': '[deleted]', 'subreddit': 'smilers'}\n",
      "{'body': \"It's not Oregon, we will put in the backups hopefully at the beginning of the 3rd quarter\", 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'UBC'}\n",
      "{'body': 'well who would have thunk it... would be interesting if there was a event for this if theres ever a possibility for stalin to take over in Georgia ', 'subreddit': 'Kaiserreich'}\n",
      "{'body': 'I’m so sorry for your loss! I’m sure she’s still super proud of you. You look awesome btw!', 'subreddit': 'FreeCompliments'}\n",
      "{'body': 'Adding it to my fix it list! \\n\\nThanks for letting me know.', 'subreddit': 'teenmom'}\n",
      "{'body': '  \\n[Gideon of the trials](https://img.scryfall.com/cards/normal/en/akh/14.jpg?1496796488) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Gideon%20of%20the%20trials) [(SF)](https://scryfall.com/card/akh/14?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!Gideon%20of%20the%20trials)  \\n[axis of mortality](http://mythicspoiler.com/ixa/cards/axisofmortality.jpg) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=axis%20of%20mortality) [(SF)](https://scryfall.com/card/xln/3?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!axis%20of%20mortality)  \\n^^^[[cardname]] ^^^or ^^^[[cardname|SET]] ^^^to ^^^call ^^^- ^^^Updated ^^^images', 'subreddit': 'magicTCG'}\n",
      "{'body': \"Plus there's LeBron James who was front and center of this whole thing last weekend. He had the N word spray painted on his house. \\n\\nHe has pretty much given as much as anyone could ask for to the city of Akron to help underpriveledged black youth. http://www.foxsports.com/nba/story/lebron-james-college-scholarships-akron-university-cavaliers-i-promise-081315\\n\", 'subreddit': 'news'}\n",
      "{'body': 'This content brought to you from \"Taiwan Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#off site feed \"Taiwan Pool\")\\n', 'subreddit': 'ImagesOfTaiwan'}\n",
      "{'body': 'Actually the comment said NO sugar /s', 'subreddit': 'cats'}\n",
      "{'body': 'oh, sorry! I havnt posted any of the Chi Cards at all. These are just all of the regulat cards. I assume that you thought the cards after the Chi Card tooltip were all Chi Cards? Because only 2 of the 10 carda utilize the Chi mechanic.', 'subreddit': 'customhearthstone'}\n",
      "{'body': 'Taro smoothies taste just like this to me. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I propose a moment of silence at 28:03', 'subreddit': 'MLS'}\n",
      "{'body': \"Decided, I'm rooting for the Twins because Bartolo\", 'subreddit': 'NewYorkMets'}\n",
      "{'body': 'Ser Emmon Costayne +4\\n\\nLord Addam Costayne\\n\\nSer Ryswin Costayne', 'subreddit': 'IronThronePowers'}\n",
      "{'body': 'But do Rebels appear with a dead Maul?', 'subreddit': 'StarWars'}\n",
      "{'body': 'So were many. If I recall Cindarella, Snow White and Beauty and the Beast were dark.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Rocket League ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"[+JavierLoustaunau](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dno8tyg/):\\n\\nMy g.f. is an extremely good writer but the distance between 'typing in a word document' and 'being published' seems intimidatingly vast to her. \\n\\nWhat would you say where the steps (big or small) from aspiring to published?\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Not sure if Springsteen is under rated but ghost of tom joad is one of the.most haunting albums i have ever heard.', 'subreddit': 'pinkfloyd'}\n",
      "{'body': 'I personally feel he nailed the \"Emotionless sentient space robot\" he was told to portray.', 'subreddit': 'destiny2'}\n",
      "{'body': \"I have the same issue. But when I download it and open up the setup log. It doesn't work.\", 'subreddit': 'discordapp'}\n",
      "{'body': 'The space dock.  No better way to seal a friendship', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'microgrowery'}\n",
      "{'body': 'Umm Salmon when I used to live in Canada Salmon was just something we ate on like weekly basis, been a while since I last ate Salmon.', 'subreddit': 'CasualConversation'}\n",
      "{'body': 'Happened to me a while back in Chicago. These guys run scams like this with \"legit\" tickets all the time on Craigslist. They run multiple listings on Craigslist and typically title them the same way, or substantially similar. The ad on the inside is also generally the same and will use a different name. \\n\\nSee if you can find more ads. The number is probably a burner phone, so if you can find another ad and call another burner phone, you can make another \"purchase\" and track them down that way. Obviously, that might be dangerous. ', 'subreddit': 'washingtondc'}\n",
      "{'body': \"See what you've just written contradicts your post. \", 'subreddit': 'relationships'}\n",
      "{'body': 'To be fair, you have to have a very high IQ to understand Rick and Morty. The humor is extremely subtle, and without a solid grasp of theoretical physics most of the jokes will go over a typical viewer\\'s head. There\\'s also Rick\\'s nihilistic outlook, which is deftly woven into his characterisation - his personal philosophy draws heavily fromNarodnaya Volya literature, for instance. The fans understand this stuff; they have the intellectual capacity to truly appreciate the depths of these jokes, to realize that they\\'re not just funny- they say something deep about LIFE. As a consequence people who dislike Rick and Morty truly ARE idiots- of course they wouldn\\'t appreciate, for instance, the humour in Rick\\'s existencial catchphrase \"Wubba Lubba Dub Dub,\" which itself is a cryptic reference to Turgenev\\'s Russian epic Fathers and Sons I\\'m smirking right now just imagining one of those addlepated simpletons scratching their heads in confusion as Dan Harmon\\'s genius unfolds itself on their television screens. What fools... how I pity them. 😂 And yes by the way, I DO have a Rick and Morty tattoo. And no, you cannot see it. It\\'s for the ladies\\' eyes only- And even they have to demonstrate that they\\'re within 5 IQ points of my own (preferably lower) beforehand.', 'subreddit': 'circlejerk'}\n",
      "{'body': 'I have never heard anyone say anything remotely negative about the possibility of male birth control, either IRL or online. What could the possible downsides be? Of course this is a good idea.', 'subreddit': 'AskWomen'}\n",
      "{'body': '/u/Mikeymc16, your submission has been automatically removed because you do not meet the minimum account age requirement for posting (violating rule #1). [Click here for information](https://www.reddit.com/r/Random_Acts_Of_Pizza/wiki/emergency_assistance) about emergency food assistance.\\n\\n---\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Random_Acts_Of_Pizza) if you have any questions or concerns.*', 'subreddit': 'Random_Acts_Of_Pizza'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Jacob plans stuff for him and tries to give Ice some organization. It's ultimately up to Ice whether he takes his advice or not, as Jacob has said multiple fucking times.\", 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"Hm. I reckon some familiarity with programming is probably a good idea (maybe a basic Python primer or something; it's arguably more useful than learning how to write code for Arduino depending on what your overall aims are) just so you can understand the general gist of what the code is doing. \\n\\nThat said, you could just replicate my whole project and then teach yourself programming/electronics on the fly by trying to add extra features (like a buzzer, or an interface that calibrates the time thresholds to suit each user's keying speeds). The latter is a good way to learn, just don't be afraid to break stuff!\", 'subreddit': 'DIY'}\n",
      "{'body': '[deleted]', 'subreddit': 'weddingplanning'}\n",
      "{'body': 'Just remember, when going to brothels there is a risk of supporting sex trafficking. You know, women who are being bought and sold to be raped for money.. \\n\\nOf course, the one you visit might be an exception, but probably not. You could always ask yourself the question risk VS reward, but as always, your cock is almost as important as freedom for women', 'subreddit': 'sydney'}\n",
      "{'body': 'Very very much! A real hottie sir ! ', 'subreddit': 'amateur_milfs'}\n",
      "{'body': 'Your post has been automatically removed as an anti-spam measure, as your account is under five days old. You are welcome to repost this content when your account has been active for five days. In that time, please also familiarize yourself with our [rules](https://www.reddit.com/r/RoastMe/about/rules/) here on /r/RoastMe to avoid any future posts you make from getting removed.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/RoastMe) if you have any questions or concerns.*', 'subreddit': 'RoastMe'}\n",
      "{'body': 'Because the shovel pass seemed too obvious', 'subreddit': 'CFB'}\n",
      "{'body': '143412881| &gt; United States Anonymous (ID: LOhNy/z9)\\n\\n2008: Elizabeth II\\n2012: Elizabeth II\\n2016: Elizabeth II\\n\\nMake America Great Again!!!\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': ' Me too!', 'subreddit': 'AskReddit'}\n",
      "{'body': 'What difficulty are you playing on?\\nI would argue that group comp, gear, stats etc matters more on Tactician / Honor Mode than the others. \\n\\nI did my first run on Classic with a Pure fighter/tank, wizard pyro/geo, another wizard aero/hydro and a Ranger.\\n\\nSome fights I had to come back for, but in general I had no problems.', 'subreddit': 'DivinityOriginalSin'}\n",
      "{'body': '1: Night King\\n2: Cersei LANNISTER\\n3: Gregor CLEGANE\\n4: Jaime LANNISTER\\n5: Jorah MORMONT\\n6: Daenerys TARGARYEN\\n7: Qyburn\\n8: Beric DONDARION\\n9: Euron GREYJOY\\n10: Jon SNOW\\n\\nFire: Beric \\n\\nFamily: Mountain CLEGANEBOWL!\\n\\nSteel: NK\\n\\nMagic: Qyburn\\n\\nLast: Dany', 'subreddit': 'freefolk'}\n",
      "{'body': 'I would probably actually die from the laughter ', 'subreddit': 'baseball'}\n",
      "{'body': 'You have no reason to feel self-conscious. You have a beautiful body.', 'subreddit': 'AsiansGoneWild'}\n",
      "{'body': 'Yeah I love it. In my top 10 movies of all time honestly. I think a lot of the criticism it gets is kind of insane really. ', 'subreddit': 'Screenwriting'}\n",
      "{'body': 'False start count: 5', 'subreddit': 'CFB'}\n",
      "{'body': 'This is greatly appreciated, we look forward to working with Lockheed and the United States even more in the future.', 'subreddit': 'worldpowers'}\n",
      "{'body': '[deleted]', 'subreddit': 'startrek'}\n",
      "{'body': 'Plus he has a sick bike.', 'subreddit': 'niceguys'}\n",
      "{'body': \"I wouldn't be against that.\", 'subreddit': 'hockey'}\n",
      "{'body': 'I agree. At *minimum* this is going to be disputed in the courts though.', 'subreddit': 'The_Donald'}\n",
      "{'body': 'Ready to hear man secretssss!', 'subreddit': 'exmormon'}\n",
      "{'body': \"I'll most likely be getting an older Jeep Wrangler TJ. I've heard that dealerships oftentimes under-report issues if they think they can make money from future repairs, which is why I was leaning towards an independent mechanic (but maybe I'm being too cynical). Sounds like it might be worth it to try a Chrysler/Jeep dealership instead.\", 'subreddit': 'nova'}\n",
      "{'body': \"My friend (Dallas fan), always loves to tell me that Jordan Reed sucks because he's injury prone. I always tell him that it doesn't matter because Reed always plays and dominates through injuries. HTTR!\", 'subreddit': 'Redskins'}\n",
      "{'body': 'Was there an opening cinematic trailer thing like the Oklahoma Ohio State game?', 'subreddit': 'CFB'}\n",
      "{'body': \"I'll talk serious and not joking for a minute. I know that sometimes the world is far from kind with us and that leaves us insecure in our own skin, believe me I've been there before. But I want you to know, kind stranger, that you are very much worthy and just having the courage of going in drag it's something that you should be proud off. This is not empty discourse, for me anyone who has the courage to dress up on a gender non conforming way it's a brave motherfucker and deserves applause specially on a world/society where people are still out casted and even killed just because of that. Never forget your self worth sis! ❤️\\n\\nPs: I hope it will come a day where you are comfortable enough to post both your boy self and your drag persona permanently. You already have a lot of people here who like how you own up the whole Dream Reign thing showing that you are a good sport and have your head in the right place.\", 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': 'Remember that game against LSU in Jerry World? That was fun.', 'subreddit': 'CFB'}\n",
      "{'body': \"I'm sorry can I ask why is ha sentret no longer obtainable legitimately? I know it will be once bank is updated for gold and silver but why isn't it right now?\", 'subreddit': 'pokemon'}\n",
      "{'body': '[deleted]', 'subreddit': 'assholedesign'}\n",
      "{'body': 'Ugh, and my Internet decided to die right now...', 'subreddit': 'nba'}\n",
      "{'body': 'Try to squeeze in an SSD in that budget, and not 100% necessary but if you can afford a higher refresh rate monitor then get that. But an SSD is #1 priority.\\n\\nEDIT: I saw your post about salvaging an old SSD, sorry. You don’t really need anything else than the stock cooler unless you’re doing anything above a light overclock. You can use the money you saved on other things', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Severely underrated ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dno92nj/):\\n\\nI would say that you can't look at it and see the end. You have to look at it and see the next step - that's her goal. Only after that she had to move on to the next step.  She can keep publishing in mind, but the distance will be discouraging. She has to do it step by step. If she has a passion for writing and for what she's writing she just has to start the journey. It's vast, but it is SO worth it. Tell her I say good luck! She'll be there before she knows it!\\n\\nEdit: forgot a word\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"I'm willing to bet Kanye IS pretty ungrateful for everything he has. Isn't he a known asshole?\", 'subreddit': 'PoliticalHumor'}\n",
      "{'body': '.... can we not, fox?', 'subreddit': 'CFB'}\n",
      "{'body': 'Dude this is me. I have an inguinal hernia (hernia in your groin) and weed makes my fucking dick and balls hurt if there’s a slight pain before I smoke. If I have pain anywhere weed just amplifies it', 'subreddit': 'Drugs'}\n",
      "{'body': 'Your submission has been automatically removed because it does not include a self review. You must use the **exact phrase** *self review:* along with a detailed self criticism of your video or channel homepage. See /r/youtubers/wiki/index for examples.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/youtubers) if you have any questions or concerns.*', 'subreddit': 'youtubers'}\n",
      "{'body': \"Because the story frames it as a cover up. It's a safe class in the data sheet, but that's because you aren't supposed to know that it's thaumiel.\", 'subreddit': 'WritingPrompts'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"This is r/RoastMe not r/SpitRoastMe ... You're doing it all wrong!\", 'subreddit': 'RoastMe'}\n",
      "{'body': 'This is a pretty cool idea!', 'subreddit': 'askfuneraldirectors'}\n",
      "{'body': \"It would certainly be viable. Such guns are no less effective than they were a century ago.\\n\\nDefinitely wouldn't be my first choice though.\", 'subreddit': 'guns'}\n",
      "{'body': \"He's got 3 goals in 4 games and doesn't look out of place. Actually among 2017 picks I think only Hischier, Yamamoto, and Chytil have done better. Small sample size though. They want him getting a taste so he goes back to captain Tri-City and dominate so he can contend for a spot next year.\", 'subreddit': 'hockey'}\n",
      "{'body': 'No.', 'subreddit': 'visualnovels'}\n",
      "{'body': 'i installed the package but the font is still not showing up on lxapparence, no clue of what to do.', 'subreddit': 'unixporn'}\n",
      "{'body': \"Haha don't worry, I'm far from bulimic and I've never done that before. I don't even want to eat pasta in general, I just wanted that cheese taste. Believe me, I INHALED my dinner after all of that haha. \", 'subreddit': 'progresspics'}\n",
      "{'body': 'Nah I prefer the calculated one sorry!', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Before everyone points and laughs at LSU for struggling in this game...\\n\\nTroy is not a pushover. They're very well coached and one of the best teams in the G5. They'd give plenty of P5 schools a run for their money. Anyone remember how they almost beat ***Clemson*** last year?\", 'subreddit': 'CFB'}\n",
      "{'body': 'And espn3', 'subreddit': 'CFB'}\n",
      "{'body': 'No worries, those heaters was just extra put in by previous owners. We have central heat so we never turn those on.', 'subreddit': 'miniSNES'}\n",
      "{'body': 'LOL, what?', 'subreddit': 'politics'}\n",
      "{'body': 'https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/', 'subreddit': 'pics'}\n",
      "{'body': \"If there aren't 100 ways to intentionally kill myself in the game I'll be disappointed.\", 'subreddit': 'DerekSmart'}\n",
      "{'body': 'Lame as hell.', 'subreddit': 'hockey'}\n",
      "{'body': 'Do you know if this is the same card? https://www.amazon.ca/Fenvi-Desktop-Wireless-Hackintosh-Supporting/dp/B01IVIHPBY/ref=lp_7768732011_1_1?srs=7768732011&amp;ie=UTF8&amp;qid=1506816014&amp;sr=8-1\\n\\nTrying to find an equivalent purchase from Canada ', 'subreddit': 'hackintosh'}\n",
      "{'body': 'I broached the conversation again, last night as we were laying down for the night. She agreed that we should choose two night during the week for romance. Thank you for your advice!', 'subreddit': 'DeadBedrooms'}\n",
      "{'body': 'I’m not sure I’d do anything. I’ll have to give mounted combat another read to refresh my memory. If I made any changes it might be there. As the knight, you need to decide when you should fight from horseback and when you should dismount. It’s pretty campaign dependent how viable mounts are as far as combat situations.', 'subreddit': 'DnD'}\n",
      "{'body': '[deleted]', 'subreddit': 'tax'}\n",
      "{'body': 'Nothing specific. All I get is this. https://image.prntscr.com/image/YaQPj863T4ug2Is2O-f9iA.png', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Hopefully it will get my boy Alohadance out of a depression ', 'subreddit': 'DotA2'}\n",
      "{'body': 'There are many more things that could be done in a hindsight. With a keyword being - *in a hindsight*.', 'subreddit': 'Warthunder'}\n",
      "{'body': 'When someone hates your kind and wants you and all your kind genocided, does morality change if they outnumber you? Not really. ', 'subreddit': 'movies'}\n",
      "{'body': \"&gt; not by the legal way. pretty crazy right? they never tried just once. If they had way less spaniards would be upset.\\n\\nPlease don't play dumb. You know full well that Rajoy has absolutely no intention of changing the constitution to allow a Catalan vote.\\n\\n\", 'subreddit': 'europe'}\n",
      "{'body': 'Fuck, and I forgot to mention that the turret chin is perfectly designed to safely deflect shells directly into the drivers head through through the thin roof armor.\\n\\n\\nOr that the engine could be disabled from being strafed by fiddycals.\\n\\nPerfect storm of ivory tower engineering. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Congratulations Vaca!  ', 'subreddit': 'survivor'}\n",
      "{'body': \"It is the new file format. Having previously worked with vegas, it's WAY too picky about file formats.\\n\\nFirst, I would try a few tools, one being VLC. Just to see if you can even play the file. Next the portable version of pazera converter suite. Use the whatever to mp4 one. (it will take any file as input and convert to .mp4)\\n\\nYou should now be able to import the footage. Though I would strongly recommend switching to adobe premiere. It's more compatible with newer formats and once you learn a few shortcuts and workflow. It's much faster than vegas. (and if you are broke, there are... other ways of getting it)\", 'subreddit': 'gopro'}\n",
      "{'body': 'Last time I checked there were three different area meetup groups for boardgames. Are they no longer active? The go-to places this sub throws out are Mission Board Games, Pawn and Pint, and Tabletop Game and Hobby. Yours is a pretty common question here. Try the search bar.', 'subreddit': 'kansascity'}\n",
      "{'body': \"20 year old women are hotter than 30 year old women. That's life \", 'subreddit': 'nba'}\n",
      "{'body': '[removed]', 'subreddit': 'Anxiety'}\n",
      "{'body': 'heh beagles are the best', 'subreddit': 'aww'}\n",
      "{'body': \"I found it and couldn't find the part where he 1 v 3 all of you.\\n\\nAnyways, in this case someone needed plague lord to deal with his Lifesteal. He nearly went full DPS exept for the OP totem. He also kinda snowballed out of control. Dude plays a lot of Kwang.\\n\\nPS Numbing Rogue works until someone can get plague lord online.\", 'subreddit': 'paragon'}\n",
      "{'body': 'I guess you just take slang literally. It adds to your wonderful (imaginary, like my dick) personality.', 'subreddit': 'SanctionedSuicide'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'SD | [Calgary Flames vs Winnipeg Jets](http://plr.livestreamsonline.net/embed/76) | Ad Overlays: 3 | Mobile: No \\n\\n\\n 1. If black screen make sure flash player is enabled. \\n\\n\\n 2. Stream Live 5 - 10 Before', 'subreddit': 'NHLStreams'}\n",
      "{'body': \"Congrats on having two of your major cards nerfed and still being the 4th best class in the game (P.S. It's not druid it's jade druid).\", 'subreddit': 'hearthstone'}\n",
      "{'body': \"They also held a office, so yes, he isn't waiting for a week would be amazing or horrifying. haha true to that sub. It's just a good addition!\", 'subreddit': 'SubredditSimulator_SS'}\n",
      "{'body': 'Go tigers. Checking in', 'subreddit': 'CFB'}\n",
      "{'body': \"Thats a lovely accent you have there, New Jersey?... Oh Austria? *Good day mate... Let's throw another shrimp on the barbie!*\", 'subreddit': 'pics'}\n",
      "{'body': \"Congratulations it's a great to have you.\", 'subreddit': 'MURICA'}\n",
      "{'body': \"N0tail carried that last game pretty hard. Still a lot of work to be done for OG though, they got outdrafted and would've lost top rax if HR hadn't made some crucial mistakes during their highground push.\", 'subreddit': 'DotA2'}\n",
      "{'body': 'Actually, depending on the state, you can.  Legally anyway.  Not always or even mostly the proper solution.', 'subreddit': 'news'}\n",
      "{'body': 'Also it has generally less traffic so it helps generate content ', 'subreddit': 'startrek'}\n",
      "{'body': 'Source?', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'add 2 days', 'subreddit': 'ClashRoyale'}\n",
      "{'body': 'A thread on Foran and gambling is more of a mid-season topic tbh.', 'subreddit': 'nrl'}\n",
      "{'body': '143418498| &gt; United States Anonymous (ID: Yx60ZaN/)\\n\\n&gt;&gt;143418325\\nhttps://www.youtube.com/watch?v=dnrT7yZEV-g [Embed]\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Because there are no citeable claims about it?', 'subreddit': 'exmuslim'}\n",
      "{'body': 'Hahaha you and me both man', 'subreddit': 'malefashionadvice'}\n",
      "{'body': 'Anyone that says Budweiser anything is better than Miller Lite is welcome to meet me after school in the parking lot', 'subreddit': 'baseball'}\n",
      "{'body': 'Great! It is awesome that you have a car you love!', 'subreddit': 'askgaybros'}\n",
      "{'body': '[+existentialadvisor](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol0s6/):\\n\\nSo...what does someone have to do to get published?', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'You need another shame flair tbh.', 'subreddit': 'neoliberal'}\n",
      "{'body': 'ouch', 'subreddit': 'teenagers'}\n",
      "{'body': \"The main reason I'm getting this game is because it feels like a reskinned BO2, because that's by far my favourite COD ever. :P\\n\\nProbably not great if you're looking for something like the very early WWII CODs, but I think most people want it to be like BO2, since that's the most popular COD game to date. I believe it's even the most played game on PSN, or at least it was at one point.\", 'subreddit': 'WWII'}\n",
      "{'body': 'Filtration system marvel to behold. It remove 80% of human solid waste ', 'subreddit': 'news'}\n",
      "{'body': \"Oh thank you so much for this post. Do not forget how truly precious this post and yourself are. I really hope the best for you, so goddamn much.\\n\\nPlease respond, I just want to know if you're okay.\\n\\nWe're seven goddamn billion people here, people are here for you. Please look at Logic - 1-800-273-8255  on youtube for me please!!!!! It is such a powerful video, it can make anyone cry. I am just hoping you are doing better, know that the world has so much affection for you, sure there are dicks who are not self aware of their problems, but besides that we are still mortal human beings, just there for each other and hoping for the best.\\n\\nI did have such a good time thanks to your post.\\n\\nIt is often so easy to tell that people are there for you, and I thank you so much for being here for us to remind us that.\\n\\nI just want to know if you feel better for posting that post. It brought me something so pure, that we rarely all feel.\\n\\nI never knew most of my grandparents, what I can tell you though is that your great nan would be so goddamn proud of you for making that post, it brought so much happiness to other people &lt;3   I just want you to know that people are here for you, pm me &lt;3\", 'subreddit': 'wholesome'}\n",
      "{'body': \"I mean, you could ult? lol Her boop and cannon dps isn't generally going to be worth bubbling for in a true 1v1 unless the zarya has literally 0 energy. But it's still going to be best not to feed her the ult charge even if you can kill her because she has 0 energy.\", 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': 'The one that stood out to me is \"mandatory class on systems of power\". Just... no thank you, no more mandatory heavily politically motivated classes. I still remember my mandatory diversity class (I think it was medicine, culture, and society).\\n\\n\\nI\\'m really liberal, I was just arguing that if a child is having horrible seizures and parents are repeatedly neglecting appropriate treatment to the point the kid will die, our government should allow doctors to intervene regardless of the parents\\' culture of origin. Pretty reasonable, I don\\'t like kids dying because of neglect.\\n\\n\\nAnother student literally said \"Well that\\'s just your western white male perspective\". It was at the same time obvious, dehumanizing, and a pointless statement and the prof backed them up. I was a little too shocked by how dumb it was to fight back at the time. A real \"is this real life\" moment, it was my first time dealing with something like that. Something that became very common in that class... along with a complete lack of any dissenting sources. Really felt like a farce coming from my hard biology background.', 'subreddit': 'Cornell'}\n",
      "{'body': \"&gt; What's your favorite flavor?\\n\\nWatermelon.\\n\\nFor everything else (shakes, cakes, whatever) then chocolate.\\n\\nExcept cheesecakes, those are their own flavor and stand alone.\", 'subreddit': 'anime'}\n",
      "{'body': 'DAE prequel show -- excuse me, successor show -- of a slightly-retconned (sorry GURM) Ned Stark, Howland Reed, and Arthur Dayne taking care of baby \"AeJon\" in a \"Three Men and a Baby\" scenario???', 'subreddit': 'asoiafcirclejerk'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'It just lets me play for another minute before crashing :/ any idea?', 'subreddit': 'RocketLeague'}\n",
      "{'body': \"Also, drug them and sell their organs, by the time they realize what happened they'll be dead..\", 'subreddit': 'LifeProTips'}\n",
      "{'body': 'Both. Sci-fi and fantasy. Or urban fantasy.\\n\\nA better term for it would probably be weird fiction: https://en.wikipedia.org/wiki/Weird_fiction', 'subreddit': 'WritingPrompts'}\n",
      "{'body': \"Seems like we're playing a 4-4-2\", 'subreddit': 'AtlantaUnited'}\n",
      "{'body': 'They should be shut down. Unfortunately the only thing more sacred in the US than a church is a corporation. ', 'subreddit': 'AdviceAnimals'}\n",
      "{'body': 'She was going to therapy for a while and is currently not. I have mentioned it a few times but I don\\'t think she wants to go back anytime soon. \\n\\n\"but always remember that leaving him for good will be a choice she has to make on her own, and can\\'t feel like a choice someone else made for her.\" I definitely agree, I just feel guilty with both ignoring him/the relationship or being more accepting, they just both seem like bad options, and I don\\'t know how to decide which one is better. ', 'subreddit': 'relationships'}\n",
      "{'body': \"Wow you're butthole is 😍😍😍\", 'subreddit': 'gonewild'}\n",
      "{'body': 'Devs that homogenize their game and reskin every piece of loot deserve all the toxicity they get.\\n\\nHow dat raid loot bruhs', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': \"I noticed that you don't mention test scores. How important are they? I'm in the process of applying to Masters programs and my test scores are my weak point because I don't have time to devote/money to take classes. \", 'subreddit': 'AskReddit'}\n",
      "{'body': '[Off https://i.imgur.com/mTC76mV.jpg](https://i.imgur.com/mTC76mV.jpg)', 'subreddit': 'Bulge'}\n",
      "{'body': \"That's cute and all but you're posting a pic on the expanse of the Internet of a stranger? Creeper. \", 'subreddit': 'funny'}\n",
      "{'body': '[In the mountains of Colorado ](https://i.imgur.com/b7ALILi.jpg)', 'subreddit': 'GalaxyS8'}\n",
      "{'body': 'B', 'subreddit': 'AskOuija'}\n",
      "{'body': 'I might host one if I can get it working.', 'subreddit': 'teenagers'}\n",
      "{'body': \"Fair enough.\\n\\nI see a bit more actual substance to some of his claims than you do, clearly.\\n\\nI agree with you, the hate crime law doesn't seem to me like a slippery slope to anything. I do agree with his proposition that you cannot codify into law preferred pronoun usage. I would never intentionally misgender someone, and I would do my best to use whatever pronoun a person prefers, but I do not want to live in a society where it's a crime if I get that wrong. I think there is reasonable room for discussion about where compassion and function meet on something like that.\\n\\nI don't agree with his stance on the nuclear family and by association his criticism of other models of being. I think the nuclear family is a model that works for many people, but I know in point of fact that it does not work for many others. I think people should be free to do what they see fit. I know of no evidence that suggests gay adoption is in any way inferior to straight adoption. I wasn't aware he had made statements to that affect, but it doesn't totally surprise me that he did.\\n\\nI agree that we should encourage people to embrace their gender and sexuality as fluid. More freedom and more ability to be true to one's reality is good. \\n\\nBut I agree with him that there is an anti-science and anti-discourse bent to some of the present political climate on the left. I'm squarely in the left on the American political spectrum, but I sense more and more that there is a tendency to dismiss anything, whether an idea, a study, or a person, that conflicts with progressive views as so self-evidently wrong, ignorant, and hateful as to not even deserve consideration. I sense that even in myself sometimes, and I don't like it. Some things are self-evidently wrong, ignorant, and hateful. But the vast majority of things about which people disagree are not that way. I don't see a lot of people on either side who seem interested in increasing the level of nuance to their opinions. I also feel that there is a chilling effect in much of public discourse. I don't even like the guy, but by merely talking about him and not just bashing him, I feel like I'm likely to get downvotes. I don't care about internet points, but I do think there's pressure to agree or be silent. That would just be one exceedingly silly example. The left does it to the right. The right does it to the left. That's probably always been so. But I feel increasingly the far left does it to people who basically share their goals.\\n\\nI studied a fair bit of postmodern philosophy in school, which I admit was several years ago now. I kind of agree with his characterization of its issues and flaws (basically that its claims are true only in the abstract and claims that cannot be functionally applied should be treated as false, even if not logically falsifiable- like, for instance, solipsism). I do not share his opinion that the whole thing has an underlying ulterior motive of destroying Western civilization. I would be interested specifically in your take on where he strays regarding postmodern philosophy, because it's been a while for me.\\n\\nI think certain academic disciplines are rather corrupt. I'm open to discussion about a lot of things, but it's wild to me that a Yale professor more or less got run out of town for questioning whether cultural appropriation is for real. Cultural appropriation, microaggressions, patriarchy, white privilege, rape culture... I think there's real, honest to God truth in this stuff, but when I really think about the world I live in, I have trouble swallowing some of it whole hog without adding at least a pinch of nuance. And if a professor at an institution can lose their job for trying to do so, how can anyone else? Shouldn't we want to know the truth about those things? Doesn't that require critical reflection?\\n\\nAnd, for the Christianity piece, I think he makes an excellent observation (or takes one, anyway, from Dostoevsky and Neitzsche). The erosion of that as a unifying force has increased the chaos in societies. But I don't share his pessimism or resentment about it. Our society is more plural. I think the Christian tradition gave societies a toolkit. I believe that any tools from that kit that still have value will be retained, and to hell with the rest. But I don't disagree with his notion (and this was more or less Neitzsche's take) that you can toss out bad tools from Christianity, and perhaps we must, but that doesn't mean you immediately have replacements that work better. I believe we'll find them, but I do think the twentieth century had it's share of nihilism, and I do think that even today people struggle to find purpose and it can harm them. I don't mourn Christianity, and I assume we'll figure that out, but it's fair to say that, absent an externally imposed value system, people struggle to create an internal one. That's a real phenomenon in my opinion.\\n\\n\\n\\n\\n\", 'subreddit': 'SubredditDrama'}\n",
      "{'body': 'SD Streams - [Ole Miss vs Alabama *ESPN*](http://sportstreams.co/live-soccer-wiz-stream-13/) | Mobile Compatible : Yes | Ad Overlays: 4 | Use Adblock', 'subreddit': 'CFBStreams'}\n",
      "{'body': 'What are your thoughts on the APA changing definitions due to political pressure?\\n\\nhttps://www.google.com/amp/s/www.lifesitenews.com/mobile/news/former-president-of-apa-says-organization-controlled-by-gay-rights-movement\\n\\n', 'subreddit': 'unpopularopinion'}\n",
      "{'body': 'Why yes it is 😂😂', 'subreddit': 'joesymon'}\n",
      "{'body': '[deleted]', 'subreddit': 'teenagers'}\n",
      "{'body': \"Hey, whatever works for you - just call and let me know when EV cars can be fully charged in 5 minutes and hold enough charge for 300+ miles, while costing under $30k\\n\\nOh, and have a way to be charged while I live in an apartment. It's not practical for everyone.\", 'subreddit': 'cars'}\n",
      "{'body': 'LMAO', 'subreddit': '90DayFiance'}\n",
      "{'body': 'Mycket bra:)', 'subreddit': 'NoFap'}\n",
      "{'body': 'Yup', 'subreddit': 'metalgearsolid'}\n",
      "{'body': \"[+261TurnerLane](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnom6d3/):\\n\\nThis is self-publishing. You literally could self-publish a book on Amazon in ten minutes. And then there are services which you pay to publish your book, like the one OP used. This form of publishing is as open and easy as it gets. If your GF wants to hold her own book in her hand, she could do it, for a price. Traditional publishing is long and hard and discouraging. When I got my first short story published in an anthology you could go to the store and buy, I cried. It was the end result of a long and hard road. I've recently finished my second full-length novel (the first one I'm trying to sell) and it's been two months of mostly rejections. But that's the game. You have to find that right agent. And then they have to find that right publisher, and so on, and three years from now I might be able to walk into a store and buy my book. \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Just let one of the teams be the Cowboys please. Patriots would be ok too.', 'subreddit': 'conspiracy'}\n",
      "{'body': 'You owe us one or something.', 'subreddit': 'Cardinals'}\n",
      "{'body': \"Is this that surprising? His buyout is doubled as long as Jurich is employed due to some strange contract language.\\n\\nHe probably should've just gone with no comment.\", 'subreddit': 'CFB'}\n",
      "{'body': 'Fairy Bottle', 'subreddit': 'TotalDramaTheGameshow'}\n",
      "{'body': 'SSD would help with the buildings and such loading slowly when you enter a game. Start with that.', 'subreddit': 'PUBG'}\n",
      "{'body': 'https://www.reddit.com/r/RocketLeague/comments/73heob/update_on_crashes_from_dirkened_still_crashing/', 'subreddit': 'RocketLeague'}\n",
      "{'body': '\\nIn a private school, sure.', 'subreddit': 'news'}\n",
      "{'body': 'Eggcellent', 'subreddit': 'Bacon'}\n",
      "{'body': \"I'd say natural boobs and a more matching build (now it's a long slim top with a more filled out bottom) would've made it better. \", 'subreddit': 'furry'}\n",
      "{'body': \"Yes hi, I'll take $20 of the OG Kush and $15 of the Super Blue Dream, thanks!\", 'subreddit': 'nba'}\n",
      "{'body': \"No, facts don't change even if idiots keep posting the same dumb shit so I do not have any reason to alter the replies.\", 'subreddit': 'conspiracy'}\n",
      "{'body': \"Yeah I've literally never purchased a Sivir skin, but I have 3 Sivir skins (4 w/ the victorious skin). All from mystery gifts.\\n\\nAnd it's not like I have gotten a ton of mystery gifts. Maybe 10 gifts since I started playing this game. Riot just loves giving me Sivir skins I guess.\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Then what would you lie about, OP? ', 'subreddit': 'funny'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"If I hold my hands by my side, the sleeves end at my knuckles, or right where the bottom of the white strip is on the hip area. I don't really have the means to take a pic of myself in the jersey, sorry. I usually like a medium jersey so the 50 is perfect for me. \", 'subreddit': 'hockeyjerseys'}\n",
      "{'body': 'I should then be able to have the option to toggle all of those features when I choose to do so.', 'subreddit': 'ios'}\n",
      "{'body': 'i thought people hated zariss?', 'subreddit': 'SWGalaxyOfHeroes'}\n",
      "{'body': 'It looks like its going to be pretty weird', 'subreddit': 'nathanforyou'}\n",
      "{'body': 'Empty seats, many empty seats...', 'subreddit': 'exmormon'}\n",
      "{'body': \"Maybe. I don't know.\", 'subreddit': 'Amd'}\n",
      "{'body': \"I don't know about his elbow, but his heel could use some lotion.\", 'subreddit': 'WTF'}\n",
      "{'body': 'yes', 'subreddit': 'CFB'}\n",
      "{'body': \"I generally recommend starting with 1 core, and if you like the game picking up a second core along with the Dunwich or Carcossa starter. Having two cores is very helpful in making sure everyone has enough copies of stuff for consistency, especially if you're playing investigators with any otherlap in available class cards.\", 'subreddit': 'arkhamhorrorlcg'}\n",
      "{'body': \"You've been with him 3 months and it sounds like you've found a lot of areas where the two of you aren't compatible.  It isn't that either of you did anything wrong, you just aren't right for one another.  If you break up it means, for example, he can find a woman who is into his martial arts and you can find a guy who is with you on supporting animals.\", 'subreddit': 'relationships'}\n",
      "{'body': \"Lol, there's this kid named Jake in my class that nobody liked so whenever he would do something I would picture that scene in my head\", 'subreddit': 'MTVScream'}\n",
      "{'body': 'Totally the pyro of an underdog. ;p', 'subreddit': 'SquaredCircle'}\n",
      "{'body': \"I'm not 100% sure what you mean, but I see that the DMZ is disabled in my router settings.\", 'subreddit': 'Comcast'}\n",
      "{'body': \"Eno's producing.\", 'subreddit': 'AskReddit'}\n",
      "{'body': '143412979| &gt; Canada Anonymous (ID: JohaoXv9)\\n\\n&gt;tfw no matter who i vote for, Canada will always go for Libshit\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Ah, yes, that’s reasonably assholish then. That part wasn’t clear from the post.', 'subreddit': 'assholedesign'}\n",
      "{'body': 'How many sex workers do you think are on askreddit right now', 'subreddit': 'AskReddit'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnomsbx/):\\n\\nI didn't pay anything :)\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'The Germans had the Schlieffen Plan, (their battle plan used in August 1914) ready in 1905. If there were no provocation, they would have created one in order to start the war.', 'subreddit': 'history'}\n",
      "{'body': 'Hehe😙', 'subreddit': 'gonewildcurvy'}\n",
      "{'body': 'It’s a melee llama hooray ', 'subreddit': 'FORTnITE'}\n",
      "{'body': 'Markmcfarlane ', 'subreddit': 'Fireteams'}\n",
      "{'body': 'Yeah 1 pro player and not even a big one at that. Lets wait and see what the real competitive pros think. Sneaky was getting pissed at his teammates for not playing soloQ enough at bootcamp this year. TSM has said they have put in more hours than ever before. Games on stage are their best practice, do you think they want that practice time to be cut by more than half? These guys want to win worlds, they want to be the best! To be the best you have to beat the best, and to beat the best you have to practice harder than the best. Anyone who doesn\\'t want to play more is lazy and just in it for the money, they don\\'t have what it takes to be the best. I dare someone to ask Faker if he thinks going to Bo1 format is a good idea for a LCS regular season. He would say \"no\" and then go back to playing soloq. ', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"You play resolutions lower than 480p on a 30khz PC CRT by messing with vertical refresh rates.  Here's a guide https://www.reddit.com/r/emulation/comments/675wk4/guide_how_to_run_retroarch_at_240p_on_your_vga/\", 'subreddit': 'crtgaming'}\n",
      "{'body': \"Different in the first, second, and fifth book.  There's more continuity in the others.  And they grow on you, or they did on me, anyway.\", 'subreddit': 'Fantasy'}\n",
      "{'body': 'Ok so why is the procedure to divert? ', 'subreddit': 'worldnews'}\n",
      "{'body': \"We bought last year, a 2bdrm Unit about 35 minutes by train to the CBD from the South East. A halfway compromise between proximity to work and having a yard for a dog. I did live in CBD apartments before this though for about 4.5 years. (With a stint in the outer West in between.) Proximity to everything is pretty hard to compete with, but we wanted the dog and I'm not going to sit around demanding I'm entitled to a 3 bdrm townhouse in Richmond.\", 'subreddit': 'melbourne'}\n",
      "{'body': 'There is a very real possibility the NFC west has three teams that are 1-3. We win against the Cardinals, and the Seahawks lose to the Colts.', 'subreddit': '49ers'}\n",
      "{'body': 'ok', 'subreddit': 'MMORPG'}\n",
      "{'body': \"That's an awesome idea and I hope they work it out soon.\", 'subreddit': 'AskWomen'}\n",
      "{'body': 'omg i love this! i share your same joy.', 'subreddit': 'stopdrinking'}\n",
      "{'body': '&gt; impossible by the laws of biology. \\n\\nIncluding a God. It seems you now agree with me the probability of a God is about the same as flying pink unicorns - 0%', 'subreddit': 'DebateReligion'}\n",
      "{'body': '[deleted]', 'subreddit': 'news'}\n",
      "{'body': 'Yeah but we also know Reddit is 98% Liberal so....', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'Was it yours? Lol?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': '292 hunter, psn: chemicologist', 'subreddit': 'Fireteams'}\n",
      "{'body': 'International', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I say it really doesn't matter if you have a controller or not.Either way its going to be difficult.\", 'subreddit': 'Cuphead'}\n",
      "{'body': '[deleted]', 'subreddit': 'Fireteams'}\n",
      "{'body': 'Damn!!! No need to wear reflective gear I  Halloween for that one.  You can see that a mile away', 'subreddit': 'gifs'}\n",
      "{'body': 'I got a feeling LSU will come to regret firing Les Miles by the end of the season.', 'subreddit': 'CFB'}\n",
      "{'body': '&gt;Graham, taken with pick 53 in last year’s national draft, didn’t debut until Round 22 and has now played more finals than home-and-away games.  \\n\\nPlays 5 games, kicks 3 goals in the GF and becomes a premiership player. Truly amazing. ', 'subreddit': 'AFL'}\n",
      "{'body': 'P.S. Facebook is cancer.', 'subreddit': 'TrueStoicism'}\n",
      "{'body': ' jot uppjepasst', 'subreddit': 'cologne'}\n",
      "{'body': 'Oh really?  I never thought UNP vs CBBE was considered a strictly better sort of deal, I thought it was a matter of preference.  My main reason for using UNP is because I find it has a wider selection of armours of styles that I like.', 'subreddit': 'skyrimmods'}\n",
      "{'body': \"Purolator. And it's been over a week since it said it was delivered. \", 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Woah! How have I lived until now without knowing this jem?', 'subreddit': 'todayilearned'}\n",
      "{'body': '“Toot toot!”', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Yes, for a number of reasons.\\n\\nDominant decks force players to either run the dominant strategy themselves or something that can win reliably against it. This severely limits the amount of strategies instead of diversifying them. If it gets to a point where you don't have at least one of the basic deck archetypes (like no aggro deck) you have a metagame that has warped around that dominant strategy.\\n\\nThere have been many cases of that happening in Vintage over the past 10 years. Legacy too: Mystical Tutor in ANT and Survival of the Fittest are great examples of this happening.\", 'subreddit': 'MTGVintage'}\n",
      "{'body': 'Given the context, you *are* doing something more than just that.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'This reminds me of when someone takes a scene from a TV show like Family Guy, makes it into a comic, and posts it to /r/funny.', 'subreddit': 'ComedyCemetery'}\n",
      "{'body': 'Yep basically every game is going to have them these days.', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Tinder', 'subreddit': 'AskReddit'}\n",
      "{'body': 'A lot of people complain about Diego Fagundez not progressing like we wanted. A big reason is that he some still does not have a weak foot as seen with that chance.', 'subreddit': 'MLS'}\n",
      "{'body': 'Imagine eyes that green on a person. ', 'subreddit': 'aww'}\n",
      "{'body': 'this looks like a job for the Magic 8 ball bot', 'subreddit': 'pokemongo'}\n",
      "{'body': \"USI Grad!  Played that course the year it was just put in.  Haven't been back in years, but would love to go back and play. \", 'subreddit': 'discgolf'}\n",
      "{'body': 'Yea, Tatum is getting a lot of praise from teammates and coaches, a lot more so than other rookies so its somewhat reassuring but as we all know, compared to most of the other top prospects, Tatum will be the one that has to work the hardest just like Jaylen did last year simply because of the fact that we are already a top 5 team getting top 5 lottery', 'subreddit': 'bostonceltics'}\n",
      "{'body': '3k?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"I agree 100% and will only add that they must value Mandog considering they used an expansion draft protection on him. I know there really wasn't anyone better (Amac? hahaha) but I think he latches on this year as the 7th man. Morin stays as does Hagg, FWIW I think Morin will eventually get into the top 4 as well, and I hope that Amac is sent down if not now, eventually during the season (but I ain't holdin' my breath). \\n\\nA pipe dream but I'd love to see a Provy - Gudas top pair, a Morin - Ghost second pair, and a Manning - Hagg third pair but I also don't think that will happen.\\n\\nI also extra-agree with getting Sanheim top pairing minutes in the AHL because he should be a top four next year assuming he uses this year to iron out the last of his defensive game. Manning is a UFA next year and I doubt we resign him.\", 'subreddit': 'Flyers'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'sorry, my bad :(', 'subreddit': 'phgonewild'}\n",
      "{'body': \"I think Ushiwakamaru's Interlude gives some as well.\", 'subreddit': 'grandorder'}\n",
      "{'body': 'This is a great all in one terrain tool - [Gaia - $45](https://assetstore.unity.com/packages/tools/terrain/gaia-42618)\\n\\nJust go look before you say it costs too much  : )', 'subreddit': 'Unity3D'}\n",
      "{'body': \"okay question What specifically unique to Europe .. because I would say in Europe the black population is from Africa and it is a smaller minority... \\n\\nconversely the Muslim population is exploding and are we counting that as part of black population?... because if you do then I wouldn't necessarily say definitely Europe\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'If u actually hit it, it explodes bug shit everywhere..', 'subreddit': 'tifu'}\n",
      "{'body': \"They are planning on introducing stuff like this. It's just gonna be a few. Just be sure to keep in mind, this is still pretty early game state. Which is a good sign for things to come \", 'subreddit': 'FortNiteBR'}\n",
      "{'body': '\"I am not going to stand up to show pride in a flag for a country that oppresses black people and people of color,\"  - Colin Kaepernick on why he started the protest\\n\\nBasically, he believes the flag/national anthem doesn\\'t deserve to be respected until racial equality is achieved.', 'subreddit': 'news'}\n",
      "{'body': 'i think its 600 xtals for the 1pc not $50 \\n\\nthe $50 is the for set selection ', 'subreddit': 'FantasyWarTactics'}\n",
      "{'body': 'I thought that was a makeshift pizza made out of garlic bread and I was about to virtually high five you', 'subreddit': 'drunk'}\n",
      "{'body': 'Or you know, you can just take care of your phone.\\n', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'My feelings are hurt! ', 'subreddit': 'gifs'}\n",
      "{'body': \"I believe they fit but I'll confirm 100% for you when I get home . I can tell you the coils that come with the AO are a bit nicer . \", 'subreddit': 'Waxpen'}\n",
      "{'body': \"Not as bad as I thought. I guess it could be perceived as irreverent, but it's just kind of cool to me. :)\", 'subreddit': 'Catholicism'}\n",
      "{'body': \"This is so true. Judaism came from Sumerian mythology. Elohim is Anu. Lucifer is Enki who is a scientist and the actual Creator of mankind possibly they weren't even goods but just a more advanced civilization or something... Any how Genesis says we created man in our image not my... So God was not alone.\\n\\nMy guess is Jews usurped a God from Sumerian mythology to be their patron God like Catholics have patron saints. Then he became jealous and to a people who believe in the existence of many God's, but only one is 'their' personal God, he would semantically be the one true God of this small subsection of humanity. But not necessarily the God of everything ever created.\\n\\nThe garden of Eden myth told by Sumerians is so similar though that it's hard to believe it's not the true source, esp since it predates Bible by like 1500 years.\", 'subreddit': 'exmormon'}\n",
      "{'body': \"Actually the more transactions on the Tangle the faster it becomes.  That's why when they get spammed it speeds up.\", 'subreddit': 'Ripple'}\n",
      "{'body': 'Who is going to be teaching Genetics? ', 'subreddit': 'jhu'}\n",
      "{'body': \"Even if you get all of them, there's a place in the far NE where its' really cold. You can only go so high up before you can't. When one of the last few quests starts, you'll be able to progress. No spoilers but at earliest is this point.\", 'subreddit': 'horizon'}\n",
      "{'body': \"It's extremely overly simplistic, but you kind of have to be when you're talking to 8 year olds. \\n\\n&gt;By your logic, they’ll get it sorted out for them later\\n\\nWill they not? Does Canada have a epidemic of 3rd grade dropouts I'm not hearing about?\", 'subreddit': 'mildlyinfuriating'}\n",
      "{'body': '[deleted]', 'subreddit': 'watchpeopledie'}\n",
      "{'body': 'LOLOL', 'subreddit': 'trees'}\n",
      "{'body': \"[+261TurnerLane](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnomxii/):\\n\\nOh interesting, the page lists First Choice Books as your publisher, which is a pay to print company, and I assumed that's how you got the physical copy you're holding in your picture. Did you make the print copy yourself? It looks very well done if that's the case.\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '* Username: /u/Momo254\\n* Join date: 2011-12-14 02:02:53\\n* Link karma: 407\\n* Comment karma: 221\\n* Confirmed trades: 20\\n* Heatware: [https://www.heatware.com/eval.php?id=103982](https://www.heatware.com/eval.php?id=103982)\\n\\n^^This ^^information ^^does ^^not ^^guarantee ^^a ^^successful ^^swap. ^^It ^^is ^^being ^^provided ^^to ^^help ^^potential ^^trade ^^partners ^^have ^^more ^^immediate ^^background ^^information ^^about ^^with ^^whom ^^they ^^are ^^swapping. ^^Please ^^be ^^sure ^^to ^^familiarize ^^yourself ^^with ^^the ^^[RULES](https://www.reddit.com/r/hardwareswap/wiki/rules/rules) ^^and ^^other ^^guides ^^on ^^the ^^[WIKI](https://www.reddit.com/r/hardwareswap/wiki/index)', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'holy moly', 'subreddit': 'AskReddit'}\n",
      "{'body': 'The murderers all got off.', 'subreddit': 'Anarcho_Capitalism'}\n",
      "{'body': 'Up until last year, a virginity test was required for single women joining the national police force in Indonesia. ', 'subreddit': 'news'}\n",
      "{'body': '143418340| &gt; United States Anonymous (ID: ZKQ7hPa6)\\n\\n&gt;&gt;143412250 (OP)\\nTrump\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Millenials (my generation) grew up with the internet, we barely remember a time when we actually had to buy newspapers, some of us weren't even alive then. If you wanted the news, you bought a paper with your money or watched it on a tv station that ran ads or was paid for buy our parent's cable subscriptions. All our lives news has been free, and for that reason we never understood its value. We don't believe it's something that's worth being paid for because we've never had too.\\n\\nHell in this sub people will copy and paste entire articles from sites and post them in the comments, robbing the site of the page views or if the site is behind a paywall, its just straight up piracy at that point. I can't post a link to torrent sites on /r/television or /r/movies but you can copy entire news articles in the comments here and no one bats an eye. Actually think about that for a second.\", 'subreddit': 'news'}\n",
      "{'body': 'Byoot!', 'subreddit': 'food'}\n",
      "{'body': 'You really don\\'t need to attend Kingdom Halls to understand the thinking of JWs.  There isn\\'t much thinking to begin with.  After all it\\'s a cult.\\n\\nIf you plan on going however, be prepared for people to try and convert you through \"Bible study\" and get invites for doing door to door ministry, or field service as it\\'s called.  And when you refuse, you will be judged, avoided, and warned against in secret.', 'subreddit': 'exjw'}\n",
      "{'body': \"She would probably be more offended by you assuming she's over forty than being called a girl.\", 'subreddit': 'educationalgifs'}\n",
      "{'body': \"better listen. So /u/KatyLiedTheBitch, what's wrong?\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Anyone got a streamable of the Price break-dance save on Hoffmann?', 'subreddit': 'hockey'}\n",
      "{'body': '6k?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': '[deleted]', 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': 'All my niggas gang bang ', 'subreddit': 'Hiphopcirclejerk'}\n",
      "{'body': 'UBW is still my favorite route but I do hope they pace the romance and character building in HF better than they did in the VN. \\n\\nOne of the most jarring things was getting character flashbacks of when Shiro and Sakura first meet and when she first started to go to his house (and why) was established right at the very end of the normal end of the route &lt;.&lt;', 'subreddit': 'grandorder'}\n",
      "{'body': \"Right place right time I guess. It's about to be really crazy in California. Going to surpass all other rec states I'm sure\", 'subreddit': 'trees'}\n",
      "{'body': '[deleted]', 'subreddit': 'tailplug'}\n",
      "{'body': '[deleted]', 'subreddit': 'asoiafcirclejerk'}\n",
      "{'body': 'Good enough, I guess.  From my understanding they tossed most of the lore out when they created their new \"Age of Sigmar\" table top game.', 'subreddit': 'totalwar'}\n",
      "{'body': \"So I'm very new to planted tanks, and this Anubias that I bought was just covered in these black specs one day (had it for about a week) \\n\\nI'm just curious if these are snail eggs (or something else) because I have a lot of snails in there already and my other Anubias nana has nothing on it\\n\\nTank is a 20 long with a Nicrew LED  that runs for about 7 hours a day. \", 'subreddit': 'PlantedTank'}\n",
      "{'body': 'Agreed not worth the time or money ', 'subreddit': 'WhatsInThisThing'}\n",
      "{'body': 'Indeed we have.', 'subreddit': 'arrow'}\n",
      "{'body': 'Happy Birthday Love!!!!!', 'subreddit': '90DayFiance'}\n",
      "{'body': 'correct. recovering an acct will 100% log out an account instantly', 'subreddit': '2007scape'}\n",
      "{'body': \"Just a note, you probably don't need as much sand as you think you do. 9-10 inches is overkill but in the safe range, even .50 AE will only make it six or seven (several YouTubers have confirmed). So you really need about half a 5 gallon bucket of sand, which is way easier to move around.\\n\\nAlternately, while these are only rated for .44 Magnum and below, if you're in an apartment and shooting 9mm, I'd probably consider this or something similar:\\n\\nhttps://www.range-systems.com/product/guardian-compact-clearing-trap/\\n\\nI live in a concrete and brick house so any exterior wall down towards the ground is a safe direction. \", 'subreddit': 'CCW'}\n",
      "{'body': 'thanks for the help everyone 6mb speeds not as bad as the tmobile subreddit had me thinking and for such a low price!', 'subreddit': 'Sprint'}\n",
      "{'body': \"I'm for this \", 'subreddit': 'steroids'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"That kid's going to die a virgin for sure.\", 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'My parents Chevrolet Caprice with power windows. Only rich people could afford that.', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': \"i'm not gonna advocate blind trust in riot, but i really hate the mentality that a lot of people have where they think they know what's best for riot as a company better than riot itself.\\n\\nthey're just going off whatever data is available to them. i highly doubt riot would opt into certain situations while knowing it's going to cost them more than what they stand to gain\\n\\nthen again, companies make mistakes too\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Stink bug?', 'subreddit': 'ImagesOfMontana'}\n",
      "{'body': 'Ha I goofed. Since herb was mentioned I looked at my herb level for my farming level. I have 40 farming...so yeah. my bad.', 'subreddit': '2007scape'}\n",
      "{'body': 'Thought they were CGI. \\n\\nFor real though, you have beautiful lips. You should be proud. ', 'subreddit': 'selfie'}\n",
      "{'body': 'Wheres the cake?', 'subreddit': 'gaming'}\n",
      "{'body': \"&gt; Literature =\\\\= Manga. You just cant compare the two.\\n\\nTrucks are not pencils but I can say that trucks are on average heavier than pencils. I am not sure why you would say that is not possible to compare two aspect of things simply because they belong to different categories.\\n\\n&gt; It has the comical and action hooks to keep a child interested while being layered with themes of Eastern spirituality and mirrorings of Eastern mythology, all while being a story primarily about self improvment and facing the world with the optimism that you CAN do what you set out to acheive.\\n\\nTypos apart, do you define a work art on the basis on whether it's interesting, entertaining or positive? Genuine question, no sarcasm. Because to me you're simply saying that Dragon Ball is nice to read, which I agree on. \", 'subreddit': 'changemyview'}\n",
      "{'body': 'Tasty! Love to see more', 'subreddit': 'gonewild'}\n",
      "{'body': 'Either way maybe I should bough out. ', 'subreddit': 'mildlyinteresting'}\n",
      "{'body': 'His ticket literally requires time travel to close with a solution.', 'subreddit': 'sysadmin'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnon5q1/):\\n\\nI got my ISBN number through them when I first finished the book. My parents made my very rough first draft into a physical copy to surprise me. The copy you see there I did not pay to print. ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'She got sent home before filming started. ', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': \"He is bringing you on his dates with HER, not the other way round, quite frankly. If all you do is sit there and watch him talk at her all night about himself, you're a part of the audience, not the participants.\", 'subreddit': 'relationships'}\n",
      "{'body': 'Wow. WTF. I really don’t know how something like this was justified back in the day. ', 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': \"That's actually a really awesome observation! I also agree with it! \\n\\nExtroverted? Maybe do more portrait shots. Introverted, maybe some landscapes and scenary or even wildlife\", 'subreddit': 'photography'}\n",
      "{'body': \"BBQ is useless without slaw and beans, don't forget the beans.  I also like slaw on cheeseburgers sloppy joes and fish sandwiches.\", 'subreddit': 'food'}\n",
      "{'body': 'As a delivery driver 2s are always nice to see. Get about 1 a month.', 'subreddit': 'nba'}\n",
      "{'body': \"&gt;the only cases where a consumer would not have an alternative would be a natural monopoly\\n\\n&gt;That's naive.\\n\\nExactly, a natural monopoly isn't the only case. You could be in a market where there's not enough demand for competition, like in a country town where there's only one supermarket or bakery in an hour drive.\\n\\nNo amount of tax cuts and removing the minimum wage is going to generate demand in places it doesn't exist.\", 'subreddit': 'australia'}\n",
      "{'body': 'Get TO .500', 'subreddit': 'CFB'}\n",
      "{'body': 'Try Google asshole.', 'subreddit': 'pics'}\n",
      "{'body': \"that's understandable, there seems to be a stigma about meeting your SO online?? But yeah not a lie heh\", 'subreddit': 'actuallesbians'}\n",
      "{'body': \"Your submission or comment was removed for the following reason(s):\\n\\n\\n---\\n\\n---\\n\\n**[Rule 6: Post Formatting](https://www.reddit.com/r/TumblrInAction/wiki/the_tia_rulebook#wiki_rule_6.3A_post_formatting)**\\n\\n**Screenshots, if used, must be full post and full context.** \\n\\n**Just posting the headline of an article will get your post removed**. This includes someone sharing a link on social media without any discussion on that post.\\n\\nWe should be able to see the post in question as well as any necessary context to understand what's being discussed.\\n\\nYou may censor any blog names to protect the bloggers, but if the post looks like a troll it will be removed and you will be asked to **send the mods a link to the blog for verification**.\\n\\n\\n---\\n\\n\\n\\n---\\n\\nIf you have any questions or comments about this action, **Use this link to send us a mod mail message** [here](https://www.reddit.com/message/compose?to=%2Fr%2FTumblrInAction&amp;subject=About my removed submission&amp;message=I'm writing to you about the following removal: https://www.reddit.com/r/TumblrInAction/comments/73gf71/say_one_of_these_in_public_i_dare_you/. %0D%0DMy issue is:). \\n\\n \\n**Any PM sent to individual mods about this action will likely be ignored.  Mod mail is the proper channel and will be your best bet for any appeals, so please use the link above.**\", 'subreddit': 'TumblrInAction'}\n",
      "{'body': 'This is interesting :0', 'subreddit': 'Guildwars2'}\n",
      "{'body': \"Way to cash in on some random lady's grief.\", 'subreddit': 'forwardsfromgrandma'}\n",
      "{'body': '20 minutes', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': '522', 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': \"Yet it's the richest city in the US and homeless rule the streets. Wtf. \", 'subreddit': 'Seattle'}\n",
      "{'body': '[deleted]', 'subreddit': 'videos'}\n",
      "{'body': 'Votar tem um custo de oportunidade elevado.\\n\\n\\nTem o deslocamento, a preparação do mesmo, a fila, o constrangimento de ter lá alguém conhecido, o constrangimento de ter de tomar uma decisão difícil, a obrigação moral de não votar de forma desinformada...\\n\\nEu irei votar, mas mesmo assim, e apesar de me ter esforçado ao máximo para estar informado sinto que o meu voto é um bocado inútil. Não percebo o suficiente disto para ter a legitimidade de decidir... ', 'subreddit': 'portugal'}\n",
      "{'body': 'Having read both, the TMM is really an in depth guide on the baby steps, while CGM covers budgeting, communication, bargaining, etc. CGM mentions the baby steps, but focuses more on the bigger picture of your finances.', 'subreddit': 'DaveRamsey'}\n",
      "{'body': 'I think a lot of people are overrating action video games (Overwatch) due to personal bias, and I think it would be an interesting study to compare this to logic-based exercises. I think the areas of memory/learning would be similar even given the different sensory info. ', 'subreddit': 'science'}\n",
      "{'body': '*\"And with that, a mighty cheer went up from the heroes of\\nStamford.  They had banished the awful WWF name forever,\\nbecause it was haunted.  Now let\\'s all celebrate with a cool steak wrap.\"*', 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'Ahah I fell in love with it at first sight ;) so much better than blue! ', 'subreddit': 'fountainpens'}\n",
      "{'body': 'Agreed fuck micro transactions never defend that shit\\n\\n\\nEdit: wow you guys are just proving my point thanks for accepting them btw', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'I wanna say it was from /u/neuralhandshake', 'subreddit': 'DetroitRedWings'}\n",
      "{'body': \"Thank you! That's something I hadn't thought of before. I guess I want to be able to think I beat it and it's gone forever, but it's not. \", 'subreddit': 'stopdrinking'}\n",
      "{'body': 'Well everyone knows that', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'me_irl'}\n",
      "{'body': 'Oh, I can be that guy without even *trying* but yeah. ', 'subreddit': 'malefashionadvice'}\n",
      "{'body': 'Traffic impact! Hah. \\n\\nThose people are coming one way or another, the question is do you want them in higher density development right next to the highway or spread out along your city edge in lower density development? ', 'subreddit': 'toronto'}\n",
      "{'body': \"Wish I knew. Never seen it before . It was sticky and wouldn't break up. \", 'subreddit': 'saplings'}\n",
      "{'body': 'First you learn lots of insane ways to do math that you never imagined even existed, then you apply it in every class you take after that. And I mean every class. There are literally no easy classes in engineering. ', 'subreddit': 'EngineeringStudents'}\n",
      "{'body': 'egg confirmed. ', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': \"It's like the MSST line hasn't heard people cheer before.\", 'subreddit': 'CFB'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Srry to say bud but that isn’t worth 2,000 ', 'subreddit': 'DBZDokkanMarketplace'}\n",
      "{'body': 'Ahhhh she’s so cute! My two-month-old’s first game will be October 14 and I’m so excited!!!', 'subreddit': 'losangeleskings'}\n",
      "{'body': \"Well you need to find a place. Don't put other people's lives in danger because you want to get high. \", 'subreddit': 'trees'}\n",
      "{'body': \"Yes man!!!! I've been eyeing them for years. 😍\", 'subreddit': 'askgaybros'}\n",
      "{'body': 'Damn that sucks ', 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': \"The older ones on Steam don't have uplay afaik, and the only way to get newer assassins creed without uplay is to torrent them so ubisoft makes it difficult for me to pay them even though I'd like to\", 'subreddit': 'GameDeals'}\n",
      "{'body': 'Okay well the best advice a commish could get is to let your league be as organic and natural as possible.  Don\\'t interfere, and if other league mates get mad at you and are like \"it\\'s not fair you should veto\" you should say \"that\\'s your own fault for not getting a trade done before this one was accepted\"\\n\\nBeing a commish is about making sure rules are in place and keeping people involved, and it\\'s fun.  You are there to police but only by extreme circumstance, like someone abandoning a team during playoffs.  A trade mid season? That\\'s nothing strange, got to let it happen.', 'subreddit': 'fantasyfootball'}\n",
      "{'body': \"143412299| &gt; United States Anonymous (ID: lbsRXVZP)\\n\\ni only voted for trump, which i now regret since i'm woke about america\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': '\"I was expecting something better but THIS will do.\"', 'subreddit': 'CODZombies'}\n",
      "{'body': 'he looks nothing like the mascot, what is this?', 'subreddit': 'CFB'}\n",
      "{'body': \"I suppose you're right. I did find it in a US parking lot after all. \", 'subreddit': 'funny'}\n",
      "{'body': 'Really?', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'pics'}\n",
      "{'body': 'So we don\\'t have to give it aids names like calling it \"the ERFWQ\" combo, just give it a name.', 'subreddit': 'Rivenmains'}\n",
      "{'body': \"Sweet iced tea is my favorite flavor. Unfortunately, I haven't encountered sweet tea ice cream, sweet tea shakes, sweet tea malts, or sweet tea candy, but it doesn't matter because cold sweet tea is the best thing to have ever graced my taste buds.\", 'subreddit': 'anime'}\n",
      "{'body': \"At least one guy is working on it(discussions in slack so far), and someone else made a post a week or two ago about one alternative.  It won't be coming with the first hardfork though, it will be a ways out.\", 'subreddit': 'btc'}\n",
      "{'body': \"Yep.  Bullying went from an ignored problem, to the diagnosis for every sort of conflict no matter the degree or circumstance. \\n\\nFrankly, I'm tired of hearing the word bullying... \", 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'Anyone got links to the last 2 goals and the scuffle at the end?', 'subreddit': 'Habs'}\n",
      "{'body': 'If you are tightening the cooler too much, it will make the paste too thin. Alternatively if there is too much paste, you will see the gpu temp rise quickly when you launch a game.\\n\\nYou could try using a grease instead of paste. Noctua paste, Phobya Hegrease, or Arctic MX-4. MX-4 over arctic silver 5.', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'https://twitter.com/TomGulittiNHL/status/909458844505559041', 'subreddit': 'devils'}\n",
      "{'body': 'the fluid simulation is really nice!, keep it up', 'subreddit': 'webdev'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': \"Had this on cassette because my car didn't have a CD player. I ran that tape into the fucking ground playing it so much. \", 'subreddit': 'Music'}\n",
      "{'body': '[MRW I try to masturbate to this as a black guy.](https://media.giphy.com/media/xT9KVJLPn98SNxXEME/giphy.gif)', 'subreddit': 'insanepeoplefacebook'}\n",
      "{'body': 'I think you just get the regular vaule and go about your day  \\nwhich is kinda dissapointing but reasonable', 'subreddit': 'PhantomForces'}\n",
      "{'body': '\"You can probably expect to see myself and some other humans in the cider line,\" I say, taking another drink.', 'subreddit': 'MLPLounge'}\n",
      "{'body': 'Your uncle must’ve been happy you finally asked him to homecoming', 'subreddit': 'RoastMe'}\n",
      "{'body': \"I don't see them being debrief on that information since they was only tasked to find Blue Team. Locke may known since he was a high ranking ONI agent, but kept it to himself unless it was necessary.\", 'subreddit': 'halo'}\n",
      "{'body': \"It was suppose to. Now they can't even get that straight. So now, it's cuz Trump durrrr.\", 'subreddit': 'news'}\n",
      "{'body': \"[+peginus](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo18k/):\\n\\nOh okay, so you paid to publish all of your other copies except the one you're holding?\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"Because the same number of people don't play each class. You could have 8 classes with 100% winrate and one class with just under 50% winrate if (literally) almost everyone plays that one class.\", 'subreddit': 'hearthstone'}\n",
      "{'body': 'I don’t dislike her for casually dating because that’s what lots of people do. But I dislike her for calling Taishi out on the very thing she is doing! She half assed her talk with Eric about seeing other guys. Though Taishi isn’t better by going cold turkey with Anna and Niki previously, at least he managed to change when he got called out.\\n\\nAnd her Instagram posts. GIRL. If you’re not bothered with the comments you’re getting, why keep making posts about them? I know she should defend herself but for someone who doesn’t know how reality show works, Cheri has been very careless on how she presented herself. And this is a show without a script so most people perceive it to be 100% real. So that makes her look really off to the Japanese and Asian viewers, the majority of the Terrace House audience.\\n\\nThat’s just my two cents on her. I just think she’s very hypocritical and obviously so sensitive to the criticism even if she claims she’s stronger than that.', 'subreddit': 'terracehouse'}\n",
      "{'body': 'That would just segregate the player base *even more*, and make finding a match even harder.', 'subreddit': 'Brawlhalla'}\n",
      "{'body': \"[That's cute.](http://www.thecanadianencyclopedia.ca/en/article/parizeaus-lobster-flap/)\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'you sound so entitled, you dont need the skin for  play you know?you like always only wanna free stuff only', 'subreddit': 'BattleRite'}\n",
      "{'body': 'I love the bubble blower! It can be used as a diversion. It can be an offensive weapon. It can shield you from enemies. It can help control certain areas. I have special power up so the bubbles are huge! I find it to be a fantastic special just got to know when to use it. ', 'subreddit': 'splatoon'}\n",
      "{'body': \"Pm'ed. Thanks!\", 'subreddit': 'Warframe'}\n",
      "{'body': \"I told him I didn't want to be too intimate because we're not officially a couple so I did mention it to him that's the thing. \", 'subreddit': 'relationship_advice'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'At least go through Milton\\'s \"Free to Choose\" video series.\\n\\nhttps://www.youtube.com/watch?v=D3N2sNnGwa4', 'subreddit': 'Conservative'}\n",
      "{'body': \"For me success is more about autonomy, integrity, and people. I'll never become a millionaire doing construction, but if I have some say in my work, I'm proud of what I do, and I spend my days around good people then I consider myself successful.\\n\\nKnowing that I have options and can afford to speak my mind, set boundaries, and work under fair conditions means a lot to me. More so than, say, a 10 or 15% pay bump.\", 'subreddit': 'RedditForGrownups'}\n",
      "{'body': 'One of the few regrets I have about college  is not going to Blacksburg for gameday while I had buddies at VT. Enter Sandman is fucking iconic', 'subreddit': 'CFB'}\n",
      "{'body': 'its extrapolated via syringe', 'subreddit': 'Showerthoughts'}\n",
      "{'body': \"I Googled a bit, but I'm not sure of specific programs or campaigns.  \\n\\nYou could contact the [Alabama ACLU](https://www.aclualabama.org/en/voting-rights-restoration) or the [Alabama Democratic Party](http://www.aldemocrats.org/) and maybe they'd be able to find something for you to help with.\", 'subreddit': 'BlueMidterm2018'}\n",
      "{'body': '[deleted]', 'subreddit': 'videos'}\n",
      "{'body': \"As a married poly person with children, I would honestly have my brakes screeching right now. Being poly is all about communication and trust. You laid out rules that were agreed to and not even a week later and she's already dismissed your comfort and boundaries? \\n\\nNo. Just no. If My husband had done that to me I would immediately rescind the poly permission and spend some time focusing on *us*. Maybe that's harsh, but from what I can see your wife is going to ignore your needs, and use your permission to be poly as an excuse to do so.\", 'subreddit': 'polyamory'}\n",
      "{'body': \"How'd you like ME andromeda?   It's super cheap now.   Heard it was huge disappointment.....\", 'subreddit': 'battlefield_one'}\n",
      "{'body': 'when you say assited boot, i assume that means you enter the bios/uefi, from which you can set the boot priority. There youll have the option to set grub/whatever you want as the highest priority. Probably the most important step is that you remove the usb with the iso, once the install is complete it is no longer needed and is likely confusing you. i hope that helps ', 'subreddit': 'linux4noobs'}\n",
      "{'body': \"&gt; There are just financial reasons why they don't\\n\\nAnd player contracts too. Dips into the financials of course but it's a lot of trouble to break a player's contract.\", 'subreddit': 'news'}\n",
      "{'body': ':( ', 'subreddit': 'TexasRangers'}\n",
      "{'body': 'Hmmmmm. Understandable ', 'subreddit': 'PvZHeroes'}\n",
      "{'body': \"Short answer, yes. He was tested and monitored throughout the month, as was I. Also, ***TRIGGER WARNING VOMIT***\\n\\nLong answer, Norovirus is extremely contagious. Just a few parts per million can make you violently ill. You can also get reinfected over and over. Which is what happened to DS and I. \\n\\nDS was ok during the day but in the middle of the night he would wake up sick. He has autism so when he would get sick he would panic and try to out run the projectile vomiting. Every night he would run through the house projectile vomiting at least once. Every room in the house was covered in vomit except my daughter's. My bed, his bed, the couch, the arm chairs, my bookshelf, nothing was left untouched. I would spend hours cleaning every night.\\n\\nHe woke me up one night by puking in my face.\\n\\nAs you can imagine, it took a long time for the kid and I to shake it. Surprisingly, DH and DD never got sick.\\n\\nEDIT: also he refused to take any meds at this point so I couldn't get anti nausea meds down him. He always ate and drank well, he was pretty much normal all day. He would just wake up every night and spend an hour or 2 puking.\", 'subreddit': 'JUSTNOFAMILY'}\n",
      "{'body': \"He's just working harder,             \\nto make the abyss better,           \\nso we can travel faster,        \\nand become stronger.\", 'subreddit': 'anime'}\n",
      "{'body': \"Most of the tribes are good in XLN limited, but none of them have enough standard support to be viable. If you want to do a tribe, though, Dinosaurs would be your best bet with things like [[Kinjalli's Caller]], [[Optec Huntmaster]], [[Ripjaw Raptor]], and [[Carnage Tyrant]].\", 'subreddit': 'magicTCG'}\n",
      "{'body': '[deleted]', 'subreddit': 'smashbros'}\n",
      "{'body': 'IT SHOULD BE MY TURN. I WAS GOING TO DO IT TWO MONTHS AGO AND THEN I DIED NIGHT 0.', 'subreddit': 'hogwartswerewolvesA'}\n",
      "{'body': 'Damn...', 'subreddit': 'martialarts'}\n",
      "{'body': 'Hi there! Your post was removed because it uses the text box. Per [rule 1](/r/AskReddit/wiki/index#wiki_-rule_1-), use of the text box is prohibited. You can resubmit your post [here](/r/askreddit/submit?selftext=true&amp;title=Emergency Room workers of Reddit, what are the craziest, most unbelievable, insane stories you have?) without the textbox.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Muslims and Sikh are peaceful because I choose to believe people are born generally good. However, having researched a multitude of religions I can tell you that Islam is quite unlike the other abrahamic religions. All have barbarism but that doesn't mean they exist at uniform degrees of intensity \", 'subreddit': 'aww'}\n",
      "{'body': 'What makes these amazing aside from what actions. Is they prove you can allow them to physically and mentally develop before training and they are more sound. They do not start training ground work until they are four and riding until almost five. I hope soon the world will take suit also that we see the health benefits of not shoeing. ', 'subreddit': 'Horses'}\n",
      "{'body': 'Use an incognito tab on chrome', 'subreddit': 'frugalmalefashion'}\n",
      "{'body': 'Tone down pitcher confidence\\n\\n\\nMake hitting less rng \\n\\n\\nFix poor fielding/throwing animations (tired of a runner going from 2nd to home on a ball hit 4 feet in front of home plate that I throw to first)\\n\\nNo way the show touches esports without #2 and #3 fixed', 'subreddit': 'MLBTheShow'}\n",
      "{'body': '[deleted]', 'subreddit': 'dirtyr4r'}\n",
      "{'body': '*Ugh*, I know though! I want her to come back and slay. She is honestly one of my all-time faves. Her personality is just so refreshing and sweet. I love her! Such a classy, talented lady!', 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': \"Hey, we've lost to both of those teams at home.\", 'subreddit': 'CFB'}\n",
      "{'body': 'Steel type:  \\n  \\n- Start with a sturdy Skarmory. Could set up spikes or toxic before death.  \\n- Air balloon Magnezone. Sturdy again since magnet pull wouldn\\'t help too much.  \\n- Heatproof Bronzong. Covers fighting and helps against fire.  \\n- Empoleon. A better fire counter. Give it an AV.\\n- Aegislash. No fighting weakness and powerful hitter.  \\n- Ace: Scizor. My 4th favorite mon and a potential mega for a \"rematch\" team.', 'subreddit': 'pokemon'}\n",
      "{'body': 'With 2cb every mg counts. A dose of 13mg for someone could be very different to 18mg dose', 'subreddit': 'DNMUK'}\n",
      "{'body': \"You've not been out clubbing for a while, have you?!\", 'subreddit': 'gifs'}\n",
      "{'body': \"[+msanteler](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoomeq/):\\n\\nShe's saying her parents paid for the first one, which assigned an ISBN number. Every copy since then (including the one she's holding in the photo) has not been pay-to-print\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"it's a masterpiece.\", 'subreddit': 'xboxone'}\n",
      "{'body': 'Just save your rp for the next time they have special loot. ', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Well duh they can keep the brown people out ', 'subreddit': 'news'}\n",
      "{'body': 'Go Rebs. ', 'subreddit': 'CFB'}\n",
      "{'body': \"Ok I'll bite. Based on my original comment your initial reply had nothing to do with my post. You are just inventing a secondary argument. If you do require my political stance: I am not an advocate for Trump and I did not vote for him. I guess I really do not understand what triggered you. Again though, I really do hope you have a good night.\", 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': 'I GET TO GET DRUNK BEFORE THE 3RD INNING!!!! THANKS CARDBROS!!!! \\n\\nEdit: GO CU!!!! ', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': 'I bet she dried them on the clothes line or with low heat in the dryer. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I'm in. Go Tigers\\n\\nEdit: Would it be a haiku for comments on every subreddit or just r/cfb?\", 'subreddit': 'CFB'}\n",
      "{'body': 'So can I post a meta link to it in another top comment?', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I think it's a special way to cook an egg. \", 'subreddit': 'technology'}\n",
      "{'body': 'This might be true for your most basic White Oaks, but you can spot a pair of super slubby japanese selvedge, or rainbow weft, or N&amp;F vugar selvedge from a mile away.\\n\\nThe build quality is also often better, turn a pair of Ciano Farmer inside out next to a pair of Uniqlo selvedge and tell me which is better built.\\n\\nThere is also a matter of inherent value in the materials. Shuttle looms put out much less fabric than their modern counterparts. Boutique mills will use specific strains of cotton to achieve desired textures and fades. The dying process is often more labor intensive  and uses real indigo (most dept. stores use artificial dyes). Also, a rare deadstock fabric that only has &lt;100 bolts in existence is going to cost more than a run of the mill (literally) fabric that is being pumped out in the 1000s of yards per day.', 'subreddit': 'malefashionadvice'}\n",
      "{'body': 'Lol at chaisson sidestepping', 'subreddit': 'CFB'}\n",
      "{'body': 'Flair up', 'subreddit': 'CFB'}\n",
      "{'body': 'Yes but not the condoms.', 'subreddit': 'nononono'}\n",
      "{'body': 'Oh yeah, I hate sticking-out tongues.', 'subreddit': 'datingoverthirty'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Dun worry Meta Coola will one shot Goks', 'subreddit': 'dbz'}\n",
      "{'body': \"Our boi Bamboe coming through in the clutch.\\n\\nCan't say I'm too relieved with them qualifying for a LAN thanks to trademark Sexy Bamboe plays [but](http://68.media.tumblr.com/ab5ee52bbbd86b1ffa2069e7758fe820/tumblr_ojbd1zglWn1qim9hqo3_400.gif) \", 'subreddit': 'DotA2'}\n",
      "{'body': '[deleted]', 'subreddit': 'PrettyGirls'}\n",
      "{'body': \"&gt; I suffer from severe anxiety, and my first kitten's behavior is pushing me to the limit, to the point where I dislike him.\\n\\nYou don't need to apologize for taking care of your mental health. I think the best thing for you to do is to rehome them pronto. Cats under 1yo are pretty easy to find homes for - they're cute! You might do well adopting an older cat. Older cats are harder for find homes for, so there are a lot of great cats looking for homes. The best thing about mature cats that their personalities are already developed so you can talk to whoever is caring for them now and find out if they are a good fit for your home.\", 'subreddit': 'Pets'}\n",
      "{'body': '[removed]', 'subreddit': 'worldnews'}\n",
      "{'body': '[ **text here** ]( **link here** ).\\n\\nWithout the spaces. ', 'subreddit': 'hogwartswerewolvesA'}\n",
      "{'body': '[deleted]', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'fun fact i got a Noctua NH-U9B SE2 ;P\\nnoctua coolers are not scrap FYI, heck just change the fans to something non-poo-ish else\\n\\ni got PRIME X370-PRO, Corsair Vengenace LPX 3200@16', 'subreddit': 'buildapc'}\n",
      "{'body': 'I can Promise you one thing. These people dont want to be greedy. But if the only thing that makes you happy is milking that ~~thong~~ money, then you are gonna do it.', 'subreddit': 'Showerthoughts'}\n",
      "{'body': '143418414| &gt; Canada Anonymous (ID: +DelM8Tk)\\n\\n&gt;&gt;143418344\\n&gt;still voted hillary\\nCuckservative, through and through\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Is there really equal rights for homosexuals? Also they didn't disrespect the flag they kneeled to it which in a lot of ways is almost more respectful. \", 'subreddit': 'news'}\n",
      "{'body': '\"I frequently dedicate more time to my career than self or family\"', 'subreddit': 'AskReddit'}\n",
      "{'body': \"What if it wasn't that they died to pass on rulership but it changed hands. So that each one was responsible for a certain amount of time before they let the next take over. Also interesting that there are currently 8 super entities that about 95% of economic wealth flows through.\\n\\nProbably just coincidence though.\", 'subreddit': 'conspiracy'}\n",
      "{'body': 'Why? Not arguing just curious ', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Thank you! Ill definitely follow :)', 'subreddit': 'Embroidery'}\n",
      "{'body': 'It looks like an ironstone concretion.', 'subreddit': 'whatsthisrock'}\n",
      "{'body': \"[I decided to post it myself after all.](https://www.reddit.com/r/anime/comments/73ifnk/spoilers_action_heroine_cheer_fruits_episode_12/) The episode is out, it's just only available in a batch on the torrent site (at least it was last time I checked), so maybe that's why the thread hadn't been posted yet.\", 'subreddit': 'anime'}\n",
      "{'body': '[removed]', 'subreddit': 'Incels'}\n",
      "{'body': 'Something along these lines perhaps? \\n\\nhttps://i.imgur.com/K1cEUn7.png', 'subreddit': 'qotsa'}\n",
      "{'body': \"Misplacing stuff, the time spent finding it again, the mess created while trying to find it, which makes me lose more stuff, and I don't have time to clean it up because I'm spending so much time looking for stuff!\", 'subreddit': 'ADHD'}\n",
      "{'body': 'You are missing your LSU flair, breaux.', 'subreddit': 'CFB'}\n",
      "{'body': 'I dont think so. I got quite broad shoulders (they annoy me when shopping for clothes since I am skinny yet my shoulders make small sizes not fit) yet my wrists are quite small (not the most jacked person but my wrist is like 4/5 times smaller than my biceps in diameter. Muscles dont change it either since most of the bulk ends up in the upper part of the forarms, not the wrists. Even then the only thing it affects are clothes sizes and fit. Noone else cares.', 'subreddit': 'IncelTears'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop21i/):\\n\\nThat is what I'm saying, I'm sorry for the confusion. My parents bought copies and the ISBN number when I was younger, but the copy I'm holding and any others were not pay-to-print\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Imo it has a much bigger impact than that. ', 'subreddit': 'Wellthatsucks'}\n",
      "{'body': 'That is an impressive collection but I feel like you are missing more than a couple from the collection.', 'subreddit': 'Mariners'}\n",
      "{'body': '&gt;The Media Needs To Stop Rationalizing ~~President Trump’s~~ Right-Wing Behavior\\n\\nFTFY', 'subreddit': 'politics'}\n",
      "{'body': 'I still hate this fucking stage. I discovered, though, that if you have a cape and bring a blue Yoshi with you, and you’re really careful, you can get to the first koopas, grab one, and fly the rest of the way. It’s not easy but it works.', 'subreddit': 'miniSNES'}\n",
      "{'body': 'I mean, does every team really need a name?', 'subreddit': 'SquaredCircle'}\n",
      "{'body': \"I'm a bit worried with Mi5s since lots of users reported that they have problems with the fingerprint sensor. Looks like I will narrow down my choice to Mi5 or A1. Thanks.\", 'subreddit': 'Xiaomi'}\n",
      "{'body': 'Quick question: does ethermine API offer a way to notify you if your rig is not working, or you can only get the calculated and reported hashrate?', 'subreddit': 'EtherMining'}\n",
      "{'body': \"Still /u/Yurika_BLADE is right; the new dailies do have a lower drop rate considering the AP cost.\\n\\nOn the JP table, it lists Chaldea (Wed) 30 AP with a 6.8% rate for an average of 439.6 AP per drop.\\n\\nMeanwhile on the NA side Germania is at a 3.6% drop rate but with half the AP cost (15) while is a 417.7 AP cost per drop, *slightly* better than the new daily. \\n\\nOn top of that, the 20 AP Monster Hunt Daily is a 5.3% rate at 20 AP cost  for a 380.3 cost/drop.\\n\\n---\\n\\nNow I'm not sure why the 40 AP Wednesday quest is not listed for Claws, but given the information present in the drop table, you actually have *more* chances to get Claws if you farm for them before the 3rd.\", 'subreddit': 'grandorder'}\n",
      "{'body': \"Sprint attack? Her only sprint attack is a gb. If you mean dodge attacks then it's because her dodge attacks are heavies. Berzerker, warlord, and highlander can execute from their dodge attacks too.\", 'subreddit': 'forhonor'}\n",
      "{'body': 'No love for SoundCloud :(', 'subreddit': 'Monstercat'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Neat.', 'subreddit': 'worldnews'}\n",
      "{'body': 'Manningham had a great 15 minutes', 'subreddit': 'nfl'}\n",
      "{'body': 'Antiqua is a classy typeface. ', 'subreddit': 'insanepeoplefacebook'}\n",
      "{'body': 'Thank you &lt;3', 'subreddit': 'BiggerThanYouThought'}\n",
      "{'body': \"But why? Why wouldn't someone like Xpecial or say, Apollo/Gate support the new system?\\n\\nBefore franchising- less people watching their games, barely any chances of winning the split\\n\\nAfter franchising- possibly more people watching their games and get more money through revenue sharing\\n\\nSounds like a win-win for a lot of players and orgs. \\n\\nEven if NA does fall behind I doubt it'll happen immediately after the change and with EU breaking its league into idk how many and having around 20 teams, they'll probably turn to a BO1 format too. But Riot knew how people would react if they announced BO1 for EU first so here we are.\", 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"Our points are not mutually exclusive. \\n\\nI'm saying he wasn't deficient. \\n\\nYou're saying he was better at math. \", 'subreddit': 'todayilearned'}\n",
      "{'body': 'Wow, do u have a video to share ? ', 'subreddit': 'Hotwife'}\n",
      "{'body': \"I normally don't really find him funny at all...but this is just hilarious.\", 'subreddit': 'OopsDidntMeanTo'}\n",
      "{'body': \"Yeah, it does. That doesn't mean I'll be able to scrape up the cash to buy a Wii U in time, though. I'd rather not be on a time limit to do that, so I'm trying to find out if that's the case. If I can download the app now and use it whenever I like, that would be ideal.\", 'subreddit': 'nintendo'}\n",
      "{'body': \"Let's say you own an apple orchard. You sell the apples you grow. But instead of branding them as what they are (apples), you brand them as bananas. They're still apples, no matter what you call them.\", 'subreddit': 'Nerf'}\n",
      "{'body': \"[Same dude :)](https://puu.sh/xMOA5/aa49a4e9e0.jpg) I've never tried a controller though...\", 'subreddit': 'RocketLeague'}\n",
      "{'body': 'of course. ', 'subreddit': 'starwarsspeculation'}\n",
      "{'body': 'The president of the United States of America is Donald Trump.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'may not need to be, but it is. thank you', 'subreddit': 'NBA2k'}\n",
      "{'body': \"It was so gross. It was like dark red/Brown and had lumps of what they said was the meat, and the rest was blood sauce. I guess it just like their culture's version of British black pudding or Chinese blood sausage. They had it with rice. \", 'subreddit': 'vegan'}\n",
      "{'body': 'Sounds good! When do we begin?', 'subreddit': 'cars'}\n",
      "{'body': 'You do scrappy comments.', 'subreddit': 'aww'}\n",
      "{'body': \"Hmm seems like that's how it is for some others here too \", 'subreddit': 'schizophrenia'}\n",
      "{'body': 'Then you bring in a Silverback Gorilla to eat the paper towel?', 'subreddit': 'oddlysatisfying'}\n",
      "{'body': 'Yes', 'subreddit': 'EliteDangerous'}\n",
      "{'body': 'Is there anyone ever who looked at this picture and said, “That looks like a smart guy who can handle the complexities of being a modern college football coach.”\\n\\nhttp://image.cdnllnwnl.xosnetwork.com/pics33/400/JW/JWULRFVBLTSLXMZ.20150213165120.jpg\\n\\n\\nI would pay money to see him and Butch Jones play tic-tac-toe.  They would both be so confused and frustrated. ', 'subreddit': 'CFB'}\n",
      "{'body': \"It's just that Okada/Tana will one day be revisited again, that's a given. We've already seen it for the title and at every big event, the only one left is G1 Finals so you might as well do it there, especially in a G1 where Okada winning is most likely  \", 'subreddit': 'SquaredCircle'}\n",
      "{'body': \"She's definitely not, so she should be safe :)\", 'subreddit': 'oldpeoplefacebook'}\n",
      "{'body': \"Here you go: https://c2.staticflickr.com/8/7172/6711904025_a02d7cafdc_b.jpg\\n\\nI'll take all your money now please\", 'subreddit': 'CryptoCurrency'}\n",
      "{'body': 'Well, 4DD345HRIN3 means added a shrine. You can find stick by going to top left corner on empty server and keeping walking up and right. If you found green Donald Trump without stick try changing the server.', 'subreddit': 'foesIO'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Watch fox for the comedic value?', 'subreddit': 'politics'}\n",
      "{'body': '[deleted]', 'subreddit': 'COMPLETEANARCHY'}\n",
      "{'body': 'Can i please not be stressed for the entire game this year?', 'subreddit': 'CFB'}\n",
      "{'body': \"I'm hoping that SpaceX makes fuel costs a major capital concern in the development &amp; testing of the BFR.  The only company I've ever heard had that as a major issue was Armadillo Aerospace, where I heard that fuel was about a third of their capital costs in the development of the Mod vehicle that they used in the Lunar Lander Challenge.  That was a whole lot of actual flying time for the vehicle, and it showed in the end with a very sturdy and reliable vehicle.  It is also something pretty common in terms of research teams doing VTOL rocket research to have a whole lot of flights.\\n\\nThat is something you have the luxury to do when you have a reusable vehicle.... which fortunately the BFR is going to be.  It wouldn't surprise me if there were over a thousand flights (perhaps just suborbital) before any passengers ever went on board.\\n\\nI'm hoping that the constellation will start going up before the BFR is ready for revenue service with satellites, but that certainly is a possibility too.  I really want to see a bunch of videos [just like this](https://www.youtube.com/watch?v=HXdjxPY2j_0) with the BFR in the meantime.\", 'subreddit': 'SpaceXLounge'}\n",
      "{'body': 'You look amazing! I love your abs', 'subreddit': 'gonewild'}\n",
      "{'body': \"Sturm is really good in crucible, just don't use it in PVE because there are too many exotics with better perks. Sadly I think the tractor Cannon is better, and that says something\", 'subreddit': 'destiny2'}\n",
      "{'body': \"Print is way too dark, but I don't think people will call you out for this. It's quite an obscure piece \", 'subreddit': 'FashionReps'}\n",
      "{'body': \"Thanks, I think I may have picked one out when I received it but haven't used it in over a year so I'll definitely look into that!\", 'subreddit': 'VisitingIceland'}\n",
      "{'body': \"In the context of referendums and whether they have the right to do it, self-determination &gt; all. They can also easily declare independence.\\n\\nIn the context of actual, successful implementation, however force &gt; all, obviously. Though I can hardly imagine Spain pulling a 1991 Yugoslavia and bombing Barcelona, especially since that would make the situation worse for Spain in the long run. Then again, 50% wanting something shouldn't be even close to enough for anything of such major importance.\\n\\nAnd as for geopolitics, military power isn't even close to &gt; all, otherwise North Korea would long since have stopped existing and the US would own all the oil in the Middle East.\", 'subreddit': 'geopolitics'}\n",
      "{'body': \"My ex had a cat who loved riding with us in the car.  One time she was camping out in the back and we stopped at Burger King and got some chicken tenders.  We went out to the car and decided to eat them while we were still parked there; we just set them in the center console.  She smelled them, wandered up to the box, and just casually took one.  Didn't eat it, but we thought that was so adorable.\", 'subreddit': 'aww'}\n",
      "{'body': '143418153| &gt; United States Anonymous (ID: 7aSzBYHR)\\n\\n2012: Gary Johnson\\n2016: Gary Johnson\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"There's so many crappy looking Yugioh cards that ended up being part of some really niche broken strategy though and that's the fun.\", 'subreddit': 'Games'}\n",
      "{'body': '[deleted]', 'subreddit': 'LearnUselessTalents'}\n",
      "{'body': \"No reason for that kind of tone, you certainly wouldn't be in any danger\", 'subreddit': 'Tinder'}\n",
      "{'body': \"I know, i'm not 100% sure about it either But i'll let it be for now.\", 'subreddit': 'tanks'}\n",
      "{'body': \"that's true. fury was remorselessly grim but it felt maybe less grounded (?) than his other darker films. training day is, imo, as close to a perfect 'life fucking sucks' movie as there is because the leads are dark, twisted, but emotionally relatable. in fury it felt like each character was relentlessly dark which is, perhaps, how life was for soldiers during ww2.\", 'subreddit': 'movies'}\n",
      "{'body': '**One** DAY **Six** HOURS !!!!!!!!\\n\\nOur 2nd longest time yet!\\n\\nCLOCK RESET!!!!! WOOO GO BROWNS!', 'subreddit': 'Browns'}\n",
      "{'body': \"Lol well when I see u in hell let's get a beer\", 'subreddit': 'CringeAnarchy'}\n",
      "{'body': \"ah okay.. i've just hit level 20. So do I wait or decrypt straight up once i've got some?\", 'subreddit': 'destiny2'}\n",
      "{'body': 'And to some, including myself, it is kind of retarded to ask someone to stand for something they have no obligation to.', 'subreddit': 'news'}\n",
      "{'body': \"I agree it was a good movie but it was not an original concept. There's very few movies like it, but almost every writer or aspiring producer/director Ik has had a similar idea.\", 'subreddit': 'horror'}\n",
      "{'body': '*Neo Yokio*', 'subreddit': 'TwoBestFriendsPlay'}\n",
      "{'body': 'Concentrate and ask again', 'subreddit': 'pokemongo'}\n",
      "{'body': 'yeah. wait for it.', 'subreddit': 'phgonewild'}\n",
      "{'body': 'Guys loved listening. First time very impressed, quality and content.', 'subreddit': 'minnesotaunited'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop0dm/):\\n\\nAll the other copies are not paid. My parents paid to print some copies when I was younger. After getting my book professionally edited I went through CreateSpace and Amazon (which is free) to publish and distribute my book', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Dad, you are fucked! ', 'subreddit': 'trailerparkboys'}\n",
      "{'body': \"Nope. It's a singular noun for a group of nouns, and therefore would need a singular verb. It describes the group of nouns as a singular entity \", 'subreddit': 'askscience'}\n",
      "{'body': \"True enough, but it's slightly more of a pain in the ass to build a mailing list that way.\", 'subreddit': 'Entrepreneur'}\n",
      "{'body': 'I just it for Rank 4 this morning.', 'subreddit': 'Warframe'}\n",
      "{'body': 'really? the only subaru issues i would regularly see are valve cover leaksm head gasket leakage at 100k miles, and timing cover leaks (on 6 cylinders). other than that, never seen any braking issues or electrical issues. Worked as a subaru mechanic for 5 years...\\n\\nthat being said, ill buy the car off of you. How close to $500 are we?', 'subreddit': 'Justrolledintotheshop'}\n",
      "{'body': \"That's pretty much how I feel. If I had a second team, it'd be Texas. I loved watching them as a kid, and their orange is one of my favorite colors. \", 'subreddit': 'CFB'}\n",
      "{'body': 'Girl, look how fucking orange you look, girl!', 'subreddit': 'educationalgifs'}\n",
      "{'body': 'Odc, Turbo, and a couple nitro', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Asicboost does not reduce the CO2 emissions of bitcoin. It just changes the % allocation of hash power to those that have it enabled from those that don't. The whole stability of bitcoin relies on the amount of hash power being committed to the network. The more committed hashpower, the stronger the network. Asicboost games the system to the advantage of a select few. \", 'subreddit': 'btc'}\n",
      "{'body': '[deleted]', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Very nice choice, what else did it have?', 'subreddit': 'CasualConversation'}\n",
      "{'body': 'Judge has been average in RF according to his defensive WAR. ', 'subreddit': 'baseball'}\n",
      "{'body': '\"Turn around.\"', 'subreddit': 'Overwatch'}\n",
      "{'body': \"Do you think manual labor positions will become reallocated in a way? I'd with our advancements we will also open up new doors for workers.  \\n\\nI also feel like humanity will be able to adapt to change in the workforce like in the past but maybe I'm being too idealistic \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"They both contain rodents dying. That's a fairly close correlation.\", 'subreddit': 'AnimalsBeingJerks'}\n",
      "{'body': 'It looks like your original comment was removed, but I am still fine in trading', 'subreddit': 'pokemontrades'}\n",
      "{'body': 'Teach an adult aged woman how to whistle if she has not yet found the talent.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'It was OK', 'subreddit': 'hockey'}\n",
      "{'body': 'First of many I hope! Let me know if you take requests', 'subreddit': 'gonewild'}\n",
      "{'body': 'Emmelyne Costayne', 'subreddit': 'IronThronePowers'}\n",
      "{'body': 'Where can I find more information on that?', 'subreddit': 'UofT'}\n",
      "{'body': \"Relax, everything will be alright. I mean, I don't know what will happen, and I honeslty never bothered to check what that leak business was about, but I'm sure there's no need to stress over it. \", 'subreddit': 'StarVStheForcesofEvil'}\n",
      "{'body': 'Blood donor of three years here. My suggestion is looking away. The pain is only for a split second and does not last.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Thank you Shannistration for your submission to /r/thatHappened! Unfortunately it was removed for the following reason(s):\\n\\n\\n* **This is either a recent repost or this specific story is being or has been posted repeatedly.**\\n\\n  Avoid reposting by using the search function and browsing the Top of All Time. Users who frequently repost may be permanently banned.\\n\\n\\n\\n\\n\\nPlease [message the moderators](https://www.reddit.com/message/compose?to=/r/thatHappened) if you have any questions.', 'subreddit': 'thatHappened'}\n",
      "{'body': 'Welp. I guess bye Brewers', 'subreddit': 'Dodgers'}\n",
      "{'body': '[removed]', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': \"People making smash displays or MvC displays...\\n\\nI'm over here making my MUGEN display smh\", 'subreddit': 'ActionFigures'}\n",
      "{'body': \"In Australia, Eb games is giving a cappy hat with every preorder. Not sure about other locations but it doesn't seem like there are any in game bonuses.\", 'subreddit': 'NintendoSwitch'}\n",
      "{'body': \"Literally spent an hour testing it. Below 190 didn't crash at all for me. At 195 literally couldn't load into the tower after 10 attempts. Can't just pass that off as RNG. It definitely became more difficult to get into the tower as I added more to my vault.\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Quick calculation shows about 8 or 9 carbs total for the glaze. Can probably be omitted.', 'subreddit': 'GifRecipesKeto'}\n",
      "{'body': 'No worries! Good luck!', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Jet sweep to the TE....', 'subreddit': 'CFB'}\n",
      "{'body': 'Lmao if only the founders of this country could see how sensitive “patriots” have done a complete 180 on what was supposed to be the most important right we have. \\n\\nNot saying the first amendment is supposed to protect people from all the consequences of their speech, but to badmouth people using it and to seemingly relish in punishment for a peaceful, non-disruptive protest is an absolute absurdity. ', 'subreddit': 'news'}\n",
      "{'body': \"Hey. I want One Piece if that's cool? It is the full game, yes?\", 'subreddit': 'SteamGameSwap'}\n",
      "{'body': \"Aho Girl and Made In Abyss were worth watching, but they didn't exactly have stiff competition.\", 'subreddit': 'TwoBestFriendsPlay'}\n",
      "{'body': '&gt;The ranking in Forza 7 is very unusual. Nvidia has confirmed ComputerBase, however, that the results are so correct, so there is no problem with the system in the editorial regarding GeForce.\\n\\n\\nI am so confused', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'I like \"descriptive annotation\" :)', 'subreddit': 'gaming'}\n",
      "{'body': 'Halfmoon &lt;3', 'subreddit': 'bettafish'}\n",
      "{'body': 'Traps are gay tbh', 'subreddit': 'Phijkchu'}\n",
      "{'body': '[deleted]', 'subreddit': 'offbeat'}\n",
      "{'body': 'Ephraim gauntlet was bigger', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"Why not just milk an hour earlier, so they're routine isn't messed up?\", 'subreddit': 'australia'}\n",
      "{'body': 'Is it only opiates though?  I told them I smoke weed like every day and they just told me not to come in high.', 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Salve, to keep it moist.  ', 'subreddit': 'gifs'}\n",
      "{'body': \"Yes. That's why ISFPs are better at handling conflict even though we abhor it too. If it doesn't happen in real life, it doesn't happen in our mind. Unless the ISFP is in Ni loop, in which case the ISFP sort of becomes paranoid.\\n\\nBut healthy INFPs don't have this problem either because their Ne sees other things to focus on soon. It's when your tertiary Si is overactive, that it forces you to keep reliving the past, analyzing it. INTPs are like that too, except they are more into analysis rather than feeling emotions about it.\", 'subreddit': 'infp'}\n",
      "{'body': '1g ket was in mylar', 'subreddit': 'DarkNetMarkets'}\n",
      "{'body': 'Roughly 9 days. I applied 9/20 and received them 9/29.', 'subreddit': 'churning'}\n",
      "{'body': \"You don't even need to jog if you just want to lose weight. Just eat whatever you want, but less of what your usual serving sizes are, it doesn't have to be drastic either. I've lost around 20 pounds over the last 20 weeks, so that's around a pound per week. Or if you want to take it a bit further, download myfitnesspal and count macros.\\n\\nI personally recommend some kind of strength training if you can commit to a gym, if not maybe start with bodyweight exercises. Not a coach or anything, just really interested in fitness haha.\", 'subreddit': 'Philippines'}\n",
      "{'body': 'Fuck.', 'subreddit': 'StarVStheForcesofEvil'}\n",
      "{'body': 'a guidline I follow when it comes to fetishizing: get the opinion of the actual people being represented as to whether or not the representation is good. in the example above, if women are going nuts about the gay male characters, but actual gay men hate how they are portrayed than that is probably fetishization.\\n\\nas a side note, I find emotional stuff sexy.', 'subreddit': 'dragonage'}\n",
      "{'body': 'No cuz Miami respects him and he said he said he wants to play in miami again', 'subreddit': 'nba'}\n",
      "{'body': \"That's a fun way of thinking about it! \\n\\nStill though, it adds a whole different layer if you have to decide how much you can afford to carry. \\n\\nMore importantly, the fact that you only have a limited amount of arrows means every shot has to count. \\n\\nBut yeah in Skyrim it's not an issue... So next time I do play it, I'll be thinking of your method haha\", 'subreddit': 'Morrowind'}\n",
      "{'body': 'I want to take a gap year to chill and get money up first. Friends are roasting me for it tho', 'subreddit': 'teenagers'}\n",
      "{'body': 'I swear this same exact post has been posted before', 'subreddit': 'playrust'}\n",
      "{'body': 'Hey selling my magni and modi 2 Uber you can check my post history ', 'subreddit': 'hardwareswap'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"If people don't want to buy my slaves, then my slave selling business will just go under. It's the will of the free market. If people want slaves, I don't see who you think you are that you're in a position to deny them this liberty. Are you anti liberty? \", 'subreddit': 'AskLibertarians'}\n",
      "{'body': \"Countries only exist for the benefit of their peoples - there's no intrinsic value in preserving them.\", 'subreddit': 'ukpolitics'}\n",
      "{'body': 'I have videos of my wedding and from the birth of both my children so I can relive those days easily enough.\\n\\nIf I could relive just one day, it would be a day in July of last year when after more than 25 years I sat down to have lunch with my best childhood friend. We talked about all that had happened over the many years in-between. He looked so different. If you saw him walking down the street, his old clothes, his long tangled and unwashed hair, nearly all his exposed skin covered in tattoos and a cigarette hanging from his mouth, you\\'d have wanted to avoid him. He looked like an absolute badass who might just be completely and unpredictably violent. Except that\\'s not who he really was on the inside. He was still the same kind and thoughtful guy I knew from my childhood. He\\'d had more than his fair share of misfortune. Some of it was his fault and some wasn\\'t but he owned every bit of it. He never blamed anyone but himself. When I asked him how he manages it all he said, \"It\\'s my life. What else can I do?\"\\n\\nAfterwards we said our goodbyes and went our separate ways, back to our families and our lives. I was so happy to have reconnected with him and I thought we\\'d continue to be a part of each other\\'s lives for many years to come.  A month later, he died unexpectedly and all alone of a heart attack. Oh how I wish I could have spent more time with him. I just didn\\'t know there wasn\\'t any time left.', 'subreddit': 'AskReddit'}\n",
      "{'body': \"...wait is there an inside joke I'm missing or are you being serious.\\n\\nNot that there's anything wrong if you were, but just wanna make sure im not missing out on something.\", 'subreddit': 'funny'}\n",
      "{'body': '[deleted]', 'subreddit': 'bloodborne'}\n",
      "{'body': 'Yes. This is talking about how they have edited the description without notifying anyone.', 'subreddit': 'xboxone'}\n",
      "{'body': 'I have no examples but I just wanted to say this is a terrific question', 'subreddit': 'nba'}\n",
      "{'body': 'He did indeed, my b, brain fart', 'subreddit': 'CFB'}\n",
      "{'body': 'Am i the only thinking \"A Piece of Shit?\"\\nGreat abbreviation', 'subreddit': 'pics'}\n",
      "{'body': \"Second the recommendation on Mans Best Friend.  My partner and I used that to get our systems down.  You won't have to think about the climbing or gear since it's bolted.\\n\\nAnother good area might be the riding hood wall.  Physical Graffiti is a 2 pitch climb on gear with a bolted anchor midway.  Right next door is Big Bad Wolf, 3 pitches of sport that goes at 5.9.\\n\\nWhatever route you choose make sure you've thought out the plan to get back down - both after the climb or if you need to bail part way.  Have a great trip.  RR is paradise.\", 'subreddit': 'tradclimbing'}\n",
      "{'body': 'Oh, what\\'re the valvetrain issues? Weak pushrods, poor quality springs?\\n\\nIf you\\'re doing heads and cam, can you buy a \"cam package\" with all new valvetrain parts to fix it?', 'subreddit': 'cars'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol8ke/):\\n\\nDepends if they decide to go the traditional route or not. ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '143418952| &gt; None Anonymous (ID: L6+x+Lup)\\n\\n&gt;&gt;143412250 (OP)\\n2010: Liberal Democrat\\n2015: Conservative\\n2016: Remain\\n2017: Liberal Democrat\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Best all around player in my books. He's clutch on offence and always solid on defence.\", 'subreddit': 'MLS'}\n",
      "{'body': 'so you are who infomercials are targeting.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Why do you ask people if they have allergies at least once a week?', 'subreddit': 'britishproblems'}\n",
      "{'body': 'Yeah, I have mine at home with the door open. Sometimes I′ll put food in it, or hide a toy. That way it isn′t a big deal when I close the door and we go somewhere.', 'subreddit': 'cats'}\n",
      "{'body': \"I don't get it\", 'subreddit': 'playrust'}\n",
      "{'body': '#Welcome to r/LateStageCapitalism\\n***\\n\\n#***Please remember that this subreddit is a SAFE SPACE for leftist discussion. Any Liberalism, capitalist apologia, or attempts to debate socialism will be met with an immediate ban. Take it to r/DebateCommunism. Bigotry, [ableism](http://www.autistichoya.com/p/ableist-words-and-terms-to-avoid.html) and hate speech will also be met with immediate bans; Socialism is an intrinsically inclusive system.***\\n\\nIf you are new to socialism, please check out our socialism crash course [here](https://github.com/dessalines/essays/blob/master/crash_course_socialism.md).\\n\\nIf you are curious to what our leftist terminology means, then please check out our glossary [here](https://github.com/dessalines/essays/blob/master/glossary_of_socialist_terms.md).\\n\\nIn addition, here are some introductory links about socialism:\\n\\n- [Albert Einstein - *Why Socialism?*](http://monthlyreview.org/2009/05/01/why-socialism/)\\n\\n- [Pyotr Kropotkin - *The Conquest of Bread*](https://theanarchistlibrary.org/library/petr-kropotkin-the-conquest-of-bread)\\n\\n- [Friedrich Engels - *The Principles of Communism*](https://www.marxists.org/archive/marx/works/1847/11/prin-com.htm)\\n\\n- [Vladimir Lenin - *The State &amp; Revolution*](https://www.marxists.org/archive/lenin/works/1917/staterev/)\\n\\n- [Rosa Luxemburg - *Reform or Revolution*](https://www.marxists.org/archive/luxemburg/1900/reform-revolution/)\\n\\n- [Karl Marx &amp; Friedrich Engels - *The Communist Manifesto*](https://www.marxists.org/archive/marx/works/1848/communist-manifesto/)\\n\\nFor an extended list of works, check out [our wiki](https://www.reddit.com/r/LateStageCapitalism/wiki/index) or [this masterlist.](http://pastebin.com/raw/7xNqzd85)\\n\\n#☭☭☭\\n\\n***\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/LateStageCapitalism) if you have any questions or concerns.*', 'subreddit': 'LateStageCapitalism'}\n",
      "{'body': \"Using your logic:\\n\\nAnd intelligence increases spell damage. So the necro spells that do spell damage will do 5% more damage for every point in intelligence.\\n\\nBut necro does not *scale* with intelligence\\n\\nOh wait, that's complete and utter nonsense, as is your post.    \", 'subreddit': 'DivinityOriginalSin'}\n",
      "{'body': 'Will answer upon getting home :p (working atm)', 'subreddit': 'runescape'}\n",
      "{'body': 'Lol after a 100 comment thread with majority ripping him I bet there will be a turn around in opinion. \"Oh we believed him him he just needed to show it\"', 'subreddit': 'Habs'}\n",
      "{'body': 'I feel like Lt. Dan on the mast of that shrimping boat during a hurricane watching this game.', 'subreddit': 'MLS'}\n",
      "{'body': \"How much have you donated?\\n\\nSince if memory serves you're a Sanders supporter, how much has he? \\n\", 'subreddit': 'politics'}\n",
      "{'body': \"Yeah it's not necessary then.\", 'subreddit': 'pokemontrades'}\n",
      "{'body': 'Your post seems at odds with itself.', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': 'They want you to pay full price - the freebies are over!', 'subreddit': 'uber'}\n",
      "{'body': \"You can never go wrong with Vanille's TMR. It is not a TM that would go to waste if you get Genji Glove. It'll be BiS for whatever healer you get.\\n\\nI would say, yeah, go ahead with it, you won't regret it.\", 'subreddit': 'FFBraveExvius'}\n",
      "{'body': 'Better stones than batteries...', 'subreddit': 'hockey'}\n",
      "{'body': \"The thing with going to places is that there isn't really any where I live just a couple pubs that are mostly filled with people who are a little too old to be out getting hammered every weekend. I want to start going places but I basically have to travel 2h to go any where decent.\", 'subreddit': 'r4r'}\n",
      "{'body': 'Just wait until the LR Trunks banner. Best LR banner rates ', 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': '[deleted]', 'subreddit': 'pcars'}\n",
      "{'body': 'whats stopping you?', 'subreddit': 'hiphopheads'}\n",
      "{'body': \"I used to have a friend like Ross several years ago. You get used to it. You start developing a sense of when they're going to pull shit and then you both laugh about it.\", 'subreddit': 'gamegrumps'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Huhuh good one.', 'subreddit': 'RocketLeague'}\n",
      "{'body': 'ghetto has nothing to do with race, you can be white, black, Hispanic, or arabic and still be ghetto. ghetto has to do with being ignorant as fuck, like the girl being ignorant and obnoxious as hell, and then getting upset when she got called out on it. ', 'subreddit': 'news'}\n",
      "{'body': 'Hey there. I just tried a randomizer (Heartgold) for the first time. It was a lot of fun, but I wanted to start over and try a new one.The problem is, whenever I try to make a new game, my randomizer file loads an identical pokemon layout. Same starters as the first time, and same encounters.\\n\\nIs this a common issue with randomizers? Is there some way to make it fully randomize on a new game? Thanks!', 'subreddit': 'pokemon'}\n",
      "{'body': 'Are you going to keep doing these when teams lose in the postseason, or was this the last one?', 'subreddit': 'baseball'}\n",
      "{'body': '[removed]', 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': \"Okay -- that's what I thought, but a cursory google search basically said that eBay sends out tax forms so I wasn't sure. Thank you!\", 'subreddit': 'personalfinance'}\n",
      "{'body': 'Link them the DynastyFF Tools website', 'subreddit': 'DynastyFF'}\n",
      "{'body': \"Props to Noah making himself look slightly more like Steven Adams to help Kanter's transition.\", 'subreddit': 'NYKnicks'}\n",
      "{'body': \"You can hide those under forum settings. Can't remember how because I set mine in 2009. Stopped going mostly after I joined reddit tho. \", 'subreddit': 'UnresolvedMysteries'}\n",
      "{'body': \"I was in a relationship with a guy for two years, we lived together and everything. Long story short, he wasn't the guy for me and I broke up with him and moved back in with my parents. I totally didn't expect his reaction, I thought he would shrug and pretend he didn't care. He did care. He cried his eyes out and begged me to stay, asked me over and over to please stay and give him one more chance. Even clung to me and kept saying please please please, I'll do everything different! It was the hardest thing I had to do in my life, walking away and purposely hurting someone to save my own happiness. It was the hardest but also the best decision I have ever made. It has been years since then and I'm sure he had a hard time at first. There was just nothing I could do for him, every conversation we had just gave him hope we would be okay again. I'm glad he could rely on his parents for support. I felt so guilty at first, especially because I started dating someone new a lot earlier than him. However years later we are both in relationships with different people who suit us much better. I honestly think he would agree now that it was for the best. This was my personal anecdote, I hope it gives you hope that down the line everything will be okay 😊 the heavy feeling and the guilt will lessen over time, you are not responsible for his happiness. You are not obligated to stay with him so he is comfortable. Sending you strength and happiness and if you want to talk I'm here! \", 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': 'Yes, much better to unquestioningly believe every narrative presented to you.\\n\\nAlso, by definition, the narrative of a conspiracy theory is *in*convenient. ', 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': \"There's a difference in forcing a turnover and recovering a fumble.\", 'subreddit': 'sports'}\n",
      "{'body': 'If any individual submits in obedience to another, they do so for a variety of reasons: love, trust, monetary matters, and fear being the first that come to mind. Of these, fear is that only one that makes sense in context, yet there really isn\\'t any hint of that in the story. And unfortunately Blizzard has kind of left us hanging in the \"so what the hell is Sylvanas doing now\" category, so as consumers of the story, we are left to make sense of our place in it. (As you can tell, I\\'m thinking of this in the larger sense of the overall story and how my actions fit into it, not just some dude sitting at a computer). Of all the Horde races, the Tauren have perhaps the least checkered past, in terms of unprovoked violence or delving into black magic fuckery. It just seems... false that my tauren shaman, with his own history and the history of his race, would willingly follow Sylvanas. And her actions thus far as Warchief have not gone beyond this single, apparently self-serving mission. ', 'subreddit': 'wow'}\n",
      "{'body': 'I guess after today Oaks has now surpassed Martin Shrekli in that desire.', 'subreddit': 'exmormon'}\n",
      "{'body': 'LOL', 'subreddit': 'CSRRacing2'}\n",
      "{'body': 'Arguable. I was hyping Fury Road up for years and no one I talked to cared about it. Until they saw it', 'subreddit': 'movies'}\n",
      "{'body': 'Perhaps this can be the next project for the makers of [PancakeBot](http://www.pancakebot.com/).', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Please run for President ', 'subreddit': 'sjwhate'}\n",
      "{'body': 'get the mdr1000x instead , you can get it refurbished for 200 or new for 250', 'subreddit': 'headphones'}\n",
      "{'body': 'Knock Knock', 'subreddit': 'KCRoyals'}\n",
      "{'body': 'In the printer profile find the starting gcode. Cut the two lines having to do with heating the nozzle and paste them just after the mesh leveling line.\\n\\nProblem solved, remember to clean your nozzle after each print.', 'subreddit': '3Dprinting'}\n",
      "{'body': \"It's not the actual muscles, but the demonstration of being able to dedicate yourself to getting muscular that's attractive. Getting in shape is -hard- for most people, and the kind of people who can keep up that kind of lifestyle tend to be good at committing to things. \\n\\nThen there's the crazies who jack themselves up to the point they look like they can barely turn their heads, and that looks like obsession (in my opinion)\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'He does. But OP has (1) given her number when she didn’t want to (2) answered his calls/replied to messages when she didn’t want to (3) got off the bus or missed it to avoid talking to him (4) felt she needed to explain why she didn’t message back (5) didn’t kick him in the balls when he hugged/ touched etc.\\n\\nMost people don’t need this kind of practice but sounds like op needs to get used to saying no as well. The phone number especially should never have been shared', 'subreddit': 'relationships'}\n",
      "{'body': 'Not going to resubmit, just inquiring, would it have been accepted if the title was something like: this man on a carrousel?', 'subreddit': 'photoshopbattles'}\n",
      "{'body': 'Sorry, meant to say not scared.', 'subreddit': 'nba'}\n",
      "{'body': \"PU and Bluehole are too busy counting their money, can't work on fixing their game. Try again in a few months \", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'hahahahaha dén var sgu god', 'subreddit': 'Denmark'}\n",
      "{'body': 'OG Ultraboost or Bait EQT?', 'subreddit': 'Sneakers'}\n",
      "{'body': 'Malarkey was the funniest character on BoB. ', 'subreddit': 'TheOrville'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Love u 2 bb', 'subreddit': 'teenagers'}\n",
      "{'body': \"Cold. Wet. Flaky. I don't remember much, I was a baby, but it's lost its authenticity by now- now its just a nuisance\", 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'leagueoflegends'}\n",
      "{'body': '[Very interesting.](https://www.youtube.com/watch?v=pkunxmvunxI&amp;t=113s)', 'subreddit': 'worldbuilding'}\n",
      "{'body': \"That's jacket money\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"i have american 3ds so i can't play mhxx, but I'll get it when i get a switch.\\n\", 'subreddit': 'MonsterHunter'}\n",
      "{'body': \"In a very concise thought:\\n\\nImagine your victory condition. Do you win with the opponent having a full hand of cards he didn't have time to cast? You're the beatdown. In contrast, do you win with the opponent having exhausted his resources while you simply overpower him after absorbing his threats? You're the control.\\n\", 'subreddit': 'EternalCardGame'}\n",
      "{'body': \"Yes, you are an asshole.\\n\\nYou had one job to do.  Feed the dog and cat.\\n\\nThey trusted you with a key to their home to do that job.  Instead of doing just that, you went through all the corners of their home and figuratively rubbed your scent in it.\\n\\nYou showed that you have no consideration or respect for your adult son and his adult wife and their decisions about their life.  Had you been considerate, you would have offered to do a deep cleaning, listing out the things that you could do for their home.  This could have been done through a phone call or a text message and you should wait for an answer before doing anything else.  \\n\\nI am very leery that you say you organized.  By that, did you straighten up a pile of papers on the table?  Or did you rearrange cupboards and furniture?  If your cleaning included more than squaring away a pile of papers so they are not on the verge of falling off the pile, you overstepped big time.  By the way your son described your actions and called you presumptuous and controlling, I suspect you did a lot more than cleaning surfaces that were dirty, other than doing laundry.\\n\\nIf you wanted to help, you need to ask in what form they would accept your help.  You don't barge in and start doing things without their knowledge and consent.\\n\\nI have a followup question for you?  Did you keep the key or did you give it back?  If you didn't give it back (why not?), have you USED the key without their prior knowledge?  If you've used the key, you are more than an asshole very much deserving of the cold shoulders thrown your way.\", 'subreddit': 'AmItheAsshole'}\n",
      "{'body': 'Came here to say that. The chorus is strong as fuck.', 'subreddit': 'marilyn_manson'}\n",
      "{'body': 'Drink plenty of water before hand.  The better hydrated you are the easier for them to find the vein, and the easier the blood will flow.  ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Why would expect people to change how they are born? I didn't choose my skin color, sex, or sexuality either \", 'subreddit': 'vegan'}\n",
      "{'body': 'This sounds great, but I guarantee you the moment that one of James / Curry / Durant / etc. decides to be a goober and take a knee, this memo will suddenly have no teeth. NBA bends over backwards all the time.', 'subreddit': 'Conservative'}\n",
      "{'body': '[+i_stay_turnt](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo19t/):\\n\\nHow long did it take for your book to publish?', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Lusk 182', 'subreddit': 'edmproduction'}\n",
      "{'body': \"Here's what I came up with: https://i.imgur.com/2HGOTps.png\\n\\n^^^^^***bleep*** ^^^^^***bloop***\", 'subreddit': 'colorizebot'}\n",
      "{'body': \"&gt; EVE is literally known as excel in space and as an old player of it I can attest to it.\\n\\nIf you choose to play excel in space, you are welcome to.  I don't because it's kind of boring.  I like to PvP.  If you never left high sec and never progressed past spreadsheets in space, I see why you would quit. However, that was your decision to play that way and it isn't required at all.  Fortunately, many players progress past those things into the more interesting parts of the game very quickly.\\n\\n&gt; It literally rewards betrayal and treachery and corruption as well as other factors\\n\\nThe game rewards being good at whatever it is that interests you.  Some people scam, and most of them don't benefit from it at all.  It's a very minor part of the game.\", 'subreddit': 'science'}\n",
      "{'body': 'done', 'subreddit': 'FreeKarma4You'}\n",
      "{'body': \"It's sad to think that people feel this extreme but it is important to remember this isn't representative of the majority of left wingers. \", 'subreddit': 'ukpolitics'}\n",
      "{'body': 'Not really, my damage has been very consistent again the world boss.', 'subreddit': 'sevenguardians'}\n",
      "{'body': \"143418468| &gt; United States Anonymous (ID: j6fQZDZD)\\n\\n&gt;&gt;143417408\\n*puts her mocha latte down*\\n*takes her poodle off her lap*\\nUM, excuse me sweety-pie. You think it's okay to just go around shitposting like you do? Maybe you don't really understand... *flips her hair* After all, you rural and suburban retards are uncivilized folk. Oopsie, did I hurt your feelings? *crouches down and undoes your zipper* ahahaha--you're blushing? How pathetic... I guess you Trumpkin farmhands aren't used to REAL women after all those hours you spend on the farm. What's that, honeybuns? No--let me guess. That's why you carry such big guns around, you tiny-cocked bigot-- *stares, shocked, at your thundercock* Ahaha.. ahah... hah... Er, listen b-baby dick... J-Just lie back and relax. It'll make it easier to accept your privilege. *strokes your cock up and down, then starts blowing you gently* It'll... mmph... all be over soon... mmm... snookems. After all, even your dick admits that female is the superior gender. Look how hard I've made you. You're pathetic. Now... mmmph... once we're finished here, we're going to go back to my room and you can atone for all those horrible years of institutional privilege by submitting to my vagina, which, by the way, has its own voice\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': 'that would be cool', 'subreddit': 'teenagers'}\n",
      "{'body': \"I'm not trying to defend anything here, just trying to clarify the stats. In fact I also think Mercy is busted as hell rn.\", 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': 'No more field goals, period. Always go for it on 4th down. Hell, no more kicking for the point after either. Always go for 2. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Good gamer tags are always great. My husband and I came across a guy in halo reach a few times whose tag was About 729 Jews (Iirc the number might be different) but it made us lol every time it popped up “you just killed about 729 Jews.... “', 'subreddit': 'xboxone'}\n",
      "{'body': 'You were doing something that could break your arm without an EMT on sight? What barbarians do you have for parents!\\n\\n- Too many people these days.', 'subreddit': 'CanadaPolitics'}\n",
      "{'body': 'Muhammad was a bitch! Do I win!?', 'subreddit': 'atheismrebooted'}\n",
      "{'body': \"It's nice to see Hosmer buy tickets behind home tonight for all the hearts he broke in KC.\", 'subreddit': 'KCRoyals'}\n",
      "{'body': 'Not sure how defending. What I am saying is that Mercy changes the game for sure and what it changed it from is no longer can you get a pick and then roll a fight. Instead people need to force her rez, take her out and win the team battle. In reality we already killed support first and more so with Mercy. Mercy isnt broken she is strong and needs a different tactic. Much of what she does isnt different to what she used to do which is rez and shift a battle. \\n\\nand then I said Ana though needs some changes because she doesnt do as much as what other supports do. Like how 76 kicked McCree out.', 'subreddit': 'Overwatch'}\n",
      "{'body': '\"I don\\'t want a F2P trash store with combat decks but I\\'ll buy an expansion with one for the same price.\"\\n\\nDo you not see how there\\'s zero difference in what you just said lol', 'subreddit': 'absolver'}\n",
      "{'body': \"&gt; Feels dehumanizing and makes me ponder how they treat others that they aren't trying to get something out of. In the end, yes it's better to know, bullet dodged and all that. Still feels bad.\\n\\n**BINGO!!!**\\n\\nThis is it exactly. It's OK not to be interested anymore, but especially in my case, where the rapport is long established professionally and I thought, a least platonically, I figured you can be cool instead of treating me like I'm a leper asking you for spare change all of a sudden. \", 'subreddit': 'datingoverthirty'}\n",
      "{'body': 'In that Behind the Scenes video, we see what looks like a Resistance fighter falling from a sizeable height. I know some have speculated that this is Paige Tico. Is it? Or is it someone else? Or does it hold no important in the actual film? ', 'subreddit': 'starwarsspeculation'}\n",
      "{'body': \"Thanks! it's the thought that counts\", 'subreddit': 'CringeAnarchy'}\n",
      "{'body': \"Well in a points league you don't need to worry about categories as much. Just play the guys with the highest point averages.\", 'subreddit': 'fantasybball'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'bta corn', 'subreddit': 'GlobalOffensiveTrade'}\n",
      "{'body': 'Well at least our defense showed up today :D', 'subreddit': 'CFB'}\n",
      "{'body': 'https://i.imgur.com/gyb9sAR.png', 'subreddit': 'smashbros'}\n",
      "{'body': 'Pretty much what i was expecting. Should i bother even trying to remain friends though?', 'subreddit': 'relationship_advice'}\n",
      "{'body': 'When you are on steam, click your profile picture on the bottom right corner. It will take you to your Steam Profile. Near the top left, there will be a website address. That is your Steam Profiel URL. You can copy-paste it to Reddit, so I can add ya!', 'subreddit': 'tf2'}\n",
      "{'body': \"Your parents did work at one time to gain the security not to in the future. You're very well aware of what I mean, you're being willfully ignorant because your argument was already crappy. Nice job, thank you for exposing that. \\n\\nI literally shared the definition of crony capitalism, if you don't like it then that's OK but you're still wrong. I'm sorry that upsets you. You have been able to get away with saying things are your own way at other points in your life but this is not one where that works. Crony capitalism is a thing, its THE thing we are discussing, and you don't get to just define things in your own twisted ways to fit your argument. You're not very good at this. \", 'subreddit': 'PenmanshipPorn'}\n",
      "{'body': \"It doesn't matter to them.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"I completely agree , some turns I'd get 8+ of them and it's just irritating , I wish we had the option to turn them off. \", 'subreddit': 'totalwar'}\n",
      "{'body': '[deleted]', 'subreddit': 'jailbreak'}\n",
      "{'body': '[removed]', 'subreddit': 'FashionReps'}\n",
      "{'body': 'I have a sk Hynix 250 for 50$', 'subreddit': 'hardwareswap'}\n",
      "{'body': \"Congrats on the purchase.  watchbuys is doing a road show in San Francisco this weekend.  I'll be there to check it out before I make the final plunge.  Super excited!\", 'subreddit': 'Watches'}\n",
      "{'body': 'Pass the mic ', 'subreddit': 'BeastieBoys'}\n",
      "{'body': '&gt;But this singular focus on the white working class — rather than the working class as a whole, in all its hues — has (perhaps unintentionally) aided and abetted neoliberalism’s ascension in the Democratic Party. The fate of workers has been lost in the shuffle, undermining both the material wellbeing and the morale of what should be the party’s voting base.\\n\\n\\n&gt;By focusing on the role of white voters in Clinton’s defeat, rather than the failure of the Democrats’ neoliberal strategy, liberal pundits and party leaders are drawing the wrong conclusions from Trump’s victory. Instead of debating how to win white workers or doubling down on the misguided strategy of courting upscale whites, Democrats must train their attention on the needs of the working class as a whole.', 'subreddit': 'politics'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoqn8c/):\\n\\nUsing CreateSpace once I had the final product ready to roll it took 3 days', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Cool. Thanks for the insight. I have a feeling I’ll be buying another lol. Got the zombie green this time around. ', 'subreddit': 'electronic_cigarette'}\n",
      "{'body': 'IN-TENSE.', 'subreddit': 'Teachers'}\n",
      "{'body': 'The googles just told me this:  Denominational families that practice infant baptism include Catholics, Eastern and Oriental Orthodox, Anglicans, Episcopalians, Lutherans, Presbyterians, Congregationals and other Reformed denominations, Methodists and some Nazarenes, and the Moravian Church.', 'subreddit': 'JUSTNOMIL'}\n",
      "{'body': 'Agreed.  PJW is a cunt too. He isnt Pro Trump. He at least is witty but i dont consider him loyal. He can fuck off too. ', 'subreddit': 'The_Donald'}\n",
      "{'body': 'I am more sick of the fake mat 4s ffs..', 'subreddit': 'NarutoBlazing'}\n",
      "{'body': \"My method is to draw attention to it afterwards even though I know it's obnoxious and kills the joke, but I'm too upset about the joke going unnoticed to not do it.\\n\\nJust end me now.\", 'subreddit': 'meirl'}\n",
      "{'body': 'Even bots are trolling us now!', 'subreddit': 'ps4homebrew'}\n",
      "{'body': \"That's a great pic.  Looks like a fatherson ish relationship and Brett has known Ben and his family since Ben was a baby\", 'subreddit': 'sixers'}\n",
      "{'body': \"Yeah, if you're looking to rewrite and misrepresent recent events, I'm probably not the right guy to talk to - I don't care about your revisionism.\", 'subreddit': 'politics'}\n",
      "{'body': '[removed]', 'subreddit': 'gonewild'}\n",
      "{'body': 'Not a fan of the color yellow, but that is one sexy yellow console. ', 'subreddit': 'xboxone'}\n",
      "{'body': '7 rings 7 fmvps is what it’s gonna take, even MJ has basically made that point ', 'subreddit': 'nba'}\n",
      "{'body': 'Haha. No... If I remember a unp wilp be developed in the future at some point maybe. I do remember a comment about that, but it would  take longer than CBBE. People like UNP. My guess is someone will update it to the new CBBE 2 stander. ', 'subreddit': 'skyrimmods'}\n",
      "{'body': \"It won't kill you in Animal Crossing!\\n\\nBut it has a chance of falling out of a tree.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Yeah maybe, I just feel the rematch coming', 'subreddit': 'WahoosTipi'}\n",
      "{'body': 'thnx', 'subreddit': 'phgonewild'}\n",
      "{'body': \"He's got a pepper guy and a legit hot sauce business, it's probably a 7 pot.\", 'subreddit': 'Peppers'}\n",
      "{'body': 'I think we end up with a soft lineup tonight, hard one tomorrow. ', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': \"Oh wow that's awfully generous and compassionate...\", 'subreddit': 'SanctionedSuicide'}\n",
      "{'body': \"Do you not remember how bad WR last year? Texans had a damn good line and still can't do shit. \", 'subreddit': 'eagles'}\n",
      "{'body': 'LLS?', 'subreddit': 'travel'}\n",
      "{'body': 'Gone as quickly as it came to be....', 'subreddit': 'buildapcsales'}\n",
      "{'body': 'Welp there it is, The biggest reddest Dildo WaltonChain has ever seen haha! ', 'subreddit': 'waltonchain'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Ludicrous! ', 'subreddit': 'The_Donald'}\n",
      "{'body': 'Bruh chill some people don’t have their phone on them 24/7. And/or she went out without her phone. \\n\\nReal worries are when someone opens a snap then never responds', 'subreddit': 'teenagers'}\n",
      "{'body': 'She’s so hot. Her snapchat is great too 👌🏼', 'subreddit': 'AsianHotties'}\n",
      "{'body': 'Correct, this is a special puke. I think the officiant probably shat and spewed at the same time. ', 'subreddit': 'holdmyfries'}\n",
      "{'body': \"Aww I feel like she's been hinting at this for a while on twitter! Happy for her, hope it's not anyone from bachelor nation though.\", 'subreddit': 'thebachelor'}\n",
      "{'body': 'I get where hes coming from. In CSGO (obviously the most competitive fps game atm) most pro players use 400 dpi. So I assume thats where he gets it from', 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': 'Not sure if this is normal but I was able to because I broke my Pixel. I got a cheap Moto g4 play and called to get it activated and transferred my number to it. That worked when I called the tech support. It was pain but worked and the plan stayed the same. Not sure if the CS tech did some exception but you could try that route. ', 'subreddit': 'Sprint'}\n",
      "{'body': \"&gt; Also remember that the Federation has multiple fleets,\\n\\nI can't remember this because I didn't know it in the first place! Can you recommend some reading to learn more about this? How are these fleets subdivided?\", 'subreddit': 'DaystromInstitute'}\n",
      "{'body': 'And the Leafs too.', 'subreddit': 'BostonBruins'}\n",
      "{'body': 'lol gl', 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': 'This is awesome ', 'subreddit': 'Defcon'}\n",
      "{'body': \"Yes, Sandmeh shows up as a random encounter on Stage 35 with Beetler and Quaken, and replaces Sgt. Burly occasionally, which actually makes the stage easier since Sandmeh has a lower attack stat.\\n\\nIf you're playing the U.S. version of Wibble Wobble, Sternynan is an S rank from the Tough Tribe, a Jibanyan reskin. He appears in the second hidden stage on the second map. You have to beat the end bosses Tanbo, Payn and Sandmeh with Sheen on your team. Then the stage unlocks.\\n\\nSandmeh is horrible. He had a purpose in the Liberty Way event this past spring. You would catch him and then crank for the Sand Suit to fuse him into Mr. Sandmeh. He would then get an attack damage boost when fighting Libertynyan S. Now he's only in this event along with other past event Yo-kai just so people who missed out can add to their Medallium. I don't really need him, I was just trying to earn all of the stars for the sake of it. It means nothing because they're not offering rewards for earning all stars for this particular event.\\n\\n\", 'subreddit': 'yokaiwatch'}\n",
      "{'body': '[THE VIDEO ON THE ROCKIES TWITTER!!!!!!!!!!](https://twitter.com/Rockies/status/914272299834007552)', 'subreddit': 'ColoradoRockies'}\n",
      "{'body': 'I totally agree.\\nI\\'ve watched all their videos and I can\\'t wait for their next one, each time. (My favorites are the *Song of Durin* and the *Fall of Gil Galad*)\\nI\\'m not as good in English as I should be to express myself proprely. So I\\'ll continue in my native language (French).\\n\\nSi vous saviez comment j\\'éprouve une profonde admiration pour les gens de cette chaîne. Ils m\\'ont aidé à travers des moments où j\\'aurais pu être fâché, triste, mais leurs chansons m\\'ont donné la motivation de continuer, parfois.\\nCes œuvres m\\'ont permis de réfléchir plus loin, de mieux comprendre l\\'immense univers de Tolkien, au-delà de ce que j\\'aurais pu m\\'imaginer. Je n\\'écoute pas les paroles, je les vis, je vie les personnages dont ils chantent l\\'histoire.\\nJe les chantonne également très souvent, peut-être un peu trop pour les gens qui m\\'entourent qui, surtout mes amis, semblent à mon grand désespoir avoir une aversion pour ce genre musical. Comme si le rap moderne signifiait quelque chose. La musique classique peut atteindre un point de mon cœur qui se trouve intouchable par le pop et les autres genres.\\nLes membres de Clamavi De Profundis sont divins.\\n\\nSorry for that French text, but I wouldn\\'t have made it in English...\\nLike you said, \"Clamavi De Profundis is an incredible group\".\\nWow that was so long, sorry,\\nMy best regards,\\nArosgan.', 'subreddit': 'lotr'}\n",
      "{'body': 'Mods! If you please! Award this good person a flair', 'subreddit': 'IncelTears'}\n",
      "{'body': \"Yeah, that appears to be part of the disk.  This wouldn't be the first disk to come apart.  That DC-10 that crash landed in Sioux City back in 89 had a fan disk come apart.  And I know of at least one Pratt &amp; Whitney F100 (F-15/16) engine that had a fan disk fail.  That caused us in the military, a LOT of work!  Fortunately, that engine that came apart, was on the test cell at the time, and not in an aircraft. \", 'subreddit': 'worldnews'}\n",
      "{'body': 'This is some high level shit', 'subreddit': 'raimimemes'}\n",
      "{'body': 'Hi, dis you lost you pvp honor and your rank if you rebirth before the end of the cd? ', 'subreddit': 'IdlePoring'}\n",
      "{'body': '143413050| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143412979\\ngood\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Polling companies are in the business of producing polls that are their customers happy, of polls designed to push narratives, and a bunch of other things.\\n\\nThey are not impartial arbiters of sacred statistics and it is naive to think so.', 'subreddit': 'Calgary'}\n",
      "{'body': 'Most of us handles the transfers our selves.\\n\\nAs for morale you can praise your players for their recent form, training level and their conduct.', 'subreddit': 'footballmanagergames'}\n",
      "{'body': 'Are you using a 60Hz rom?', 'subreddit': 'nesclassicmods'}\n",
      "{'body': \"Does it have a crew which can respond intelligently? Or are they restricted to staying buttoned up inside the vehicle and won't intervene if I, say, get on top of it? If the former, I'm completely dead. If the latter, I might, just might, survive. Unescorted tanks aren't great at urban environments, and it has to find me to kill me.  \\n\\n\\nI'm on a UK-style housing estate - lots of two-story brick buildings, hedges, fences. Roads aren't in a grid, it's all cul-de-sacs and tight bends. Not much line of sight. Now the Abrams has thermal and night vision kit, but that's mostly directional and generally trained on the forward arc (as are the weapons). It won't be able to see through, say thick hedgerows, walls, buildings, or fences. Obviously if it actually sees me I'm dead. But since I know the estate quite well I can probably evade it reasonably easily, for a small time at least.  \\nSo my plan is to collect some firelighters and kindling materials, and a load of ceramic plant pots. I'm also buying some duct tape, rope, some food and water. And a few of those foil emergency blankets. I'm going to make a bunch of small fires all over the place, and use them to heat the ceramic pots. These are to serve as false heat signatures all over the place.  \\nMeanwhile, I'll be in an alley across the street, behind a fence, covered in the foil blankets to conceal body heat. The tank will probably use the road to get to my address because my cul-de-sac is relatively inaccessible without driving through other houses. That puts me right behind it, at which point I run and climb right on top of the turret. That'll take like 5 seconds, so hopefully I'm not spotted.  Once on top I lie spread eagled, and securely rope/tape myself to whatever stanchions and protuberances there are on top, in order to avoid exhausting myself or falling off. Then wait for 24 hours to win!  \", 'subreddit': 'whowouldwin'}\n",
      "{'body': \"That's awesome. I love his videos!\", 'subreddit': 'drums'}\n",
      "{'body': 'That IS his thesis ', 'subreddit': 'Kappa'}\n",
      "{'body': 'Odd to think a still-updated game is older than me.', 'subreddit': 'Steam'}\n",
      "{'body': \"One of those comments I wish I could upvote more than once. A lot of people now are giving serious Kinnock in '92 vibes, luckily Corbyn and co aren't quite there yet. But some of the social media comments are definitely making me cringe as hard as watching the Sheffield Rally does.\\n\\nhttps://www.youtube.com/watch?v=ROKXlvYMKQc\\n\\nWe are not alright yet guys. There isn't even an election on the cards right now.\", 'subreddit': 'LabourUK'}\n",
      "{'body': \"That's actually wet\", 'subreddit': 'GlobalOffensiveTrade'}\n",
      "{'body': 'Lord', 'subreddit': 'CFB'}\n",
      "{'body': \"TFC should sub out Bradley and Altidore so they're well rested for the panama game, just saying \", 'subreddit': 'MLS'}\n",
      "{'body': 'Unpaid mods are superior in both quality and ofc, price. Do you think Bethesda will like competition for their new way of forcing people to pay more money? Do you think they like sharing the game with unpaid mods when that money could be theirs? They released this now so people get used to it so when they announce a new game with only creation club the backlash will be less and people will already be used to paid mods.', 'subreddit': 'fo4'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"What tier(s) in arena are you in? You don't need full horse emblem or armour emblem to stay in T20. Granted I have min-maxed a little so I don't have to reroll score as often, it's definitely doable and enjoyable so long as you can counter the common heroes you run into\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"I too saw this on this week's New Yorker. Laughed then and had another laugh now.\", 'subreddit': 'sports'}\n",
      "{'body': 'Thanks a ton! :D ', 'subreddit': 'pokemontrades'}\n",
      "{'body': \"That's what I'm thinking too. I think I'll do that. Thanks ! :D\", 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': \"Hey, I have no friends too.. But I don't have anyone's contact info either. You're not alone\", 'subreddit': 'socialanxiety'}\n",
      "{'body': '10 team ppr:\\n\\nGiving: Michael Thomas &amp; Chris Thompson\\n\\nGetting: Keenan Allen &amp; Lesean McCoy\\n\\nMy other WRs: OBJ, Alshon Jeffery, TY Hilton\\n\\nMy other RBs: Ingram, Crowell, Gillislee, Duke Johnson, Riddick, Smallwood ', 'subreddit': 'fantasyfootball'}\n",
      "{'body': '[deleted]', 'subreddit': 'teenagers'}\n",
      "{'body': \"[+Zaorish9](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnovlhe/):\\n\\n[Edit: I removed this comment after the original poster's dishonest marketing ploy was revealed.]\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'I want Ice as a friend now.', 'subreddit': 'LivestreamFail'}\n",
      "{'body': 'If only the 30% of people giving Trump his abysmal approval rating felt the same, maybe we could end this farce and move forward.', 'subreddit': 'worldnews'}\n",
      "{'body': 'I beg to differ :\\\\^)\\n\\nhttps://i.imgur.com/qNIlEUY.png', 'subreddit': 'thedavidpakmanshow'}\n",
      "{'body': 'Since I met my girlfriend, I fantasize about her when I jerk it. Super weird to me., but yeah. Anytime I go at it with something else on my mind, she pops into my head and stays until the deed is done.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'How does a person get away with using chewing tobacco in school and not ending up in deep shit?', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I often wonder why owners feel the need to bathe their cats.... ', 'subreddit': 'cats'}\n",
      "{'body': 'Cars are also stupid... just sprint faster and you go the same speed. ', 'subreddit': 'PeopleFuckingDying'}\n",
      "{'body': 'Obama was one corrupt SOB', 'subreddit': 'The_Donald'}\n",
      "{'body': 'agreed', 'subreddit': 'drunk'}\n",
      "{'body': 'Your comment has been removed because:\\n\\nGendered slurs are strictly scrutinized; please see our [gendered slurs policy guide.](/r/askwomen/w/genderedslurs)  \\nIf you edit your comment, let us know and it may be reinstated. \\n\\n\\n\\n**[Have questions about this moderator action? CLICK HERE!](http://www.reddit.com/message/compose/?to=/r/AskWomen&amp;subject=Why+was+this+removed?&amp;message=\\\\[My+comment\\\\]\\\\(https://www.reddit.com/r/AskWomen/comments/73i316/what_was_the_most_ridiculous_case_of_someone/dnqgo50/\\\\)+was+removed+and+I+do+not+understand+the+reason+given+by+the+mod+who+acted upon+it.)**    \\n\\n\\n[AskWomen rules](/r/askwomen/w/rules) | [AskWomen FAQ](/r/askwomen/w/index)  \\n[reddit rules](http://www.reddit.com/rules/) | [reddiquette](http://www.reddit.com/wiki/reddiquette)', 'subreddit': 'AskWomen'}\n",
      "{'body': \"Goku always wants to test himself, and also the writers don't have a whole saga to work with.  They had a little more than an hour so they had to build some tension, so they saved super saiyan till the end.\", 'subreddit': 'dbz'}\n",
      "{'body': \"Meh. First time I've seen it posted. I'm sure for many others too. \", 'subreddit': 'Jokes'}\n",
      "{'body': 'just get the first down, it was 2 yards', 'subreddit': 'CFB'}\n",
      "{'body': 'Fucking gross. Have my upvote', 'subreddit': 'unpopularopinion'}\n",
      "{'body': \"Can't say I'm not nervous, but I'm definitely less nervous than in 2015. I still can't believe we made it to the GF haha\", 'subreddit': 'nrl'}\n",
      "{'body': 'E', 'subreddit': 'AskOuija'}\n",
      "{'body': \"No way. He gave up 2 innocent lives to save his own. Stank gum didn't know about them until he told them. Stank gum tortured him to find max, he didn't even know about hope and glory until chum brought it up. He should have told him where max was and never even mentioned them. And he stole my Damn car! :-(\", 'subreddit': 'MadMaxGame'}\n",
      "{'body': '[deleted]', 'subreddit': 'ffxiv'}\n",
      "{'body': '[deleted]', 'subreddit': 'freeuse'}\n",
      "{'body': 'will do ', 'subreddit': 'bmx'}\n",
      "{'body': 'This deal is getting worse all the time!', 'subreddit': 'forza'}\n",
      "{'body': \"It's not the post-processing. I had that enabled pre-patch, and still experienced similar drops as the above poster. Disabling the post-processing effects via engine.ini also does not help.\\n\\nThe shadows should have been a performance boost, since they were changed to baked-in static ones.\\n\\nMy guess is that they are overusing a sub-component somewhere causing the system to cap out early, though I have little knowledge/experience there. I just rarely see the GPU usage above 40%, or any single thread on any core of the CPU go above 60% usage.\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': '1463 chainz', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"The flip side is that unlike Chase, which strictly segregates your consumer and biz accounts, Amex seems pretty liberal about intermingling everything. And Amex has a habit of not doing hard pulls for longstanding customers. A foreign Amex won't count against 5/24 since it's a separate credit profile but I wouldn't necessarily want to count on Amex ignoring that they already have their own profile on you.\", 'subreddit': 'churning'}\n",
      "{'body': \"Last time I checked Crawford isn't licensened by the British Board of Boxing so he would be ineligible. Errol Spence Jr. won foreign fighter of the year tho so make of that what you will. \\n\\nIt's an award to celebrate British Boxing. They have a separate award for non British fighters, presumably who have fought in the U.K. Or against British opposition \", 'subreddit': 'Boxing'}\n",
      "{'body': \"There is no shortage of Magpul and Hexmag 10 rounders in 30 round bodies in stores. I wouldn't worry about it.\", 'subreddit': 'CAguns'}\n",
      "{'body': 'Oh yeah. Forgot about that.', 'subreddit': 'CFB'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnox5hl/):\\n\\nThank you :)', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Amsa vs army 2-1 for amsa 2 stock comeback game 3', 'subreddit': 'smashbros'}\n",
      "{'body': '[removed]', 'subreddit': 'science'}\n",
      "{'body': 'https://q39kc.com/', 'subreddit': 'kansascity'}\n",
      "{'body': \"I'm from NJ. And Obama did step up and worked quite well win Christie and gave our state plenty of aid and support after Sandy wrecked us. \\n\\nQuick edit: I am not a fan of Christie but he did do a great job standing up for the us and getting us what we needed to recover and help those hit the hardest. \", 'subreddit': 'TrumpCriticizesTrump'}\n",
      "{'body': 'yes you can, but skimming throught a legal thread read that sometimes prosecutors think it is better to play it safe and just convict for one crime instead of 2 or 3, becuz then you need more evidence, trials, etc. So just to get them to jail they focus on one type of crime.', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': 'My delivery person never comes to my door and I live on the first floor. It bothers my soul. ', 'subreddit': 'UnethicalLifeProTips'}\n",
      "{'body': 'I can trade at anytime :)', 'subreddit': 'pokemontrades'}\n",
      "{'body': \"Your submission was automatically flagged. [Message the moderators immediately after submitting](https://www.reddit.com/login?dest=https%3A%2F%2Fwww.reddit.com%2Fmessage%2Fcompose%3Fto%3D%252Fr%252Fweb_design) if you feel this should be an exception. Be certain that your submission follows our posting guidelines and reddit's rules.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/web_design) if you have any questions or concerns.*\", 'subreddit': 'web_design'}\n",
      "{'body': \"It's there. You weren't paying close enough attention. Detention for you\", 'subreddit': 'lego'}\n",
      "{'body': 'Hey guys, just got a tesarion. What runes should i use on him?', 'subreddit': 'summonerswar'}\n",
      "{'body': \"I remember reading the list (it had been posted on the InRev TS a week or so before that post was made), and it looked like it was internal.  I'm not going to lie, at the time I was upset, but it's still scummy to bring it up now\", 'subreddit': 'Warthunder'}\n",
      "{'body': 'Let me guess, NYC or SF?', 'subreddit': 'FFBraveExvius'}\n",
      "{'body': 'I think you are missing the Contents model? ', 'subreddit': 'learnprogramming'}\n",
      "{'body': 'Até lá eles vão arrumar mais recibos com data errada e outros sujeitos dizendo que ouviram alguém falar que o Lula é culpado de alguma coisa.\\n\\n\\n*Aliás, dá uma olhada nos tópicos disso na outra comunidade. Dá até pena de tão engraçado...', 'subreddit': 'BrasildoB'}\n",
      "{'body': \"*You're* an iPhone \", 'subreddit': 'oldpeoplefacebook'}\n",
      "{'body': '143413876| &gt; United States Anonymous (ID: rZGCaIJJ)\\n\\n&gt;&gt;143412250 (OP)\\nnigger\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"[Mirror of Amazing!! Irving Lozano scores two goals with PSV!](https://streamable.com/zo6nb)\\n\\n *** \\nIf the original post is already a Streamable link, and I posted a Streamable version of it, it acts as a mirror in case the original is taken down. If I still offended you, well... can't please every human. \\n\\n I'm a bot. Leave me some feedback if possible! :)\", 'subreddit': 'soccer'}\n",
      "{'body': \"Jimiller band sucks if you're an actual dead head \", 'subreddit': 'festivals'}\n",
      "{'body': 'god damn it, you got me with that ending', 'subreddit': 'DatGuyLirik'}\n",
      "{'body': 'Already there bby', 'subreddit': 'teenagers'}\n",
      "{'body': 'Fuck BourManPig', 'subreddit': 'Braves'}\n",
      "{'body': \"Yes. Employment at will means you can leave anytime you want and your employer can ask you to leave anytime they want.\\n\\nPeople have been fired for making dumb social media posts, which is not something I generally agree with. However, if you where to be fired because you identified yourself as a member of a political party that your boss doesn't like... I def. think that's bullshit in its highest form.\\n\", 'subreddit': 'news'}\n",
      "{'body': 'I played a ton of racing games as a kid. my racing skills developed from them have saved me from countless accidents in real life. my town is full or horrible drivers.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Notre Dame and USC? You just seem to like overrated teams', 'subreddit': 'CFB'}\n",
      "{'body': 'Its almost winter here in Canada, and my university has a shit ton of international students and so far every year i love seeing the face of international students experiencing snow! 1 of my best friends i met in first year was from india and when it snowed she was freaking out and was so happy, it was a great time, we built snowmen and rolled around in it until we got cold. Would love to hear your experiences!', 'subreddit': 'AskReddit'}\n",
      "{'body': 'First day in my blind up here in Manitoba. Geese are finally starting to group up so shooting was good.', 'subreddit': 'Waterfowl'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '(in the ship?)', 'subreddit': 'RPGStuck'}\n",
      "{'body': \"Great, I'll be here if you want to play. I'm G_Rod12 on psn and i think i already added you :^)\", 'subreddit': 'GamerPals'}\n",
      "{'body': \"Honestly I agree with this. He hasn't proven to be a dirty player in the past, and massive hits generally result in fumbles and huge momentum swings.  \\n  \\nI can't tell you how many times I've seen fumbles caused by leading with the helmet (towards the ball/arms) on a hard hit. \", 'subreddit': 'nfl'}\n",
      "{'body': 'Why is it clear that earth is flat?', 'subreddit': 'flatearth'}\n",
      "{'body': 'I CAN’T WIIIIIIN\\nWITH OR WITHOUTTT YOU', 'subreddit': 'CoDCompetitive'}\n",
      "{'body': 'CAREFUL NED\\n\\nCAREFUL NOW', 'subreddit': 'traaaaaaannnnnnnnnns'}\n",
      "{'body': \"Did you have insurance on it? I have no insurance but my charging port went like 10 months in and they gave me a free 6p. Now, another 5 months later or so with the replacement and I swear I'm the only person with no battery issues on the 6p. I use almost the whole battery and it dies at like 5% ...maybe that's still not acceptable tho\", 'subreddit': 'ProjectFi'}\n",
      "{'body': \"I'm sixteen, and it was last week. I'm living in a new town and moving into highschool is incredibly hard I find. I feel so out of place and alone, and it was overwhelming. \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'on my way! ', 'subreddit': 'DCents'}\n",
      "{'body': 'Human at sight, monster at heart.', 'subreddit': 'howardstern'}\n",
      "{'body': 'What is the difference?', 'subreddit': 'fantasybball'}\n",
      "{'body': 'And he also said that Puerto Rico \"wants everything done for them.\"\\n\\nLike, Donald. They\\'re *dying*. The fuck is wrong with you?', 'subreddit': 'hamiltonmusical'}\n",
      "{'body': \"I'm white, but things like police brutality, black poverty and inner city violence.  They don't hit close to home for me.  I'm not trying to be the great white hope. I just think it's stupid we're paying attention to the kneeling and not the words behind it.  Then again, this is the population that argued about bathrooms for months. \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Not at all to be honest. I actually kind of like it. Even if you said it to me in a derogatory way I wouldn’t really care. It’s such an old word that it basically just means American. It’s nothing insulting these days.  To southerners, they use it to refer to anyone from the north. ', 'subreddit': 'AskAnAmerican'}\n",
      "{'body': 'Yeah.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Is it the same size as a real one? And how recently did that become available?', 'subreddit': 'Silverbugs'}\n",
      "{'body': 'Just start where goth starts. The music.', 'subreddit': 'goth'}\n",
      "{'body': 'Could be fun!:)', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Why tho', 'subreddit': 'porninfifteenseconds'}\n",
      "{'body': 'Down voted for not killing anything', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'It’s doing exactly the same dance as “cool guy”', 'subreddit': 'funny'}\n",
      "{'body': \"As long as you can stand and mumble a few words they'll serve you.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"that is a tibetan letter. \\n\\nhttps://www.omniglot.com/images/writing/tibetan_cons.gif\\n\\nIt is the last letter listed in the link. I'm not sure what it's significance is. \\n\\nEdit: I'm sure it is the Tibetan om. \", 'subreddit': 'GREEK'}\n",
      "{'body': 'A chimney?', 'subreddit': 'philadelphia'}\n",
      "{'body': 'wikileaks', 'subreddit': 'AskReddit'}\n",
      "{'body': \"I'm pretty sure that's the plan.\\n\\nElon referred to the passenger variant coming in multiple configurations, with the Mars cabins just being one of them.\", 'subreddit': 'spacex'}\n",
      "{'body': 'I love gen three so much. I can’t wait either. ', 'subreddit': 'pokemongo'}\n",
      "{'body': 'A hokujo pot and cup was $25 to ship. ', 'subreddit': 'tea'}\n",
      "{'body': \"I'm using both 1 &amp; 4 of your options. Slowly creeping up, early 80s now\", 'subreddit': 'Diablo3Necromancers'}\n",
      "{'body': 'Is that just a straw cut and put in there or is it split in the middle being 1 half of a straw?', 'subreddit': 'airsoft'}\n",
      "{'body': '[+Laetitian](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoj81y/):\\n\\n&gt; You have to [look it](https://www.youtube.com/watch?v=Do5qzzCnjgc&amp;feature=youtu.be&amp;t=50) and see the next step', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '/u/waterguy12 check this out ', 'subreddit': 'me_irl'}\n",
      "{'body': 'Nerf genji pls kthx', 'subreddit': 'Overwatch'}\n",
      "{'body': 'So should I be thanking my Lucky Stars that I pulled both Cloud usb and Zack csb because of anniversary?', 'subreddit': 'FFRecordKeeper'}\n",
      "{'body': \"The shorter the trip, the bigger the tip.\\nIf you're ordering to an apartment complex or business, give the name.\\nDouble check the address.\", 'subreddit': 'UberEATS'}\n",
      "{'body': '[all of the most recently released japanese language free vns](https://vndb.org/v/all?q=;fil=tagspoil-0.olang-ja;rfil=patch-0.released-1.type-complete.freeware-1;o=d;s=rel)\\n\\n[all of the most recently released free vns that are in english](https://vndb.org/v/all?q=&amp;fil=&amp;rfil=type-complete.patch-0.freeware-1.released-1.lang-en&amp;s=rel&amp;o=d)\\n\\n[all of the most recently released free japanese VNs that are also in english](https://vndb.org/v/all?q=&amp;fil=olang-ja&amp;rfil=type-complete.patch-0.freeware-1.released-1.lang-en&amp;s=rel&amp;o=d)\\n\\nlearn to use filters thanks', 'subreddit': 'visualnovels'}\n",
      "{'body': \"Hey everyone!\\n\\nI'm thinking on throwing up an NM tomorrow but wanted to see how people feel about it first.\\n\\nIt is a Kershaw knockout (assisted opening) I don't have the box there are some snail trails on the blade from opening a few cardboard boxes other than that the knife is in great condition. Thinking of doing 10 @ $5\\n\\nWhat does everyone think?\", 'subreddit': 'KnifeRaffle'}\n",
      "{'body': 'I loved his reaction: \"Holy shit!\"', 'subreddit': 'Skookum'}\n",
      "{'body': 'Snopes: Mostly false. It was 1.2 billion dollars. (if I remember correctly)', 'subreddit': 'The_Donald'}\n",
      "{'body': '[Original post](https://www.reddit.com/r/snackexchange/comments/73ighu/soy_sauce_flavored_ramen_cups_located_in_central/) by /u/Shamitro in /r/snackexchange\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This crosspost was performed automatically as a part of the ImagesOfNetwork.)\\n[^(Learn more about the ImagesOfNetwork, how you can help, and other Frequently Asked Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^, ^or [^(visit us on Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove my post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post) ^| [^(\"The bot got this one wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong) ^| [^(\"Stop Crossposting My Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)  ^(])\\n\\n[](#match \"California\")\\n', 'subreddit': 'ImagesOfCalifornia'}\n",
      "{'body': 'Lol I did', 'subreddit': 'Random_Acts_Of_Amazon'}\n",
      "{'body': 'PM', 'subreddit': 'AVexchange'}\n",
      "{'body': 'hello\\nif you still want to play Add me on steam Themasterseb\\n', 'subreddit': 'playrustlfg'}\n",
      "{'body': \"That's not really a great reason to agree with him though - he consistently treats his own family as pawns to be disposed of as he sees fit and is incredibly comfortable with sexual violence. He's also an enormous hypocrite, railing against Tyrion's whores while both sleeping with Shae and having a tunnel built to his chambers from a whorehouse down the street. Tywin knows his way around a battlefield, but he's an enormous piece of shit.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'What does jacob do to benefit the streams? Exactly. Delete this post.', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"Let's be real though, Apple is not helping themselves with gouging people for shit like chargers out of the box. 8+ with a 5W charger? It's almost insulting. \", 'subreddit': 'apple'}\n",
      "{'body': 'I couldn’t remember which splitter it was...lol. I guess I’ll just retake the LSAT!', 'subreddit': 'lawschooladmissions'}\n",
      "{'body': \"that's digital blackface\", 'subreddit': 'h3h3_productions'}\n",
      "{'body': '[removed]', 'subreddit': 'churning'}\n",
      "{'body': 'Countdown is [here!](https://www.piliapp.com/timer/countdown/#stop=1506817812294,all=00:30:00)\\n\\nRemember No spoilers!\\n\\nPretend it’s July 12, 1996 and [How do U Want It by Tupac](https://www.youtube.com/watch?v=w9KWYwczHEw) and [California Love](https://www.youtube.com/watch?v=5wBTdfAkqGU) are at the top of the charts (really it was officially July 13th but come on) and [Independence Day is #1 at the box office](https://www.youtube.com/watch?v=kA2WzBi2grE)', 'subreddit': 'MMA'}\n",
      "{'body': 'Hahahaha so funny video ', 'subreddit': 'entertainment'}\n",
      "{'body': 'on what exchange? basically no volume on Binance', 'subreddit': 'ethtrader'}\n",
      "{'body': \"...see, I had never made this connection before, and I am glad you pointed it out to me, I still don't like it, but that there is a logical explanation for why he dresses like that is actually kinda satisfying to know.\", 'subreddit': 'grandorder'}\n",
      "{'body': 'The Slenderverse has gotten old to the point where it is frowned upon to create something with Slenderman in it. This reason is why I create my own monsters. ', 'subreddit': 'ARG'}\n",
      "{'body': \"This is the first I've seen of these. Are there glams? I imagine they're just solid black?\", 'subreddit': 'funkopop'}\n",
      "{'body': \"Did Donald tell you it's ok to believe that?\", 'subreddit': 'BannedFromThe_Donald'}\n",
      "{'body': 'Sorry I think I dropped this /s. ', 'subreddit': 'pokemon'}\n",
      "{'body': \"In Britain, in the days when people stayed in their towns or villages for their whole lives, the local vicar or priest held all the family records and would have prevented close family marriages. It was another reason for 'Banns' - so that locals could give a reason, other than bigamy, for a proposed marriage not taking place.\", 'subreddit': 'gameofthrones'}\n",
      "{'body': \"&gt; Everyone hate him here\\n\\nThat's why he has absolute majority in the parliament. Oh wait...\", 'subreddit': 'europe'}\n",
      "{'body': '[removed]', 'subreddit': 'oneplus'}\n",
      "{'body': 'Fuck Michael. \\n\\nI mean yeah, the Klingon killed the Captian. But capturing him alive was the only way to prevent war, and YOU SET YOUR PHASER TO KILL!!\\n\\nI though someone who was raised Vulcan would be able to control their emotions.', 'subreddit': 'Alt_Hapa'}\n",
      "{'body': '143414594| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143414467\\nNo. Hurtful is when people look at me with hatred, disgust, and contempt for no other reason than my gender identity.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': '*Your post has been removed for the following reason(s):*\\n\\n**Off Topic**\\n\\n* Posts or submissions that are not primarily asking or discussing legal questions  are removed.  \\n\\n\\n\\n*If you feel this was in error, message the [moderators.](http://www.reddit.com/message/compose?to=%2Fr%2FLegalAdvice)*', 'subreddit': 'legaladvice'}\n",
      "{'body': \"I wiggled and flashlight spammed a scav player from behind and didn't shoot, he looked at me, proceeded to shoot and still died.\\n\\nThis is why you don't trust people. It's only rarely worth it. \", 'subreddit': 'EscapefromTarkov'}\n",
      "{'body': '$25 shipped', 'subreddit': 'ecigclassifieds'}\n",
      "{'body': \"Not on PC. You can play it in private matches but it's not in public playlists.\", 'subreddit': 'MW2'}\n",
      "{'body': \"Yeah man, I stayed watching nick and CN for\\n a long time. Toonami was my jam back in the day. \\n\\nThere was one cartoon that aired on toonami, I cant remember the name of it, but it was about this mechanic dude who found a giant Gundam style robot from another planet, and then totally reworked all the parts into this badass mech. Then the alien who originally owned it comes looking for it (and she is a hot af redhead) and she can't pilot it anymore because he totally fucked it up, so they gotta work together, and crazy shit happens. \", 'subreddit': 'trees'}\n",
      "{'body': 'Use a trusted exchange for escrow. Seems like the only practical way to do this. Many people want to scam or bluff. No vaporware, just old fashioned escrow.', 'subreddit': 'btc'}\n",
      "{'body': 'I mean there’s sugar in it but I wouldn’t call ketchup sweet (at lest Heinz). It’s more complex than that. But I agree on pickles. Dill = excellent. Sweet = shit. :)', 'subreddit': 'PenmanshipPorn'}\n",
      "{'body': \"Morals aren't inherently good.  He lost the ones he had that made him interesting.  He was a noble evil that was focused on making his group prosperous, sort of a Robin Hood type character turned worse.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"I agree and disagree. The problem is these people don't see that behaviour as being bigoted. Many probably believe they actually are defending the sanctity of marriage. I think it'd be important not to call someone a bigot, but to say you believe it is a bigoted view to have and to explain why you think that.\", 'subreddit': 'australia'}\n",
      "{'body': 'Pumas cant buy a try', 'subreddit': 'rugbyunion'}\n",
      "{'body': \"I'm sure Arin's story about having a horrible trip is less likely than having a regular high, but I still don't like those odds.\", 'subreddit': 'gamegrumps'}\n",
      "{'body': \"As a side note, if you happen to have a decklist I'd love to see it\\n\", 'subreddit': 'EDH'}\n",
      "{'body': 'Haha, thanks, but I just meant that these colors are notorious for never drying.  Apache Sunset laid down thick on Tomoe River can smear weeks later.', 'subreddit': 'fountainpens'}\n",
      "{'body': \"https://redditenhancementsuite.com/ is as close as I can give ya. My gf set it up and I'm using her computer.\", 'subreddit': 'politics'}\n",
      "{'body': 'The \"Cat\\'s Eye\" chroma is my favorite one, the one with mostly the original colorscheme rearraged a big. The red/red-orange blades are beautiful.', 'subreddit': 'Talonmains'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnojk6f/):\\n\\nWhat is typing? lol', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'I took a cab across Puerto Rico on my way to a walled and guarded resort. The homes looked like a hurricane had just blown through and that was before Maria. What a dump!', 'subreddit': 'The_Donald'}\n",
      "{'body': 'Nice pic, looks good', 'subreddit': 'gonewild'}\n",
      "{'body': 'HAHA FUNNY', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Heck again. \\n\\nI had written it but I guess I accidentally deleted it. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '[deleted]', 'subreddit': 'NeutralPolitics'}\n",
      "{'body': 'Has to work better than Shireen. ', 'subreddit': 'CFB'}\n",
      "{'body': \"Calculus is too basic. I make calculus' mom bite the pillow. \", 'subreddit': 'iamverysmart'}\n",
      "{'body': 'Go Briscoe', 'subreddit': 'NASCAR'}\n",
      "{'body': 'My dad always said it was a mouse on a motorcycle. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Doesn't take more effort and it's fine quality. But why you even arguing about this?\", 'subreddit': '2007scape'}\n",
      "{'body': \"No, they weren't. Stop trying to pass off less-than-half-truths as supporting your case.\", 'subreddit': 'ukpolitics'}\n",
      "{'body': 'Oh another USA only service..', 'subreddit': 'Animedubs'}\n",
      "{'body': \"Arizona is having trouble keeping the OL healthy. They couldn't even run well against Dallas. C.johnson my look appealing but Ellington or Williams could take over if they just happen to run more efficiently. \", 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'I remember seeing the pitch, thinking back on my chemistry classes and thinking \"there is no way they\\'d be needing crowdfunding if this was a real thing.\"', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I blame Russia and idiots beliveing all the trolls on Facebook and reddit.  ', 'subreddit': 'AskReddit'}\n",
      "{'body': '[removed]', 'subreddit': 'news'}\n",
      "{'body': 'Corn', 'subreddit': 'Atlanta'}\n",
      "{'body': 'One of them is red u monkey', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Thanks for posting to /r/dirtykikpals, /u/SoRandomWow! We encourage all of our users here to verify themselves, to possibly get more/better responses, as well as help us in dealing with sellers, scammers, etc. For information on how to verify, please check our sidebar or message us at [modmail](https://www.reddit.com/message/compose?to=%2Fr%2Fdirtykikpals).\\nAlso, be sure to familiarize yourself with the rules of this subreddit, as well as helpful tips [here](https://redd.it/6ojo0r).\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dirtykikpals) if you have any questions or concerns.*', 'subreddit': 'dirtykikpals'}\n",
      "{'body': 'I mixed up their helmets and the fact that Mansell tested for Jordan in 1996.\\n\\nBut yes, it is Brundle. :)', 'subreddit': 'formula1'}\n",
      "{'body': \"But he's not someone I want to be a close friend , I don't feel comfortable speaking about anything personal with him , I find him kind of annoying and he's quite aggressive. I want to make new friends but he's making it difficult by always being there. It's been 3 weeks and I haven't been able to have a conversation with anyone without him being there. I want to be my own thing , separate from him\", 'subreddit': 'relationships'}\n",
      "{'body': '*can you text it over to my number please?*', 'subreddit': 'conspiracy'}\n",
      "{'body': 'Just let them read Robert Crumb comics, those are healthy and artistic for the child.', 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'I run 2 in my roach forest along with 1 wotf and i think she is fine. The main reason i like her is that she goes on curve better than wotf, because turn  5 is usually time where you want to play Teena/EPM/use DoD or whatever else. Turn 6 is often period where some combo pieces are already gathered, so its time to stall/clear the board while waiting for lethal range and/or more combo pieces.', 'subreddit': 'Shadowverse'}\n",
      "{'body': 'Nice amp.', 'subreddit': 'malelivingspace'}\n",
      "{'body': 'Hah! I had to hop off the cab in a minute, so I could only write \"Retinol and Lactic Acid alternate PM, and sunscreen AM.\"\\n\\n\\nHere\\'s the full routine:\\n\\n\\nAM:\\n\\n\\nWash with water&gt; Timeless CE Ferulic&gt; Hydrating Toner&gt; Jojoba/Grapeseed Oil&gt; CeraVe moisturizing lotion&gt;Sunscreen\\n\\nPM 1:\\n\\nDIY oil cleanser if I have sunscreen/makeup on&gt; CeraVe hydrating cleanser&gt; Hydrating Toner&gt; Hylamide SubQ serum&gt; TO Lactic Acid 5%/Alpha Skin Care 10% Glycolic gel&gt; Rosehip seed/Passion fruit oil&gt; CeraVe PM/Stratia LG&gt; CeraVe healing ointment\\n\\nPM2:\\n\\nSame up to serum, then Oil&gt; TO Advanced Retinoid 2%&gt; Moisturizer&gt; Occlusive\\n\\n\\nI wait the suggested time between products. With sunscreen it\\'s usually more than 30 mins+ after moisturizer. I work from home, so I have the luxury of stretching out my AM routine. I also don\\'t have to wear sunscreen or makeup every day.\\n\\n\\nToners I use: Klairs Supple Preparation, Hada Labo Gokujyun Moist, Kiku Masamune Sake High Moist\\n\\n\\nSunscreens: Neogen Day-Light on dry days, Anessa Aqua Booster Mild SPF 50 on humid days\\n\\n\\nOh, and I have combo leaning dry skin, btw. Hope this helps!', 'subreddit': 'SkincareAddiction'}\n",
      "{'body': \"Didn't know that was a thing.\", 'subreddit': 'CasualConversation'}\n",
      "{'body': \"It may be a different DIL completely.  This story sadly is all too familiar.  She's getting hammered with the responses though. \", 'subreddit': 'JUSTNOMIL'}\n",
      "{'body': '[deleted]', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'I would greatly appreciate a run. 290+ Hunter. \\n\\nGT: chugachninja ', 'subreddit': 'DestinySherpa'}\n",
      "{'body': 'No, and you should really just read the all-ages version.', 'subreddit': 'visualnovels'}\n",
      "{'body': \"If we see an escalation of the kind of attacks we saw against individual node operators of XT, Classic, Unlimited, etc., then I wouldn't be surprised if we start to see rumblings of residential ISPs prohibiting running of Bitcoin nodes in their terms of service, because why deal with this childish nonsense in the first place if you're trying to run a business?\", 'subreddit': 'btc'}\n",
      "{'body': 'Ahh. Glad you looked into it more. \\nMaybe adding a silicone tube to the end of a vaporizer  or use it with a water tool will give you cooler vapor if needed. ', 'subreddit': 'vaporents'}\n",
      "{'body': \"I don't see the confusion. \\n\\nr/btc says s2x will cripple and nullify core. \\nr/bitcoin says s2x will die out like bcash did\\n\\nyou're just misreading what I typed. \", 'subreddit': 'Bitcoin'}\n",
      "{'body': 'Probably going straight to Iran.', 'subreddit': 'interestingasfuck'}\n",
      "{'body': \"Hooked up with a guy and it was basically doomed from the start.\\n\\nHe showed up 3 hours late to my place cause he decided it was a good idea to sell one of his old game consoles before he came over and the person took forever to pick it up. Once we started fooling around I had to tell him to take his clothes off, he refused to take off his tshirt. He didn't know what foreplay was and I had to tell him to do it.\\n\\nSurprise surprise I couldn't finish cause I just wasn't feeling into it at that point and I don't think he did either. This is why I don't do hookups anymore.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Looks like a goddamn fire hazard to me', 'subreddit': 'techsupportgore'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'I understand *why* they’re doing it, but PSUs blatantly obvious Heisman stat padding still rubs me the wrong way. ', 'subreddit': 'CFB'}\n",
      "{'body': 'Comrade! I am real puerto rican/asian/canadian man.\\n\\nI am not Ruski. Cyka!', 'subreddit': 'russiansfacebook'}\n",
      "{'body': 'this happened to me expect my game just crashed and reset my player', 'subreddit': 'NBA2k'}\n",
      "{'body': \"#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n  \\n[Kinjalli's Caller](https://img.scryfall.com/cards/normal/en/xln/18.jpg?1505309002) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Kinjalli%27s%20Caller) [(SF)](https://scryfall.com/card/xln/18?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!Kinjalli%27s%20Caller)  \\n[Optec Huntmaster](http://mythicspoiler.com/ixa/cards/otepechuntmaster.jpg) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Otepec%20Huntmaster) [(SF)](https://scryfall.com/card/xln/153?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!Otepec%20Huntmaster)  \\n[Ripjaw Raptor](http://mythicspoiler.com/ixa/cards/ripjawraptor.jpg) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Ripjaw%20Raptor) [(SF)](https://scryfall.com/card/xln/203?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!Ripjaw%20Raptor)  \\n[Carnage Tyrant](http://mythicspoiler.com/ixa/cards/carnagetyrant.jpg) - [(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Carnage%20Tyrant) [(SF)](https://scryfall.com/card/xln/179?utm_source=mtgcardfetcher) [(MC)](http://magiccards.info/query?q=!Carnage%20Tyrant)  \\n^^^[[cardname]] ^^^or ^^^[[cardname|SET]] ^^^to ^^^call\", 'subreddit': 'magicTCG'}\n",
      "{'body': 'Ok Eagerjewbear-\\n\\n\\nWhat should who have done better? Trump?\\n\\nPre-stage the USNS Comfort prior to the hurricane, just like what was done for hurricane Matthew. \\n\\nDispatch another two LHDs to accompany the Wasp. \\n\\nOpen transport for power companies and personnel on the SE Atlantic coast as soon as the SJ airport was cleared. \\n\\nIssue National Guard mobilization to support the Puerto Rico units (they have a few of their own NG units there, but not enough)\\n\\nYeah helicopters would perform supply airdrops of food and water while the USCG performed air medevac to SJ to await the Comfort, however would be in a central triage location with access to supplies as needed. \\n\\nIn summary, what they are saying they are going to do really soon, but about 5 days ago. \\n\\n\\n', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': \"This may be related to your system, as you aren't getting any connections. Did you allow both incoming and outgoing connections for `monerod`?\", 'subreddit': 'Monero'}\n",
      "{'body': \"What I am suggesting is raising the healing rate of healers other than mercy to help them with the challenge that is keeping teammates up. While some kits have alternate perks, I'd say that they still leave you in a spot where you are unable to capitalize because you lose teammates that would help you capitalize too quickly. \", 'subreddit': 'Competitiveoverwatch'}\n",
      "{'body': 'And soundtrack!', 'subreddit': 'boston'}\n",
      "{'body': 'Anything you saw in a porn. Stick to the basics for the first time. ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'What ever happened to Hugh Jaynuss?', 'subreddit': 'magicTCG'}\n",
      "{'body': 'R', 'subreddit': 'darkjokes'}\n",
      "{'body': 'Boooooooss', 'subreddit': 'starcitizen'}\n",
      "{'body': '[removed]', 'subreddit': 'baseball'}\n",
      "{'body': \"and Buster Posey's mask was never seen again\", 'subreddit': 'baseball'}\n",
      "{'body': \"143415951| &gt; Australia Anonymous (ID: CSDg7ZEw)\\n\\n&gt;&gt;143415817\\nHow'd you go from a lolcuck to an establishment cuck?\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': '**SD** | [Los Angeles Angels vs Seattle Mariners](http://plr.livestreamsonline.net/embed/24) | Ad Overlays: 3 | Mobile: No \\n\\n\\n 1. If black screen make sure flash player is enabled. \\n\\n\\n 2. Stream Live 5 - 10 Before', 'subreddit': 'MLBStreams'}\n",
      "{'body': 'Someone will take his seat talk to the bride.  Make sure you go though', 'subreddit': 'relationship_advice'}\n",
      "{'body': 'while channel surfing I came across the \"speech\" part of this protest on c-span.. \\n\\nthis particular speaker was some short, overweight and unattractive black woman bitching about something - watched for about 5 minutes..  she made sure to include \"white\" and \"racist\" / \"racists\" in every sentence.\\n\\nwhatever her message was, if her message was about anything else other than all white people are racists, it was lost on me.\\n\\nI guess if she believes all whites are racists, then I can believe all blacks are crybaby niggers.\\n', 'subreddit': 'OffensiveSpeech'}\n",
      "{'body': '[deleted]', 'subreddit': 'keto'}\n",
      "{'body': \"I'm sure Hillary doesn't\", 'subreddit': 'ShitPoliticsSays'}\n",
      "{'body': '[+Lukavian](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo383/):\\n\\nThis is not a comforting thing to hear from a writer... ;)', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'You see it when you read old stuff. It means the flu and flu-like symptoms. I\\'ve always pronounced it \"ayg\". It\\'s actually pronounced \"ay-gyoo.\" ', 'subreddit': 'Damnthatsinteresting'}\n",
      "{'body': 'at 70, all of it can be burned in a party of 4', 'subreddit': 'ffxiv'}\n",
      "{'body': \"The Warring States period is perfect for a a total War game. It's a super interesting period because it sits right at two important events in chinese technological history: The shift from bronze to iron, and the introduction of cavalry into chinese military tactics.\", 'subreddit': 'totalwar'}\n",
      "{'body': 'Man is he ever lucky, to be surrounded by all those nuns, unf. ', 'subreddit': 'funny'}\n",
      "{'body': \"Fun Fact: If you stopped drinking water the day after Rhys Hoskins' last home run you would probably be dead right now.\", 'subreddit': 'fantasybaseball'}\n",
      "{'body': 'I wasn’t comparing the US system to the Iranian system. I was simply saying how the US system works and came to the conclusion that the US system is indeed and in fact broken. It does not serve the people. It serves the wealthy and influential. ', 'subreddit': 'worldnews'}\n",
      "{'body': \"I'm using a large step stool as we speak. Laptop shelf on top, foot shelf below.\", 'subreddit': 'AskWomen'}\n",
      "{'body': \"No just check it once it's empty there will be a funny message for each chest\", 'subreddit': 'JRPG'}\n",
      "{'body': \"It's on your computer. It's up to you to fix it\\n\", 'subreddit': 'mozilla'}\n",
      "{'body': '34', 'subreddit': 'The_Division'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I forget who posted it, but a while back, someone asked a similar question, requesting people's top 5 podcasts, and in one person's response, they put The Bright Sessions.  I hadn't heard of it before, but I fucking loved it.\\n\\n  It's basically the audio logs of a therapist who works with people with superpowers, and it takes itself way more seriously than you'd expect, and it's so much better for it.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"You're right it is. \\n\\nAnd I messed up. The guy's name is Winston. \", 'subreddit': 'DnD'}\n",
      "{'body': 'Dear Pandarth_Omega,\\n\\nLuckily for you - or maybe not so much - I read your entire reply. In this comment I will be clearing some things up, setting the records straight, while replying to some of your remarks directly. Therefore I will quote your previous post in order to achieve full transparency. I feel we need this in both of our lives right now. Correct me if I\\'m being to presumptuous. \\n\\n**Honest**\\n&gt;I thank you for your honest reply\\n\\nI must confess. I wasn\\'t honest at all, for I have never read the 4th issue of The Thanos Imperative. So I merely guessed that Thanos said \"No\" in this issue. My conclusion - as you have correctly pointed out - is flawed and playfull mockery. \\n\\n**Fun**\\n&gt;As it seems you have not quite understood the latter point, let me put it to you as plainly as I can. This post was made for fun.\\n\\nWhile I was teasing you with my clearly false theory - on which I will elaborate later - it wasn\\'t my intention to give you the impression that I took your theory fully serious or that I didn\\'t enjoy myself. Although this section isn\\'t titled **honest**, I still believe I am when I state that my opening sentence was fully sincere. I shall quote myself here - one thing I always enjoy doing - to show you to what sentence I\\'m referring: \"I intensely enjoyed reading your - how shall I call it? - Theory.\" Your writing style, wit and the theory itself are entertaining. I was merely inspired by your post to reply with another theory. And since I do not believe in the meaning you get from those numbers (D23), I decided to exaggerate in an even more far-fetched theory. \\n\\n**I told you so**\\n&gt;your telling me \"I told you so\" has no meaning\\n\\nIn my modest opinion. I told you so always has meaning. If nothing else, it may hint at a character flaw of the one posing this statement. \\n\\n**My clearly false theory**\\n&gt;I can only imagine how long it took you to accomplish this.\\n\\nIt took me 10 minutes. I think. \\n\\n&gt; Your theory was built on three numbers which were never shown in the film; to discover them one would have to pay very close attention to the film and, likely as not, watch it multiple times. \\n\\nI googled \"Captain America Winter Soldier Numbers\" in order to get some data quickly. I quickly found this picture: https://i.pinimg.com/originals/a9/8f/9b/a98f9bce6365417da8056ae3339d8e7c.jpg\\nI choose three numbers - which only had in common that Captain America himself was involved - and tried googling some dates. Luckily I found that The Thanos Imperative was released on one of the possible dates. As you can see, this is no achievement at all. Of course, while having fun, I was also trying to point out that giving meaning to numbers isn\\'t that difficult. And while I will admit, the numbers in your theory have a clearer connection to each other, I still think it isn\\'t a meaningful one. I cannot disprove this - and since you\\'re not that serious about it, I probably won\\'t have to - but I simply highly, highly doubt a director or marketing team would put in coded messages about trailer drops for a movie that far ahead of time. It is way more likely that this is just coincidence (if the trailer does drop at October 14th). \\n\\n**The Trailer**\\n&gt;As I mentioned in my original post, a quick look through recent activity will reveal that the most popular answer is \"sometime in October\"\\n\\nThis is likely. I think the trailer will drop around Thor: Ragnarok. This strategy will remind moviegoers that the Thor film might be worth checking out because he\\'s in Infinity War. And we all want to know which events will lead-up to that movie. But this is merely a prediction of Marvels overall strategy. You probably wouldn\\'t be surprised at this point that I too think the Infinity War trailer will be dropped at the end of October or the beginning of November. \"Around October\" sounds accurate to me. \\n\\nu/AdmiralSnackBar69\\'s analysis is therefore much more reliable then your fun-to-read-theory. He doesn\\'t look at a coded message - which we just entertainingly assume it might be a coded message - but he looks at production trends. \\n\\n**Closing**\\n\\nIt was never my intend to be hurtful or mock you as an author. I really enjoyed reading your text. So maybe I didn\\'t formulate my reply as carefully as I could have. English isn\\'t my first language, so communicating can be difficult sometimes. That being said, I don\\'t feel you took it the wrong way. Your reply was equally entertaining. This exchange of simple words we\\'ve had today was so fun and inspiring, that I think we should become electronic pen pals. \\n\\nTo summarize: \\n\\n- I had fun reading your original post and your reply. \\n\\n- I was fully aware you were not entirely serious. \\n\\n- My theory was very much incorrect. \\n\\n- I think a trailer will drop in October/November.\\n\\n- While I was mocking, I hope you know by now it wasn\\'t meant to be hurtful, personal or hateful. \\n\\n\\nHopefully, I cleared some things up. If not, if everything I said you already knew or assumed, then consider my post simply as meaningless rambling, attention seeking or a waste of your valuable time. ', 'subreddit': 'marvelstudios'}\n",
      "{'body': 'Will Butch Jones be fired by the end of the Bama game?', 'subreddit': 'CFB'}\n",
      "{'body': 'Who said you had to put up with it? ', 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': 'plenty of hall of famers in action tonight, count me in', 'subreddit': 'MMA'}\n",
      "{'body': \"Your post (probably) hasn't broken any rules, but we see these kinds of things a lot. Look at our [most overdone items here](https://www.reddit.com/r/mildlyinteresting/search?q=flair%3A%28overdone%29&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/mildlyinteresting) if you have any questions or concerns.*\", 'subreddit': 'mildlyinteresting'}\n",
      "{'body': \"agree with this. i mean right now hes on pace for 2,800 all purpose yards, about 50 receptions, and over 30 tds. This is ridiculous and impossible basically. I think Hunt owners realize that, and are expecting big games from him, but tempering those expectations a little. \\n\\nThink i read somewhere that they estimated regression to maybe 1200 yards the rest of the season (which still leaves him with over 1700), and something like 14 td's. those are STILL ridiculous numbers, so even if he comes close to that he is worth holding onto, or trading for someone great.\\n\\nI may regret it, but im gonna hold onto him. People were saying to trade DJ while his value was up so high after the first few weeks he took over, and now those people look silly. he could fall off, sure. but he could take you to the ship as well. Im all for the solid, respectable numbers\", 'subreddit': 'fantasyfootball'}\n",
      "{'body': '\"We chose to paint the guest room Mama\\'s Cameo with an accent of Madeline\\'s Muff.\"', 'subreddit': 'funny'}\n",
      "{'body': \"All you have to do to be eligible for the rate stats are average 3ABs over 162 games. Which is reasonable. One could argue that if Trout wasn't hurt he would have a larger lead based on his previous seasons....\", 'subreddit': 'baseball'}\n",
      "{'body': \"Yours don't look like this? http://i.imgur.com/dOupf.jpg\", 'subreddit': 'xboxone'}\n",
      "{'body': \"I domt entirely trust it. Thats an extremely gated video. You dont get to see it start up cpu intensive programs, you dont get to see them do anything intensive either. It reminds me of the demos apple used to pull, when things didnt have real full functionality yet. Gimme more information about what's running that, let me see it in action entirely. How hot is it getting, what's the cooling system, and how is the battery affected?\", 'subreddit': 'windowsphone'}\n",
      "{'body': 'He was about to OBJ that shit ', 'subreddit': 'CFB'}\n",
      "{'body': 'Props to the fort tonight, chants have been 🔥', 'subreddit': 'MLS'}\n",
      "{'body': '[I think Shiggy is the most relate-able though](https://i.imgur.com/6gMD049.png)', 'subreddit': 'BokuNoHeroAcademia'}\n",
      "{'body': 'IDK what press you are using but the one time I did this I left the die in place, raised the ram all the way up and then put the proper shell plate over the shell while simultaneously attaching it to the ram.  Lowered ram and yanked the shell out.', 'subreddit': 'reloading'}\n",
      "{'body': 'How can you determine if the internet is worth a damn before you move in somewhere?', 'subreddit': 'AskReddit'}\n",
      "{'body': \"But, it's so weird to see him at Sam's Club in Logan. I feel like I need to hide until I can get some fig leaves.\", 'subreddit': 'exmormon'}\n",
      "{'body': 'Anti bethesda AND pro CDPR circlejerk all in one comment!\\n\\nDing ding ding!', 'subreddit': 'PS4'}\n",
      "{'body': 'Mixing fighting tired and fighting fresh is suboptimal. Huh. I hope for your sake your opponents think the same thing ;)', 'subreddit': 'amateur_boxing'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': \"Okay thanks. I'm in phx so won't get my hopes up but hopefully will be pleasantly surprised! \\n\\nHas anyone figured out if these are just repackaged Gardein or other brand? They've been out of stock for sooo long I can't recall. \", 'subreddit': 'vegan'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop508/):\\n\\nRight? lol', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '[deleted]', 'subreddit': 'kotor'}\n",
      "{'body': '907', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Even better, involve her, let HER send out the invitations to those people. Make her think its about making amends to HER, that it\\'s about HER. \\n\\nThat way when DH does the mic drop moment with a speech ending \"And of course I\\'d like to thank my mother for making all of this possible, after all if it wasn\\'t for her lies about us having a shotgun wedding and a miscarriage none of us would be here tonight\" it hit\\'s her fully. ', 'subreddit': 'JUSTNOMIL'}\n",
      "{'body': 'Glorious rack too', 'subreddit': 'ladyladyboners'}\n",
      "{'body': \"I think Kentaro will continue to get better and challenge Brandon at the end.  None of the others are consistently good enough or have a newer style that pleases the judges.  Margarita may be a surprise if they are considering JC Penney styling.  Kenya is a little too old fashioned.  I am old and  I like many of her designs but again, I am old and designers don't design for baby boomers.  Same reasoning applies for Amy.\", 'subreddit': 'ProjectRunway'}\n",
      "{'body': 'Lol haha XD le pickle Rick amirite?', 'subreddit': 'justlegbeardthings'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Thanks bud. I’m gonna look into it', 'subreddit': 'Trucks'}\n",
      "{'body': '[deleted]', 'subreddit': 'NBA2k'}\n",
      "{'body': \"It doesn't even say that he kills them in the book. He just gets rid of them somehow. Maybe he kills them. Maybe he banishes them and they become pirates and Indians.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'I think it really depends on how the season plays out tbh. If either team is an absolute lock in the playoffs it’s possible.', 'subreddit': 'Texans'}\n",
      "{'body': 'The weather matches your mood', 'subreddit': 'depression'}\n",
      "{'body': 'Oh yeah totally agree. I think she had a boob job done for sure.', 'subreddit': '90DayFiance'}\n",
      "{'body': 'Wait I honestly don’t know what happened to Cross. Did he just leave or what?', 'subreddit': 'greysanatomy'}\n",
      "{'body': 'I am the same, no animosity, but I have noticed institutional (?) discrimination. Many \"multicultural\" savings accounts are only eligible for foreign women, and if you look at government welfare systems they\\'re made for foreign women. I guess that\\'s more just sexism than racial discrimination - anything related to the family will be written for \"mom\".\\n\\nJust look up something about Chuseok for foreign spouses, it will tell you how to be a good little wife:)', 'subreddit': 'korea'}\n",
      "{'body': 'Oh, so you want to see my historical financials to prove my healthcare costs went up?\\n\\nYawn.', 'subreddit': 'politics'}\n",
      "{'body': 'Yea because the cartel are based off of U.S. state parks and not off Mexico. ', 'subreddit': '4chan'}\n",
      "{'body': 'I had to stop taking my Saint Bernard to the dog park because he started getting aggressive with unneutered males. Will training help or that’s just him becoming an adjunct jerk?', 'subreddit': 'dogs'}\n",
      "{'body': 'Also removal of life steal and %damage', 'subreddit': 'Diablo'}\n",
      "{'body': 'We are still the Titans', 'subreddit': 'nfl'}\n",
      "{'body': \"I like it, it's a way to use speedy cam/cf at a wider formation, almost acting like wingers but without getting isolated in the wings.\", 'subreddit': 'FIFA'}\n",
      "{'body': \"There's more chance of him learning Irish than ever meeting anyone on this sub in person.\", 'subreddit': 'northernireland'}\n",
      "{'body': 'ye fam', 'subreddit': 'Alt_Hapa'}\n",
      "{'body': 'I\\'m guessing because if the send her to the \"fat farm\" then Jane can take her leave from work for a bit. \\n', 'subreddit': 'Frasier'}\n",
      "{'body': 'Embiid is also like 28', 'subreddit': 'nba'}\n",
      "{'body': 'Thanks! 😁 do you think aguero will be back in two weeks tho?', 'subreddit': 'FantasyPL'}\n",
      "{'body': 'DMac with another try-saving tackle', 'subreddit': 'rugbyunion'}\n",
      "{'body': \"Agreed, this Shen is better than the old, and I don't dislike the new Q, I think it is a nice ability but feels like it could be a bit better still.\", 'subreddit': 'Shen'}\n",
      "{'body': 'Fleshlight? ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Just go to mcpedl.com . You can find lots of free maps, textures and addons. The marketplace is completely optional, don't get why people think it's the only way to get stuff\", 'subreddit': 'Minecraft'}\n",
      "{'body': '143414681| &gt; United States Anonymous (ID: zR7qrd9W)\\n\\n2012: Aleppo\\n2016: Trump and Trump\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'He and most of the population who actually think not supporting certain animal abuses is terrible because \"If they can\\'t be abused they wouldn\\'t exist in the first place and that\\'s worse!\"', 'subreddit': 'todayilearned'}\n",
      "{'body': 'Lucky! About 20 days shy of a year here!', 'subreddit': 'NFA'}\n",
      "{'body': '[removed]', 'subreddit': 'woahdude'}\n",
      "{'body': 'Of all the possibilities... Cumnog and Thotness wtf', 'subreddit': 'tumblr'}\n",
      "{'body': \"Wiggins is a bit terrifying.\\n\\nHe's getting that max deal, and I know that I shouldn't be concerned about him being worth it, but I am a bit concerned about him for a few different reasons.\\n\\nAlso interesting to note the average age of the guys above the line versus the average age of the guys below it in general.\", 'subreddit': 'nba'}\n",
      "{'body': 'Ward almost had it!!!!!', 'subreddit': 'CFB'}\n",
      "{'body': '[Imgur](https://i.imgur.com/jxB6GQW.jpg)', 'subreddit': 'photoshopbattles'}\n",
      "{'body': 'why', 'subreddit': 'freefolk'}\n",
      "{'body': 'Our D-linemen are making them jump by just being there.', 'subreddit': 'CFB'}\n",
      "{'body': '\"Oh you\\'re in for quite a journey boy.\"', 'subreddit': 'roleplayfantasy'}\n",
      "{'body': 'Lauren back at it with her assault and battery charges.', 'subreddit': 'survivor'}\n",
      "{'body': 'Anarchy, as a political concept, is a naive floating abstraction: .\\xa0.\\xa0. a society without an organized government would be at the mercy of the first criminal who came along and who would precipitate it into the chaos of gang warfare. But the possibility of human immorality is not the only objection to anarchy: even a society whose every member were fully rational and faultlessly moral, could not function in a state of anarchy; it is the need of\\xa0objectivelaws and of an arbiter for honest disagreements among men that necessitates the establishment of a government.\\n\\n“The Nature of Government,”\\nThe Virtue of Selfishness, 112\\n\\n¶\\n\\nIf a society provided no organized protection against force, it would compel every citizen to go about armed, to turn his home into a fortress, to shoot any strangers approaching his door—or to join a protective gang of citizens who would fight other gangs, formed for the same purpose, and thus bring about the degeneration of that society into the chaos of gang-rule, i.e., rule by brute force, into perpetual tribal warfare of prehistorical savages.\\n\\nThe use of physical force—even its retaliatory use—cannot be left at the discretion of individual citizens. Peaceful coexistence is impossible if a man has to live under the constant threat of force to be unleashed against him by any of his neighbors at any moment. Whether his neighbors’ intentions are good or bad, whether their judgment is rational or irrational, whether they are motivated by a sense of justice or by ignorance or by prejudice or by malice—the use of force against one man cannot be left to the arbitrary decision of another.\\n\\n“The Nature of Government,”\\nThe Virtue of Selfishness, 108\\n\\n', 'subreddit': 'Anarcho_Capitalism'}\n",
      "{'body': '[deleted]', 'subreddit': 'oldpeoplefacebook'}\n",
      "{'body': \"In Surah Al-Ahzab verse 33. Shi'as use this verse to claim that the ahl al-bayt cannot make any mistakes and are completely infallible. \\n\\nedit: By the way, I'm not claiming that they are infallible nor do I believe that's what the verse is implying. I was just trying to figure out what exactly shi'as mean when they say infallible. \", 'subreddit': 'islam'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'I passionately  wait for the moment that graves is meta again :[ ', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'Not rating a guy highly (or not talking about him) because he gets injured a lot is not overlooking him. Availability matters.', 'subreddit': 'nfl'}\n",
      "{'body': \"Ignoring the fact that Kaepernick literally chose to kneel as his form of protesting police brutality because he talked to vets that said that would be the most respectful way to do it. \\n\\nI'm curious as to why you'd think kneeling is an act of disrespect and what situation you've seen someone kneel before this that you thought was disrespectful. \", 'subreddit': 'news'}\n",
      "{'body': \"I didn't release her, that's bullshit, I did NOT...oh hi Eclipsa.\", 'subreddit': 'StarVStheForcesofEvil'}\n",
      "{'body': \"Man, all respect, I was very lucky, had no real problems stopping working for someone else and moving to working for myself, I was already somewhat in the industry, the transition was very smooth for me. Your a hustler, I respect that 100%. People don't realize that hard work really does pay off, you might fall down sometimes but the real winners are those who don't quit and pick themselves up! I'll pop a beer for you bro, wish you all the best!\", 'subreddit': 'drunk'}\n",
      "{'body': \"That doesn't sound right, but I don't know enough about irregular verbs to dispute it.\", 'subreddit': 'rickandmorty'}\n",
      "{'body': 'Sux. So sorry :(', 'subreddit': 'cats'}\n",
      "{'body': 'Is it really that much wow', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'I figured, thanks though and good luck', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Cures alcoholism...', 'subreddit': 'depression_help'}\n",
      "{'body': 'DC VegFest is always a great time!!', 'subreddit': 'PlantBasedDiet'}\n",
      "{'body': 'Nothing wrong with respecting your dad. Even if it means hanging out somewhere you’d rather not be. I mean, he’s your dad. Mine died over 10 years ago and I’d love to hang out with him, even if it was at the opera, ballet, rock concert, poetic reading, or a couple hours sitting in the dark sleeping with my eyes open. He would take me to Wendy’s afterwards. ', 'subreddit': 'exmormon'}\n",
      "{'body': 'incels: the most fragile of all manchildren', 'subreddit': 'IncelTears'}\n",
      "{'body': 'Yeah if you have man city, there probably won’t be any chants\\nAnd if you are losing the stadium will magically empty itself', 'subreddit': 'FIFA'}\n",
      "{'body': \"The poor bloke isn't even to blame for this one.\", 'subreddit': 'Eve'}\n",
      "{'body': 'What is that link? It wants me to download something?? \\n\\nSketch... lol', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'spitting in public. ', 'subreddit': 'AskReddit'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': '**HD** Stream : [NCAAF Ole Miss vs Alabama *ESPN* **HD** Stream](http://nowwatchtvlive.cc/espn-live-stream-watch-espn-sportscenter-online-free/)\\n\\nSD TV Streams : [NCAAF Ole Miss vs Alabama *ESPN*  English Stream](http://nowwatchtvlive.cc/live-sports/stream-1-13.php)\\n \\nAd Overlays: 4 | Mobile: Yes | NSFW: Yes | Chat', 'subreddit': 'CFBStreams'}\n",
      "{'body': 'I can see it now \"Hillary Clinton says her movie bombed because America is sexist.\"', 'subreddit': 'Conservative'}\n",
      "{'body': 'I meant \"command\" the printer. \\n\\nI mean, those are OK specs, but for a \"gaming\" setup it\\'s a bit low. The CPU is OK but the 940M should struggle in recent games, right? \\n\\nI meant no offense. I just found it weird that it would be a gaming pc with those specs, when the same specs are perfectly on par with what I\\'d expect from the companion PC of a 3d printer, since you still need some GPU power to design the pieces and manipulate them in software.  \\nThat just seemed logical.\\n\\nHence my question. Sorry if I offended you in any way. ', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Archived for your convenience\\n\\nSnapshots:\\n\\n1. *This Post* - [archive.org](https://web.archive.org/20171001000150/https://i.redd.it/5qcnbrtc04pz.jpg), [_megalodon.jp\\\\*_](http://megalodon.jp/pc/get_simple/decide?url=https://i.redd.it/5qcnbrtc04pz.jpg \"could not auto-archive; click to resubmit it!\"), [archive.is](https://archive.is/oLpqJ)\\n\\n*^(I am a bot.) ^\\\\([*Info*](/r/SnapshillBot) ^/ ^[*Contact*](/message/compose?to=\\\\/r\\\\/SnapshillBot))*', 'subreddit': 'MGTOW'}\n",
      "{'body': 'salt', 'subreddit': 'Eve'}\n",
      "{'body': '200 for a fucking cd player?', 'subreddit': 'gaming'}\n",
      "{'body': 'I will once I get more lucky occurrences like this lol :D', 'subreddit': 'PenmanshipPorn'}\n",
      "{'body': \"I've been in one USF game thread so i haven't seen this, but yeah i wouldn't be surprised if we had our fair shares of jerks on our side as well and the rest of us apologize for it\", 'subreddit': 'CFB'}\n",
      "{'body': \"You should definitely talk to your doctor about this. Don't hurt yourself! Hugs \", 'subreddit': 'BipolarReddit'}\n",
      "{'body': 'Lol, not sure if you are kidding or not', 'subreddit': 'DBZDokkanBattle'}\n",
      "{'body': 'Exactly. If it weren’t for posts talking about female nature, we’d likely stop gaining many new members, and we’d lose quite a few as well. Take away the reminder of why we left, and sooner or later biology will take control and we’ll reenter the game (no matter how stupid it would be)', 'subreddit': 'MGTOW'}\n",
      "{'body': '[+dmwe225](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnos1l4/):\\n\\nHighly recommend she read Stephen King\\'s \"On Writing\"', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Am I am right in saying there was similar shortages of food in 1846/7 in most of Europe? ', 'subreddit': 'europe'}\n",
      "{'body': \"Also Jules Verne did a good job of describing how to put a man on the moon using a giant cannon and  HG Wells provides details on Cavorite and the sphere built using it to get to the moon.\\n\\nThe point is ancient Hebrews knew how to build a ship. So in telling the story of Noah they just expanded the dimensions.  On the key issues, how food was stored, how Noah got enough gopher wood to build the Ark, and a a lot of other details the Bible is silent.\\n\\nPerhaps the story of Noah's Ark is the ancient version of BattleStar Galatica ?\", 'subreddit': 'Christianity'}\n",
      "{'body': 'They rent too as do other tool rental places like Aurora Rents.', 'subreddit': 'SeattleWA'}\n",
      "{'body': 'No people just like using new ways to insult people. Pretending otherwise would be a lie.', 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'Night bud. Glad you appreciate my over-sharing! &lt;3', 'subreddit': 'anime'}\n",
      "{'body': '[deleted]', 'subreddit': 'teenagers'}\n",
      "{'body': \"A decentralized grid is probably the way of the future. There remains much to be worked out, but that's the fun part.\", 'subreddit': 'energy'}\n",
      "{'body': \"That's only cause everybody else sucked though \", 'subreddit': 'neoliberal'}\n",
      "{'body': 'I know its crap quality and everything but i always like the aesthetic of it. Plus being an introvert myself adds to that\\n\\nEdit: its also the brand that first got me into streetwear in general', 'subreddit': 'streetwear'}\n",
      "{'body': \"Taking a personal stance here: I know a few good people in the community who have succeeded and are happy today after starting tulpamancy with possession. Compared to them Koomer and Ougigi are not only an exception, but as many others have pointed out today and in the past, they had several other underlying problems that are probably more connected to their issues involving tulpamancy than the skills they practiced, which further distances the problems they had from the actual practice.\\n\\nAlso, you call your post an opinion here, [but in your blog its title doesn't reflect the same stance](https://i.imgur.com/y15xA2D.png), although you do mention its something along that line in the very last line of your article.\\n\\nHonestly, as much as it is indeed your opinion, I don't think its helpful in any way to newcomers. In fact, I think you're just painting the skills related to tulpamancy as the villains of their story.\\n\\n**Edit:** reapproved the post. Sorry for the disruption.\", 'subreddit': 'Tulpas'}\n",
      "{'body': 'check your spelling bud', 'subreddit': 'Marvel'}\n",
      "{'body': \"she works in NYC, of course she's going to try to get discovered as an actress at every opportunity\", 'subreddit': 'funny'}\n",
      "{'body': 'Thaaanks', 'subreddit': 'RAOfCSGO'}\n",
      "{'body': \"Lol Orel's annoyed\", 'subreddit': 'Dodgers'}\n",
      "{'body': 'not really a reference, just look at her painting of berkut and rinea', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'lol its ok just scared the hell out of me!', 'subreddit': 'EscapefromTarkov'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '/r/ShittyMapPorn?', 'subreddit': 'im14andthisisdeep'}\n",
      "{'body': \"https://www.rei.com/product/828505/rei-co-op-camp-folding-cot\\n\\nIt's a super basic platform for sleeping on.\", 'subreddit': 'politics'}\n",
      "{'body': 'What the fuck is that supposed to mean?', 'subreddit': 'AskLibertarians'}\n",
      "{'body': \"It's actually a percentage slider.\", 'subreddit': 'ProgrammerHumor'}\n",
      "{'body': 'Shitty doctors', 'subreddit': 'todayilearned'}\n",
      "{'body': 'great drawing.', 'subreddit': 'Shadowverse'}\n",
      "{'body': 'No', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'Jeremy and I had already left the band by then. That sucked to hear about those thieves. ', 'subreddit': 'IAmA'}\n",
      "{'body': 'just shave your face period. Why does every second urbanite today look like a cast member of chapo trap house', 'subreddit': 'neoliberal'}\n",
      "{'body': \"IE: That the planets Luke visited in-story before landing on Ahch-To - as seen briefly in TFA with the whole map subplot - were important for his journey to said planet where the First Jedi Temple resides. (Couldn't really explain that in a headline.)\\n\\nAlso, it's been ages since we had a thread on one of his tweets, so I figured that it's worth making one - especially considering that there's not a lot on r/TheSupremePablo right now. I'd really like to figure out the specifics of Luke's journey.\", 'subreddit': 'StarWarsLeaks'}\n",
      "{'body': \"Not all hero's wear capes.\", 'subreddit': 'detroitlions'}\n",
      "{'body': 'Yeah they an be very strict. I love Columbus because of all the brick buildings and my family is here ', 'subreddit': 'funny'}\n",
      "{'body': 'Very hot ', 'subreddit': 'twinks'}\n",
      "{'body': 'Is it on youtube yet?', 'subreddit': 'Strongman'}\n",
      "{'body': \"Not surprised you can't remember. Adrenaline fucks with the memory.\", 'subreddit': 'motorcycles'}\n",
      "{'body': 'It should be noted that it probably only works if most of your pelvis isnt covered by your belly.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I suggest glassesshop.com. You can get your first pair for free and shipping is only $5. You can add anti reflection for $5 more. ', 'subreddit': 'freebies'}\n",
      "{'body': \"RT I think is the same as Mouse 1 in the control scheme. So he's saying don't spam beam \", 'subreddit': 'Overwatch'}\n",
      "{'body': 'You forgot to post your IGSRep page. I have tried to find it for you [Click Here](http://www.reddit.com/r/IGSRep/search?q=DogsOnWeed%27s+IGS+Rep+Page&amp;restrict_sr=on).\\n\\nI have removed this thread. You are required to have a IGSRep page and post it in your thread to trade on this subreddit. Feel free to repost this trade and add your IGSRep page. If you cannot repost this, please message the mods and we will see about reactivating this thread. It is always required to post your IGSrep page, but once you upgrade to \"Trader\" flair Automod will become a little more forgiving.\\n\\n**[Click here to create a IGSRep!](https://www.reddit.com/r/IGSRep/wiki/index)**\\n\\n\\n*Tip: Do not use URL shortners*\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/indiegameswap) if you have any questions or concerns.*', 'subreddit': 'indiegameswap'}\n",
      "{'body': \"Yep. The auction house will be in this game just says coming soon at the moment. They're probably waiting til Tuesday to activate it.\", 'subreddit': 'forza'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnotht2/):\\n\\nEXCELLENT BOOK', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"It's like Marshall is infecting them all\", 'subreddit': 'rugbyunion'}\n",
      "{'body': \"More importantly, where's the turkey?\", 'subreddit': 'ExpectationVsReality'}\n",
      "{'body': 'Lol', 'subreddit': 'dankmemes'}\n",
      "{'body': 'Scathatch or whatever the crazy sex witch was called in Roanoke.\\n\\nLady Gaga played her during the first part of the season, which was the dramatic reenactment show. So they\\'re saying what if Gaga herself was an in universe actor playing that part on the \"show\". ', 'subreddit': 'AmericanHorrorStory'}\n",
      "{'body': 'All the feels.', 'subreddit': 'firefly'}\n",
      "{'body': '#FuuUuuUUuUUuuUUuUUuuuuck off!\\n\\nI *like* midichlorians!', 'subreddit': 'AskReddit'}\n",
      "{'body': '143413598| &gt; United States Anonymous (ID: eFvRVltr)\\n\\n&gt;&gt;143413258\\nYour gender hurt my feelings\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': '/r/politics basically blames everything on white males. I wish I could find somewhere to discuss politics without all that bullshit. Maybe you and me could start our own identity politics free sub reddit together.', 'subreddit': 'news'}\n",
      "{'body': 'https://www.youtube.com/watch?v=nLsCC0LZxkY in case you wanna see the video. ', 'subreddit': 'Drugs'}\n",
      "{'body': \"The pure joy on the guy's face lmao\", 'subreddit': 'MMA'}\n",
      "{'body': \"No, I'm saying many of them are repeat offenders, that's why they had to highlight two specific people and say they had equal backgrounds\", 'subreddit': 'nfl'}\n",
      "{'body': '😂😂😂😂😂', 'subreddit': 'nintendo'}\n",
      "{'body': 'Hmmm ok thanks,  one more thing, my mav lacks 4 skillups 3 on his s1 and 1 on his s2, do you think that will be a problem?', 'subreddit': 'summonerswar'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"I'm glad that you lived to tell the tale.\", 'subreddit': 'dirtyr4r'}\n",
      "{'body': \"Where's that Daily Dose photoshop\", 'subreddit': 'photoshopbattles'}\n",
      "{'body': 'Thank you for this.', 'subreddit': 'EliteDangerous'}\n",
      "{'body': 'Congrats on a year! ', 'subreddit': 'stopdrinking'}\n",
      "{'body': \"He's right, at some point it was patched and defenders got a bit weaker. They were still very OP, but they used to be absolutely insane on the first days/weeks.\", 'subreddit': 'FIFA'}\n",
      "{'body': 'Do you know why they removed it from rotation? Was there glitches or anything on that map ? ', 'subreddit': 'Rainbow6'}\n",
      "{'body': 'Seattle?', 'subreddit': 'nekoatsume'}\n",
      "{'body': 'FUCK OFF AND DIE', 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'That’s what I got on my first win... absolutely wounded.', 'subreddit': 'forhonor'}\n",
      "{'body': \"I wish there weren't any armed vehicles at all. This is Grand Theft Auto, not Twisted Metal. \", 'subreddit': 'gtaonline'}\n",
      "{'body': \"Few will try to paint a picture of perfect innocince. Most will present a facade of honesty by telling you about half the story, if the topic of their sexual history ever comes up. And at the opposite end of the spectrum are the few girls who will look you in the eye and tell you the (supposedly) entire nauseating truth, in which case you'll almost always wish they haden't.\", 'subreddit': 'Incels'}\n",
      "{'body': 'Kirk  bulking ', 'subreddit': 'CFB'}\n",
      "{'body': 'Easily doable if I sit and fix it in Lightroom. I was more looking at the colors and what the photo consists of to even worry about the horizon. Thanks! ', 'subreddit': 'Cyberpunk'}\n",
      "{'body': 'Ah, good to know. So many weird events with carousing.', 'subreddit': 'CrusaderKings'}\n",
      "{'body': 'To be fair, Troy gave Clemson a scare last year. They are competent.', 'subreddit': 'CFB'}\n",
      "{'body': 'Haha try again. Last time we were shut out AT HOME was 1923. ', 'subreddit': 'ockytop'}\n",
      "{'body': \"It was her idea but then they collaborated and we don't see the whole process we see a snippet \", 'subreddit': 'rupaulsdragrace'}\n",
      "{'body': 'Your desperation to seem edgy and relevant is only outdone with how pathetically sad this comment is.', 'subreddit': 'CHIBears'}\n",
      "{'body': \"Because they're heavies, same as her dodge attacks. Most characters sprints counts as lights. But as for why... I guess since she's an assassins and supposed to hunt down targets.\", 'subreddit': 'forhonor'}\n",
      "{'body': \"$10 to mail a single sheet of paper? Those were for everyone you rat fuck. \\n\\nYou're lying to yourself if you think a few dollars going towards holocaust awareness compensates for such a selfish act.\", 'subreddit': 'nathanforyou'}\n",
      "{'body': 'Dragging small scenes for too long as episode fillers', 'subreddit': 'AskReddit'}\n",
      "{'body': \"well yes but I'm talking about the retards here that are like 17 and claim to be incel, fucking lol at these literal children\", 'subreddit': 'Incels'}\n",
      "{'body': '\"Ya!\"', 'subreddit': 'CampHalfBloodRP'}\n",
      "{'body': 'Top 5 athlete? What are your standards to get into the top 5? \\n\\nHe might not even be the greatest athlete in America right now. I would argue that hes not even the best basketball player right now and then reluctantly agree that he still is, as of the last series he played. That says something but top 5 is rediculous.', 'subreddit': 'nba'}\n",
      "{'body': 'I never once got the impression that Alphamon was trying to protect Meicoomon.', 'subreddit': 'digimon'}\n",
      "{'body': \"Hmmm... I've had it all wrong this whole time. I swear they taught me in our public education system that we, as Americans, have the right to peaceful protests.\\n\\nI guess Kyrie is right. We really need to question/research everything that we're told.   \\n\\n* Edit:  Wait a second, after research I've confirmed that we do have the right to protest. So,  WTF  is traitorous about protesting? \", 'subreddit': 'bostonceltics'}\n",
      "{'body': '~$80-$100', 'subreddit': 'hardwareswapaustralia'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '..how?\\n', 'subreddit': 'eu4'}\n",
      "{'body': 'can anyone estimate whats the most likely way forward to fix that plane? they will try to replace engine on that remote location, or send it back on 3 engines to an AF/Airbus base? and how long will it take for it to be added back to service?', 'subreddit': 'aviation'}\n",
      "{'body': \"Whoosh! Nice job, fellow comet. I'm starting my own beard journey. I might just copy this way of showing progress\", 'subreddit': 'beards'}\n",
      "{'body': 'I guess the beauty and the beast rt score is close enough? \\n\\nWay off on spiderman though...', 'subreddit': 'RedLetterMedia'}\n",
      "{'body': \"Doesn't the study have to be done post-mortem?\", 'subreddit': 'nfl'}\n",
      "{'body': 'SoDak baby!', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'google'}\n",
      "{'body': 'Donate clan? Someone pulled the same thing in there.', 'subreddit': 'ClashRoyale'}\n",
      "{'body': \"That he's perfect and deserves a perfect person and i am far from perfect\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"You're the real hero u/Archaeo\", 'subreddit': 'hockey'}\n",
      "{'body': 'https://i.imgur.com/0HfzqNp.jpg\\n\\nIt finally came in ', 'subreddit': 'Miata'}\n",
      "{'body': 'Yep. As always its a case of *beware the simple narrative.*\\n\\n\"Evil breeders are churning out puppies with cleft pallets because people find them cute\" /gasp from audience.\\n\\nIf its simple and unnuanced then its probably bullshit.', 'subreddit': 'aww'}\n",
      "{'body': \"&gt; (I like Obama, but that Peace prize was a joke). \\n\\nTo be fair, I'm pretty sure Obama thinks the same thing\", 'subreddit': 'politics'}\n",
      "{'body': 'https://i.imgur.com/caMm6N8.jpg', 'subreddit': 'funny'}\n",
      "{'body': \"Damn, just paid for him and he's a beast. Enjoy!\", 'subreddit': 'NHLHUT'}\n",
      "{'body': 'This submission has been automatically removed because you did not include a question mark (\"?\") in your title. Reddit does not allow post titles to be edited, so if you would like, you can post the question again.  Please write your title in proper question format, and include a question mark, thank you.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*', 'subreddit': 'AskReddit'}\n",
      "{'body': \"[+LostVaultDweller](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokq7s/):\\n\\nYou can try to find an agent who pushes your novel to publishers. Look into that (obviously I'm expecting you to do a ton of research on your own. Just presenting an option you may have not known about)\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'When Porsche was added in FM6 this was one of the first things I noticed about every 911. They handle differently than any other car in the game because of the rear engine layout. Once you figure out how to modulate the throttle and brake through corners in order to get your suspension leveled correctly they are ridiculously fast cars. \\n\\nI also had a ton of trouble driving the GT2 RS in the demo but once I figured out that you drive this car with the throttle it became incredibly fast. ', 'subreddit': 'forza'}\n",
      "{'body': 'Hey this is an absolutely fantastic prompt, with the exception of the unnecessary apostrophe in the title.\\n\\nTwo won\\'t play **food chain magnate**. They got absolutely demolished in their first game, to the point where they were \"irrelevant bystanders\" after turn 3. Never again, they said.\\n\\nOne won\\'t play **alchemists** because it reminds him of being in college and he goes to game night to forget school and work.\\n\\nOne won\\'t play **resistance or secret Hitler** because he\\'s a (young) retired Afghanistan war vet with ptsd and those games trigger him too much. He gets way to stressed out.\\n\\nTwo friends won\\'t play **arboretum** , and these are two really good gamers, not snowflakes. But the hand restriction is so stressful that they can\\'t have fun with it. Of those two, one won\\'t play **bazaar** because the thought process required to play optimally is so brain burny that he doesn\\'t think it\\'s worth it. That being said, he\\'ll play fcm and battlecon with me all day, ironically.\\n\\nAnother friend has kids 10, 8, 6, who all like game, but they won\\'t play **escape curse of the temple** again because the last game ended in tears, shouting, and rage. The mom and 10 year old did fine, but the 8&amp;6 struggled, and I split the group up poorly. So idiotic, I should have been with the 8&amp;6, not with the 10.\\n\\nI almost won\\'t play **splendor** anymore because it removes all socialization and leads to some outburst when someone grabs the card someone else wanted.\\n\\nOne guy won\\'t play **acquire** because he ran out of money real fast, couldn\\'t buy stocks for like 6 turns, didn\\'t have the tiles to force a merger, and walked away feeling like it was a luck fest that punished you too hard for being careless with your money. Dunno, me neither..?\\n\\nOne won\\'t play **Chinatown** because the endgame is too mathy and predictable (easy to solve).\\n\\nAnd another won\\'t play **dominion** anymore because it\\'s too themeless and too pure, ie he prefers Clank, where deck building is a means to an end, not an end. By the way, he\\'s never played a dominion expansion.\\n\\nI won\\'t play any of the **legendary** series games, **monopoly, munchkin, galaxy trucker, mice n mystics, or Robinson Crusoe** because I don\\'t feel I have enough player agency. I feel like the game is playing ME', 'subreddit': 'boardgames'}\n",
      "{'body': 'Bet anything they did ', 'subreddit': 'politics'}\n",
      "{'body': '[deleted]', 'subreddit': 'NoFap'}\n",
      "{'body': \"I saw John Cena get hit by a chair with my own eyes,  and dumbasses still say it's fake\", 'subreddit': 'nba'}\n",
      "{'body': \"Power Jump's number calculation is total malarkey, it's supposed to deal one Normal Jump's damage x2 +2 but it instead deals around 80-90% a Normal Jump's damage, past 10 damage it becomes apparent that it's terribly broken.\\n\\nSpeaking of dumb active Badges, why on earth do you get Hammer Throw in Chapter 4? Vivian and Flurrie have already replaced it, and Spike Shield replaces it for Mario after Chapter 4 is over, so what's the point?\", 'subreddit': 'papermario'}\n",
      "{'body': '[deleted]', 'subreddit': 'Guildwars2'}\n",
      "{'body': 'As a capybara main, this... terrifies me on so many levels.', 'subreddit': 'Tierzoo'}\n",
      "{'body': \"Your reality TV garbage is pure cultural marxist bullshit as well. A couple dozen people fighting over someone who isn't even a prize? Way to cheapen and destroy what relationships are supposed to be about. Pushing degeneracy is what the left is all about - and cheap meaningless relationships are part of destroying marriage and family as well.\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'What does jacob do to benefit the streams? Exactly. Delete this post.', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'Please tell how?', 'subreddit': 'BikiniBottomTwitter'}\n",
      "{'body': 'DMac another try saver', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'I would *love* to see them.', 'subreddit': 'Music'}\n",
      "{'body': 'Yes they both were at one point. ', 'subreddit': 'montreal'}\n",
      "{'body': 'exactly ', 'subreddit': 'StarVStheForcesofEvil'}\n",
      "{'body': \"Fantastic, well done!\\n\\nYou are probably far more critical of your playing than anyone else is, at least if they are not professional musicians. Chances are it sounded better to them than you think.\\n\\nI'm all scientist, but would exchange it all to be able to play piano really well. I've tried to learn (seriously) but it's simply not in me to be able to create music beyond the most basic level. I'm resigned to listening.\", 'subreddit': 'aspergers'}\n",
      "{'body': '143417319| &gt; United States Anonymous (ID: Jx077B7U)\\n\\n&gt;&gt;143417265\\nLMAO\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': '[deleted]', 'subreddit': 'SubredditDrama'}\n",
      "{'body': \"I can't minimize comments unless it's a top level comment. This app is terrible I wish I kept alien blue\", 'subreddit': 'redditmobile'}\n",
      "{'body': \"I get the feeling we're already seeing that\", 'subreddit': 'CFB'}\n",
      "{'body': \"Wow this sounds a lot like me and my very recent now ex. We broke up a lot too and always got back together and had exactly the same problems you did and I said the exact same thing she did. The hardest thing is you two were best friends and now coping without that is so difficult. I don't really think there is a best way to move on other than time. Time is key to help yourself get over this. \\n\\nI just sent an email to my ex explaining that what we had was more of a friendship than a relationship. We were together since we were 14 and had been together 5 years. And we did love eachother but sometimes love is not enough. Have some you time and figure out who you are again before you even consider getting back with her because you'll hurt eachother again. Easier said than done, believe me I know!!\\n\", 'subreddit': 'relationship_advice'}\n",
      "{'body': 'You are completely off base. ', 'subreddit': 'btc'}\n",
      "{'body': '😉', 'subreddit': 'Random_Acts_Of_Amazon'}\n",
      "{'body': \"Yeah, that certainly removes some of the extra work. I'll have to start thinking up Foci for my elves, dwarves, and porcine-humanoids!\", 'subreddit': 'numenera'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Beck was here from 2010-2014. Get your facts straight before mouthing off.', 'subreddit': 'Huskers'}\n",
      "{'body': \"Maybe you should post a screen shot where you have gotten to round 50 with no power in an hour. I can assure you, you have not.\\n\\n\\n\\nI have an excel sheet I tracked round times and cryptids spawned every round. My numbers are solid and correct. \\n\\n\\n\\nLike I said, for high rounds, past round 80, then yes, use no power because rounds will stay at 2:20 seconds after round 21 to 1000. \\n\\n\\n\\nIf you're grinding for keys, it's the other maps. Literally impossible to get to round 50 in 1 hour on Beast. 41 all day on other maps. 32-35 on Beast no power. \\n\\n\\n\\nhttps://i.imgur.com/4zjUgRP.jpg\", 'subreddit': 'CODZombies'}\n",
      "{'body': 'Working as of 8 EST', 'subreddit': 'soccerstreams'}\n",
      "{'body': \"Holy fuck Mississippi State. You're going to give Michigan a run for its money with that 100+ yards in penalties\", 'subreddit': 'CFB'}\n",
      "{'body': 'Pretty sure that’s BYU ', 'subreddit': 'hmmm'}\n",
      "{'body': \"That's what I was saying before... the curve seemed off.\", 'subreddit': 'mapmaking'}\n",
      "{'body': \"I mean, g2 and faze are both 5 star player teams and they're both top 4. \", 'subreddit': 'GlobalOffensive'}\n",
      "{'body': 'Y tho?', 'subreddit': 'CFB'}\n",
      "{'body': 'President Fujimori of PERÚ. He ended hyper inflation, during his government terrorism ended (almost completely), and Peru started becoming one of the best places to be in South America. \\nThen he became corrupt, stole millions of dollars, changed the constitution to remain in power for 3 terms (only 2 allowed legally), among other things.\\nHe flew the country to Japan (he had double citizenship). Then he was asked to leave Japan, went to Chile, and then was deported to Peru. He was condemned to 25 years in prison for corruption and human rights violations.', 'subreddit': 'history'}\n",
      "{'body': 'That was kind of my thought, as well. Social interaction with like-minded individuals as opposed to the non-empathetic masses. I imagine having a place like that would help a lot of people. \\n\\nDefinitely have some sort of filter system at the door, too.', 'subreddit': 'SanctionedSuicide'}\n",
      "{'body': \"It's been broken for a long time now. But Obama wasn't the result of the broken system; he won in spite of it.\", 'subreddit': 'worldnews'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol9ox/):\\n\\nAgents, traditionally, are the first step to pushing to publishers. ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Damn it. My friend got them Y E E Z Y Boost 350 V2. I need them Y E E Z Y S man ', 'subreddit': 'dankmemes'}\n",
      "{'body': 'Thanks that worked. I just talked to everyone eventually found out I had to talk to the groundskeeper to open a quest to the guy in the top middle.', 'subreddit': 'NintendoSwitch'}\n",
      "{'body': 'Lmao at that patchy beard, it looks like middle schoolers growing out their peach fuzz.', 'subreddit': 'hiphopheads'}\n",
      "{'body': 'Legitimately see people completely missing the point on my FB feed saying that “Tebow did it before it was cool and was mocked, how is this any different?”. \\n\\nThe bible belt is a weird place to be.', 'subreddit': 'news'}\n",
      "{'body': 'Placebo', 'subreddit': 'AskReddit'}\n",
      "{'body': 'You betcha', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'CFB'}\n",
      "{'body': \"Gabbadiggio's Fresca House of Muzzerel Primero Italiano. \\n\\nDunno if the Valley is too far away from you or not, just stay on the 5 until Castaic Junction. Free garlic bread!\", 'subreddit': 'LosAngeles'}\n",
      "{'body': 'Samsung galaxy S5, A few dings and wear and tear along the bezel but otherwise in pristine condition. One very faint scratch on the screen from rubbing against something in my bag... more of a scuff than anything.', 'subreddit': 'GalaxyNote8'}\n",
      "{'body': 'Good lord, like two thirds of these comments are in favor of what he did. Most socially inept, Reddity thread ever.', 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'The bear had it coming. Just ask anyone.', 'subreddit': 'cats'}\n",
      "{'body': \"Ohhhh i see haha what a funny coincidence lmao, but hey! it's the thought that counts!&lt;3\", 'subreddit': 'RyzeMains'}\n",
      "{'body': \"Mansplaining? Sorry. Not a native English speaker. What's that mean?\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'How long does that free supercharging last?', 'subreddit': 'teslamotors'}\n",
      "{'body': \"I assume it's going to be a while, but considering the changes to headphones, I really do not want to be 'wasting' my RP. I know I 'could' buy headphones, but I only have about 800 diamonds that I may need to spend for the LE theme, so buying headphones is out of the question :( Even then, buying headphones is barely sufficient compared to how much I used to farm (easily 100+ headphones used per day on relaxing days)\", 'subreddit': 'SuperstarJYPNation'}\n",
      "{'body': 'hes still got 30 upvotes ', 'subreddit': 'politics'}\n",
      "{'body': 'Melee Llama', 'subreddit': 'FORTnITE'}\n",
      "{'body': 'Triad and a dump pouch', 'subreddit': 'Nerf'}\n",
      "{'body': 'Wtf it looks like they put him at 2x normal speed and left the other guy the same', 'subreddit': 'sports'}\n",
      "{'body': 'Wow, just wasted 20 minutes of my life poking around this blog. He never directly approaches questions. He just bitches about the wording they chose, and cries about how Jeremy called the Maxwell institute unofficial apologists. \\n\\nI love how he gets upset that Jeremy won\\'t accept the standard apologist lines, and says \"well I\\'ve got nothing new to tell you, fairmormon and farms already gave the answers.\" Then why does your blog exist Mr Cornell? Is it, perhaps, priestcraft? ', 'subreddit': 'exmormon'}\n",
      "{'body': '[deleted]', 'subreddit': 'DirtySnapchat'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'I\\'m sorry, but that analysis misses the fact that Trump wasn\\'t the obstacle to ObamaCare repeal. Trump went along with whatever Ryan or McConnell proposed, despite that being contrary to his campaign promises. Same will be true with tax \"reform\" (ie. tax cuts). Trump will promote and support anything Ryan puts forth, as long as he can claim victory if it passes. He has no agenda except his image. ', 'subreddit': 'worldnews'}\n",
      "{'body': 'Yeah, it seems like you already got a lot of solid advice on what tests and such are next to try. I hope you guys have some better luck soon. I know how not fun the \"trying\" part can be after those first 6 months! ', 'subreddit': 'relationships'}\n",
      "{'body': \"Congrats on the pull! That's some decent coin right there!\", 'subreddit': 'MaddenUltimateTeam'}\n",
      "{'body': 'And you see i have the REM wire in the back of the radio and the REM wire from the amp twisted together but there is no transfer I guess. I got some electrical tape gunk on it. Could it be That or just a faulty wire? I could restrip it I guess. ', 'subreddit': 'CarAV'}\n",
      "{'body': \"The system is broken because Americans are constantly fooled into thinking that the Democrat and Republican politicians have their best interests at heart despite constant proof to the contrary, and yet still heatedly will support them because... well I don't even really know why to tell you the truth.  Political parties should be abolished in favor of electing people.  People who actually care about the population and not just about filling their coffers.  Trump just took advantage of the broken political system we already have.\\n\\nEdit: A word poorly spellchecked.\", 'subreddit': 'worldnews'}\n",
      "{'body': 'Veruna forced a woeful smile. It quickly faded away as he creeped up to Jalvere and whispered into his ear.\\n\\n\"Nute Gunray made me kill him. It was the only way.\"', 'subreddit': 'galactic_senate'}\n",
      "{'body': 'please include links to your TSV threads - I need to see that you are an active hatcher in the community - and are these all for FFA? or do any of them match your TSV?', 'subreddit': 'SVExchange'}\n",
      "{'body': \"Title--Battle of Naguabo\\n\\n\\nGenre--Military Sci-Fi\\n\\n\\nWord Count--5233\\n\\n\\nType of Feedback--General feedback is appreciated, but very interested in thought about the action sequences.  This is a middle chapter from a novel-length work, so you're getting things bit out of context.  There's also profanity and graphic violence, so if that's not your thing, you may want to stay clear.\\n\\n[Link to Document](https://docs.google.com/document/d/1X_PQf-j3XKB1IFaJVAFpNX2UkmUJml2lVtsxuedCaIQ/edit?usp=sharing)\", 'subreddit': 'writing'}\n",
      "{'body': \"Agreed. In one of my current fics, the age difference between the primary ship is approximately eighteen years (early 20s and mid-40s), so... it's a thing, lol.\", 'subreddit': 'FanFiction'}\n",
      "{'body': 'I recently found out I can\\'t donate blood anymore, and was pretty gutted as it\\'s an amazing literally life saving thing to do. So despite a lifelong fear of needles and all things related, my incredible boyfriend is donating for the first time tomorrow, to \"make up\" for the blood I can no longer donate.\\n\\nMy advice would be drink plenty of water and have a filling meal, and try and eat something sugary like a chocolate bar an hour or so before (also helps with my nerves). The most painful bit is ripping off the plaster after wards - I promise!', 'subreddit': 'AskReddit'}\n",
      "{'body': 'In Flames and Mastodon. ', 'subreddit': 'Music'}\n",
      "{'body': 'Hello dog!', 'subreddit': 'aww'}\n",
      "{'body': 'one random please', 'subreddit': 'edc_raffle'}\n",
      "{'body': 'No problem! Thank you for the feedback! Working on being more descriptive, very wide variety of guests for the clean radio to the comedian unfiltered radio. ', 'subreddit': 'buffalobills'}\n",
      "{'body': 'No tire culpa para afuera? dije que aca son muy forros apra arriesgarse asi y que no nos darian mucha bola. L ocual es verdad porque aunque seamos del G20 honestamente nuestra \"influencia\" no se nota.', 'subreddit': 'argentina'}\n",
      "{'body': 'SO SORRY sold but forgot to remove post', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'If you accidentally go the wrong way trying to leave the taint, you might run into a starfish. For some of us, that might actually be the correct way', 'subreddit': 'WorldOfWarships'}\n",
      "{'body': \"I'm into it and you bring up great points there.  But try convincing an open borders or La Rasa advocate that English be a requisite and the official language, heads will explode. \", 'subreddit': 'SeattleWA'}\n",
      "{'body': \"I've heard good things about that one too. I've actually owned the Intel 7620 (I think) years ago. I even got use out of the bluetooth.\\n\\nThe gigabyte one seems to be the newer version of that one.\\n\\nJust curious if I'd see any benefit in spending more.\\n\\n------\\n\\nAnd I do use pcpartpicker... Links on amazon were just quicker here. Thanks.\", 'subreddit': 'buildapc'}\n",
      "{'body': 'HesaJew would get thrown off reddit tho', 'subreddit': 'survivor'}\n",
      "{'body': 'Lonegunman5....304 hunter', 'subreddit': 'Fireteams'}\n",
      "{'body': 'Rip off my nut sack and call me Lord Varys can we get on with the game at the scheduled time please! FUCK ', 'subreddit': 'CFB'}\n",
      "{'body': '[removed]', 'subreddit': 'nbastreams'}\n",
      "{'body': 'Great extension there by Ward', 'subreddit': 'CFB'}\n",
      "{'body': 'Brandon Jennings belongs pretty high on that last like top 5 ', 'subreddit': 'nba'}\n",
      "{'body': 'I\\'m just blown that as soon as the left stops telling us to boycott the team suddenly the right, who loves to pretend they aren\\'t just as prone to flying off the handle over every little thing in their newsfeeds, is suddenly telling me to boycott the team over this flavor of the month outrage. \\n\\nIf this doesn\\'t prove both sides are equally retarded I don\\'t know what will. Racial inequality is a massive problem that nobody wants to address, and these NFL protests are doing a good job of highlighting that. Unfortunately, it\\'s going to take a few more decades of \"liberal propaganda\" for people to realize that most racism isn\\'t in-your-face klansman shit, and this protest is doing nothing to address that divide. ', 'subreddit': 'Redskins'}\n",
      "{'body': 'I always cringe when announcers talk about how good Davis is or how great his career has been. Not putting the blocked kicks on him but close games scare me because of the mix of our iffy red zone offense and terrible kicking game.', 'subreddit': 'CFB'}\n",
      "{'body': 'I know what you mean. I dont like to do the math:( Cant believe I have been poisioning my body for this long.', 'subreddit': 'stopdrinking'}\n",
      "{'body': 'FYI - those are for the little cups that are used during communion', 'subreddit': 'cade'}\n",
      "{'body': '143413518| &gt; United States Anonymous (ID: qrXQpZdq)\\n\\nKerry\\nObama\\nObama\\nTrump\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Yeah okay go do all of vHoF without a food buff and let me know how you feel about it.', 'subreddit': 'elderscrollsonline'}\n",
      "{'body': '[deleted]', 'subreddit': 'Guildwars2'}\n",
      "{'body': \"And what's that reason if you want to answer?\\n\\nI mean Kickstarter is not really preordering, it's mostly pledging money for a thing you want to support.\", 'subreddit': 'NintendoSwitch'}\n",
      "{'body': \"I'm not a troll. This is very real and Monday we'll see how and if this employment situation continues.\\n\\nShe's not cleaning any bottles. My daughter doesn't spit up exorcist style. She'll dribble some extra milk down her chin while eating so that's a quick wipe and it's done. There have been no diaper blowouts. Dirty onesies get tossed into a laundry basket and new ones are always available because I'm the one who has washed them and folded them. There is no diaper laundry because we use disposable ones and the disposable ones are what's taking put most of the space int he garbage that she's not taking out.\\n\\nWith how much time she spends texting on her phone and then streaming TV shows, oh she gets more than 15 mine very 2 hours and she's free to eat lunch whenever the baby's down for a nap which means hours worth of lunch time.\\n\\nI'm far from over working this girl. I'm trying to get her to actually do some work.\", 'subreddit': 'Nanny'}\n",
      "{'body': '[deleted]', 'subreddit': 'LateStageCapitalism'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Love is, uh, hard to explain.  I guess.\\n\\n[spoiler](/s \"Most of that route doesn\\'t even feel like they\\'re romantically attracted, like in Hanako\\'s.  In that route it\\'s all in act 4 and hinges one of three ways based on two choices.  I liked how that was handled, felt very real. In Rin\\'s, Hisao monologues about how he\\'s not sure how or why he fell for Rin, it just happened.\") ', 'subreddit': 'katawashoujo'}\n",
      "{'body': '\"street food in Akihabara\"\\n\\nthat\\'s when I knew this person has no clue as to what the fuck is anywhere ', 'subreddit': 'JapanTravel'}\n",
      "{'body': 'Well ya thats why I dont understand, you can get the same chain going with OK innate skills,  dont see the reason why you would put a pod on him', 'subreddit': 'FFBraveExvius'}\n",
      "{'body': 'Wow, I love these! Great job, your execution is awesome! ', 'subreddit': 'RandomActsOfPolish'}\n",
      "{'body': '[+eqleriq](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoj457/):\\n\\n&gt; My g.f. is an extremely good writer but the distance between \\'typing in a word document\\' and \\'being published\\' seems intimidatingly vast to her.\\n&gt; \\n&gt; What would you say where the steps (big or small) from aspiring to published?\\n\\nHuh? You can upload a PDF to amazon and be \"published\"\\n', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '[deleted]', 'subreddit': 'brasil'}\n",
      "{'body': 'Eh, maybe during the regular season, but they were playing pretty well with Rondo in the first games of the playoffs until they lost him', 'subreddit': 'nba'}\n",
      "{'body': \"You'll be back. They all come back.\\n\", 'subreddit': '2meirl4meirl'}\n",
      "{'body': \"That escalated at the end.  I'll allow it.\", 'subreddit': 'Whatcouldgowrong'}\n",
      "{'body': 'I could see that but then you have to take into account that the computer would have to have anticipated him sitting on the desk ahead of time to ensure that his orientation from the outset was such that both his desk and her desk would align.', 'subreddit': 'DaystromInstitute'}\n",
      "{'body': \"Jay 305 - Thuggin\\n\\nGame - My Flag\\n\\nMeek Mill - Body Count\\n\\n7 Mile Clee - Famous Where I'm From\\n\\nWilly Northpole - Straight Jacket\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"Sorry, I was out today! Let me know when you're next available!\", 'subreddit': 'SVExchange'}\n",
      "{'body': 'It sounds like a therapist would be something to consider. Mine helped me a great deal. ', 'subreddit': 'ADHD'}\n",
      "{'body': \"Apparently yes, she was happy that i didn't learn anything worse at school! Jokes on her I never learn anything there anyway! \", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Yes.', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'Because they are there and weaken the glass. ', 'subreddit': 'LifeProTips'}\n",
      "{'body': 'I will, bc we were just approved to pick him up tomorrow. Already in love.', 'subreddit': 'aww'}\n",
      "{'body': \"Au final très peu d'info actuellement. On verra bien une fois que l'enquête aura avancée.\", 'subreddit': 'france'}\n",
      "{'body': 'If you tag more than 3 people, they wont get notified btw. ', 'subreddit': 'marvelstudios'}\n",
      "{'body': 'Ah must be thinking of walls', 'subreddit': 'diablo3'}\n",
      "{'body': \"If you're doing your schoolwork, it's not procrastinating anymore. If you're not procrastinating, you can't be doing your schoolwork. So I guess you are procrastinating after all. Wait a minute... ;-)\\n\\nI actually did do my test before the FDA crackdown, but after that the 23andme web interface changed for me along with all new customers. However, I believe all users can still [download their raw data](https://customercare.23andme.com/hc/en-us/articles/212196868-Accessing-and-Downloading-Your-Raw-Data), and then there are is some other software that could be used to analyze that data ([example](http://medicalfuturist.com/analyse-your-dna-in-your-living-room/)), which maybe skirts the FDA rules by virtue of being more DIY. I've always thought it would be interesting to explore, but haven't gotten around to it amongst everything else.\\n\\nThanks for pointing out the genes in the sidebar, somehow I just skimmed the lefthand text and totally missed those despite them being RED!\", 'subreddit': 'CPTSD'}\n",
      "{'body': \"I'm surprised they don't run more with Fitzgerald. Would keep the Defense more honest down to down. \", 'subreddit': 'CFB'}\n",
      "{'body': 'Not gonna lie, this is probably the weakest korea has been or is going to be in a while with the implosion of KT; SSG and SKT are experiencing slumps and are wildly inconsistent and have crown and huni as liabilities, as well as the fact that LZ is a very inexperienced roster that may choke under pressure', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"All sakki is, is believing your own feints. Believing your own feints makes your body move in such a way that the opponent TRULY believes that a punch not only is about to be thrown, but is *actually* thrown, and moves accordingly.\\n\\nIt's just a higher level version of a deceptive technique. Actual feinting does work like this. The more convincing you can make it, the better, and the lies that are the most convincing are the ones that you can get yourself to believe, even if just for a moment. Long enough for the opponent to react in the wrong way.\", 'subreddit': 'hajimenoippo'}\n",
      "{'body': 'how bad is wvu engineering?', 'subreddit': 'EngineeringStudents'}\n",
      "{'body': \"\\nAs a gay man, I've sat through 30+ years of watching heteros get lovey-dovey in Star Trek, but if you can't sit through a bit of man-on-man affection, it is totally your prerogative to stop watching the show. \\n\", 'subreddit': 'StarTrekDiscovery'}\n",
      "{'body': 'Thinking of getting my wife the Lelo wand but anytime I go in to have a look at one, the display model is always out of damn battery.\\n\\n&amp;nbsp;\\n\\nIs it a rotary vibration or is it magnetic? I want to get her one of the ones that leave that super weird tingle, they are almost unbearable to touch with your hand for any length of time due to the sensitivity of the nerves.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'As fuck.', 'subreddit': 'nba'}\n",
      "{'body': 'Honestly Im suprised people still think Gamepedia is SOMEHOW relevant. They seem unreasonable every time a new unit gets released. Pure Joke. ', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'I agree with what you\\'re saying, but at the same time in this situation the only time oxford commas would ever be truly necessary would be either in a complex sentence or if one is writing to a person who lacks the ability to use context clues.\\n\\nIn your example, we\\'d start with the base sentence:\\n\\n\"I had dinner with my uncle and a madman and an accordion player.\"\\n\\nThis obviously isn\\'t proper grammar, so we\\'d replace the first \"and\" with a comma since we\\'ve created a list:\\n\\n\"I had dinner with my uncle, a madman and an accordion player.\"\\n\\nIn this case anyone with a grasp on context clues would understand what one was trying to say. If this was being said aloud then it would be the burden of the person pronouncing it to clarify what they were saying.\\n\\nI will submit that there are situations where an oxford comma is necessary, but they are few and far inbetween. Meanwhile our society expounds into our heads at a young age that we must always use the oxford comma whenever we write regardless of the situation.\\n\\nI still use them, but only because of I\\'ve been oppressed by our society which worships them to the point where that oppression is now internalized in all within. Break the cycle, fight the Punctriarchy.', 'subreddit': 'dirtyr4r'}\n",
      "{'body': 'Vocês reclamam de tudo. Esse banner tá mil vezes melhor que aquelas imagens boring de cidades cinzas. Depois que a pessoa entende onde deve clicar, só erra se tiver probleminha de coordenação. ', 'subreddit': 'brasil'}\n",
      "{'body': \"If you went to international school, I'd imagine it's probably passably American to any Brit, and to at least a substantial chunk of Americans. If you went somewhere with regional accents (Great Lake states or the South) they may notice something off but again this all totally speculation and guesswork. \", 'subreddit': 'AskEurope'}\n",
      "{'body': 'im cool with that. ', 'subreddit': 'nba'}\n",
      "{'body': \"Not far, maybe a 10 minute drive without traffic (though given that it's I4, when does that ever happen?). Like I said, we really have no excuse other than laziness for not going to a Solar Bears game.\", 'subreddit': 'hockey'}\n",
      "{'body': \"Those shirts can be used to mop up Maria's waters. \\n\\n\\n\\n\\n\\nMaybe that is their plan with those shirts... hmmmm\\n\\n\\n\\n\\n\\nOh nevermind. It's definitely Trump being racist\\n\\n\\n\\n\\n\\nProblems would be gone without Trump like Obama handled the floods in Louisiana or Sandy.\\n\\n\\n\\n\\n\", 'subreddit': 'news'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Friend of mine\\'s family owns a pumpkin patch. They just opened it up this week. Lo was in heaven with all the \"balls\" everywhere.  [Definately one for the memory book.](http://imgur.com/a/O2g1I)', 'subreddit': 'beyondthebump'}\n",
      "{'body': 'Steam treated basins.  Then off to the slaughter.', 'subreddit': 'videos'}\n",
      "{'body': 'Not OP, but also a unicorn.  A friend and coworker was part of a couple but I didn’t know her very well.  He and I were texting one night and he jokingly (or so I thought) asked if I’d join them for a threesome.  I laughed it off, saying maybe if his girl asked I’d consider it.  Five seconds later she fb messages me saying he wasn’t joking but we should all get to know each other first.  We started hanging out a lot and they told me about the lifestyle and I met a lot of their like-minded friends.  The opportunity to have friendships and safe sexual relationships without the burden of a daily relationship was heaven to me after a divorce.  Three years later:  I have lots of fun, lots of great friends, the original couple and I still talk almost daily and hang out in some form every week.  ', 'subreddit': 'sex'}\n",
      "{'body': \"[+NotClever](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnojicv/):\\n\\nI was going to say that the parent was probably talking about the journey to finding a publisher for the book, but it's not entirely clear that OP's book is not self published. Clearly there are paper copies, but the website doesn't say anything about a publisher and a google search doesn't turn up any results from publisher websites, so maybe OP is just self published?\\n\\nEdit: Further down OP does confirm that this is self published.\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Definitely \"Piranha\" !!', 'subreddit': 'horror'}\n",
      "{'body': 'Ok, thanks again!', 'subreddit': 'Shadowverse'}\n",
      "{'body': \"Stay strong, my friend. What I found helpful when I start to get urges is to get up, wash my face with cold water, and read a good book.\\n\\nif you don't have a book to read, My Man Jeeves by Wodestone is a pretty funny book, you can pull it off of gutenberg.org and have a read - it'll certainly take your mind off of porn!\", 'subreddit': 'pornfree'}\n",
      "{'body': 'Same here tbh, I always make sure the grammar etc. are as perfect as can be.', 'subreddit': 'AmazonMerch'}\n",
      "{'body': \"I don't know what you're talking about, /u/budude2, it's right there on the 'hot' page.\\n\\n\\\\&gt;__&gt;\", 'subreddit': 'baylor'}\n",
      "{'body': 'haha OH that makes sense :P\\n\\nYeah idk where another option is\\n', 'subreddit': 'spiderbro'}\n",
      "{'body': 'Nice way to look back for the ball.  Hope that continues. ', 'subreddit': 'CFB'}\n",
      "{'body': 'ALMOST. ALMOST. ALMOST.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'That requires effort beyond reading a price tag.', 'subreddit': 'gaming'}\n",
      "{'body': 'Okay, så er der åbenbart en del jurister på samme tid. Godnat :o)', 'subreddit': 'Denmark'}\n",
      "{'body': 'Depends - pve content like a fractal for deadeye is a big no. Chronmancer is always needed in pve. I don’t play wvw so I can’t help you there, but I love my deadeye for pvp. For looks, you can always by outfits ', 'subreddit': 'Guildwars2'}\n",
      "{'body': 'StalkerSOUP is a hardcore overhaul mod, definitely not for beginners.', 'subreddit': 'stalker'}\n",
      "{'body': 'Be careful around Termini train station at night. ', 'subreddit': 'rome'}\n",
      "{'body': 'Thanks man. Good day to you ', 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'Its not gay if its rape', 'subreddit': 'worldnews'}\n",
      "{'body': 'Well that was time well spent. How far have you gotten in F/SN?\\n\\n&gt; Actually I underslept so much I am currently sick.\\n\\n[](#k-on-hug)', 'subreddit': 'anime'}\n",
      "{'body': \"that's pretty cheap, the whole registration and inspection process cost me ~$300   \\na small price to pay muh rooooooads\", 'subreddit': 'Anarcho_Capitalism'}\n",
      "{'body': '[deleted]', 'subreddit': 'unpopularopinion'}\n",
      "{'body': '...and?', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Go Hokies. ', 'subreddit': 'CFB'}\n",
      "{'body': \"Sounds like an easy way to make money. \\n\\nRich guy: are the satellites up?\\n\\nEngineers: uhh..... yeah. They'll be there in 40 years. \", 'subreddit': 'todayilearned'}\n",
      "{'body': 'I\\'ve seen every episode at least 15 times no joke, some more.\\n\\n\"Stop this maadnesss! LOOOOOK AT YOURSELVES!!!!\"\\n\\nClassic.', 'subreddit': 'childfree'}\n",
      "{'body': '[removed]', 'subreddit': 'CatastrophicFailure'}\n",
      "{'body': \"Yeah, I don't know everything that went on there. Just that they implemented the policy, most people stopped going there and after a few months they changed it back. \", 'subreddit': 'AskReddit'}\n",
      "{'body': \"THAT'S MY DOG'S NAME!\", 'subreddit': 'CFB'}\n",
      "{'body': 'looks very airy for an Indica, whats the density like overall?', 'subreddit': 'DNMUK'}\n",
      "{'body': 'This would assume a good corner. A good corner would be closer to to the upper of my price range, a bad corner could possible go below my price range.', 'subreddit': 'GlobalOffensiveTrade'}\n",
      "{'body': \"I've never seen it \", 'subreddit': 'gaming'}\n",
      "{'body': 'Alt answer: a Tootsie Roll', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Interesting information. ', 'subreddit': 'ExplainBothSides'}\n",
      "{'body': 'go to the bar....but during the day!!! &gt;:D', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Number one. Here's a good way to look at it:\\n\\nIf you were starting a team and could pick any QB that's ever played, there aren't many you could put above him. Elway's combination of mobility, toughness, a rocket arm and fierce competitiveness make him a compelling choice regardless of who he's compared to. Let's give give it a shot:\\n\\n**Brady**: Simply one of the greatest to ever play the game, period. He's also had the benefit of playing for THE greatest head coach of all time. That has to be factored into this evaluation. Would Brady have been as great under multiple head coaches? Although we'll never know the answer definitively, history shows that it'd be very difficult to replicate Brady's success under different coaches.\\n\\nElway however went to five Super Bowls under two different head coaches. Neither of these coaches would even be in the running of GOAT. Also if you compare the QBs head-to-head, Elway is more athletic and mobile than Brady while being just as competitive (Brady's best attribute). Advantage Elway\\n\\n**Favre**: I never understood why Favre was treated with such reverence when he played. Whenever I think of Favre, I think of back-breaking picks in the playoffs. That alone disqualifies him from consideration. \\n\\n**Peyton Manning** : Great QB. Gaudy stats. Four Super Bowl appearances with four different head coaches. I wouldn't start him over Elway though. Elway is more mobile and tougher, but his biggest advantage is that he played his best ball under pressure. That's Peyton's biggest weakness. That's a big weakness of most QBs. \\n\\n**Marino**: Well he was a great QB to be sure. I think his stats were better than Elway overall. But Marino had a downside. I read recently he had less than 100 yards rushing for his ENTIRE career. So even if Marino was a better passer than Elway, I'd consider his lack of mobility to be a significant disadvantage. Also--and this probably isn't all his fault necessarily-- he played for two Super Bowl winning coaches and only had one SB appearance.  Elway took three Reeves-coached squads to the Super Bowl that had no business being there. That's how good he was.  Quick who was Elway's top receiver on those teams? Who was his running back? I can't remember and I actually WATCHED those teams! \\n\\n**Montana** : Joe Montana was a fucking great QB and arguably is the GOAT. As a fan of both the Broncos and Cowboys (don't ask), I had the pleasure of having my heart ripped out by this muthafucker for over a decade. Accurate, cool under fire with the heart of a champion.  But here's the thing about Montana...dude was fragile by NFL standards. If you put a solid lick on him, he wouldn't get up. That's why he didn't finish his career as a 49er. Toughness is important in this position. Elway was a tough sumamabitch. \\n\", 'subreddit': 'DenverBroncos'}\n",
      "{'body': '143414170| &gt; United States Anonymous (ID: qrXQpZdq)\\n\\n&gt;&gt;143413934\\nI live in california you sensitive faggot clinton was winning anyway\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'God dammit our O Line fucking sucks. ', 'subreddit': 'CFB'}\n",
      "{'body': '/r/legaladvise may be interested ', 'subreddit': 'cars'}\n",
      "{'body': \"Yup, even extremely immigration strict Australia.\\n\\nI heard once that once you're rich enough, immigration is not really something you have to worry about, it's taxes.\", 'subreddit': 'europe'}\n",
      "{'body': 'jacob couldnt test the teradick while ice wasnt in na? jacob couldnt say bring s8 backup setup since ur going out with a car? what did jacob plan for today?', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': 'Fool me once, fool me twice, fool me chicken soup with rice. ', 'subreddit': 'polandball'}\n",
      "{'body': 'Exactly how I feel, it wasnt his strength to begin with. ', 'subreddit': 'leagueoflegends'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnok19y/):\\n\\nI am, in fact, self published :)', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Hi there! Your post was removed because it uses the text box. Per [rule 1](/r/AskReddit/wiki/index#wiki_-rule_1-), use of the text box is prohibited. You can resubmit your post [here](/r/askreddit/submit?selftext=true&amp;title=5 most successful/least successful presidents in your opinion) without the textbox.\\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/AskReddit) if you have any questions or concerns.*', 'subreddit': 'AskReddit'}\n",
      "{'body': 'or is it ;p', 'subreddit': 'milliondollarextreme'}\n",
      "{'body': 'Give Emmure a listen, one of their newer songs Flag of the Beast is pretty good. Another band is King 810, but quite a few people here do not like them. Try their song Trauma Model or Alpha &amp; Omega from their most recent album. They also have some songs similar to rapcore. Deez Nuts is a good rapcore band.', 'subreddit': 'Metalcore'}\n",
      "{'body': \"Yeah, I personally support it, the war on drugs was lost long ago. I say tax it and use it for a healthcare system where I don't have a $6000 deductible.\", 'subreddit': 'Atlanta'}\n",
      "{'body': \"That's what she said.\", 'subreddit': 'motorcycles'}\n",
      "{'body': \"&gt; there are a number of scientific flaws associated with that method that I won't go into right now\\n\\nCould you though?  this is news to me.  I was planning on weighing out my oil or measuring with a syringe as I'm only making about six vials the first time.  Could you just measure your oil in a volumetric flask and then pour it into a beaker?  I feel like you're overthinking this somewhat but I might just not be understanding the question.  \", 'subreddit': 'steroids'}\n",
      "{'body': '2,581', 'subreddit': 'counting_in_the_wild'}\n",
      "{'body': \"Why not cut out the middleman and just get it on with the ambulance? \\n\\nLike I'm talking hard, rough, sirens on full blast it's go time fucking.\\n \", 'subreddit': 'NSFWIAMA'}\n",
      "{'body': 'Why is it that some of us become suicidal? Brains are weird dude...\\n\\n\\n^^^^Okay ^^^^not ^^^^just ^^^^some ^^^^of ^^^^us', 'subreddit': 'funny'}\n",
      "{'body': \"You can't suggest democrats aren't good on reddit. The circle jerk on here is ridiculous. No one wants to admit faults in their own party and always wind up pointing fingers when people say bad things like the fact the other party does it excuses them. Both parties disgust me. \", 'subreddit': 'business'}\n",
      "{'body': 'When I logged onto Reddit ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'After you get unbanned if you want to troll to spite Riots shit system hmu', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"I'm so mad at how the market doesn't appreciate Al Ewing.\\n\\nAnd how Marvel doesn't appreciate Laura Kinney as much as the character deserves.\", 'subreddit': 'comicbooks'}\n",
      "{'body': \"So if you could force a quantum state without breaking the entanglement instead of just reading it's state, then you'd be sending information.... but that currently is not possible?\\n\\nSo instead of sending information as a transmission, it's just measuring the state of a particle, which would measure the same for anywhere it exist entangled?\", 'subreddit': 'conspiracy'}\n",
      "{'body': \"Most any site that sells shoes has a quick and easy print-label return exchange policy, usually 14 to 30 days.  Check the site's return/exchange policy.\", 'subreddit': 'malefashionadvice'}\n",
      "{'body': \"Ya they're always in stock at my local target, Walgreens and Walmart. \", 'subreddit': 'MakeupAddiction'}\n",
      "{'body': 'Khuda Qasam, Awesome!!! Where was this diamond content hidden? Great job OP. Gave me a good amount of laugh.', 'subreddit': 'indianpeoplequora'}\n",
      "{'body': '[deleted]', 'subreddit': 'WahoosTipi'}\n",
      "{'body': 'Colorgaurd bear...', 'subreddit': 'gifs'}\n",
      "{'body': \"Think he's reading into Moreno's comments a bit too much. Anyone who watches us closely will have noticed that we don't press anywhere near as much anymore, and it's a conscious change.\\n\\nCould be many reasons for it. One is that we have the CL this season so we can't afford to play such a high energy game every week. We don't have that deep of a squad, and Klopp thus far has only really rotated in the full back areas. City/Spurs have rotated far more effectively than us. Also worth remembering we have the CL playoff round which added games for us, while having to deal with the absence of Clyne, Lallana, Coutinho and dealing with bringing back players from injury (Hendo/Mane/Sturridge).\\n\\nAnother is that pressing, while effective, when it isn't working, leaves your midfield and defence exposed. Notably when Lallana is missing, we don't have a natural doggish presser aside from Firmino, so Klopp is being pragmatic in the absence of Lallana in order to protect our already vulnerable defence. I can imagine Klopp remembers playing a high pressing game away at Bournemouth or Leicester and then getting caught by simple long balls into the channels.\\n\\nI remember when Klopp first arrived, the number of hamstring/calf injuries was ridiculous. These injuries have slowed down somewhat, but a less intensive pressing game will have helped.\", 'subreddit': 'soccer'}\n",
      "{'body': 'The Left has leaned right over the years. Both parties have. Obamacare originated with the Heritage Foundation in the 80\\'s. The Left absolutely refused to pass it when Gingrich was pushing it in the \\'90\\'s. Once Obama was president, the Left loved it and the Right hated it. \\n\\nThe NSA\\'s domestic spying is literally fascist. Not the Left\\'s new definition which is \"Trump supports it\", but actually fascist. Both the Right and the Left largely support it out of fear, but it\\'s a very far right program. It wouldn\\'t have been allowed before 9/11. \\n\\nTrump has changed his position on things over the years. Most people do. There\\'s no shame in that, as long as the changes don\\'t happen from interview to interview. Trump has been consistently leaning farther and farther to the right over the last several decades. \\n\\nI have no idea why you think the country has been moving Left. I\\'ve been watching politics since the 90\\'s. We\\'ve moved to the Right. You\\'d think people on the Right would consider that a victory, but somehow y\\'all have twisted it into an imaginary defeat. ', 'subreddit': 'ShitPoliticsSays'}\n",
      "{'body': '&gt; I mean if these friends are so far away, it makes sense they don’t want to stay so far from home for a long time.\\n\\nI\\'ve offered to drive by them but still they don\\'t seem to want to hang out with any frequency. Granted, I only do that once or twice before I feel too rejected to keep asking. \\n\\n&gt; If people act differently in real life than texting, it might have to do with your person. How’s your personal hygiene? You take care of yourself, make an effort to look nice?\\n\\nI take extremely good care of myself, to the point of where women I\\'ve met have actually told me they assumed I was a b**ch before they started talking to me, solely because I am \"so beautiful.\" I hate even saying that because it sounds so arrogant, and I think I\\'m a really down-to-earth, kind person. :(', 'subreddit': 'relationships'}\n",
      "{'body': \"I absolutely hate Venom's L3\", 'subreddit': 'ContestOfChampions'}\n",
      "{'body': '[removed]', 'subreddit': 'medicalschool'}\n",
      "{'body': \"&gt; It makes fun of people that are totally fine with animated CP because somehow that makes it less shameful. It doesn't.\\n\\nIt does jerking off to something that hurts no one is significantly less shameful then jerking off to something that permanently harms a kid.\", 'subreddit': 'starterpacks'}\n",
      "{'body': \"Go to psyonix's twitter for the answer. \\nOr you could use the search bar because this has been asked a million times today. \\n\\nThe trails are disabled for now because they are causing crashes. \", 'subreddit': 'RocketLeague'}\n",
      "{'body': \"They weren't a country until the XIX century.\", 'subreddit': 'neoliberal'}\n",
      "{'body': 'Whoa, you really are a kid. Vote Bernie when the time  comes, junior.\\n\\nThen your kids may know some basic abundance in a collapsing world.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'On one hand, It only looks like a small cut, yes there was blood but it doesnt have to be painful. I  think that many of us have bled a lot more after shaving, and is it not necessary need to be painful, is just annoying.\\nOn the other hand, a proper healing should have been done after they noticed he was bleeding. I dont care about their schedule, if it is not flexible enough for them  to have a 10/20 minutes pause to heal Daehwi, YMC sucks. ', 'subreddit': 'kpop'}\n",
      "{'body': 'And it’s not as if he didn’t get looks in our system\\n\\nIt seemed like he was open constantly but just bricked almost every single shot ', 'subreddit': 'nba'}\n",
      "{'body': \"Still haven't answered my question. \", 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': \"\\nadobe.com/go/edu-validate\\n\\nbcbla Redacted for privacy (it's where I work)\\n\\ncc.com/shows/the-daily-show-with-trevor-noah\\n\\ndrivetexas.org\\n\\nexpedia.com\\n\\nnothin for f\\n\\ngoogle (duh)\\n\\nhulu\\n\\naaaand I'm bored\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'But this is presumably also while Frank is shooting back at her. Someone who tagged Spider-Man, twice.', 'subreddit': 'CharacterRant'}\n",
      "{'body': 'Even hollow moon cannot be compared to flat earth. The moon [rings like a bell](https://www.popsci.com/does-moon-sound-like-bell), and craters should be deeper. So, why not? It is not falsified per say. Flat earth is falsified every time you look at the sky.', 'subreddit': 'conspiracyundone'}\n",
      "{'body': \"I'd pay good money to see Huma on roller skates. Amirite?\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'Traffic is really fucked today.', 'subreddit': 'WTF'}\n",
      "{'body': 'Kyrtasaurus and ShishkaBobSaget both with multiple clears. ', 'subreddit': 'Fireteams'}\n",
      "{'body': '[removed]', 'subreddit': 'Incels'}\n",
      "{'body': \"I'm not an ocelot and you're not a snake. We're men, with names. \\n\\n\\nI'm Adamska...\", 'subreddit': 'pics'}\n",
      "{'body': \"Fun fact apparently Mr Satan's his gimmick name. The creator has stated his real name is Mark.\\n\\nAnd I just realised while typing that sentence in this subreddit how hilarious that is.\\n\\nhttp://www.kanzenshuu.com/2009/04/21/mr-satans-real-name-revealed/\", 'subreddit': 'SquaredCircle'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '[deleted]', 'subreddit': 'Guildwars2'}\n",
      "{'body': 'no', 'subreddit': 'gtaonline'}\n",
      "{'body': 'ive not lost any data ive bounced between xbox and pc but pc mainly', 'subreddit': 'forza'}\n",
      "{'body': 'woops, thanks', 'subreddit': 'gunpolitics'}\n",
      "{'body': 'Too many ground balls on our end.  Come on guys.', 'subreddit': 'WahoosTipi'}\n",
      "{'body': \"Too many people don't want to see us get pooped on by OSU. It sucks but I get why people don't wanna go to this game. \", 'subreddit': 'CFB'}\n",
      "{'body': 'I gotchu, msg: The Sweed', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'I just made one of these the other day, but max weight. I really like that this build has decent mid range so you can still dominate in the paint and stretch the floor a little bit if you’re on a team with other bigs. \\n\\nBtw do you know what the rebounding badges go up to?  I was hoping gold for hustle rebounder and putback king but I don’t think so. \\n', 'subreddit': 'NBA2k'}\n",
      "{'body': 'Some of us do realize it.\\n\\nAnd it is weird.', 'subreddit': 'news'}\n",
      "{'body': \"Bitch can't even stand for a picture. Unfit!\", 'subreddit': 'The_Donald'}\n",
      "{'body': 'no, we regret hiring coach O', 'subreddit': 'CFB'}\n",
      "{'body': '[deleted]', 'subreddit': 'IAmA'}\n",
      "{'body': 'Gotta love the the bender musings! ', 'subreddit': 'HighlySuspect'}\n",
      "{'body': \"Funny.  This was a common sight in the 70's. Only they were rolled higher- as high as the could reach.  \", 'subreddit': 'Justridingalong'}\n",
      "{'body': '[removed]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'It is listed under payouts, but is under the minimum payout amount so no.', 'subreddit': 'GenesisMining'}\n",
      "{'body': 'No, but it changes how much they score, are you looking for big potential or someone who has a safe floor.', 'subreddit': 'fantasyfootball'}\n",
      "{'body': 'Pretty sure he did.', 'subreddit': 'BlackPeopleTwitter'}\n",
      "{'body': 'Do it. Eat your vegetables.', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'Have pink spiralis ', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'I want really want Ice to crash his car so he can learn a lesson of not looking at phone/chat while driving. Dont want anyone to get hurt or anything...maybe just crash into a pole or something ', 'subreddit': 'Ice_Poseidon'}\n",
      "{'body': \"I am now going to write a poem based on that because I find it intetesting. I'll edit this comment when finished if you're curious\", 'subreddit': 'TumblrInAction'}\n",
      "{'body': 'Killing your literacy eating into Spain bro. Should have taken Belgium/Netherlands instead if you want to blob in Europe. Steal Quebec from the Brits and return the French-Canadians to their sisterland.', 'subreddit': 'victoria2'}\n",
      "{'body': \"Reasonable skepticism, I can respect that. It says it's 3000 lm, 500 ANSI lm. [link.](https://www.amazon.com/Nebula-Portable-Multi-media-supported-Projector/dp/B073P3JHTH)\", 'subreddit': 'Android'}\n",
      "{'body': \"[+LFAH94](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnonalv/):\\n\\nBrianna, as someone who's trying to do the same (while racing against the college graduation clock), how did you manage to self-publish without paying anything?\\nThere are decent self publishers here in Canada but even a short run of 10 paperbacks is going to set me back at least a few hundred bucks. \\nAny suggestions?  \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': '143413955| &gt; United States Anonymous (ID: msELbNeP)\\n\\n&gt;&gt;143413905\\nDual citizenship?\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': \"Accessorize is good for nothing besides looks but I think you want maybe more an example where the function is being disused or not used at all like with outdoor wear (North Face etc) as fashion. Actually I can't really think of any good examples right now, but feel that they are many, hm.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'This!\\n\\nEvery player passes in the same pattern and it takes 2-3 passes to get to goal and score. \\n\\nWtf!', 'subreddit': 'FIFA'}\n",
      "{'body': '*Who is a Pineapple under the sea*', 'subreddit': 'NatureIsFuckingLit'}\n",
      "{'body': \"i know you're trolling . it is all good my friend. I too like to gun down uppity 12 year olds.\", 'subreddit': 'Conservative'}\n",
      "{'body': 'I mean most of the time Athel Loren and Ulthuan are more or less safe for it´s inhabitants. I don´t know how many Invasions Malekith had, but compared to the Millenia he´s been the Witch King I´d say it´s still a relatively quiet life. Same goes for Athel Loren.', 'subreddit': 'totalwar'}\n",
      "{'body': '&gt; \"When I visited your campus, it felt magical.\\n\\nYou do realize that these kids have to write ridiculously long essays and multiple application forms during their senior year, and you\\'re asking them to customize each application form.\\n\\nUh, no.\\n\\nSo trying to make a more or less \"generic\" application that can be \"tuned\" to each college is hardly a sin.  If anything, it\\'s clever thinking.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"It could be we got more enjoyment out of it playing cooperatively in the same room...but it IS the sort of game that if it's not a persons genre type, no matter who you are playing with it would be boring. Thanks for the comment mehoff88 :)\", 'subreddit': 'oculus'}\n",
      "{'body': 'That’s the one. I’ve got pics of him in his cage somewhere. It was quite sad actually. \\n\\n\\nMy wonderful mom used to give my brothers and I(8, 6, and 4) a lit cigarette and a handful of blackcats to light and throw at each other on the beach. Ah yes, the good ol days. ', 'subreddit': 'WTF'}\n",
      "{'body': 'Was there a game delay? ', 'subreddit': 'WahoosTipi'}\n",
      "{'body': \"He's clearly someone that doesn't understand that half of the world still has to be more stupid than the average person. He's much below the average here lol\", 'subreddit': 'sports'}\n",
      "{'body': 'Hong Kong moves at twice the speed, and sad to say a \"princess syndrome\" exist and they are far too picky, so I hear a lot of hk guys end up shacking up with mainlander girls, yet the irony HK girls then blame HK men for doing this...like wat!?\\n\\nBecause of these high expectations, there are quite a few \"loa gaw por\" or old tiger women, they expect a man to earn way more than them, they want a tool. Yet, the sad state of if a wm enters, he can be a bum and they go for it. Aaaand LKF is a shithole too, but I would like to think there is hope in HK at least, compared to the garbage UK offers. \\n\\nI have been thinking lately too, working hard but you need other parts in life to make you happy. I hear stories of people who just travel a lot when they are still studying, I wish I did this when I was younger (thus have less commitment to things) I still could but I am in a different situation whereby I need to fend for myself. ', 'subreddit': 'aznidentity'}\n",
      "{'body': \"Think of Oregeron at Ole Miss. It's 2.0 of that\", 'subreddit': 'CFB'}\n",
      "{'body': '&gt;Sweden has much fewer people walking outside of designated cross walks.\\n\\nAre you Swedish? If not how can you possibly know this?', 'subreddit': 'toronto'}\n",
      "{'body': \"I don't think this series has made be this emotional before.. DR2's second chapter topped the list for a while, but this fucked me up. When Kaito started coughing up blood again, I honestly couldn't help but cry. I don't want him to fucking die, he needs to survive this god damn game. Gonta being executed because of something he can't even remember, it's gotta be the most messed up 'motive' for a killer thus far. Kokichi's obviously planning some fucked up shit, and quite frankly I'm kinda scared to see where this is going. Just, don't die Kaito. \", 'subreddit': 'danganronpa'}\n",
      "{'body': 'Ill do that, GT: Huf AP\\nIll be on in about an hour if thats cool?', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Today I got there for rope drop. Did guardians twice (once was a fast pass). Cars, screamin, then went over to Disneyland and did Space Mountain and Haunted Mansion. Left park at 10:10am. Rope drops are far and away the best time. ', 'subreddit': 'Disneyland'}\n",
      "{'body': 'I also love how he Robbie starts a calm stroll towards a knocked down Rory. As if saying \"okay, I knocked him down, now let\\'s go for the finish.\" Mark Hunt did the same relaxed walk toward Bigfoot in their first fight.', 'subreddit': 'MMA'}\n",
      "{'body': \"I wish more SM's cared more about their financial future. I talk to my friends all the time about it and I can tell they get annoyed or just don't give a fuck. It's sad when I'm explaining why the TSP is great to a 40 year old who thinks gold is the number one best investment known to man. But whatever. I'm gonna be squared away so fuck em.\", 'subreddit': 'MilitaryFinance'}\n",
      "{'body': 'I wonder too. The hourly on GDAX seems like it is reaching the tipping point of a rising wedge, and Asia is just waking up. I have a tight stop loss set just in case.\\n\\nEDIT: welp, there someone just sold 700 coins at once...\\nEDIT 2: I wonder how low this one will go, guessing near $50 since that is the support where we had the most volume', 'subreddit': 'LitecoinMarkets'}\n",
      "{'body': \"I have to agree with a lot of the critiques here.  The pilot showed a lot of promise, but the show just went down hill from there.  The show had so much potential to be an accurate portrayal of the origins of hip hop and how the culture of the 70s in NYC really influenced not only a generation but changed the landscape of the music industry in general.  \\n\\nBut sadly they just didn't portray it very realistically and it wasn't, like someone else said, raw or real enough.  And don't get me started on the whole rape scene where Leslie Lesgold forces Jackie Moreno to go down on her because he once coerced her into giving him a blowjob and then fired her way back when she interned for him.  Really?  Such BS and so completely unrealistic.  I lived through that era, women got used and sexually exploited all the time and not just in the music industry.  Back then it was the unofficial and deeply repulsive status quo that in order for a woman to get a ahead, pardon the pun, she had to give head and much more. If she didn't play that game she got black listed and her career was over.  \\n\\nOnce a woman gained stardom then it was different story, but even then the male power players of the day ran the show and their word was law.  Back then no woman would dare pull some revenge crap like Leslie did, because no male power player then would ever allow a woman to have that kind of power.  Sorry ladies but rape revenge like what Leslie did, however empowering and entertaining, was not only not practical but due to the culture of the time would never have happened. So yeah that was the big BS exclamation point on the ending of that show, that and the ridiculous amount of money Netflix spent on it.  The Get Down deserved to get cancelled.  Overall though, with all of Netflix's other hit shows, one miss like the get down shouldn't hurt their overhead much at all.  They are still hitting it out of the park with shows like Luke Cage and Stranger Things, as well as a lot of their other original shows and the ones they currently got in development.  The Get Down will barely end up as a hiccup in what has become Netflix's original show money making machine.  \\n\", 'subreddit': 'hiphopheads'}\n",
      "{'body': \"He's funny but man from everything I've seen this guy is an insufferable douche\", 'subreddit': 'youtubehaiku'}\n",
      "{'body': \"He's black, even in daylight couldn't spot the fucker.\", 'subreddit': 'watchpeopledie'}\n",
      "{'body': 'God dammit', 'subreddit': 'CFB'}\n",
      "{'body': 'Happy Bottle!', 'subreddit': 'TotalDramaTheGameshow'}\n",
      "{'body': '**SHOWTIME**', 'subreddit': 'danganronpa'}\n",
      "{'body': '[deleted]', 'subreddit': 'Guildwars2'}\n",
      "{'body': \"Ishizu's scream in Japanese freaked me out. \", 'subreddit': 'DuelLinks'}\n",
      "{'body': 'already got piper. im very unlucky with brawl boxes. i got mortis at level 10, though. i have piper at 42 trophies and 3 upgrades, aiming to get 6 upgrades by the end of today', 'subreddit': 'Brawlstars'}\n",
      "{'body': 'I needed to see this.', 'subreddit': 'Minecraft'}\n",
      "{'body': \"I don't have black rlcs\", 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': \"Isn't there some saying about just renting their food?\", 'subreddit': 'AskReddit'}\n",
      "{'body': '\"no\"\\n\\n\"wrong\"\\n\\n\"puppet\"', 'subreddit': 'politics'}\n",
      "{'body': 'Are these posts allowed?', 'subreddit': 'KingdomHearts'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop6dj/):\\n\\nGo through CreateSpace and Amazon. That's what I did! You just have to buy the ISBN number\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"I've never heard of anyone saying that. Do you have a link to who said it happened to them? With that as well this would be a very hard thing to prove/collect data on.\", 'subreddit': 'OnePieceTC'}\n",
      "{'body': '&gt;And yeah, if I walk into my school\\'s libertarian club meetings, or even conservative meetings, and say \"hey guys let me tell you what\\'s great about the other side,\" they\\'ll be happy to spend forever and a day telling me why I\\'m wrong. I can\\'t say the same about the left\\n\\nthen you\\'re clearly doing it wrong, because the biggest complaint about leftists amongst leftists is that we spend too much time yelling at each other about how the other person is doing leftism wrong\\n\\ni dunno what you want me to say - none of what you\\'re saying here aligns with the real world.', 'subreddit': 'onguardforthee'}\n",
      "{'body': 'Well sorry for trying. Mr.Akkuron', 'subreddit': 'nba'}\n",
      "{'body': 'No', 'subreddit': 'asoiaf'}\n",
      "{'body': 'Among their 10 fans, 3 of them are toxic downvoters', 'subreddit': 'Braves'}\n",
      "{'body': \"They wouldn't let me donate platelets today because they had shutdown the login computer already. Lame\", 'subreddit': 'CFBOffTopic'}\n",
      "{'body': 'Lol.', 'subreddit': 'hockey'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'First one on the floor? Hope he keeps it up.', 'subreddit': 'lakers'}\n",
      "{'body': '[deleted]', 'subreddit': 'AnythingGoesNews'}\n",
      "{'body': '[deleted]', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Can’t protest during the anthem in hockey. Your knees would get cold.', 'subreddit': 'Predators'}\n",
      "{'body': 'Goat. I saw a goat. I need help', 'subreddit': 'The_Donald'}\n",
      "{'body': \"The ones released in Australia (the mini version) seem to be selling on eBay way cheaper than the NES classic was. I missed out on the NES classic mini and the listings were around $300-$500 each. So obviously I didn't get one.\\n\\nFor the SNES Classic mini, I missed out on preorder again, went on eBay and they were selling on preorder for around $40-$50 more than the release price. Which isn't *too* bad, I guess. Maybe no one bought the ridiculously overpriced ones so they had to lower them, who knows.\\n\\nBut that said, I went to the local department store 6 hours after they opened expecting them to have sold out hours earlier, but they still had plenty so I was able to get one (they said they received about 50-60, and they had a line up in the morning but it's a small town. Some major stores got hundreds and hundreds. When the NES was released they got 12. And a lineup of furious people yelling at the employees for not having enough)\\n\\nMaybe Nintendo learned after the NES fiasco, but it was still cool to get one and not have to pay stupid amounts for it.\\n\", 'subreddit': 'gaming'}\n",
      "{'body': 'He said not complex', 'subreddit': 'bloodborne'}\n",
      "{'body': \"&gt; it's funny that people can be trolled into basically believing *anything* is a Nazi symbol\\n\\nYeah, LOL! [What could've given them *that* idea?!](https://rationalwiki.org/wiki/Alt-right_glossary#Kekistan)\", 'subreddit': 'onguardforthee'}\n",
      "{'body': 'Turns out “the Cave” is what Kit and Rose call their bedroom', 'subreddit': 'freefolk'}\n",
      "{'body': 'The end of the video had it seem like its the making of a JAV Porno and I was super stoked to see what happened next.', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'I fucking love KFC', 'subreddit': 'ProRevenge'}\n",
      "{'body': \"I'm not sure what's causing the popping issue although my guess would be disk. Make sure you're not running anything in the background that could be using your system resources.\\n\\nAlso with a 1080 you should definitely be able to run Ultra- You can hit almost 60fps on a 1070\", 'subreddit': 'GrandTheftAutoV_PC'}\n",
      "{'body': \"I'll drink to that!\", 'subreddit': 'CFB'}\n",
      "{'body': \"it's not like there aren't a dozen other cabinet members doing the same thing.  And trump's maralago bills are 3x price's charges\\n in a weekend.\", 'subreddit': 'politics'}\n",
      "{'body': 'She could have won that, 1 more hit would have been enough but for some reason she turned around for a second', 'subreddit': 'Awesomenauts'}\n",
      "{'body': 'If you didnt already, try tank-karma. Best thing in the world.', 'subreddit': 'karmamains'}\n",
      "{'body': 'It\\'s only forfiet for threatening or infringing on someones rights. The sort of speech that needs protecting is precisely tge sort that someone is going to find \"disrespectful\"', 'subreddit': 'news'}\n",
      "{'body': 'Pic? ', 'subreddit': '704nascarheat'}\n",
      "{'body': \"Google search says yes, though I've never had sprout pods before.\", 'subreddit': 'gardening'}\n",
      "{'body': 'He is a rookie too. ', 'subreddit': 'Habs'}\n",
      "{'body': 'And tasty for sure 😋', 'subreddit': 'gonewildchubby'}\n",
      "{'body': \"Trying to overturn the 1st ammendment. \\n\\nAs much as these idiots want things like hate speech laws, if such a law were to get passed, that'd be it.\\n\\nThat's what would cause the 2nd Civil War.\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'Apenas o socialismo pode libertar o país ', 'subreddit': 'portugal'}\n",
      "{'body': 'Feedback for you guys straight from your neighbouring city Roosendaal!', 'subreddit': 'kotk'}\n",
      "{'body': 'Can you post the original picture without the desktop shortcuts?', 'subreddit': 'outrun'}\n",
      "{'body': 'http://www.ishtar-collective.net/categories/books-of-sorrow\\n\\nAt the bottom listed in order', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'As a new dad, this hit me right in the feels.', 'subreddit': 'pcmasterrace'}\n",
      "{'body': 'I voted, but better not to vote than to vote for him', 'subreddit': 'politics'}\n",
      "{'body': \"143412679| &gt; None Anonymous (ID: eG6B9PIj)\\n\\n&gt;&gt;143412250 (OP)\\n2008: didn't vote\\n2012: Ron Paul\\n2016: didn't vote\\n\\t\\t\\t\", 'subreddit': '4chan4trump'}\n",
      "{'body': \"It's a walking mop too. I always think poodles are best for not shedding, but after boarding s couple, they really hang onto scents and dirt after just a couple days \", 'subreddit': 'pics'}\n",
      "{'body': \"That factors in, but it's the opposite side of the same coin that causes (Or at least used to) her to get a ton of upvotes on every post with images.\\n\\nShitty treatment is shitty, her builds getting upvoted past other more imaganitive / better builds is also shitty.\", 'subreddit': 'gifs'}\n",
      "{'body': '&gt;To stop Catalonia’s self-determination, Madrid has gone so far as to suspend its autonomy and freeze all its financial assets.\\n\\nkinda like when Abe Lincoln suspended the right of habeus corpus and locked up the Maryland legislature specifically to stop them from voting to secede from the union?', 'subreddit': 'worldnews'}\n",
      "{'body': '[Original post](https://www.reddit.com/r/60fpsporn/comments/73i1ob/knees_together/) - [Subreddit](https://www.reddit.com/r/60fpsporn)', 'subreddit': 'nsfw_rising'}\n",
      "{'body': 'omg lol i laughed so hard at your title. def my kinda lesbo', 'subreddit': 'RightwingLGBT'}\n",
      "{'body': 'You can also use nickel and dimes to buy Hippy Army MPE and Frat Army FGF.  I may have actually done that back before I stockpiled the foods.', 'subreddit': 'kol'}\n",
      "{'body': 'The only writer to try to write a utopian culture that works. These are wonderful, fantastic books.', 'subreddit': 'sciencefiction'}\n",
      "{'body': 'Without EDA bch would already be gone. ', 'subreddit': 'btc'}\n",
      "{'body': 'I laughed at that one, as well.\\n\\n\"Wait, you can change stuff at halftime?\" \\n\\n-Dan Mullen', 'subreddit': 'CFB'}\n",
      "{'body': 'https://www.youtube.com/watch?v=IfXMN3VhikA', 'subreddit': 'FragReddit'}\n",
      "{'body': \"Oooh, y'all got $1.74 billion today I just heard!\", 'subreddit': 'stocks'}\n",
      "{'body': 'Man you aren’t even op and your responding to everyone great work!!', 'subreddit': 'AskReddit'}\n",
      "{'body': 'The Amity Affliction', 'subreddit': 'Metalcore'}\n",
      "{'body': 'U 2', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Yup', 'subreddit': 'metalgearsolid'}\n",
      "{'body': 'Lol. Making me feel old. Back in the day when we still used film in cameras, we had to keep them safe when in the camera and out of it. So we put them in film canisters.\\nhttps://www.bhphotovideo.com/images/images2500x2500/General_Brand_Plastic_Film_Canisters_With_263775.jpg', 'subreddit': 'ProtectAndServe'}\n",
      "{'body': \"Unless you've cycled on it, it's hard to say whether or not it is perfectly good. There was one I used to ride on which I gave up on after having several punctures. The ones adjacent to roads tend to end up accumulating broken glass and other debris which is brushed onto them.\", 'subreddit': 'britishproblems'}\n",
      "{'body': \"Congratulations, your skill attempt was successful!\\n\\nPlease make sure to edit your original bio with a note that states that you've learned this skill. Thanks!\", 'subreddit': 'IronThroneRP'}\n",
      "{'body': 'yep', 'subreddit': 'Nioh'}\n",
      "{'body': \"Stop you're stressing me out\", 'subreddit': 'meirl'}\n",
      "{'body': '[deleted]', 'subreddit': 'Fishing'}\n",
      "{'body': 'Could have wished that was real.', 'subreddit': 'ClashRoyale'}\n",
      "{'body': 'This submission was automatically removed because it did not have the correct title format.\\n\\nTitles need to follow the format: **Gender/Age/Height [Weight Before &gt; Weight After = Total Amount Lost] Personal title**\\n\\nFor example:\\n\\n**F/23/5\\'5\" [189lbs &gt; 169lbs = 20lbs] Weight loss progress**  \\n\\n- Gender must be **ONE letter** \"M\", \"F\", \"T\", etc.  Do Not write out \"Male\"  \\n- Do not add extra spaces.\\n- Do not use the \"~\" character\\n- Use Brackets [ ],    DO NOT USE ( ) Parentheses \\n- For best results, copy/paste the above example, and replace the information with your own.  \\n\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/progresspics) if you have any questions or concerns.*', 'subreddit': 'progresspics'}\n",
      "{'body': 'Why would Gohan suddenly know Kaioken?', 'subreddit': 'dbz'}\n",
      "{'body': 'They turned them down but it was not with a \"We\\'re not interested in selling.\" reason it was with a \"The premium is too small.\"  Which translates to \"Come up with more money and we will listen.\"\\n', 'subreddit': 'NASCAR'}\n",
      "{'body': \"I can't say as though I would be particularly interested in going to my ex's wedding invited. \", 'subreddit': 'hockey'}\n",
      "{'body': \"Some peopke honestly can't handle that truth , use to though it was not so easy to self publish (well she has spent thousands of dollars)\", 'subreddit': 'Drama'}\n",
      "{'body': \"I too would like to know what's the other guy is asking \", 'subreddit': 'steroids'}\n",
      "{'body': 'Hey fuck you, he was ours first ', 'subreddit': 'CFB'}\n",
      "{'body': \"I saw Young Sheldon, it was total garbage, and I'v been a Big Bang Theory fan for years now--little kid is an insulting ass,and the supporting cast is woefully anemic\", 'subreddit': 'AskReddit'}\n",
      "{'body': 'This is so true. Also, makes me happy to have pulled a single Reinhardt in my 5 months of play. 4★, but +atk and upgraded him to 5★. Now he carries me!', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': 'Oh boy I went to read a bit that subreddit to see how bad was it and I was not dissapointed !', 'subreddit': 'bladeandsoul'}\n",
      "{'body': \"Hoeneß and his idea of who to put in charge of certain positions is unprofessional. I'm all for ex-players remaining with the club, but if they occupy an important position, they have to be somewhat qualified. The people we have lost and not adequately replaced in the last year or so is disappointing. \\n\\n\", 'subreddit': 'fcbayern'}\n",
      "{'body': 'Username checks out.', 'subreddit': 'apple'}\n",
      "{'body': \"You think Trump will be charged with treason and I'm the stupid one....\", 'subreddit': 'news'}\n",
      "{'body': '[+SharkBaitDLS](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoqth8/):\\n\\nYou actually don’t even need to buy an ISBN if you don’t mind your publisher being listed as “Independently Published”. So it’s possible to do with zero cost. ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Pics 4 and 6 ', 'subreddit': 'whatsthisbird'}\n",
      "{'body': 'N', 'subreddit': 'AskOuija'}\n",
      "{'body': 'Sometimes They Come Back', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Currently:\\n\\n•LJ Peretti Park Sq.\\n\\n•OGS\\n\\n•Lane Black Raspberry\\n\\n', 'subreddit': 'PipeTobacco'}\n",
      "{'body': \"So you have no evidence at all is what you're saying? You're just making wild, baseless assumptions that have no grounding in reality, strictly to push a narrative against a politician you don't like?\", 'subreddit': 'conspiracy'}\n",
      "{'body': \"Yeah, I've got that down. I just don't really know how you would go about actually making a realistic looking one.\", 'subreddit': 'BendyAndTheInkMachine'}\n",
      "{'body': 'Read a bumper sticker once that said \"dress for the slide not door the ride\" made a lot of sense. ', 'subreddit': 'AskReddit'}\n",
      "{'body': \"Haha whatever gets us through the raid my friend.  We all have to learn somehow! The amount of times I've gone flying over those ramps...\", 'subreddit': 'destiny2'}\n",
      "{'body': \"That's their placeholder art:  Its Studio MDHR lead designer and founder Jared Moldenhauer.\\n\\nhttps://twitter.com/tonycoculuzzi/status/665156098610065408\\n\", 'subreddit': 'Cuphead'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'In the US it varies from state to state.', 'subreddit': 'amibeingdetained'}\n",
      "{'body': 'God forbit ', 'subreddit': 'gaming'}\n",
      "{'body': 'Why the long face?', 'subreddit': 'Overwatch'}\n",
      "{'body': \"And more content is ON PTP. Almost twice as much in fact, and that's without even subtracting non-movie content on HDB.\", 'subreddit': 'trackers'}\n",
      "{'body': 'What I generally do is roll my eyes and scoff under my breath. Not very effective as far as activism goes but it makes me feel better. ', 'subreddit': 'vegan'}\n",
      "{'body': 'Oh sweet.  That was nice of her.', 'subreddit': 'fivenightsatfreddys'}\n",
      "{'body': \"I don't like either person very much, if I was American I would have voted for Hillary, but even I think Pence would be a better president than Trump.\", 'subreddit': 'CringeAnarchy'}\n",
      "{'body': 'Why don\\'t you just \"rent\" one and never return it.  Worst case scenario, they make you pay for it which is what you wanted all along.', 'subreddit': 'Connecticut'}\n",
      "{'body': 'Sure is ', 'subreddit': 'lego'}\n",
      "{'body': 'Yup', 'subreddit': 'Skookum'}\n",
      "{'body': '[deleted]', 'subreddit': 'toronto'}\n",
      "{'body': 'It’s the least that I can do after all that this sub gave to me 😂', 'subreddit': 'IBO'}\n",
      "{'body': \"I honestly don't know. I've heard a few names thrown around but none of them seem realistic. Thankfully I'm not being paid to worry about this problem so I'll let those who are figure it out.\", 'subreddit': 'CFB'}\n",
      "{'body': 'https://streamable.com/chgmu\\n\\nIs this your own footage as live? Please if you could download periscope, everyone would love to watch this and you could really change the world ', 'subreddit': 'IAmA'}\n",
      "{'body': 'Son of a trailer park electrician', 'subreddit': 'RoastMe'}\n",
      "{'body': 'Why', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Wonderful picture, she looks like a meme', 'subreddit': 'cats'}\n",
      "{'body': 'Ya go get swabbed', 'subreddit': 'Herpes'}\n",
      "{'body': '[deleted]', 'subreddit': 'PoliticalHumor'}\n",
      "{'body': 'It’s so surprising how little caps he has. ', 'subreddit': 'soccer'}\n",
      "{'body': \"And who says the Metro just isn't classy?\", 'subreddit': 'JusticeServed'}\n",
      "{'body': '747', 'subreddit': 'Conting'}\n",
      "{'body': 'Your framing is pretty good for an accidental photo.', 'subreddit': 'funny'}\n",
      "{'body': 'Way to stick with it Ward.', 'subreddit': 'CFB'}\n",
      "{'body': 'Classic D-Frag perfection.', 'subreddit': 'manga'}\n",
      "{'body': '[Here](https://gist.github.com/GenuineMP5/b65c481fbc3a164f29ade6943f3df82f) is the file. I used the default as a base and modified from there. I used the function that originally moved to the Plover layer as my game function, but I may have accidentally changed something. Everything works except for going to that layer (which does work, just only after the second key press). \\n\\nThanks!', 'subreddit': 'olkb'}\n",
      "{'body': '8h a day. 5 days a week. 5 days (sorry I counted it wrong) a month I have to work the night shift.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Well, there probably is, but not the same number of people as say Chicago.', 'subreddit': 'nfl'}\n",
      "{'body': 'Since you aren\\'t taking a substantial part of the source material, and you are transforming it by creating your own \"performance\" for comedic effect, you should be fine. You are in no way usurping or replacing the original work, which weighs greatly in your favor for fair use.', 'subreddit': 'podcasting'}\n",
      "{'body': \"Yes, it is a male sex organ.  Being attracted to a woman who has non-standard sex organs doesn't make you gay.  I mean, if someone is solely fixated on the penis, and doesn't care for the rest of the person, that might mean something different.  But that's not what people are typically doing.  Bailey Jay is considered by a lot of people to be good looking and *also* she had a penis.  Random penises are not what these people are into - they're just more open about non-standard genitalia.\", 'subreddit': 'SubredditDrama'}\n",
      "{'body': \"I think it's the horn that's giving him smarts.\", 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Ok. I fixed the typo.', 'subreddit': 'Rockband'}\n",
      "{'body': 'Délicieuse !! ', 'subreddit': 'gonewild'}\n",
      "{'body': '[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoteu8/):\\n\\nThis is very true', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'he likes raw fish', 'subreddit': 'opieandanthony'}\n",
      "{'body': 'Literally any church.', 'subreddit': 'montreal'}\n",
      "{'body': 'Diarrhea.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'You need to get the three override holotapes but that is a matter of fighting through everything until the last bit before the boss fight.', 'subreddit': 'Fallout'}\n",
      "{'body': 'This submission has been removed.\\r\\n\\r\\nViolation:\\r\\n\\r\\n## [Rule 0: No threads that are answered by the Wiki, Searching r/Fitness, or Google](https://www.reddit.com/r/Fitness/wiki/rules#wiki_rule_.230)\\r\\n\\r\\n[/r/Fitness Rules](http://www.reddit.com/r/Fitness/wiki/rules) | [/r/Fitness Wiki](http://www.reddit.com/r/fitness/wiki/index)\\r\\n\\r\\n**PLEASE NOTE: I am a bot account, but I have removed this thread by request from a human moderator. If you have a question about this removal, please [send a message to ModMail](https://www.reddit.com/message/compose?to=%2Fr%2FFitness) and include a link to your thread.**', 'subreddit': 'Fitness'}\n",
      "{'body': 'YA BUT MY IDEOLOGUES COULD BEAT UP YOUR IDEOLOGUES!', 'subreddit': 'canada'}\n",
      "{'body': '\"Oh fuck the acid\\'s really kicking in. Act like a human. What does that *even mean*?\"   ', 'subreddit': 'funny'}\n",
      "{'body': 'I was so young and just really dug the \"techno\" song from the movie and playing as Sonya on Sega, lol. Then I went to Spencer\\'s and got a full body condom and we ran around the mall wearing them like idiots. Gotta love 90\\'s slumber parties.', 'subreddit': 'movies'}\n",
      "{'body': 'Bro ur a lover not a fighter. You got flower shirts on.', 'subreddit': 'malefashionadvice'}\n",
      "{'body': 'I wish i had your problem ', 'subreddit': 'sex'}\n",
      "{'body': '[deleted]', 'subreddit': 'muacjdiscussion'}\n",
      "{'body': 'my b lol oh well', 'subreddit': 'paydaytheheist'}\n",
      "{'body': 'Looks a bit Scarlet Crusadey.\\nEither way, hot as fuck.', 'subreddit': 'Kappa'}\n",
      "{'body': '[deleted]', 'subreddit': 'starterpacks'}\n",
      "{'body': '143413374| &gt; United States Anonymous (ID: lXCJyWOh)\\n\\n&gt;&gt;143413305\\nthanks leaf\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Playing on a team is a privilege, though. Nobody is entitled a spot on a team, and coaches kick people off of teams for whatever they want', 'subreddit': 'news'}\n",
      "{'body': 'Just disable it. Then when it is up and running reenact windows defender.', 'subreddit': 'vertcoin'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"Amusingly, it's not even Roman. The earliest attestation of the Bellamy Salute is *The Oath of the Horatii* in 1784. Hitler and Mussolini couldn't even get their fucking Roman cosplaying right. \", 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': 'How about one at 7:00?', 'subreddit': 'MLS'}\n",
      "{'body': \"Sorry, your Karma is less than 0. This could mean you're a bot or a troll.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/lfg) if you have any questions or concerns.*\", 'subreddit': 'lfg'}\n",
      "{'body': 'I hope they can get you straightened out.   Good luck!', 'subreddit': 'collapse'}\n",
      "{'body': 'You are right my friend. And it could also be neither too :)', 'subreddit': 'TwoXChromosomes'}\n",
      "{'body': \"I've had an experience like that before. The passenger was upset because he couldn't drink his Bud Lite in my car. \", 'subreddit': 'uberdrivers'}\n",
      "{'body': \"Red Wings are notorious for a hard break in period. But when it's done they create an exact mold to your foot\", 'subreddit': 'goodyearwelt'}\n",
      "{'body': \"Depends on who you ask LOL! I like some of it but it doesn't taste like the real thing\", 'subreddit': 'vegetarian'}\n",
      "{'body': \"Okay I'll try that, thank you very jukmifgguggh \", 'subreddit': 'jukmifgguggh'}\n",
      "{'body': \"Sad story time. \\n\\nI was 6 years old and had family friends over just before Christmas. My mom was single and working a ton to support my sister and I at the time. \\n\\nAnyways, we didn't want our friends to leave so we hid in the laundry room.  Low and behold, I stumble across the technodrome hidden behind the furnace. We book it out of there, and agree not to say anything. \\n\\nSo they leave, and it's just the 3 of us at the kitchen table. I was so excited about it that I kept dropping hints until it was obvious and she just broke down crying. I had never seen her cry before, and granted she was stressed from everything but its probably the worst memory I have with her. She tried so hard to make a perfect Christmas and circumstances (as I know I was a kid and didn't do it on purpose) just took that away from her, as was common in general for her. \\n\\nI didn't even ask for that present but she knew I loved TMNT and got me the best present ever. It was my last Christmas with her. \\n\\n \", 'subreddit': 'nostalgia'}\n",
      "{'body': '\"I don\\'t like cake\"', 'subreddit': 'CampHalfBloodRP'}\n",
      "{'body': 'I appreciate you cursing the top players, one god already fallen into the depth of losers.', 'subreddit': 'smashbros'}\n",
      "{'body': 'Check out this [r/bestof of some guys experience of Burning Man](https://www.reddit.com/r/bestof/comments/6yt3zs/redditor_explains_the_logistics_behind_and_what/) ', 'subreddit': 'bonnaroo'}\n",
      "{'body': 'I love your breasts... I want to put my face in-between them', 'subreddit': 'ghostnipples'}\n",
      "{'body': '[deleted]', 'subreddit': 'hockey'}\n",
      "{'body': 'Such sagacity ', 'subreddit': 'Jokes'}\n",
      "{'body': 'So just to be clear, \"lift,\" \"timing,\" and \"angle,\" all ultimately refer to the amount of valve overlap caused by the cams? ', 'subreddit': 'cars'}\n",
      "{'body': \"Lol I factored it in, but didn't realize this bike had a 12k service coming up. I have the $ but that doesn't mean I'm not broke. Buy it and almost immediately spend $1,200? That sucks no matter who you are. \\n\\nMy fault for not researching that, but I only expected tires and normal fluids for awhile\", 'subreddit': 'Triumph'}\n",
      "{'body': 'Seize information technology.', 'subreddit': 'ChapoTrapHouse'}\n",
      "{'body': \"It's possible to stream it, but it's not the same, you know? Unfortunately we are in different states, so getting married in front of her is not really an option unless we just say our vows in front of her.\", 'subreddit': 'wedding'}\n",
      "{'body': \"I don't watch basketball, but are you serious? That's laughably bad.\", 'subreddit': 'sports'}\n",
      "{'body': 'redline concealment DCR 2.0\\n\\nhttp://redlineconcealmentholsters.com/product.sc?productId=14', 'subreddit': 'CCW'}\n",
      "{'body': 'To save her life if she was on fire.', 'subreddit': 'AskReddit'}\n",
      "{'body': 'sure thing Day Day', 'subreddit': 'xboxone'}\n",
      "{'body': 'Would you be able to elaborate how you got it under control? Currently struggling with the same thing. ', 'subreddit': 'Showerthoughts'}\n",
      "{'body': 'Showalter would play a better Vince though. ', 'subreddit': 'SquaredCircle'}\n",
      "{'body': \"But it's actually kinda cool though.\", 'subreddit': 'trashy'}\n",
      "{'body': 'I used to rock the psp version of this shit back in the day and I loved it. Regardless of the quality, I just enjoyed that a crash racing game had some platforming in it.', 'subreddit': 'crashbandicoot'}\n",
      "{'body': '[removed]', 'subreddit': 'pics'}\n",
      "{'body': 'His momma listens to AM radio.', 'subreddit': 'cars'}\n",
      "{'body': 'Damn,People can turn into real assholes while on the karts with bumping etc.', 'subreddit': 'AMA'}\n",
      "{'body': 'anyone have a stream?', 'subreddit': 'CFB'}\n",
      "{'body': 'After the first 4 or 5 days, I just stopped using up the raid orbs.  3 a day for the daily and thats it.  \\n\\nIf I tried to use every orb, I think I would of ended up quitting the game before the event was over.', 'subreddit': 'FFBraveExvius'}\n",
      "{'body': \"Making a big batch of soup, sipping a hot toddy with Bushmill's Black Bush (highly recommend), listening to some [Mike Love](https://youtu.be/SheUtYVritw) (also highly recommend), switching my plans from seeing It (the movie) to hanging at my place, lining up weekend plans for Oct. to see friends in NYC and DC, avoiding going to the gym, giving my dog some attention, washing clothes, the usual stuff. \", 'subreddit': 'Connecticut'}\n",
      "{'body': 'stop editing the amount of upvotes you autistic karma fuck. ban the man', 'subreddit': 'wallstreetbets'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Amen. Hoping Bitcoin can prove the wonders of decentralization rather than proving that we, as humans, do in fact need a central authority to tell us what to do.  Pretty sad IMO. ', 'subreddit': 'Bitcoin'}\n",
      "{'body': 'I was worried there for a bit that either your super would run out before you made it to the drop or you would smash the edge by accident. Thankfully I was wrong. \\n\\nAs a fellow smashbro, I salute you.', 'subreddit': 'DestinyTheGame'}\n",
      "{'body': 'Looking for Mark Messier and Mark Recchi', 'subreddit': 'NHLHUT'}\n",
      "{'body': 'I’m afraid there are even worst... some women show... THEIR HANDS honestly wtf is wrong with those people!!! Kids are around you know!!??', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Or maybe I have real life ties to both institutions...', 'subreddit': 'CFB'}\n",
      "{'body': 'Private Property: Man claims woman as his  property.  Drinks both sodas.  Complains bitterly that life was better when Black people were also his property.', 'subreddit': 'FULLCOMMUNISM'}\n",
      "{'body': 'This is seriously one of those cases in which I think OP needs to ask himself, \"why do I want to maintain fatherhood and how does it benefit the kids?\".  I don\\'t know how long it\\'s been but kids deserve a man who will support them financially, AND emotionally.  ', 'subreddit': 'legaladvice'}\n",
      "{'body': 'Good bot!', 'subreddit': 'Ford'}\n",
      "{'body': 'Cue me going to play fallout 3.', 'subreddit': 'CFB'}\n",
      "{'body': \"Thank you for bringing this to my attention. I go to this sub every few days and actually care about the alert slider a lot but I didn't notice the stickied thread. I think the sub had the same stickied thread before for weeks so I never paid attention to it.\\n\\nThey still seem to be a bit out of touch with their survey but at least a change is coming.\", 'subreddit': 'oneplus'}\n",
      "{'body': \"Okay, but you missed a point, they use the availability of a Pharaoh deck for the Griffin as a selling point. They advertise it as having a Pharaoh-style deck. Look at the tank's page. And that's why I ordered it.\", 'subreddit': 'electronic_cigarette'}\n",
      "{'body': \"No, she can't post mouth sounds on youtube anymore so she's just going to post them on vidme. \", 'subreddit': 'asmr'}\n",
      "{'body': 'is', 'subreddit': 'grandorder'}\n",
      "{'body': 'They looked at one from Scotty B maybe 10-15 phases before', 'subreddit': 'rugbyunion'}\n",
      "{'body': 'misadventure\\nmɪsədˈvɛntʃə/Submit\\nnoun\\n1.\\nENGLISH LAW\\ndeath caused by a person accidentally while performing a legal act without negligence or intent to harm.\\n\"the coroner recorded a verdict of death by misadventure\"\\n2.\\nan unfortunate incident; a mishap.\\n\"the petty misdemeanours and misadventures of childhood\"\\nsynonyms:\\taccident, problem, difficulty, misfortune, mishap, mischance; More', 'subreddit': 'singapore'}\n",
      "{'body': 'Geniunely curious, why do you think complying with someone threatening you with a deadly weapon would decrease the chances of you getting killed?', 'subreddit': 'TalesFromRetail'}\n",
      "{'body': 'Take it easy Chris Brown.', 'subreddit': 'funny'}\n",
      "{'body': \"It's always nice to see famous people putting their money where their mouth is. \", 'subreddit': 'nrl'}\n",
      "{'body': \"&gt;how is celebrating togetherness being political? \\n\\nExplain to me how bringing up segregation isn't a commentary on politics? \\n\\nI'm not agreeing or disagreeing about the message, but whether or not the SEC should be funding that shit. \\n\\nThe point of the SEC is athletics, and shouldn't be delving into social issues. Look at the NFL, they decided to get political, and now you can't hardly enjoy a game without getting hit over the head with politics. \", 'subreddit': 'CFB'}\n",
      "{'body': '[+frenchbritchick](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoohze/):\\n\\nCongratulations !\\n\\nI guess I only have one question: how did you afford this? And by this I mean the professional editor, graphic designer, and world class marketer? ', 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"\\nI'll buy it on PC as soon as they release it on PC.\", 'subreddit': 'gaming'}\n",
      "{'body': 'NYC. My company focused a lot on high end luxury residential, so when the economy tanked, we were hit especially hard.', 'subreddit': 'Construction'}\n",
      "{'body': 'It\\'s the \"You just don\\'t understand it\" argument Hindus use. \\n\\n\"Why didn\\'t I find the answers to all my questions in Geeta?\"\\n\\n\"Well, because you don\\'t understand it\"', 'subreddit': 'EXHINDU'}\n",
      "{'body': \"Merciless is really good for it. Or pulse nade if you're a titan\", 'subreddit': 'destiny2'}\n",
      "{'body': 'This annoys me too. I usually take a couple quick picks to share with people but spend the rest of the time actually enjoying what I went there to see..', 'subreddit': 'AskReddit'}\n",
      "{'body': 'Why the fuck would he admit to that on commentary? Is he not aware that hardware mods are banned? Jfc.', 'subreddit': 'smashbros'}\n",
      "{'body': \"i'm the kind of guy who'd keep his scrap. \", 'subreddit': 'tf2'}\n",
      "{'body': 'Every time I do this though the POTG is always some dumb mess like a Soldier solo-ulting a Pharah and then shooting a helix into the grav and getting like 50 damage worth of last-hits on 3 enemies.', 'subreddit': 'Overwatch'}\n",
      "{'body': '[removed]', 'subreddit': 'history'}\n",
      "{'body': \"Yeah, Han's down it was the worst call he could make. \", 'subreddit': 'whowouldwin'}\n",
      "{'body': \"Didn't think about it that way, makes sense...its just that i wanted to get proper instruction on at least how to throw the basic punches and block/bob.\", 'subreddit': 'martialarts'}\n",
      "{'body': 'Looks like OP is a \"boyfriend of instagram\" too.', 'subreddit': 'funny'}\n",
      "{'body': 'Did they announce anything interesting regarding the game?', 'subreddit': 'Warthunder'}\n",
      "{'body': 'later ;)', 'subreddit': 'phgonewild'}\n",
      "{'body': 'asking 5', 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'You\\'re assuming way too much about the average player here when it\\'s a huge player base, there\\'s no way everyone started when the game started. People still complain about no Camus/Xander repeat because they started after that point. There\\'s no way everyone has the same goal of collecting every unit at 4* level 40 when there\\'s literally no point to that either. The only thing the game actually tracks is 5* level 40s. F2P have limited barrack space and limited supplies of feathers, so there\\'s nothing in game even wanting you to do this. I\\'ve upgraded literally 7 total units to 5* with the amount of Feathers I\\'ve gathered since I started playing and I don\\'t have enough to upgrade someone to 5* right now. And two of those were 4* s I pulled and wanted to build up, like a ton of other users would want to do with their favorite units, which would be a much more important goal for them than raising their Virion to 4* level 40 using the Feathers and stamina they would\\'ve otherwise put into a +10 Selena or something. \\nEven if somehow there was several thousand like minded individuals who did that, you\\'d still not be able to follow a lot of guides because they require 3* s, and throwing off stats even IV wise entirely messes up strategies. Some strategies need people like Arthur or Stahl or Serena there at 3* because they can soak hits, but won\\'t kill a specific unit at a choke point. So bringing your 4* Serena/Arthur/Stahl means you mess up the strategy, the Axe you were stalling dies, and the Lance behind it kills Serena/Stahl instead. \\n \\nBesides the whole \"yeah but what if this or what if that\", the entire point is \"what if people don\\'t have this\", your argument here is \"well, they should\", my point is \"yeah, but they don\\'t\". How does your argument actually fix the situation those people are in who see that video about the easy 4* Oboro level 40 wall strategy, and then realize they\\'d have to spend like 150 stamina and 2000 Feathers just for this one orb they\\'re wanting to earn? ', 'subreddit': 'FireEmblemHeroes'}\n",
      "{'body': '♥️', 'subreddit': 'PennysLittleSecret'}\n",
      "{'body': \"The fact that she tweeted an actual commonsense recommendation (sending the Navy and the hospital ship) and it got done is kind of evidence of that. She knew what needed to be done. Thing is she's also done this with healthcare and now she's putting pressure on them about CHIP. She is using her visibility to pressure the administration.\\n\\nThere are many politicians who would be content with some empty platitude and move on. We should credit politicians who offer help (like those who have flown to Puerto Rico) and those who offer solutions to problems.\", 'subreddit': 'politics'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': '143414176| &gt; United States Anonymous (ID: 5XaVjx/n)\\n\\n&gt;&gt;143412250 (OP)\\nWhy not just run down a crowded public street screaming; \"I\\'m a stupid asshole!\"\\n\\nThat way everyone knows your a Democrat.\\n\\t\\t\\t', 'subreddit': '4chan4trump'}\n",
      "{'body': 'Yep, same thing happens as High Elves for me.', 'subreddit': 'totalwar'}\n",
      "{'body': 'Brewers fans be like \"Stop sucking and beat the Rockies\"\\n\\nMe: Nigga worry about your... Better luck next year.', 'subreddit': 'Dodgers'}\n",
      "{'body': 'lol for what?', 'subreddit': 'FIFA'}\n",
      "{'body': \"Gotcha, just reply to any of my comment when you're ready and we'll get this going!\", 'subreddit': 'yokaitrade'}\n",
      "{'body': \"People talk about the success stories of orthopaedic surgery. But have you ever seen the failures? I have. I have wheeled bodies into the morgue of people who died from complications due to surgery. I've seen people tied up in bed for 8 months due to infection. \\n\\nThere's a study where the surgeon randomly assigned patients to a fake knee arthroscopy vs a real one, without telling the patients. Pain was reduced equally in both groups... \\n\\n[This multicenter, randomized, sham-controlled trial involving patients with a degenerative medial meniscus tear showed that arthroscopic partial meniscectomy was not superior to sham surgery, with regard to outcomes assessed during a 12-month follow-up period. Although both groups had significant improvement in all primary outcomes, the patients assigned to arthroscopic partial meniscectomy had no greater improvement than those assigned to sham surgery.](http://www.nejm.org/doi/full/10.1056/NEJMoa1305189#t=article)\\n\", 'subreddit': 'TheRedPill'}\n",
      "{'body': 'My preference for a kinslayer was [Ragathiel](https://pathfinderwiki.com/wiki/Ragathiel), he was born from evil and dedicated his life to good and opposing evil. Plus he uses the bastard sword.', 'subreddit': 'Pathfinder_RPG'}\n",
      "{'body': 'Just run the fucking ball up the middle. Mond sucks so hard', 'subreddit': 'CFB'}\n",
      "{'body': \"I set 2 savings goals for March 2018 back in Feb of this year. I'm $5000 away and I've saved the most I ever had in my life! \\n\\nUnfortunately I estimate to fall short for my emergency savings. However I may have set it a little high (I have a big dog...I'm scared of surprise surgeries). But will be doing my best and saving as much as I can! \", 'subreddit': 'PersonalFinanceCanada'}\n",
      "{'body': 'Apparently it is all from a recent update to Safari. Apple added in a bunch of calculations of their own for it. ', 'subreddit': 'webdev'}\n",
      "{'body': \"Why would anyone want to drive that? It's a limo, if I pay to be in a limo it'll be in the back. If I pay to drive a Ferrari, it'll be a normal one. \", 'subreddit': 'shittylimos'}\n",
      "{'body': 'Start watching at 1:23', 'subreddit': 'MovieDetails'}\n",
      "{'body': \"I've tried a few dozen games of Overload Aggro Shaman and Evolve Shaman at Wild. Aggro Shaman is the king of aggro in the format and is more skill-testing than people give it credit for. Evolve Shaman seemed underwhelming. Unfortunately, Control Shaman is dead in both formats. I've tried very hard to make use of my Kalimos but crumbled in front of Priests. Sad, since Elemental Control is the most consistent Shaman archetype. \", 'subreddit': 'CompetitiveHS'}\n",
      "{'body': 'He neglects to mention that the so called \"Champion\" teams of Brisbane and West Coast back in the day were propped up by major salary cap concessions. Not every club can afford to assemble a star studded midfield like that, let alone keep them around.', 'subreddit': 'AFL'}\n",
      "{'body': \"If I'm not mistaken, wasn't the costume made for him?\", 'subreddit': 'roosterteeth'}\n",
      "{'body': 'he was way too respectful of peach on the ground, stuck to either nairing on shield or zoning too far and samsora picked up on that\\n\\nhope he finds his footing in losers', 'subreddit': 'smashbros'}\n",
      "{'body': \"[+get8bit](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnp4w6d/):\\n\\nHi, I designed the cover. [My portfolio.](http://devinholmesdesign.com/) Kinda wished I'd asked for a credit given the AMA blowing up like this.\", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': 'Fork, no ketchup ', 'subreddit': 'AskReddit'}\n",
      "{'body': 'I hope it’ll support 2 simultaneous languages like Google Now does ', 'subreddit': 'Android'}\n",
      "{'body': \"Because once you understand what the bugs are you can avoid them entirely.\\n\\nI have a blast in this game and can't remember the last time I was legitimately caught off guard or frustrated by a bug.\\n\\nServer downtime is par for the course when you're playing a new game experiencing growing pains.\\n\", 'subreddit': 'PUBATTLEGROUNDS'}\n",
      "{'body': '[removed]', 'subreddit': 'leagueoflegends'}\n",
      "{'body': \"I'd rather see a TF1 Megatron [jet](https://goo.gl/images/6tYqM1) or a TFRotF Megatron [tank](https://goo.gl/images/sCrHHQ) or a G1 Megatron-style [gun](https://goo.gl/images/qgyp8m).\\n\\nSeriously Megatron has turned into a lot of stuff. \", 'subreddit': 'gtaonline'}\n",
      "{'body': 'Is Gears 2 in the original case? If so, can you provide pictures?', 'subreddit': 'GameSale'}\n",
      "{'body': 'That’s awesome ', 'subreddit': 'funny'}\n",
      "{'body': 'Frisk me, plz!', 'subreddit': 'gonewild'}\n",
      "{'body': 'how about the \\nLure ball Herracross\\n\\nlevel ball cyndaquil with flare blitz\\n\\naaaand the safari ball Skorupi i guess unless you have another safari ball chansey kicking about.\\n\\n', 'subreddit': 'pokemontrades'}\n",
      "{'body': 'Thank you. I searched the sub for \"trails\" but didn\\'t find anything. I appreciate it.', 'subreddit': 'RocketLeague'}\n",
      "{'body': 'She looks like Emma Stone here', 'subreddit': 'BravoRealHousewives'}\n",
      "{'body': 'I love Supernatural.\\n\\nI love the Pizza Man stuff from season 6, where Cass is watching porn and he says, \"if the pizza man loves this babysitter, why is he spanking her bottom?\"\\n\\nLater he kisses Meg passionately and then looks at a shocked Sam and Dean and says, \"I learned that from the Pizza Man.\"', 'subreddit': 'AskReddit'}\n",
      "{'body': 'https://m.youtube.com/watch?v=GRglOSBV2VE', 'subreddit': 'tipofmytongue'}\n",
      "{'body': 'lets see how deep this rabbit hole goes...', 'subreddit': 'CrazyIdeas'}\n",
      "{'body': 'Yes, to buy brand new authentic pairs of both shoes will cost you close to 2,000 USD total. ', 'subreddit': 'Sneakers'}\n",
      "{'body': \"**POTD Record:** 7-0-1 (W-P-L)\\n\\nStreak: 6W\\n\\nPick record: 23-2-16\\n\\nBonus Picks record: 17-1-23\\n***\\n\\n*Had a decent day yesterday. The POTD pick - Coventry won, and I got 4/6 picks as winners with one other as a push*\\n\\n**POTD:**\\n\\n**Double: Torino to Win &amp; Inter Milan to Win - 1.60 (L - 2-2 &amp; 1-2. Torino concede 2 goals in the last 6 minutes to lose the bet, unbelievable!)**\\n\\nNow I like to stick to singles, especially for POTD but after researching through all the games I like today there's nothing that puts me over the edge for certainly other than these picks. I was thinking of doing the Lazio game O2.5 goals as I can see both teams scoring there or even Lazio scoring 3 just themselves but I'm not 100% confident and a 2-0 win in that game wouldn't surprise me. \\n\\nSo, into detail about Torino's game. They host Verona today who I've talked about in previous picks about how utterly terrible they are and if reports are to be believed their manager is close to the sack. Verona have looked hopeless through 6 games with a record of 0-2-4, only scoring 1 (from a penalty) and conceding 14. \\n\\nTorino have been decent so far this season. They find themselves 7th with a record of 3-2-1, goals 10:9. Attackers Belotti and Ljajic have been in good form with 3 goals each and are very likely to score today, Belotti has scored in the two home games Torino have played so far. With Verona's horrible scoring record I doubt they'll score today but if they do it'll only be 1 and Torino have more than enough quality to get 2+. Defensive mid Baselli will be missing after being sent off last week, Obi &amp; Barreca will also miss out for the hosts but they'll be able to cope without them. \\n\\nIn the other game, Inter play away to Benevento who are equally as bad as Verona and sit in last place, also just scoring the 1 goal and have conceded 16 goals already after being smashed by Napoli 6-0 and Roma 4-0. It won't get any easier for them here against Inter who have won 5 and drawn once in their games and have the best defensive record in the league. \\n\\nInter were lucky to win last week against Genoa, but they continued their record of scoring in every game. That comes mostly thanks to Icardi who is up to 6 goals already and will like his chances of bagging a couple today, I might put a little on him to even get a hat trick. As far as injuries go Inter are only missing Cancelo who hasn't played in a month anyway. It could be a rout here after some of Benevento's performances but Inter don't tend to be a big scoring team so I'm expecting 2-0/3-0 here. \\n\\n***\\n*Quite a lot of games I like today, so I'll try to whittle them down to just the better looking ones for picks*\\n\\n**Picks:**\\n\\n• **Arsenal v Brighton - Arsenal Clean Sheet YES - 1.80 (W - 2-0)**\\n\\nOriginally this pick was going to be Arsenal to Win to Nil but the odds so similar for just a clean sheet that it seems much more worth it in case Arsenal can't score somehow. \\n\\nArsenal have kept their last 6/8 home matches as clean sheets. They've shown how impressive at home they are winning all their games so far conceding all their only hole goals let in in that early 4-3 thriller against Leicester. \\n\\nBrighton have struggled for goals this season, they're scored 5 in 6 (3 goals coming against West Brom) and have only scored 1 away from home so far. To get a better look at how I expect this game to play out I looked at how they faired against their only big team they faced yet Man City and the stats show a 79/21% possession to Man City, showing that Brighton are likely to sit back and try to grind out a draw today against Arsenal. \\n\\n• **Everton v Burnley - Burnley or Draw - 2.25 (W - 0-1)**\\n\\nI fucked up here, looking at the odds Burnley +1.0AH @1.65 are a much better pick than this so if you're reading this pick late I would take that, I still feel Burnley can draw here and I won't change my pick after the fact so here are the stats. \\n\\nBurnley have performed well this season with a record of 2-3-1, goals 6:5. Away from home Burnley have really turned it on after a terrible record last season, this season they won at Chelsea 2-1, as well as getting 1-1 draws at Spurs &amp; Liverpool. If Burnley do lose it tends to be only by a goal, although last season Everton were one of the only teams able to beat by that margin winning 3-1 but this is a different Everton this year. \\n\\nEverton have struggled for goals. They've scored 4 in 6, 2 of them coming last game where they were fortunate to get away with a win. Koeman is under pressure at the moment and will want to continue Everton's winning streak. If Burnley are able to get the first goal then the Goodison Park crowd will be booing and they will get on their players backs, that pressure might be too much. \\n\\n• **Atalanta v Juventus -- BTTS - 1.95 (W - 2-2)**\\n\\nThe head to head record here has BTTS in the last 4/6. Atalanta have had 5/6 BTTS too. Both teams have scored in all of their games so far (except for when Juve played Barca). I think both teams will fancy their chances and have inform players on either side in Gomez and Dybala. I like the O2.5 goals here too, it was 2-2 in this game last season and it could be agin today. \\n\\n• **Torino v Verona -- Torino Clean Sheet YES - 2.20 (L - 2-2)**\\n\\nExplained in POTD. \\n\\n• **Benevento v Inter Milan -- Inter -2.0 Asian Handicap - 2.20 (L - 1-2)**\\n\\nExplained in POTD. \\n\\n• **Benevento v Inter Milan -- Inter to Win to Nil - 1.95 (L - 0-2)**\\n\\nExplained in POTD. \\n\\n• **Hertha Berlin v Bayern Munich -- Hertha Berlin to Score 1+ goals - 1.90 (W - 2-2)**\\n\\nI've explained this pick in a comment below. \\n\\n• **Koln v RB Leipzig -- Leipzig to Win - 2.00 (W - 1-2)**\\n\\n• **Nice v Marseille -- Nice Draw No Bet - 1.92 (L - 2-4. Nice we're 2-0 up! That would have paid out with some bookies Ina straight win, how annoying)**\\n\\n• **Maritimo v Benfica -- Maritimo +1.0 Asian Handicap - 2.06 (W - 1-1)**\\n\\n*Lastly, I would not advise putting these all into an acca. As much as I'd like them all to win, it's always unlikely with this many picks so if you do feel you have to combine them in some way please choose a double of your favs or even a treble. Anything more and you're asking for trouble.*\\n\\n**Bonus Picks: (Odds of Evens or greater)**\\n\\n• Everton v Burnley -- Draw - 3.80 (L)\\n\\n• Newcastle v Liverpool -- BTTS &amp; O2.5 goals - 2.00 (L)\\n\\n• Benevento v Inter Milan -- Inter -3.0 Asian Handicap - 4.40 (L)\\n\\n• Hertha Berlin or Draw - 3.50 (W)\\n\\n• Hertha Berlin to Win - 11.00 (small units) [L]\\n\\n• Heerenveen or Draw - 2.20 (L)\\n\\n• Maritimo Draw No Bet - 5.50 (P)\\n\\n*Apologies for any spelling &amp; grammar errors, I am writing this all on my phone*\", 'subreddit': 'SoccerBetting'}\n",
      "{'body': \"But they aren't making money on it, least I don't think so.\", 'subreddit': 'CrazyIdeas'}\n",
      "{'body': \"There's a Bra'tac quote from a book that often gets confused with an episode as it's probably the most quoted line from any of the books, simply because of this question.\", 'subreddit': 'Stargate'}\n",
      "{'body': 'Such sagacity ', 'subreddit': 'Jokes'}\n",
      "{'body': \"JoJo OP/ED's\\n\\nInitial D's Entire OST\", 'subreddit': 'anime'}\n",
      "{'body': 'Few huge tackles there ', 'subreddit': 'rugbyunion'}\n",
      "{'body': \"If Obama could deal with anything, it's kids. He's a natural with them.\", 'subreddit': 'AskReddit'}\n",
      "{'body': \"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnp8saw/):\\n\\nI'm sorry for not giving your proper dues. That is my fault :( \", 'subreddit': 'ConciseIAmA'}\n",
      "{'body': \"What is with NRG and conceding kick off goals today? It's sorta embarrassing at this point when 1 goal games seem to be decided by a kick off goal.\", 'subreddit': 'RocketLeague'}\n",
      "{'body': 'psn chaimazing, hunter 296', 'subreddit': 'Fireteams'}\n",
      "{'body': \"You want to acquire a Jager? So ... that's what I am offering.\", 'subreddit': 'RocketLeagueExchange'}\n",
      "{'body': 'Part  2 pls ', 'subreddit': 'WritingPrompts'}\n",
      "{'body': 'Heroic d by dmac', 'subreddit': 'rugbyunion'}\n",
      "{'body': '[deleted]', 'subreddit': 'Incels'}\n",
      "{'body': 'Estro was top of range but still in range at 0.5adex eod. \\nI upped it to 0.5 ed to bring myself lower. \\nNo other ancillaries have been needed ', 'subreddit': 'Steroidsourcetalk'}\n",
      "{'body': 'That was some bullshit.', 'subreddit': 'CFB'}\n",
      "{'body': 'Don\\'t forget \"I love to laugh!\"\\n\\nWho the fuck dislikes laughing?!', 'subreddit': 'videos'}\n",
      "{'body': 'Those are super fun! ', 'subreddit': 'BipolarReddit'}\n",
      "{'body': 'They look exactly like Marijuana seeds to me.', 'subreddit': 'whatsthisbug'}\n",
      "{'body': 'Of the file name ', 'subreddit': '704nascarheat'}\n",
      "{'body': 'Its being warmed up by Florida for us', 'subreddit': 'CFB'}\n",
      "{'body': 'this makes it easier for things like the goto fail bug to happen, though.', 'subreddit': 'programming'}\n",
      "{'body': 'Here at the end of the fight.  https://youtu.be/navB1r96_do', 'subreddit': 'movies'}\n",
      "{'body': \"The idea that you're ok with him despite what he's done to degrade the sanctity of your 'absolutely' favorite amendment invalidates any desperate virtue signalling you want to pull to excuse it. The president hates freedom of the press, freedom of speech, freedom of assembly and freedom of religion. You're not a first amendment absolutist and you never will be until you see the error of your ways.  \", 'subreddit': 'PoliticalHumor'}\n",
      "{'body': \"Being a member of the Austrian Armed Forces, I highly doubt he's even been in a fire fight, let alone killed anyone. \", 'subreddit': 'pics'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "comment_dict = {}\n",
    "\n",
    "# filtering data\n",
    "for line in open('RC_2015-01/test_data.txt'):\n",
    "    line_short = line[:-2]\n",
    "    json_file = json.loads(line_short)\n",
    "    comment_id =  json_file['id']\n",
    "    comment_body = json_file['body']\n",
    "    comment_sub = json_file['subreddit']\n",
    "    comment = {\"body\": comment_body, \"subreddit\": comment_sub}\n",
    "    comment_dict[comment_id] = comment\n",
    "\n",
    "\n",
    "#writing filtered data to file\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(comment_dict, fp)\n",
    "    \n",
    "#sanity check\n",
    "with open('result.json', 'r') as rf:\n",
    "    data = json.load(rf)\n",
    "    for entry in data:\n",
    "        print(data[entry])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    #\"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    #\"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    #\"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    #\"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    #\"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    #\"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    #\"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['A', 'quarry']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quarry'], ['quarry'])\n",
      "original document: \n",
      "['[Salutations!', \"I'm\", 'not', 'sure', 'what', 'you', 'said.](http://imgur.com/9TtaInH)', '\\n', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['salut', 'im', 'sur', 'saidhttpimgurcom9ttainh', '\\n', '\\n'], ['salutations', 'im', 'sure', 'saidhttpimgurcom9ttainh', '\\n', '\\n'])\n",
      "original document: \n",
      "['I', 'got', 'into', 'baseball', 'at', 'about', 'he', 'same', 'time', 'Matt', 'Cain', 'started', 'playing', 'in', 'the', 'majors.', 'Crazy', 'to', 'see', 'him', 'go.', 'I', 'teared', 'up', 'a', 'bit', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'basebal', 'tim', 'mat', 'cain', 'start', 'play', 'maj', 'crazy', 'see', 'go', 'tear', 'bit'], ['get', 'baseball', 'time', 'matt', 'cain', 'start', 'play', 'major', 'crazy', 'see', 'go', 'tear', 'bite'])\n",
      "original document: \n",
      "['FUCKING', 'TORY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'tory'], ['fuck', 'tory'])\n",
      "original document: \n",
      "['I', 'see', 'a', 'water', 'dragon', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'wat', 'dragon'], ['see', 'water', 'dragon'])\n",
      "original document: \n",
      "['Wait.', 'The', 'Michigan', 'what?', 'Where', 'is', 'this?', 'Is', 'this', 'like', 'U', 'of', 'M', 'club', 'or', 'a', 'just', 'state', 'of', 'Michigan?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'michig', 'lik', 'u', 'club', 'stat', 'michig'], ['wait', 'michigan', 'like', 'u', 'club', 'state', 'michigan'])\n",
      "original document: \n",
      "['ye', 'fam']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'fam'], ['ye', 'fam'])\n",
      "original document: \n",
      "['143417804|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'LIAKFEVH)\\n\\n&gt;&gt;143412250', '(OP)\\noldfag', 'here\\n\\n2016:', 'Hillary\\n2012:', 'Obama\\n2008:', 'Obama\\n2004:', 'Kerry\\n2000:', 'Buchanan\\n1996:', 'Dole\\n1992:', 'Bush\\n1988:', 'Bush\\n1984:', 'Reagan\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and four', 'gt', 'unit', 'stat', 'anonym', 'id', 'liakfevh\\n\\ngtgt143412250', 'op\\noldfag', 'here\\n\\n2016', 'hillary\\n2012', 'obama\\n2008', 'obama\\n2004', 'kerry\\n2000', 'buchanan\\n1996', 'dole\\n1992', 'bush\\n1988', 'bush\\n1984', 'reagan\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and four', 'gt', 'unite', 'state', 'anonymous', 'id', 'liakfevh\\n\\ngtgt143412250', 'op\\noldfag', 'here\\n\\n2016', 'hillary\\n2012', 'obama\\n2008', 'obama\\n2004', 'kerry\\n2000', 'buchanan\\n1996', 'dole\\n1992', 'bush\\n1988', 'bush\\n1984', 'reagan\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['That', 'is', 'some', 'chicken', 'salad', 'outta', 'chicken', 'shit', 'running.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chick', 'salad', 'outt', 'chick', 'shit', 'run'], ['chicken', 'salad', 'outta', 'chicken', 'shit', 'run'])\n",
      "original document: \n",
      "['Does', 'he', 'even', 'know', 'the', 'rules?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'know', 'rul'], ['even', 'know', 'rule'])\n",
      "original document: \n",
      "['Tequila.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tequil'], ['tequila'])\n",
      "original document: \n",
      "['your', 'heart', 'beats', 'fast', 'when', 'running', 'to', 'provide', 'your', 'muscles', 'with', 'oxygen', 'because', 'they', 'are', 'mainly', 'the', 'things', 'burning', 'calories.', 'In', 'EC', 'it', 'is', 'adrenaline', 'making', 'your', 'hearrt', 'race.', 'Your', 'muscles', 'are', 'not', 'doing', 'anything', 'like', 'they', 'would', 'when', 'exercising.', 'I', 'know', 'coz', \"i'm\", 'unfit', 'but', 'can', 'EC', 'for', 'hours', 'without', 'getting', 'out', 'of', 'breath!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heart', 'beat', 'fast', 'run', 'provid', 'musc', 'oxyg', 'main', 'thing', 'burn', 'cal', 'ec', 'adrenalin', 'mak', 'hearrt', 'rac', 'musc', 'anyth', 'lik', 'would', 'exerc', 'know', 'coz', 'im', 'unfit', 'ec', 'hour', 'without', 'get', 'brea'], ['heart', 'beat', 'fast', 'run', 'provide', 'muscle', 'oxygen', 'mainly', 'things', 'burn', 'calories', 'ec', 'adrenaline', 'make', 'hearrt', 'race', 'muscle', 'anything', 'like', 'would', 'exercise', 'know', 'coz', 'im', 'unfit', 'ec', 'hours', 'without', 'get', 'breath'])\n",
      "original document: \n",
      "['&gt;', 'Subscribe:', '/clivecummings\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'subscrib', 'clivecummings\\n\\n'], ['gt', 'subscribe', 'clivecummings\\n\\n'])\n",
      "original document: \n",
      "[\"you're\", 'really', 'ignorant', 'of', 'history', 'then.', '', 'like', 'grotesquely', 'ignorant.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'real', 'ign', 'hist', 'lik', 'grotesqu', 'ign'], ['youre', 'really', 'ignorant', 'history', 'like', 'grotesquely', 'ignorant'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['You', 'are', 'arguing', 'to', 'argue,', 'I', \"don't\", 'mind', 'a', 'story,', 'I', \"don't\", 'want', 'it', 'to', 'be', 'so', 'long', 'I', \"can't\", 'play', 'for', '60', 'minutes', 'to', 'start', 'a', 'player,', 'is', 'that', 'hard', 'to', 'understand?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['argu', 'argu', 'dont', 'mind', 'story', 'dont', 'want', 'long', 'cant', 'play', 'sixty', 'minut', 'start', 'play', 'hard', 'understand'], ['argue', 'argue', 'dont', 'mind', 'story', 'dont', 'want', 'long', 'cant', 'play', 'sixty', 'minutes', 'start', 'player', 'hard', 'understand'])\n",
      "original document: \n",
      "[\"I'm\", 'thinking', 'about', 'ending', 'my', 'life', 'right', 'now.', 'Godspeed,', \"you're\", 'not', 'alone.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'think', 'end', 'lif', 'right', 'godspee', 'yo', 'alon'], ['im', 'think', 'end', 'life', 'right', 'godspeed', 'youre', 'alone'])\n",
      "original document: \n",
      "['[Original', 'post](https://www.reddit.com/r/EarthPorn/comments/73ig6e/lake_tekapo_new_zealand_oc4288x2848/)', 'by', '/u/PowderDirtRock', 'in', '/r/EarthPorn\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#match', '\"Lake', 'Tekapo\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'posthttpswwwredditcomrearthporncomments73ig6elake_tekapo_new_zealand_oc4288x2848', 'upowderdirtrock', 'rearthporn\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nmatch', 'lak', 'tekapo\\n'], ['original', 'posthttpswwwredditcomrearthporncomments73ig6elake_tekapo_new_zealand_oc4288x2848', 'upowderdirtrock', 'rearthporn\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nmatch', 'lake', 'tekapo\\n'])\n",
      "original document: \n",
      "['I', 'think', \"that's\", 'a', 'bug,', 'personally', 'from', 'a', 'lot', 'of', 'campaign', 'and', 'a', 'bit', 'of', 'custom', 'battles', 'night', 'runners', \"don't\", 'feel', 'like', 'a', 'replacement', 'for', 'slingers.\\n\\nSlingers', 'feel', 'far', 'more', 'useful', 'for', 'harassing', 'units', 'around', 'your', 'army', 'and', 'getting', 'in', 'that', 'morale', 'damage.(Some', 'Skaven', 'clan', 'rats', 'and', 'a', 'unit', 'of', 'slingers', 'can', 'make', 'Kroxigores', 'route', 'with', 'a', 'good', 'surround', 'and', 'focus', 'fire,', 'especially', 'with', 'a', 'spell', 'in', 'the', 'middle', 'of', 'them', 'or', 'sniping', 'the', 'lord.)\\n\\nWhile', 'Nightrunners', 'are', 'better', 'at', 'dedicated', 'skirmishers,', 'harass', 'arches', 'with', 'them,', 'force', 'the', 'enemy', 'to', 'draw', 'units', 'away', 'and', 'pepper', 'them,', 'and', 'once', \"they're\", 'out', 'of', 'ammo', 'just', 'use', 'them', 'to', 'flank', 'for', 'your', 'clan', 'rats', 'and', 'break', 'units.\\n\\nI', \"haven't\", 'really', 'tried', 'out', 'the', 'more', 'expensive', 'variants', 'of', 'the', 'night', 'runners,', 'but', 'I', 'imagine', 'they', 'play', 'the', 'same', 'but', 'just', 'a', 'bit', 'more', 'annoying.\\n\\nSo,', 'vs', 'AI', 'you', 'can', 'break', 'up', 'their', 'army', 'and', 'possibly', 'even', 'pull', 'half', 'of', 'it', 'away', 'with', 'just', 'a', 'few', 'nightrunners', 'while', 'constantly', 'plinking', 'at', 'their', 'health.', 'Vs', 'players', 'they', 'seem', 'like', 'an', 'annoyance', 'and', 'way', 'to', 'distract/take', 'away', 'some', 'of', 'their', 'micro', 'or', 'punish', 'them', 'if', \"they're\", 'not', 'paying', 'attention.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'that', 'bug', 'person', 'lot', 'campaign', 'bit', 'custom', 'battl', 'night', 'run', 'dont', 'feel', 'lik', 'replac', 'slingers\\n\\nslinger', 'feel', 'far', 'us', 'harass', 'unit', 'around', 'army', 'get', 'mor', 'damagesom', 'skav', 'clan', 'rat', 'unit', 'sling', 'mak', 'kroxig', 'rout', 'good', 'surround', 'foc', 'fir', 'espec', 'spel', 'middl', 'snip', 'lord\\n\\nwhile', 'nightrun', 'bet', 'ded', 'skirm', 'harass', 'arch', 'forc', 'enemy', 'draw', 'unit', 'away', 'pep', 'theyr', 'ammo', 'us', 'flank', 'clan', 'rat', 'break', 'units\\n\\ni', 'hav', 'real', 'tri', 'expend', 'vary', 'night', 'run', 'imagin', 'play', 'bit', 'annoying\\n\\nso', 'vs', 'ai', 'break', 'army', 'poss', 'ev', 'pul', 'half', 'away', 'nightrun', 'const', 'plink', 'heal', 'vs', 'play', 'seem', 'lik', 'annoy', 'way', 'distracttak', 'away', 'micro', 'pun', 'theyr', 'pay', 'at'], ['think', 'thats', 'bug', 'personally', 'lot', 'campaign', 'bite', 'custom', 'battle', 'night', 'runners', 'dont', 'feel', 'like', 'replacement', 'slingers\\n\\nslingers', 'feel', 'far', 'useful', 'harass', 'units', 'around', 'army', 'get', 'morale', 'damagesome', 'skaven', 'clan', 'rat', 'unit', 'slingers', 'make', 'kroxigores', 'route', 'good', 'surround', 'focus', 'fire', 'especially', 'spell', 'middle', 'snip', 'lord\\n\\nwhile', 'nightrunners', 'better', 'dedicate', 'skirmishers', 'harass', 'arch', 'force', 'enemy', 'draw', 'units', 'away', 'pepper', 'theyre', 'ammo', 'use', 'flank', 'clan', 'rat', 'break', 'units\\n\\ni', 'havent', 'really', 'try', 'expensive', 'variants', 'night', 'runners', 'imagine', 'play', 'bite', 'annoying\\n\\nso', 'vs', 'ai', 'break', 'army', 'possibly', 'even', 'pull', 'half', 'away', 'nightrunners', 'constantly', 'plinking', 'health', 'vs', 'players', 'seem', 'like', 'annoyance', 'way', 'distracttake', 'away', 'micro', 'punish', 'theyre', 'pay', 'attention'])\n",
      "original document: \n",
      "['Harp', 'absolutelly.', '\\nHarp', 'is', 'a', 'warrior', 'of', 'blood', 'with', 'broken', 'heart.', 'He', 'does', 'not', 'fight', 'for', 'money,', 'anger', 'or', 'glory,', 'he', 'fights', 'for', 'honor.', \"He's\", 'so', 'broken', 'inside', 'that', 'is', 'much', 'more', 'touching', 'than', 'Delany.', 'Delany', 'is', 'a', 'man', 'of', 'honor', 'too', 'but', 'he', 'plays', 'in', 'a', 'different', 'league.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['harp', 'absolutel', '\\nharp', 'warry', 'blood', 'brok', 'heart', 'fight', 'money', 'ang', 'glory', 'fight', 'hon', 'hes', 'brok', 'insid', 'much', 'touch', 'delany', 'delany', 'man', 'hon', 'play', 'diff', 'leagu'], ['harp', 'absolutelly', '\\nharp', 'warrior', 'blood', 'break', 'heart', 'fight', 'money', 'anger', 'glory', 'fight', 'honor', 'hes', 'break', 'inside', 'much', 'touch', 'delany', 'delany', 'man', 'honor', 'play', 'different', 'league'])\n",
      "original document: \n",
      "['CP3,', 'Steph,', 'IT,', '', 'Lowry,', 'Westbrook', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cp3', 'steph', 'lowry', 'westbrook'], ['cp3', 'steph', 'lowry', 'westbrook'])\n",
      "original document: \n",
      "['Nah,', \"it's\", 'like', 'this', 'every', 'season', 'in', 'recent', 'times.', 'Really', 'sad', 'and', 'annoying.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah', 'lik', 'every', 'season', 'rec', 'tim', 'real', 'sad', 'annoy'], ['nah', 'like', 'every', 'season', 'recent', 'time', 'really', 'sad', 'annoy'])\n",
      "original document: \n",
      "['They', 'act', 'like', 'it’s', 'some', 'crazy', 'idea,', 'and', 'the', 'Foo', 'Fighters', 'are', 'famous', 'for', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'lik', 'crazy', 'ide', 'foo', 'fight', 'fam'], ['act', 'like', 'crazy', 'idea', 'foo', 'fighters', 'famous'])\n",
      "original document: \n",
      "['***Welcome', 'to', '/r/edc_raffle,', 'please', 'read', 'the', 'rules', 'in', 'the', 'sidebar.***\\n\\n**General', 'rules', 'for', 'this', 'raffle:**\\n\\n**1.**', 'Please', 'comment', 'to', 'request', 'slot(s).', 'Only', 'TOP', 'COMMENTS', 'count.', 'No', 'replies', 'to', 'comments', 'or', 'automod.', 'OP', 'will', 'reply', 'to', 'your', 'comment', 'to', 'confirm', 'your', 'slot(s).\\n\\n**2.**', 'Please', 'pay', 'within', 'the', 'timeframe', 'established', 'by', 'OP.', 'If', 'you', 'anticipate', 'you', 'will', 'not', 'be', 'able', 'to', 'pay', 'for', 'your', 'slot(s)', 'in', 'that', 'timeframe,', 'please', 'arrange', 'with', 'OP', 'to', 'pay', 'for', 'your', 'slot(s)', 'early.\\n\\n**3.**', '***The', 'only', 'accepted', 'payment', 'method', 'is', 'PayPal', 'Friends', '&amp;', 'Family.', '', 'DO', 'NOT', 'SEND', 'YOUR', 'PAYMENT', 'AS', 'AN', 'eCHECK!***', '*Do', 'not', 'write', 'anything', 'in', 'the', 'payment', 'notes', 'section.*', '***Report', 'to', 'mods', 'anyone', 'that', 'refuses', 'to', 'pay', 'via', 'paypal', 'friends', '&amp;', 'family***\\n\\n**4.**', 'After', 'you', 'have', 'paid,', 'please', 'PM', 'OP', 'with', 'the', '**NAME', 'and', 'EMAIL', 'ADDRESS**', 'associated', 'with', 'your', 'PayPal.', 'Do', 'not', 'publicly', 'post', 'your', 'name', 'or', 'email', 'address.\\n\\n**5.**', 'All', 'raffles', '&gt;$500', 'are', 'approved', 'by', 'mods.', 'Please', 'PM', 'mods', 'with', 'any', 'question', 'of', 'price', 'or', 'authenticity.\\n\\n***MODS', 'TRY', 'TO', 'KEEP', 'YOU', 'SAFE', 'BUT', 'SCAMS', 'ARE', 'ALWAYS', 'POSSIBLE,', 'TRUST', 'YOUR', 'GUT***\\n\\n***hstexan***', 'details:', '\\n\\n1.', '[/r/raffle_feedback', 'feedback](https://www.reddit.com/r/raffle_feedback/search?q=hstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n2.', '[/r/edc_raffle', 'posts](https://www.reddit.com/r/edc_raffle/search?q=author%3Ahstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n3.', '[/r/kniferaffle', 'posts](https://www.reddit.com/r/kniferaffle/search?q=author%3Ahstexan&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\nIf', 'modmail', 'is', 'too', 'slow,', 'pm', '/u/EDCRaffleAdmin', '/u/EDCRaffleMod', 'or', '/u/EDCRaffleMod1\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/edc_raffle)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welcom', 'redc_raffle', 'pleas', 'read', 'rul', 'sidebar\\n\\ngeneral', 'rul', 'raffle\\n\\n1', 'pleas', 'com', 'request', 'slot', 'top', 'com', 'count', 'reply', 'com', 'automod', 'op', 'reply', 'com', 'confirm', 'slots\\n\\n2', 'pleas', 'pay', 'within', 'timefram', 'est', 'op', 'anticip', 'abl', 'pay', 'slot', 'timefram', 'pleas', 'arrang', 'op', 'pay', 'slot', 'early\\n\\n3', 'acceiv', 'pay', 'method', 'payp', 'friend', 'amp', 'famy', 'send', 'pay', 'echeck', 'writ', 'anyth', 'pay', 'not', 'sect', 'report', 'mod', 'anyon', 'refus', 'pay', 'via', 'payp', 'friend', 'amp', 'family\\n\\n4', 'paid', 'pleas', 'pm', 'op', 'nam', 'email', 'address', 'assocy', 'payp', 'publ', 'post', 'nam', 'email', 'address\\n\\n5', 'raffl', 'gt500', 'approv', 'mod', 'pleas', 'pm', 'mod', 'quest', 'pric', 'authenticity\\n\\nmods', 'try', 'keep', 'saf', 'scam', 'alway', 'poss', 'trust', 'gut\\n\\nhstexan', 'detail', '\\n\\n1', 'rraffle_feedback', 'feedbackhttpswwwredditcomrraffle_feedbacksearchqhstexanampsortnewamprestrict_sronamptall\\n\\n2', 'redc_raffle', 'postshttpswwwredditcomredc_rafflesearchqauthor3ahstexanampsortnewamprestrict_sronamptall\\n\\n3', 'rkniferaffle', 'postshttpswwwredditcomrkniferafflesearchqauthor3ahstexanampsortnewamprestrict_sronamptall\\n\\nif', 'modmail', 'slow', 'pm', 'uedcraffleadmin', 'uedcrafflemod', 'uedcrafflemod1\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoredc_raffle', 'quest', 'concern'], ['welcome', 'redc_raffle', 'please', 'read', 'rule', 'sidebar\\n\\ngeneral', 'rule', 'raffle\\n\\n1', 'please', 'comment', 'request', 'slot', 'top', 'comment', 'count', 'reply', 'comment', 'automod', 'op', 'reply', 'comment', 'confirm', 'slots\\n\\n2', 'please', 'pay', 'within', 'timeframe', 'establish', 'op', 'anticipate', 'able', 'pay', 'slot', 'timeframe', 'please', 'arrange', 'op', 'pay', 'slot', 'early\\n\\n3', 'accept', 'payment', 'method', 'paypal', 'friends', 'amp', 'family', 'send', 'payment', 'echeck', 'write', 'anything', 'payment', 'note', 'section', 'report', 'mods', 'anyone', 'refuse', 'pay', 'via', 'paypal', 'friends', 'amp', 'family\\n\\n4', 'pay', 'please', 'pm', 'op', 'name', 'email', 'address', 'associate', 'paypal', 'publicly', 'post', 'name', 'email', 'address\\n\\n5', 'raffle', 'gt500', 'approve', 'mods', 'please', 'pm', 'mods', 'question', 'price', 'authenticity\\n\\nmods', 'try', 'keep', 'safe', 'scam', 'always', 'possible', 'trust', 'gut\\n\\nhstexan', 'detail', '\\n\\n1', 'rraffle_feedback', 'feedbackhttpswwwredditcomrraffle_feedbacksearchqhstexanampsortnewamprestrict_sronamptall\\n\\n2', 'redc_raffle', 'postshttpswwwredditcomredc_rafflesearchqauthor3ahstexanampsortnewamprestrict_sronamptall\\n\\n3', 'rkniferaffle', 'postshttpswwwredditcomrkniferafflesearchqauthor3ahstexanampsortnewamprestrict_sronamptall\\n\\nif', 'modmail', 'slow', 'pm', 'uedcraffleadmin', 'uedcrafflemod', 'uedcrafflemod1\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoredc_raffle', 'question', 'concern'])\n",
      "original document: \n",
      "['Indeed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indee'], ['indeed'])\n",
      "original document: \n",
      "[\"He's\", 'gorgeous,', 'what', 'kind', 'of', 'tail', 'is', 'that,', 'rounded?', 'very', 'pretty']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'gorg', 'kind', 'tail', 'round', 'pretty'], ['hes', 'gorgeous', 'kind', 'tail', 'round', 'pretty'])\n",
      "original document: \n",
      "['No', 'it', 'is', 'not', 'open.', 'Before', 'we', 'started', 'to', 'date', 'she', 'told', 'me', 'that', 'if', 'I', 'cheat', 'on', 'her', 'that', 'she', 'will', 'never', 'talk', 'to', 'me', 'again', 'and', 'I', 'told', 'her', 'the', 'same.', 'Her', 'passed', 'relationships', 'ended', 'because', 'he', 'guy', 'cheated', 'on', 'her', 'so', 'I', 'would', 'think', 'she', 'would', 'do', 'that', 'to', 'someone', 'because', 'she', 'knows', 'how', 'that', 'feels.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'start', 'dat', 'told', 'che', 'nev', 'talk', 'told', 'pass', 'rel', 'end', 'guy', 'che', 'would', 'think', 'would', 'someon', 'know', 'feel'], ['open', 'start', 'date', 'tell', 'cheat', 'never', 'talk', 'tell', 'pass', 'relationships', 'end', 'guy', 'cheat', 'would', 'think', 'would', 'someone', 'know', 'feel'])\n",
      "original document: \n",
      "['Wow,', 'what', 'a', 'body!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'body'], ['wow', 'body'])\n",
      "original document: \n",
      "['Well', 'you', 'got', 'a', 'pretty', 'good', 'laugh', 'out', 'of', 'me', 'so', 'good', 'fucking', 'job', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'got', 'pretty', 'good', 'laugh', 'good', 'fuck', 'job', 'lol'], ['well', 'get', 'pretty', 'good', 'laugh', 'good', 'fuck', 'job', 'lol'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'really', 'asking', 'how', 'to', 'do', 'it,', \"I'm\", 'just', 'asking', 'if', 'it', 'would', 'be', 'possible/make', 'sense', 'in', 'a', 'world.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'real', 'ask', 'im', 'ask', 'would', 'possiblemak', 'sens', 'world'], ['im', 'really', 'ask', 'im', 'ask', 'would', 'possiblemake', 'sense', 'world'])\n",
      "original document: \n",
      "['[removed]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['OP,', 'what', 'did', 'you', 'do?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op'], ['op'])\n",
      "original document: \n",
      "['Yes,', 'also', 'if', 'you', 'get', 'P', 'grade', 'in', 'all', 'levels', '(not', 'bosses)', 'you', 'unlock', 'a', 'filter']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'also', 'get', 'p', 'grad', 'level', 'boss', 'unlock', 'filt'], ['yes', 'also', 'get', 'p', 'grade', 'level', 'boss', 'unlock', 'filter'])\n",
      "original document: \n",
      "['That', 'part', \"couldn't\", 'be', 'anymore', 'irrelevant.', \"I'm\", 'talking', 'about', 'how', 'she', 'would', 'probably', 'find', 'him', 'doing', 'whatever', 'it', 'is', 'you', 'suggested', 'he', 'should', 'have', 'done', 'weird']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['part', 'couldnt', 'anym', 'irrelev', 'im', 'talk', 'would', 'prob', 'find', 'whatev', 'suggest', 'don', 'weird'], ['part', 'couldnt', 'anymore', 'irrelevant', 'im', 'talk', 'would', 'probably', 'find', 'whatever', 'suggest', 'do', 'weird'])\n",
      "original document: \n",
      "['Thanks.', 'It', 'looks', 'like', 'some', 'Amazon', 'merchants', 'are', 'comparable', 'to', 'Uline.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'look', 'lik', 'amazon', 'merch', 'comp', 'ulin'], ['thank', 'look', 'like', 'amazon', 'merchants', 'comparable', 'uline'])\n",
      "original document: \n",
      "['Sauce?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sauc'], ['sauce'])\n",
      "original document: \n",
      "['Fuck', 'I', 'got', 'nervous', 'and', 'spent', '$87', 'on', 'books.', 'I', \"should've\", 'actually', 'read', 'this', 'post', 'first', 'lmao.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'got', 'nerv', 'spent', 'eighty-seven', 'book', 'shouldv', 'act', 'read', 'post', 'first', 'lmao'], ['fuck', 'get', 'nervous', 'spend', 'eighty-seven', 'book', 'shouldve', 'actually', 'read', 'post', 'first', 'lmao'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['\"NOTCH', 'NOTCH', 'NOTCH\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['notch', 'notch', 'notch'], ['notch', 'notch', 'notch'])\n",
      "original document: \n",
      "['You', 'are', 'looking', 'to', 'create', 'a', 'reddit', 'bot?', 'You', 'will', 'want', 'to', 'check', 'out', 'the', '[reddit', 'API](https://www.reddit.com/dev/api).', 'Many', 'people', 'do', 'this', 'in', 'Python', 'and', 'there', 'are', 'many', 'tutorials', 'on', 'the', 'internet', 'showing', 'how', 'to', 'do', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'cre', 'reddit', 'bot', 'want', 'check', 'reddit', 'apihttpswwwredditcomdevap', 'many', 'peopl', 'python', 'many', 'tut', 'internet', 'show'], ['look', 'create', 'reddit', 'bot', 'want', 'check', 'reddit', 'apihttpswwwredditcomdevapi', 'many', 'people', 'python', 'many', 'tutorials', 'internet', 'show'])\n",
      "original document: \n",
      "['The', 'outcome', 'of', 'this', 'roll', 'should', 'determine', 'some', 'of', 'the', 'results', 'of', 'this', 'post.', '[Read', 'more', '»](https://www.reddit.com/r/worldpowers/wiki/codeofethics#wiki_rng)\\n\\n/u/rollme', '[[1d20', '/u/Razqn', '**Overall', 'Success**]]', '[[1d20', '/u/Razqn', '**Secrecy**]]', '\\n\\n#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/worldpowers)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['outcom', 'rol', 'determin', 'result', 'post', 'read', 'httpswwwredditcomrworldpowerswikicodeofethicswiki_rng\\n\\nurollme', '1d20', 'urazqn', 'overal', 'success', '1d20', 'urazqn', 'secrecy', '\\n\\namp009\\n\\namp009\\n\\namp009\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorworldpow', 'quest', 'concern'], ['outcome', 'roll', 'determine', 'result', 'post', 'read', 'httpswwwredditcomrworldpowerswikicodeofethicswiki_rng\\n\\nurollme', '1d20', 'urazqn', 'overall', 'success', '1d20', 'urazqn', 'secrecy', '\\n\\namp009\\n\\namp009\\n\\namp009\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorworldpowers', 'question', 'concern'])\n",
      "original document: \n",
      "['This', 'shits', 'so', 'güd', \"it's\", 'from', 'the', 'future.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shit', 'gud', 'fut'], ['shit', 'gud', 'future'])\n",
      "original document: \n",
      "['*', '**[Ice', 'Block](https://media-Hearth.cursecdn.com/avatars/330/731/28.png)**', 'Mage', 'Spell', 'Epic', 'Classic', '🐘', '^[HP](http://www.hearthpwn.com/cards/28),', '^[HH](http://www.hearthhead.com/cards/ice-block),', '^[Wiki](http://hearthstone.gamepedia.com/Ice_Block)', '', '\\n3', 'Mana', '-', 'Secret:', 'When', 'your', 'hero', 'takes', 'fatal', 'damage,', 'prevent', 'it', 'and', 'become', 'Immune', 'this', 'turn.', '', '\\n\\n^(Call/)^[PM](https://www.reddit.com/message/compose/?to=hearthscan-bot)', '^(', 'me', 'with', 'up', 'to', '7', '[[cardname]].', ')^[About.](https://www.reddit.com/message/compose/?to=hearthscan-bot&amp;message=Tell%20me%20more%20[[info]]&amp;subject=hi)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ic', 'blockhttpsmediahearthcursecdncomavatars33073128png', 'mag', 'spel', 'ep', 'class', 'hphttpwwwhearthpwncomcards28', 'hhhttpwwwhearthheadcomcardsiceblock', 'wikihttphearthstonegamepediacomice_block', '\\n3', 'man', 'secret', 'hero', 'tak', 'fat', 'dam', 'prev', 'becom', 'immun', 'turn', '\\n\\ncallpmhttpswwwredditcommessagecomposetohearthscanbot', 'sev', 'cardnam', 'abouthttpswwwredditcommessagecomposetohearthscanbotampmessagetell20me20more20infoampsubjecthi'], ['ice', 'blockhttpsmediahearthcursecdncomavatars33073128png', 'mage', 'spell', 'epic', 'classic', 'hphttpwwwhearthpwncomcards28', 'hhhttpwwwhearthheadcomcardsiceblock', 'wikihttphearthstonegamepediacomice_block', '\\n3', 'mana', 'secret', 'hero', 'take', 'fatal', 'damage', 'prevent', 'become', 'immune', 'turn', '\\n\\ncallpmhttpswwwredditcommessagecomposetohearthscanbot', 'seven', 'cardname', 'abouthttpswwwredditcommessagecomposetohearthscanbotampmessagetell20me20more20infoampsubjecthi'])\n",
      "original document: \n",
      "['10', 'team', 'standard.', 'Who', 'is', 'my', 'best', 'play?\\n\\nFleener', 'vs', 'MIA\\n\\nClay', '@ATL\\n\\nMartavis', '@BAL\\n\\nDuke', 'Johnson', 'vs', 'CIN\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'team', 'standard', 'best', 'play\\n\\nfleener', 'vs', 'mia\\n\\nclay', 'atl\\n\\nmartavis', 'bal\\n\\nduke', 'johnson', 'vs', 'cin\\n\\n'], ['ten', 'team', 'standard', 'best', 'play\\n\\nfleener', 'vs', 'mia\\n\\nclay', 'atl\\n\\nmartavis', 'bal\\n\\nduke', 'johnson', 'vs', 'cin\\n\\n'])\n",
      "original document: \n",
      "[\"There's\", 'also', 'a', 'bit', 'hacky', 'way', 'to', 'use', 'Google', 'maps', 'location', 'sharing', 'api', 'which', 'is', 'less', 'configurable', 'but', 'for', 'just', 'very', 'basic', 'tracking', 'requires', 'no', 'other', 'apps', 'and', 'pretty', 'much', 'zero', 'battery', 'overhead', '(as', 'long', 'as', 'you', 'were', 'using', 'Google', 'maps', 'location', 'services', 'already)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'also', 'bit', 'hacky', 'way', 'us', 'googl', 'map', 'loc', 'shar', 'ap', 'less', 'config', 'bas', 'track', 'requir', 'ap', 'pretty', 'much', 'zero', 'battery', 'overhead', 'long', 'us', 'googl', 'map', 'loc', 'serv', 'already'], ['theres', 'also', 'bite', 'hacky', 'way', 'use', 'google', 'map', 'location', 'share', 'api', 'less', 'configurable', 'basic', 'track', 'require', 'apps', 'pretty', 'much', 'zero', 'battery', 'overhead', 'long', 'use', 'google', 'map', 'location', 'service', 'already'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['LOL', 'agreed.\\n\\n', 'No', 'perfect', 'angel', 'then?', 'Atleast', 'they', 'fixed', 'it', 'fast?', 'Could', 'have', 'told', 'people', 'to', 'fuck', 'off', 'with', 'spin', 'statements']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'agreed\\n\\n', 'perfect', 'angel', 'atleast', 'fix', 'fast', 'could', 'told', 'peopl', 'fuck', 'spin', 'stat'], ['lol', 'agreed\\n\\n', 'perfect', 'angel', 'atleast', 'fix', 'fast', 'could', 'tell', 'people', 'fuck', 'spin', 'statements'])\n",
      "original document: \n",
      "['Welcome', 'to', 'r/mma', 'this', 'must', 'be', 'your', 'first', 'time']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welcom', 'rmma', 'must', 'first', 'tim'], ['welcome', 'rmma', 'must', 'first', 'time'])\n",
      "original document: \n",
      "['2', 'posts', 'in', 'a', 'row', 'asking', 'very', 'similar', 'vague', 'homework-sounding', 'questions.', '\\nPeople', 'here', 'are', 'passionate', 'about', 'their', 'work', 'and', 'love', 'to', 'share', 'information', 'but', 'they', 'aren’t', 'going', 'to', 'do', 'your', 'homework', 'for', 'you!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'post', 'row', 'ask', 'simil', 'vagu', 'homeworksound', 'quest', '\\npeople', 'pass', 'work', 'lov', 'shar', 'inform', 'ar', 'going', 'homework'], ['two', 'post', 'row', 'ask', 'similar', 'vague', 'homeworksounding', 'question', '\\npeople', 'passionate', 'work', 'love', 'share', 'information', 'arent', 'go', 'homework'])\n",
      "original document: \n",
      "['Probably', 'not,', 'but', '24', 'hours', 'is', \"doable!\\n\\nYou'd\", 'get', 'Mind', 'Controller', 'to', 'onyx(!)', 'by', 'the', 'end', 'if', 'you', 'kept', 'up', 'that', 'pace', 'fielding', 'for', '19', 'hours,', 'and', 'if', 'you', 'do', 'it', 'in', 'the', 'right', 'spot', '(or', 'just', 'put', 'in', 'one', 'last', 'field', 'at', 'the', 'end)', 'you', 'could', 'pretty', 'easily', 'get', 'Illuminator', 'too.', \"You'd\", '*just*', 'get', 'Connector', 'platinum', 'by', 'the', 'end', 'of', 'it,', 'and', 'could', 'get', 'Recruiter', 'platinum', 'with', 'virtually', 'no', 'effort', 'on', 'your', 'part', 'if', \"it's\", 'prepped', 'in', 'advance.\\n\\nThe', '3', 'extra', 'golds', 'is', 'where', 'things', 'start', 'to', 'break', 'down', 'a', 'bit.', 'Spec', 'Ops', 'is', 'pretty', 'easy,', 'but', \"you'd\", 'need', 'to', 'have', 'a', 'lot', 'of', 'easy', '4', 'portal', 'missions', 'in', 'the', 'same', 'area', 'to', 'do', 'it', 'and', 'still', 'have', 'time', 'for', 'your', 'last', 'two.', 'Translator', 'gold', 'is', 'doable', 'in', 'that', 'timeframe,', 'but', 'would', 'take', 'up', 'a', 'majority', 'of', 'that', 'time', 'and', 'leave', 'you', 'one', 'gold', 'short.', 'I', 'expect', 'Spec', 'Ops/Pioneer/Explorer', 'would', 'be', 'the', 'best', 'combo,', 'since', 'doing', 'Pioneer', 'would', 'inherently', 'get', 'you', 'half-way', 'to', 'Explorer.', \"It's\", 'plausible', 'that', 'Spec', 'Ops/Pioneer/Recharger', 'might', 'be', 'faster', 'with', 'some', 'good', 'key', 'management,', 'but', 'with', 'how', 'temperamental', 'Niantic', 'is', 'for', 'recharging', \"I'd\", 'rather', 'just', 'go', 'for', 'the', 'thousand', 'additional', 'unique', \"hacks.\\n\\nI'd\", 'be', 'astonished', 'if', 'it', 'ever', 'happened,', 'but', 'it', 'is', 'theoretically', 'possible.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'twenty-four', 'hour', 'doable\\n\\nyoud', 'get', 'mind', 'control', 'onyx', 'end', 'kept', 'pac', 'field', 'nineteen', 'hour', 'right', 'spot', 'put', 'on', 'last', 'field', 'end', 'could', 'pretty', 'easy', 'get', 'illumin', 'youd', 'get', 'connect', 'platin', 'end', 'could', 'get', 'recruit', 'platin', 'virt', 'effort', 'part', 'prep', 'advance\\n\\nth', 'three', 'extr', 'gold', 'thing', 'start', 'break', 'bit', 'spec', 'op', 'pretty', 'easy', 'youd', 'nee', 'lot', 'easy', 'four', 'port', 'miss', 'are', 'stil', 'tim', 'last', 'two', 'transl', 'gold', 'doabl', 'timefram', 'would', 'tak', 'maj', 'tim', 'leav', 'on', 'gold', 'short', 'expect', 'spec', 'opspioneerexpl', 'would', 'best', 'combo', 'sint', 'pion', 'would', 'inh', 'get', 'halfway', 'expl', 'plaus', 'spec', 'opspioneerrecharg', 'might', 'fast', 'good', 'key', 'man', 'tempera', 'niant', 'recharg', 'id', 'rath', 'go', 'thousand', 'addit', 'un', 'hacks\\n\\nid', 'aston', 'ev', 'hap', 'theoret', 'poss'], ['probably', 'twenty-four', 'hours', 'doable\\n\\nyoud', 'get', 'mind', 'controller', 'onyx', 'end', 'keep', 'pace', 'field', 'nineteen', 'hours', 'right', 'spot', 'put', 'one', 'last', 'field', 'end', 'could', 'pretty', 'easily', 'get', 'illuminator', 'youd', 'get', 'connector', 'platinum', 'end', 'could', 'get', 'recruiter', 'platinum', 'virtually', 'effort', 'part', 'prepped', 'advance\\n\\nthe', 'three', 'extra', 'golds', 'things', 'start', 'break', 'bite', 'spec', 'ops', 'pretty', 'easy', 'youd', 'need', 'lot', 'easy', 'four', 'portal', 'missions', 'area', 'still', 'time', 'last', 'two', 'translator', 'gold', 'doable', 'timeframe', 'would', 'take', 'majority', 'time', 'leave', 'one', 'gold', 'short', 'expect', 'spec', 'opspioneerexplorer', 'would', 'best', 'combo', 'since', 'pioneer', 'would', 'inherently', 'get', 'halfway', 'explorer', 'plausible', 'spec', 'opspioneerrecharger', 'might', 'faster', 'good', 'key', 'management', 'temperamental', 'niantic', 'recharge', 'id', 'rather', 'go', 'thousand', 'additional', 'unique', 'hacks\\n\\nid', 'astonish', 'ever', 'happen', 'theoretically', 'possible'])\n",
      "original document: \n",
      "['I', \"don't\", 'have', 'much', 'interest', 'in', 'consoles', 'anymore,', 'so', 'if', 'they', 'want', 'my', 'money', \"they'll\", 'have', 'to', 'port', 'it...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'much', 'interest', 'consol', 'anym', 'want', 'money', 'theyl', 'port'], ['dont', 'much', 'interest', 'console', 'anymore', 'want', 'money', 'theyll', 'port'])\n",
      "original document: \n",
      "['Like', 'David', 'Hasselhoff.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'david', 'hasselhoff'], ['like', 'david', 'hasselhoff'])\n",
      "original document: \n",
      "['Ditto', '-', 'I', 'do', 'some', 'events', 'work', 'on', 'the', 'side', 'of', 'clinical', 'school', 'and', 'love', 'how', 'it', 'gets', 'me', 'out', 'and', 'meeting', 'non-veterinary-related', 'people', 'every', 'so', 'often.', \"I'm\", 'a', 'bit', 'of', 'a', 'gearhead,', 'and', 'I', 'get', 'a', 'lot', 'of', 'enjoyment', 'reading', 'about', 'different', \"people's\", 'rigs,', 'newly', 'released', 'gear', 'etc.\\n\\n\\n...oh,', 'and', 'I', 'guess', 'making', 'images', \"I'm\", 'really', 'satisfied', 'with', 'is', 'neat', 'too', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ditto', 'ev', 'work', 'sid', 'clin', 'school', 'lov', 'get', 'meet', 'nonveterinaryrel', 'peopl', 'every', 'oft', 'im', 'bit', 'gearhead', 'get', 'lot', 'enjoy', 'read', 'diff', 'peopl', 'rig', 'new', 'releas', 'gear', 'etc\\n\\n\\noh', 'guess', 'mak', 'im', 'im', 'real', 'satisfy', 'neat'], ['ditto', 'events', 'work', 'side', 'clinical', 'school', 'love', 'get', 'meet', 'nonveterinaryrelated', 'people', 'every', 'often', 'im', 'bite', 'gearhead', 'get', 'lot', 'enjoyment', 'read', 'different', 'people', 'rig', 'newly', 'release', 'gear', 'etc\\n\\n\\noh', 'guess', 'make', 'image', 'im', 'really', 'satisfy', 'neat'])\n",
      "original document: \n",
      "['please', 'tell', 'me', \"there's\", 'video\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'tel', 'ther', 'video\\n'], ['please', 'tell', 'theres', 'video\\n'])\n",
      "original document: \n",
      "['Just', 'raptor', 'and', 'the', 'secret', 'one', '(requires', 'beating', 'pof)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rapt', 'secret', 'on', 'requir', 'beat', 'pof'], ['raptor', 'secret', 'one', 'require', 'beat', 'pof'])\n",
      "original document: \n",
      "['holy.', 'crap.', 'I', 'want', 'to', 'wait', 'to', 'tell', 'DH', 'in', 'the', 'morning', 'and', 'make', 'sure', 'I', 'get', 'a', 'positive', 'then', 'too.', 'Longest', '13', 'hours', 'of', 'my', 'life!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['holy', 'crap', 'want', 'wait', 'tel', 'dh', 'morn', 'mak', 'sur', 'get', 'posit', 'longest', 'thirteen', 'hour', 'lif'], ['holy', 'crap', 'want', 'wait', 'tell', 'dh', 'morning', 'make', 'sure', 'get', 'positive', 'longest', 'thirteen', 'hours', 'life'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['She', 'is', 'pretty', 'amazing.', 'I', 'get', 'her', 'to', 'close', 'to', '100%', 'ta', 'and', 'she', 'hits', 'pretty', 'close', 'to', 'dmg', 'cap', 'while', 'also', 'being', 'somewhat', 'tanky.', 'Of', 'course', 'needs', 'atleast', '3', 'turns', 'to', 'get', 'going.', 'Also', 'nice', 'as', 'you', 'can', 'choose', 'if', 'you', 'want', 'aggro', 'up', 'or', 'down', 'in', 'combat.', 'She', 'is', 'also', 'pretty', 'easy', 'to', 'use,', 'as', 'you', 'basically', 'just', 'use', 'her', '#3', 'turn', '1', 'and', 'if', 'you', 'dont', 'need', 'to', 'never', 'touch', 'her', 'again,', 'which', 'is', 'very', 'nice', 'to', 'attack', 'mashing.', 'And', 'she', 'cant', 'lose', 'her', 'buffs', 'as', 'opposed', 'to', 'six']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'amaz', 'get', 'clos', 'one hundred', 'ta', 'hit', 'pretty', 'clos', 'dmg', 'cap', 'also', 'somewh', 'tanky', 'cours', 'nee', 'atleast', 'three', 'turn', 'get', 'going', 'also', 'nic', 'choos', 'want', 'aggro', 'comb', 'also', 'pretty', 'easy', 'us', 'bas', 'us', 'three', 'turn', 'on', 'dont', 'nee', 'nev', 'touch', 'nic', 'attack', 'mash', 'cant', 'los', 'buff', 'oppos', 'six'], ['pretty', 'amaze', 'get', 'close', 'one hundred', 'ta', 'hit', 'pretty', 'close', 'dmg', 'cap', 'also', 'somewhat', 'tanky', 'course', 'need', 'atleast', 'three', 'turn', 'get', 'go', 'also', 'nice', 'choose', 'want', 'aggro', 'combat', 'also', 'pretty', 'easy', 'use', 'basically', 'use', 'three', 'turn', 'one', 'dont', 'need', 'never', 'touch', 'nice', 'attack', 'mash', 'cant', 'lose', 'buff', 'oppose', 'six'])\n",
      "original document: \n",
      "['Could', 'you', 'let', 'us', 'see', 'the', 'data', 'when', \"you're\", 'finished', 'please?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'let', 'us', 'see', 'dat', 'yo', 'fin', 'pleas'], ['could', 'let', 'us', 'see', 'data', 'youre', 'finish', 'please'])\n",
      "original document: \n",
      "['10,000', 'career', 'total', 'yards', 'for', 'J.T.', 'Barrett']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten thousand', 'car', 'tot', 'yard', 'jt', 'barret'], ['ten thousand', 'career', 'total', 'yards', 'jt', 'barrett'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['It', 'will', 'be,', 'bow', 'battles', 'day', '1', 'was', 'everything', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bow', 'battl', 'day', 'on', 'everyth'], ['bow', 'battle', 'day', 'one', 'everything'])\n",
      "original document: \n",
      "['gg']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gg'], ['gg'])\n",
      "original document: \n",
      "['But', 'the', 'US', 'states', 'are', 'similar', 'economically', 'developed.', 'Ignore', 'DC', 'and', 'you', 'have', 'GDPs/capita', 'ranging', 'between', '65k', 'to', '30k', 'dollars.', 'In', 'Europe', 'ignoring', 'Luxemburg', 'you', 'have', 'GDPs/capita', 'ranging', 'between', '79k', 'to', '1,900', 'dollar.', \"\\n\\nIt's\", 'obvious', 'that', 'countries,', 'that', 'have', 'such', 'a', 'low', 'GDPs/capita', \"aren't\", 'sensitive', 'towards', 'social', 'issues,', 'because', \"they're\", 'not', 'even', 'first', 'world', 'countries,', 'but', 'emerging', 'countries.', 'Also', 'the', 'cultural', 'difference', 'is', 'more', 'severe,', 'not', 'only', 'because', 'of', 'the', 'culture', 'itself,', 'but', 'the', 'fact', 'that', 'those', 'countries', 'had', 'until', 'the', '90s', 'totalitarian', 'governments,', 'therefore', 'they', 'have', 'experience', 'with', 'democracy', '(including', 'minority', 'rights)', 'not', 'even', 'for', '30', 'years', 'now.', '\\n\\n\"Therefore', 'to', 'say', '\"Europe', 'is...\"', 'and', 'than', 'referring', 'to', 'those', 'countries', 'is', 'a', 'double', 'standard.', \"It's\", 'the', 'same', 'like', 'looking', 'at', 'Singapore', 'and', 'say', '\"Asia', 'is', 'rich\"', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'stat', 'simil', 'econom', 'develop', 'ign', 'dc', 'gdpscapita', 'rang', '65k', '30k', 'doll', 'europ', 'ign', 'luxemburg', 'gdpscapita', 'rang', '79k', 'one thousand, nine hundred', 'doll', '\\n\\nits', 'obvy', 'country', 'low', 'gdpscapita', 'ar', 'sensit', 'toward', 'soc', 'issu', 'theyr', 'ev', 'first', 'world', 'country', 'emerg', 'country', 'also', 'cult', 'diff', 'sev', 'cult', 'fact', 'country', '90s', 'totalit', 'govern', 'theref', 'expery', 'democr', 'includ', 'min', 'right', 'ev', 'thirty', 'year', '\\n\\ntherefore', 'say', 'europ', 'refer', 'country', 'doubl', 'standard', 'lik', 'look', 'singap', 'say', 'as', 'rich'], ['us', 'state', 'similar', 'economically', 'develop', 'ignore', 'dc', 'gdpscapita', 'range', '65k', '30k', 'dollars', 'europe', 'ignore', 'luxemburg', 'gdpscapita', 'range', '79k', 'one thousand, nine hundred', 'dollar', '\\n\\nits', 'obvious', 'countries', 'low', 'gdpscapita', 'arent', 'sensitive', 'towards', 'social', 'issue', 'theyre', 'even', 'first', 'world', 'countries', 'emerge', 'countries', 'also', 'cultural', 'difference', 'severe', 'culture', 'fact', 'countries', '90s', 'totalitarian', 'governments', 'therefore', 'experience', 'democracy', 'include', 'minority', 'right', 'even', 'thirty', 'years', '\\n\\ntherefore', 'say', 'europe', 'refer', 'countries', 'double', 'standard', 'like', 'look', 'singapore', 'say', 'asia', 'rich'])\n",
      "original document: \n",
      "['Why', 'is', 'Peyper', 'even', 'mentioning', 'repeated', 'infringements', 'to', 'Read?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peyp', 'ev', 'ment', 'rep', 'infr', 'read'], ['peyper', 'even', 'mention', 'repeat', 'infringements', 'read'])\n",
      "original document: \n",
      "['After', 'a', 'touchdown,', 'people', 'would', 'think', 'the', 'player', 'is', 'Teebowing.', 'Pass', 'out', 'a', 'paper,', 'people', 'toss', 'it.', '\\n\\nThis', 'was', 'the', 'best', 'way', 'to', 'do', 'what', 'they', 'meant', 'to', 'do']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['touchdown', 'peopl', 'would', 'think', 'play', 'teebow', 'pass', 'pap', 'peopl', 'toss', '\\n\\nthis', 'best', 'way', 'meant'], ['touchdown', 'people', 'would', 'think', 'player', 'teebowing', 'pass', 'paper', 'people', 'toss', '\\n\\nthis', 'best', 'way', 'mean'])\n",
      "original document: \n",
      "['I', 'should', 'add', 'that', 'I', 'am', 'in', 'love', 'with', 'this', 'phone.', 'It’s', 'everything', 'I', 'wanted', 'and', 'it', 'runs', 'beautifully.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad', 'lov', 'phon', 'everyth', 'want', 'run', 'beauty'], ['add', 'love', 'phone', 'everything', 'want', 'run', 'beautifully'])\n",
      "original document: \n",
      "['**Remember', 'OP', 'is', 'a', 'real', 'person', 'who', 'has', 'taken', 'a', 'risk', 'by', 'posting', 'photos', 'of', 'herself', 'to', 'the', 'internet.', 'Please', 'keep', 'your', 'comments', 'respectful.**', 'If', 'in', 'doubt,', 'ask', 'yourself', '\"how', 'would', 'I', 'feel', 'if', 'someone', 'posted', 'this', 'comment', 'about', 'me?\"', 'Rude', 'comments', 'may', 'result', 'in', 'a', 'ban.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/curvy)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'op', 'real', 'person', 'tak', 'risk', 'post', 'photo', 'internet', 'pleas', 'keep', 'com', 'respect', 'doubt', 'ask', 'would', 'feel', 'someon', 'post', 'com', 'rud', 'com', 'may', 'result', 'ban\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorcurvy', 'quest', 'concern'], ['remember', 'op', 'real', 'person', 'take', 'risk', 'post', 'photos', 'internet', 'please', 'keep', 'comment', 'respectful', 'doubt', 'ask', 'would', 'feel', 'someone', 'post', 'comment', 'rude', 'comment', 'may', 'result', 'ban\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorcurvy', 'question', 'concern'])\n",
      "original document: \n",
      "['\\\\-', 'a', 'guard', 'at', 'a', 'Siberian', 'labour', 'camp,', 'AD', '1921', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guard', 'sib', 'labo', 'camp', 'ad', 'one thousand, nine hundred and twenty-on'], ['guard', 'siberian', 'labour', 'camp', 'ad', 'one thousand, nine hundred and twenty-one'])\n",
      "original document: \n",
      "['Ask', 'them', 'as', 'nicely', 'as', 'you', 'can,', 'amd', 'offer', 'to', 'share', 'profits.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ask', 'nic', 'amd', 'off', 'shar', 'profit'], ['ask', 'nicely', 'amd', 'offer', 'share', 'profit'])\n",
      "original document: \n",
      "['NOW', 'LISTEN', 'HERE', 'YOU', 'KNIFE-EARED', 'PIECE', 'OF', 'SHIT!\\n\\nIF', 'YOU', 'GO', 'ANY', 'FURTHER', 'WITH', 'YOUR', 'PISS-STAINED', 'PUBIC', 'HAIR', 'YOU', 'CALL', 'A', 'WIG,', \"I'M\", 'GONNA', 'WRECK', 'YOUR', 'SHIT', 'SO', 'HARD', 'YOU', \"WON'T\", 'EVEN', 'BE', 'ABLE', 'TO', 'WALK', 'WITH', 'YOUR', '**LIMP', 'DICK!**', \"\\n\\nI'M\", 'GONNA', 'SHOVE', 'MY', 'FOOT', 'SO', 'FAR', 'UP', 'YOUR', 'SHAVEN', 'PERFECT', 'LITTLE', 'ARSE', 'THAT', 'YOUR', 'BREATH', 'IS', 'GONNA', 'SMELL', 'LIKE', 'SHOE', 'POLISH!', 'THEN', \"I'M\", 'GONNA', 'TAKE', 'THAT', 'LITTLE', 'RED', 'ANAL', 'BEAD', 'ON', 'YOUR', 'BELT', 'AND', 'PUSH', 'IT', 'IN', 'YOUR', 'FACE!', \"\\n\\n**I'M\", 'GONNA', 'FLAGELLATE', 'YOU', 'WITH', 'MY', \"FUCKIN'\", \"BEARD!**\\n\\nI'M\", 'GONNA', 'BUILD', 'A', 'PAIR', 'OF', 'RUNIC', 'MECHANICAL', 'BALLS,', 'AND', 'USE', 'SURGICAL', 'PRECISION', 'TO', 'SEW', 'THEM', 'TO', 'YOUR', 'GROIN', 'WHERE', 'YOUR', 'MANHOOD', 'OUGHT', 'TO', 'BE', 'JUST', 'SO', 'I', 'CAN', 'KICK', 'THEM', 'WITH', 'MY', 'IRON', \"FUCKIN'\", 'FEET,', 'YOU', 'TWAT!\\n\\n\\nLISTEN', 'TO', 'ME', 'YOU', 'POLE-PROPORTIONED', 'DENDROPHILES!', \"I'M\", 'GONNA', 'TAKE', 'YOUR', \"FUCKIN'\", 'ARROWS,', 'AND', 'SHOVE', 'THEM', 'IN', 'BETWEEN', 'YOUR', 'POLISHED', 'FINGERNAILS!', \"\\n\\nI'M\", 'GONNA', 'TAKE', 'THAT', 'BOWSTRING', 'OF', 'YOURS', 'AND', 'STRING', 'YOU', 'UP', 'BY', 'YOUR', \"FUCKIN'\", 'FORESKIN,', 'UNTIL', 'GRAVITY', 'GIVES', 'YOU', 'A', 'BOTCHED', 'CIRCUMCISION!', 'AND', 'PLAY', 'IT', 'LIKE', 'A', \"FUCKIN'\", \"VIOLIN!\\n\\n\\nI'M\", 'GOING', 'TO', 'HEADBUTT', 'YOU', 'UNTIL', \"THERE'S\", 'NOTHING', 'BUT', 'A', 'BUTT', 'LEFT!', \"\\n\\nI'M\", 'GONNA', 'COLOUR', 'THAT', 'PANSY', 'WHITE', 'SKIRT', 'RED,', 'USING', 'YOUR', \"FUCKIN'\", 'BONE', 'MARROW!', '\\n\\nYOU', 'BETTER', 'BELIEVE', \"YOU'RE\", 'GOING', 'TO', 'WISH', 'YOU', 'WERE', 'NEVER', 'BORN,', \"'CAUSE\", \"I'LL\", 'MAKE', 'IT', 'SEEM', 'LIKE', 'YOU', 'NEVER', 'WERE,', 'YE', 'ATROCIOUS', \"FUCKIN'\", 'BOIL', 'ON', 'THE', 'FACE', 'OF', 'REALITY!', '\\n\\nI', 'WILL-']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['list', 'knifeear', 'piec', 'shit\\n\\nif', 'go', 'pissstain', 'pub', 'hair', 'cal', 'wig', 'im', 'gonn', 'wreck', 'shit', 'hard', 'wont', 'ev', 'abl', 'walk', 'limp', 'dick', '\\n\\nim', 'gonn', 'shov', 'foot', 'far', 'shav', 'perfect', 'littl', 'ars', 'brea', 'gonn', 'smel', 'lik', 'sho', 'pol', 'im', 'gonn', 'tak', 'littl', 'red', 'an', 'bead', 'belt', 'push', 'fac', '\\n\\nim', 'gonn', 'flagel', 'fuckin', 'beard\\n\\nim', 'gonn', 'build', 'pair', 'run', 'mech', 'bal', 'us', 'surg', 'precid', 'sew', 'groin', 'man', 'ought', 'kick', 'iron', 'fuckin', 'feet', 'twat\\n\\n\\nlisten', 'poleproport', 'dendrophil', 'im', 'gonn', 'tak', 'fuckin', 'arrow', 'shov', 'pol', 'fingernail', '\\n\\nim', 'gonn', 'tak', 'bowst', 'string', 'fuckin', 'foreskin', 'grav', 'giv', 'botch', 'circumcid', 'play', 'lik', 'fuckin', 'violin\\n\\n\\nim', 'going', 'headbut', 'ther', 'noth', 'but', 'left', '\\n\\nim', 'gonn', 'colo', 'pansy', 'whit', 'skirt', 'red', 'us', 'fuckin', 'bon', 'marrow', '\\n\\nyou', 'bet', 'believ', 'yo', 'going', 'wish', 'nev', 'born', 'caus', 'il', 'mak', 'seem', 'lik', 'nev', 'ye', 'atrocy', 'fuckin', 'boil', 'fac', 'real', '\\n\\ni'], ['listen', 'knifeeared', 'piece', 'shit\\n\\nif', 'go', 'pissstained', 'pubic', 'hair', 'call', 'wig', 'im', 'gonna', 'wreck', 'shit', 'hard', 'wont', 'even', 'able', 'walk', 'limp', 'dick', '\\n\\nim', 'gonna', 'shove', 'foot', 'far', 'shave', 'perfect', 'little', 'arse', 'breath', 'gonna', 'smell', 'like', 'shoe', 'polish', 'im', 'gonna', 'take', 'little', 'red', 'anal', 'bead', 'belt', 'push', 'face', '\\n\\nim', 'gonna', 'flagellate', 'fuckin', 'beard\\n\\nim', 'gonna', 'build', 'pair', 'runic', 'mechanical', 'ball', 'use', 'surgical', 'precision', 'sew', 'groin', 'manhood', 'ought', 'kick', 'iron', 'fuckin', 'feet', 'twat\\n\\n\\nlisten', 'poleproportioned', 'dendrophiles', 'im', 'gonna', 'take', 'fuckin', 'arrows', 'shove', 'polish', 'fingernails', '\\n\\nim', 'gonna', 'take', 'bowstring', 'string', 'fuckin', 'foreskin', 'gravity', 'give', 'botch', 'circumcision', 'play', 'like', 'fuckin', 'violin\\n\\n\\nim', 'go', 'headbutt', 'theres', 'nothing', 'butt', 'leave', '\\n\\nim', 'gonna', 'colour', 'pansy', 'white', 'skirt', 'red', 'use', 'fuckin', 'bone', 'marrow', '\\n\\nyou', 'better', 'believe', 'youre', 'go', 'wish', 'never', 'bear', 'cause', 'ill', 'make', 'seem', 'like', 'never', 'ye', 'atrocious', 'fuckin', 'boil', 'face', 'reality', '\\n\\ni'])\n",
      "original document: \n",
      "['So', 'what', 'are', 'you', 'doing', 'on', 'an', 'Australia', 'specific', 'subreddit?', 'Just', 'avoid', 'anything', 'that', 'will', 'likely', 'spoil', 'it', 'for', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['austral', 'spec', 'subreddit', 'avoid', 'anyth', 'lik', 'spoil'], ['australia', 'specific', 'subreddit', 'avoid', 'anything', 'likely', 'spoil'])\n",
      "original document: \n",
      "['Oh,', 'I', 'have', 'been', 'going', 'crazy', 'on', 'that', 'report', 'button.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'going', 'crazy', 'report', 'button'], ['oh', 'go', 'crazy', 'report', 'button'])\n",
      "original document: \n",
      "['Videos', 'in', 'this', 'thread:\\n\\n[Watch', 'Playlist', '&amp;#9654;](http://subtletv.com/_r73i549?feature=playlist&amp;nline=1)\\n\\nVIDEO|COMMENT\\n-|-\\n[Zeno', 'Appears', 'First', 'Time', '', 'Dragon', 'Ball', 'Super', 'Episode', '41', 'English', 'Sub](http://www.youtube.com/watch?v=pmXIbiHSyDw)|[+2](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqgh8c?context=10#dnqgh8c)', '-', 'Erasing', 'is', 'super', 'high', 'tier', 'reality', 'warping', 'so', 'he', 'can', 'logically', 'do', 'anything', 'below', 'it.', 'He', 'also', 'created', 'the', 'multiverse', 'so', 'it', 'would', 'make', 'sense', 'he', 'can', 'do', 'lots', 'of', 'other', 'hax.', 'Also,', 'the', 'time', 'rings', 'make', 'them', 'acausal', 'so', 'GER', \"won't\", 'work.', 'Here', 'is', 'a', 'source', 'on', 'the', '...\\n[Zeno', 'destroys', 'Zamasu](http://www.youtube.com/watch?v=iTIef3cRFVM)|[+2](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqi5sd?context=10#dnqi5sd)', '-', 'Here.\\n[How', 'Powerful', 'is', 'Zeno', 'in', 'Dragon', 'Ball', 'Super?](http://www.youtube.com/watch?v=wGGKmp6xRX4)|[+1](https://www.reddit.com/r/whowouldwin/comments/73i549/_/dnqijnw?context=10#dnqijnw)', '-', 'Here', 'is', 'a', 'video', 'that', 'describes', 'how', 'powerful', 'Zeno', \"is.\\nI'm\", 'a', 'bot', 'working', 'hard', 'to', 'help', 'Redditors', 'find', 'related', 'videos', 'to', 'watch.', \"I'll\", 'keep', 'this', 'updated', 'as', 'long', 'as', 'I', 'can.\\n***\\n[Play', 'All](http://subtletv.com/_r73i549?feature=playlist&amp;ftrlnk=1)', '|', '[Info](https://np.reddit.com/r/SubtleTV/wiki/mentioned_videos)', '|', 'Get', 'me', 'on', '[Chrome](https://chrome.google.com/webstore/detail/mentioned-videos-for-redd/fiimkmdalmgffhibfdjnhljpnigcmohf)', '/', '[Firefox](https://addons.mozilla.org/en-US/firefox/addon/mentioned-videos-for-reddit)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['video', 'thread\\n\\nwatch', 'playl', 'amp9654httpsubtletvcom_r73i549featureplaylistampnline1\\n\\nvideocomment\\n\\nzeno', 'appear', 'first', 'tim', 'dragon', 'bal', 'sup', 'episod', 'forty-one', 'engl', 'subhttpwwwyoutubecomwatchvpmxibihsydw2httpswwwredditcomrwhowouldwincomments73i549_dnqgh8ccontext10dnqgh8c', 'eras', 'sup', 'high', 'tier', 'real', 'warp', 'log', 'anyth', 'also', 'cre', 'multivers', 'would', 'mak', 'sens', 'lot', 'hax', 'also', 'tim', 'ring', 'mak', 'acaus', 'ger', 'wont', 'work', 'sourc', '\\nzeno', 'destroy', 'zamasuhttpwwwyoutubecomwatchvitief3crfvm2httpswwwredditcomrwhowouldwincomments73i549_dnqi5sdcontext10dnqi5sd', 'here\\nhow', 'pow', 'zeno', 'dragon', 'bal', 'superhttpwwwyoutubecomwatchvwggkmp6xrx41httpswwwredditcomrwhowouldwincomments73i549_dnqijnwcontext10dnqijnw', 'video', 'describ', 'pow', 'zeno', 'is\\nim', 'bot', 'work', 'hard', 'help', 'reddit', 'find', 'rel', 'video', 'watch', 'il', 'keep', 'upd', 'long', 'can\\n\\nplay', 'allhttpsubtletvcom_r73i549featureplaylistampftrlnk1', 'infohttpsnpredditcomrsubtletvwikimentioned_videos', 'get', 'chromehttpschromegooglecomwebstoredetailmentionedvideosforreddfiimkmdalmgffhibfdjnhljpnigcmohf', 'firefoxhttpsaddonsmozillaorgenusfirefoxaddonmentionedvideosforreddit'], ['videos', 'thread\\n\\nwatch', 'playlist', 'amp9654httpsubtletvcom_r73i549featureplaylistampnline1\\n\\nvideocomment\\n\\nzeno', 'appear', 'first', 'time', 'dragon', 'ball', 'super', 'episode', 'forty-one', 'english', 'subhttpwwwyoutubecomwatchvpmxibihsydw2httpswwwredditcomrwhowouldwincomments73i549_dnqgh8ccontext10dnqgh8c', 'erase', 'super', 'high', 'tier', 'reality', 'warp', 'logically', 'anything', 'also', 'create', 'multiverse', 'would', 'make', 'sense', 'lot', 'hax', 'also', 'time', 'ring', 'make', 'acausal', 'ger', 'wont', 'work', 'source', '\\nzeno', 'destroy', 'zamasuhttpwwwyoutubecomwatchvitief3crfvm2httpswwwredditcomrwhowouldwincomments73i549_dnqi5sdcontext10dnqi5sd', 'here\\nhow', 'powerful', 'zeno', 'dragon', 'ball', 'superhttpwwwyoutubecomwatchvwggkmp6xrx41httpswwwredditcomrwhowouldwincomments73i549_dnqijnwcontext10dnqijnw', 'video', 'describe', 'powerful', 'zeno', 'is\\nim', 'bot', 'work', 'hard', 'help', 'redditors', 'find', 'relate', 'videos', 'watch', 'ill', 'keep', 'update', 'long', 'can\\n\\nplay', 'allhttpsubtletvcom_r73i549featureplaylistampftrlnk1', 'infohttpsnpredditcomrsubtletvwikimentioned_videos', 'get', 'chromehttpschromegooglecomwebstoredetailmentionedvideosforreddfiimkmdalmgffhibfdjnhljpnigcmohf', 'firefoxhttpsaddonsmozillaorgenusfirefoxaddonmentionedvideosforreddit'])\n",
      "original document: \n",
      "['Exactly.', 'Generation', 'Z', 'already', 'gets', 'this.', 'See', 'this', '\\nhttps://www.reddit.com/r/MGTOW/comments/73cbkv/not_drinking_or_driving_teens_increasingly_put/?st=J87ZADUH&amp;sh=a8ba3e7c\\n\\n\\nThe', 'ones', 'bitching', 'about', 'that', 'article', \"don't\", 'get', 'it.', \"They're\", 'still', 'trapped', 'in', 'the', 'Matrix.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'gen', 'z', 'already', 'get', 'see', '\\nhttpswwwredditcomrmgtowcomments73cbkvnot_drinking_or_driving_teens_increasingly_putstj87zaduhampsha8ba3e7c\\n\\n\\nthe', 'on', 'bitch', 'artic', 'dont', 'get', 'theyr', 'stil', 'trap', 'matrix'], ['exactly', 'generation', 'z', 'already', 'get', 'see', '\\nhttpswwwredditcomrmgtowcomments73cbkvnot_drinking_or_driving_teens_increasingly_putstj87zaduhampsha8ba3e7c\\n\\n\\nthe', 'ones', 'bitch', 'article', 'dont', 'get', 'theyre', 'still', 'trap', 'matrix'])\n",
      "original document: \n",
      "['WTF.', 'I', 'thought', 'u', 'had', '1', 'year', 'from', 'your', 'most', 'recent', 'survey.', 'So', 'stocking', 'up', 'is', 'impossible', 'now?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wtf', 'thought', 'u', 'on', 'year', 'rec', 'survey', 'stock', 'imposs'], ['wtf', 'think', 'u', 'one', 'year', 'recent', 'survey', 'stock', 'impossible'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'thought', 'it', 'was', 'always', 'possible,', 'just', 'not', 'worth', 'the', 'cost', 'of', 'processing', 'until', 'recently', '(referring', 'to', 'the', 'Canadian', 'oil', 'sands).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'alway', 'poss', 'wor', 'cost', 'process', 'rec', 'refer', 'canad', 'oil', 'sand'], ['think', 'always', 'possible', 'worth', 'cost', 'process', 'recently', 'refer', 'canadian', 'oil', 'sand'])\n",
      "original document: \n",
      "['143413486|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'QRDnEOyg)\\n\\n2016==&gt;', 'Trump', '(and', 'loving', 'the', 'shitstorm)\\n2012==&gt;', 'Mitt', \"(didn't\", 'like', 'him', 'but', 'it', 'was', 'a', 'vote', 'against', 'Obama)\\n2008==&gt;', '3rd', 'party\\n2004==&gt;', 'Bushy\\n2000==&gt;', 'Bushy\\n1996==&gt;', '3rd', 'party\\n1992==&gt;', 'Ross', 'Perot', '(woot)\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, four hundred and eighty-six', 'gt', 'unit', 'stat', 'anonym', 'id', 'qrdneoyg\\n\\n2016gt', 'trump', 'lov', 'shitstorm\\n2012gt', 'mit', 'didnt', 'lik', 'vot', 'obama\\n2008gt', '3rd', 'party\\n2004gt', 'bushy\\n2000gt', 'bushy\\n1996gt', '3rd', 'party\\n1992gt', 'ross', 'perot', 'woot\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, four hundred and eighty-six', 'gt', 'unite', 'state', 'anonymous', 'id', 'qrdneoyg\\n\\n2016gt', 'trump', 'love', 'shitstorm\\n2012gt', 'mitt', 'didnt', 'like', 'vote', 'obama\\n2008gt', '3rd', 'party\\n2004gt', 'bushy\\n2000gt', 'bushy\\n1996gt', '3rd', 'party\\n1992gt', 'ross', 'perot', 'woot\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"Didn't\", 'know', 'Marlins', 'had', 'downchop', 'fairies...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'know', 'marlin', 'downchop', 'fairy'], ['didnt', 'know', 'marlins', 'downchop', 'fairies'])\n",
      "original document: \n",
      "['I', 'can', 'drive', 'stick', 'and', 'have', 'a', 'CDL,', \"it's\", 'class', 'B', 'though,', 'but', 'I', 'can', 'still', 'move', 'trailers', 'to', 'where', 'they', 'are', 'needed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['driv', 'stick', 'cdl', 'class', 'b', 'though', 'stil', 'mov', 'trail', 'nee'], ['drive', 'stick', 'cdl', 'class', 'b', 'though', 'still', 'move', 'trailers', 'need'])\n",
      "original document: \n",
      "['You', \"don't\", 'lose', 'any', 'vertical', 'space', 'if', 'you', 'paint', 'the', 'ceiling,', 'and', 'it', 'just', 'kinda', 'fades', 'away', 'when', \"it's\", 'painted', 'like', 'that.', 'It', 'also', 'fits', 'with', 'the', 'more', 'modern', 'look', 'we', 'are', 'shooting', 'for', 'in', 'the', 'basement.', \"It's\", 'a', 'really', 'common', 'way', 'to', 'do', 'ceilings', 'in', 'commercial', 'spaces-', 'you', 'may', 'have', 'been', 'in', 'a', 'store', 'or', 'restaurant', 'that', 'has', 'an', 'all', 'black', 'ceiling', 'and', 'not', 'even', 'noticed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'los', 'vert', 'spac', 'paint', 'ceil', 'kind', 'fad', 'away', 'paint', 'lik', 'also', 'fit', 'modern', 'look', 'shoot', 'bas', 'real', 'common', 'way', 'ceil', 'commerc', 'spac', 'may', 'stor', 'resta', 'black', 'ceil', 'ev', 'not'], ['dont', 'lose', 'vertical', 'space', 'paint', 'ceiling', 'kinda', 'fade', 'away', 'paint', 'like', 'also', 'fit', 'modern', 'look', 'shoot', 'basement', 'really', 'common', 'way', 'ceilings', 'commercial', 'space', 'may', 'store', 'restaurant', 'black', 'ceiling', 'even', 'notice'])\n",
      "original document: \n",
      "['His', 'dad', 'was', 'actually', 'a', 'contestant', 'on', 'the', 'life-or-death', 'game', 'show', '*Wheel', 'of', 'Fortune*', 'as', 'a', 'kid.', 'When', 'he', 'failed', 'to', 'guess', 'the', 'puzzle,', 'he', 'was', 'dumped', 'into', 'a', 'pit', 'of', 'hungry', 'crocodiles', 'and', 'Pat', 'Sajak', 'claimed', 'the', '\"i\"', 'in', \"Firk's\", 'surname', 'as', 'his', 'personal', 'trophy.', 'His', 'son', 'Martin,', 'distraught', 'at', 'losing', 'both', 'his', 'father', '*and*', 'his', 'vowels', 'to', 'that', 'madman', 'Saijak', 'vowed', 'revenge', 'and', 'has', 'spent', 'the', 'last', '15', 'years', 'honing', 'his', 'skills', 'with', 'the', 'blade', 'and', 'plotting', 'the', 'moment', 'in', 'which', 'he', 'is', 'able', 'to', 'crush', 'Saijak', 'and', 'his', 'henchman', 'Vanna', 'White', 'and', 'reclaim', 'the', 'letter', 'that', 'is', 'rightfully', 'his.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dad', 'act', 'contest', 'lifeordea', 'gam', 'show', 'wheel', 'fortun', 'kid', 'fail', 'guess', 'puzzl', 'dump', 'pit', 'hungry', 'crocodil', 'pat', 'sajak', 'claim', 'firk', 'surnam', 'person', 'troph', 'son', 'martin', 'distraught', 'los', 'fath', 'vowel', 'madm', 'saijak', 'vow', 'reveng', 'spent', 'last', 'fifteen', 'year', 'hon', 'skil', 'blad', 'plot', 'mom', 'abl', 'crush', 'saijak', 'henchm', 'vann', 'whit', 'reclaim', 'let', 'right'], ['dad', 'actually', 'contestant', 'lifeordeath', 'game', 'show', 'wheel', 'fortune', 'kid', 'fail', 'guess', 'puzzle', 'dump', 'pit', 'hungry', 'crocodiles', 'pat', 'sajak', 'claim', 'firks', 'surname', 'personal', 'trophy', 'son', 'martin', 'distraught', 'lose', 'father', 'vowels', 'madman', 'saijak', 'vow', 'revenge', 'spend', 'last', 'fifteen', 'years', 'hone', 'skills', 'blade', 'plot', 'moment', 'able', 'crush', 'saijak', 'henchman', 'vanna', 'white', 'reclaim', 'letter', 'rightfully'])\n",
      "original document: \n",
      "['Maritime', 'provinces', 'would', 'make', 'it', 'to', 'the', 'end,', 'New', 'Brunswick', 'being', 'completely', 'looked', 'over', 'by', 'the', 'others.', '\\n\\nSaskatchewan', 'would', 'go', 'first.', 'We', 'would', 'feel', 'too', 'guilty', 'to', 'vote', 'off', 'the', 'territories.', 'Quebec', 'Ontario', 'and', 'BC', 'would', 'all', 'try', 'to', 'be', 'alphas,', 'make', 'alliances', 'and', 'the', 'screw', 'each', 'other', 'over', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['maritim', 'provint', 'would', 'mak', 'end', 'new', 'brunswick', 'complet', 'look', 'oth', '\\n\\nsaskatchewan', 'would', 'go', 'first', 'would', 'feel', 'guil', 'vot', 'territ', 'quebec', 'ontario', 'bc', 'would', 'try', 'alpha', 'mak', 'al', 'screw'], ['maritime', 'provinces', 'would', 'make', 'end', 'new', 'brunswick', 'completely', 'look', 'others', '\\n\\nsaskatchewan', 'would', 'go', 'first', 'would', 'feel', 'guilty', 'vote', 'territories', 'quebec', 'ontario', 'bc', 'would', 'try', 'alphas', 'make', 'alliances', 'screw'])\n",
      "original document: \n",
      "['Definitely', 'better', 'than', 'huffing', 'glue.', \"Don't\", 'huff', 'glue.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'bet', 'huff', 'glu', 'dont', 'huff', 'glu'], ['definitely', 'better', 'huff', 'glue', 'dont', 'huff', 'glue'])\n",
      "original document: \n",
      "['Which', 'always', 'seemed', 'silly', 'to', 'me.', 'Either', 'the', 'position', 'specific', 'numbers', 'are', 'important', 'enough', 'that', 'he', 'should', 'have', 'to', 'change', 'now', 'that', 'he', 'is', 'a', 'RB,', 'or', 'they', \"aren't\", 'important', 'so', 'why', 'bother', 'with', 'them', 'at', 'all?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'seem', 'sil', 'eith', 'posit', 'spec', 'numb', 'import', 'enough', 'chang', 'rb', 'ar', 'import', 'both'], ['always', 'seem', 'silly', 'either', 'position', 'specific', 'number', 'important', 'enough', 'change', 'rb', 'arent', 'important', 'bother'])\n",
      "original document: \n",
      "['I', 'heard', 'Mete', 'got', 'hit', 'and', 'it', 'sounded', 'dirty.', 'Is', 'he', 'okay?', 'Is', 'the', 'other', 'guy', 'still', 'breathing?', 'Please', 'tell', 'me', 'Price', 'dusted', 'off', 'his', 'blocker!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heard', 'met', 'got', 'hit', 'sound', 'dirty', 'okay', 'guy', 'stil', 'breath', 'pleas', 'tel', 'pric', 'dust', 'block'], ['hear', 'mete', 'get', 'hit', 'sound', 'dirty', 'okay', 'guy', 'still', 'breathe', 'please', 'tell', 'price', 'dust', 'blocker'])\n",
      "original document: \n",
      "['I', 'think', 'being', 'out', 'to', 'each', 'other', 'is', 'really', 'important,', 'so', 'good', 'to', 'hear', 'you', 'both', 'are.\\n\\nAnd', \"I'm\", 'certainly', 'not', 'trying', 'to', 'push', 'you', 'into', 'or', 'away', 'from', 'hormones', 'or', 'anything', 'else.', \"I'm\", 'just', 'warning', 'against', 'slipping', 'into', 'denial', 'or', 'repression.', 'As', 'long', 'as', \"you're\", 'honest', 'with', 'yourself,', 'you', 'can', 'make', 'an', 'informed', '(if', 'scary!)', 'decision.\\n\\nFBI...Female', 'But', 'Incognito?', 'I', 'might', 'steal', 'that...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'real', 'import', 'good', 'hear', 'are\\n\\nand', 'im', 'certain', 'try', 'push', 'away', 'hormon', 'anyth', 'els', 'im', 'warn', 'slip', 'den', 'repress', 'long', 'yo', 'honest', 'mak', 'inform', 'scary', 'decision\\n\\nfbifemale', 'incognito', 'might', 'ste'], ['think', 'really', 'important', 'good', 'hear', 'are\\n\\nand', 'im', 'certainly', 'try', 'push', 'away', 'hormones', 'anything', 'else', 'im', 'warn', 'slip', 'denial', 'repression', 'long', 'youre', 'honest', 'make', 'inform', 'scary', 'decision\\n\\nfbifemale', 'incognito', 'might', 'steal'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'void', 'walker', 'for', 'everything.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'void', 'walk', 'everyth'], ['im', 'void', 'walker', 'everything'])\n",
      "original document: \n",
      "['*', '**[Dragonfire', 'Potion](https://media-Hearth.cursecdn.com/avatars/329/334/49648.png)**', 'Priest', 'Spell', 'Epic', 'MSoG', '🐘', '^[HP](http://www.hearthpwn.com/cards/49648),', '^[HH](http://www.hearthhead.com/cards/dragonfire-potion),', '^[Wiki](http://hearthstone.gamepedia.com/Dragonfire_Potion)', '', '\\n6', 'Mana', '-', 'Deal', '5', 'damage', 'to', 'all', 'minions', 'except', 'Dragons.', '', '\\n\\n^(Call/)^[PM](https://www.reddit.com/message/compose/?to=hearthscan-bot)', '^(', 'me', 'with', 'up', 'to', '7', '[[cardname]].', ')^[About.](https://www.reddit.com/message/compose/?to=hearthscan-bot&amp;message=Tell%20me%20more%20[[info]]&amp;subject=hi)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dragonfir', 'potionhttpsmediahearthcursecdncomavatars32933449648png', 'priest', 'spel', 'ep', 'msog', 'hphttpwwwhearthpwncomcards49648', 'hhhttpwwwhearthheadcomcardsdragonfirepotion', 'wikihttphearthstonegamepediacomdragonfire_potion', '\\n6', 'man', 'deal', 'fiv', 'dam', 'min', 'exceiv', 'dragon', '\\n\\ncallpmhttpswwwredditcommessagecomposetohearthscanbot', 'sev', 'cardnam', 'abouthttpswwwredditcommessagecomposetohearthscanbotampmessagetell20me20more20infoampsubjecthi'], ['dragonfire', 'potionhttpsmediahearthcursecdncomavatars32933449648png', 'priest', 'spell', 'epic', 'msog', 'hphttpwwwhearthpwncomcards49648', 'hhhttpwwwhearthheadcomcardsdragonfirepotion', 'wikihttphearthstonegamepediacomdragonfire_potion', '\\n6', 'mana', 'deal', 'five', 'damage', 'minions', 'except', 'dragons', '\\n\\ncallpmhttpswwwredditcommessagecomposetohearthscanbot', 'seven', 'cardname', 'abouthttpswwwredditcommessagecomposetohearthscanbotampmessagetell20me20more20infoampsubjecthi'])\n",
      "original document: \n",
      "['The', 'adoption', 'fees', 'for', 'my', 'cats', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['adopt', 'fee', 'cat'], ['adoption', 'fee', 'cat'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['You', 'may', 'find', 'these', 'links', 'helpful:\\n\\n-', '[Budgeting', 'wiki', 'page](http://www.reddit.com/r/personalfinance/wiki/budgeting)\\n-', '[Finance', 'spreadsheets](http://www.reddit.com/r/personalfinance/wiki/tools#wiki_redditor_created.3A)\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/personalfinance)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'find', 'link', 'helpful\\n\\n', 'budget', 'wik', 'pagehttpwwwredditcomrpersonalfinancewikibudgeting\\n', 'fin', 'spreadsheetshttpwwwredditcomrpersonalfinancewikitoolswiki_redditor_created3a\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorpersonalfin', 'quest', 'concern'], ['may', 'find', 'link', 'helpful\\n\\n', 'budget', 'wiki', 'pagehttpwwwredditcomrpersonalfinancewikibudgeting\\n', 'finance', 'spreadsheetshttpwwwredditcomrpersonalfinancewikitoolswiki_redditor_created3a\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorpersonalfinance', 'question', 'concern'])\n",
      "original document: \n",
      "['V1', '=', 'we', 'won']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['v1'], ['v1'])\n",
      "original document: \n",
      "['that', 'would', 'be', '9:30', 'EST,', 'its', '8:30', 'CST']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'nine hundred and thirty', 'est', 'eight hundred and thirty', 'cst'], ['would', 'nine hundred and thirty', 'est', 'eight hundred and thirty', 'cst'])\n",
      "original document: \n",
      "['Or', 'at', 'least', 'let', 'Arcia', 'wear', 'his', 'glasses.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'let', 'arc', 'wear', 'glass'], ['least', 'let', 'arcia', 'wear', 'glass'])\n",
      "original document: \n",
      "['Josh', 'Jackson', 'refused', 'to', 'practice', 'for', 'Boston.', 'Didnt', 'wanna', 'be', 'a', '3rd', 'option.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['josh', 'jackson', 'refus', 'pract', 'boston', 'didnt', 'wann', '3rd', 'opt'], ['josh', 'jackson', 'refuse', 'practice', 'boston', 'didnt', 'wanna', '3rd', 'option'])\n",
      "original document: \n",
      "['I', \"haven't\", 'watched', 'much', 'of', 'you', 'guys', 'this', 'season.', \"I'm\", 'guessing', 'this', 'is', 'normal?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hav', 'watch', 'much', 'guy', 'season', 'im', 'guess', 'norm'], ['havent', 'watch', 'much', 'guy', 'season', 'im', 'guess', 'normal'])\n",
      "original document: \n",
      "['what', 'no?', 'im', 'playing', 'titanfall', '2', 'while', 'sinking', 'some', 'tins', 'listening', 'to', 'a', 'podcsat,', 'ive', 'been', 'top', 'spot', 'in', 'my', 'team', 'pretty', 'much', 'every', 'game']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'play', 'titanfal', 'two', 'sink', 'tin', 'list', 'podcs', 'iv', 'top', 'spot', 'team', 'pretty', 'much', 'every', 'gam'], ['im', 'play', 'titanfall', 'two', 'sink', 'tin', 'listen', 'podcsat', 'ive', 'top', 'spot', 'team', 'pretty', 'much', 'every', 'game'])\n",
      "original document: \n",
      "['TheFear', '290', 'titan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thefear', 'two hundred and ninety', 'tit'], ['thefear', 'two hundred and ninety', 'titan'])\n",
      "original document: \n",
      "['ahhhh', 'i', 'remember', 'the', 'days', 'of', 'water-bottles,', 'pen', 'tubes,', 'tape', 'and', 'sockets....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahhhh', 'rememb', 'day', 'waterbottl', 'pen', 'tub', 'tap', 'socket'], ['ahhhh', 'remember', 'days', 'waterbottles', 'pen', 'tube', 'tape', 'sockets'])\n",
      "original document: \n",
      "['She', 'looks', 'beautiful.', 'I', 'hope', \"she's\", 'feeling', 'beautiful,', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'beauty', 'hop', 'she', 'feel', 'beauty'], ['look', 'beautiful', 'hope', 'shes', 'feel', 'beautiful'])\n",
      "original document: \n",
      "['Type', \"''screenshot.render''\", 'in', 'the', 'console', 'and', 'press', 'enter,', 'the', 'console', 'can', 'be', 'opened', 'by', 'pressing', '`', 'under', 'the', 'esc', 'key.\\nYou', 'can', 'then', 'find', 'the', 'screenshot', 'in', 'documents', '-&gt;', 'battlefield', '1', '-&gt;', 'screenshots.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['typ', 'screenshotrender', 'consol', 'press', 'ent', 'consol', 'op', 'press', 'esc', 'key\\nyou', 'find', 'screenshot', 'docu', 'gt', 'battlefield', 'on', 'gt', 'screenshots'], ['type', 'screenshotrender', 'console', 'press', 'enter', 'console', 'open', 'press', 'esc', 'key\\nyou', 'find', 'screenshot', 'document', 'gt', 'battlefield', 'one', 'gt', 'screenshots'])\n",
      "original document: \n",
      "[\"That's\", 'not', 'really', 'a', 'punishment.', 'I', 'guess', 'detention', 'would', 'be', 'a', 'better', 'approach.\\n\\nEdit:', 'Man,', 'I', 'sure', 'hope', 'this', 'comment', 'is', 'controversial', 'because', 'of', 'some', 'well-considered', 'opinion', 'on', 'discipline', 'and', 'teaching', 'and', 'not,', 'say,', 'because', '[71%](https://www.reddit.com/r/dataisbeautiful/comments/5700sj/octhe_results_of_the_reddit_demographics_survey/)', 'of', 'of', 'reddit', 'users', 'are', 'either', 'in', 'highschool,', 'college,', 'or', 'recently', 'graduated', 'and', 'are', 'salty', 'about', 'not', 'getting', 'to', 'use', 'their', 'toy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'real', 'pun', 'guess', 'det', 'would', 'bet', 'approach\\n\\nedit', 'man', 'sur', 'hop', 'com', 'controvers', 'wellconsid', 'opin', 'disciplin', 'teach', 'say', '71httpswwwredditcomrdataisbeautifulcomments5700sjocthe_results_of_the_reddit_demographics_survey', 'reddit', 'us', 'eith', 'highschool', 'colleg', 'rec', 'gradu', 'sal', 'get', 'us', 'toy'], ['thats', 'really', 'punishment', 'guess', 'detention', 'would', 'better', 'approach\\n\\nedit', 'man', 'sure', 'hope', 'comment', 'controversial', 'wellconsidered', 'opinion', 'discipline', 'teach', 'say', '71httpswwwredditcomrdataisbeautifulcomments5700sjocthe_results_of_the_reddit_demographics_survey', 'reddit', 'users', 'either', 'highschool', 'college', 'recently', 'graduate', 'salty', 'get', 'use', 'toy'])\n",
      "original document: \n",
      "['I', 'was', 'constantly', 'bullied', 'as', 'a', 'child', 'for', 'crying.', 'Nowadays', 'I', \"can't\", 'cry', 'at', 'all,', 'even', 'when', 'I', 'desperately', 'need', 'it', 'because', 'my', 'body', '*needs*', 'to', 'get', 'stress', 'out.\\n\\nSo', 'instead', \"I've\", 'bottled', 'bad', 'emotions', 'for', 'years', 'and', 'now', \"they're\", 'manifesting', 'with', 'mind-shattering', 'panic', 'attacks.\\n\\nOh', 'joy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['const', 'bul', 'child', 'cry', 'nowaday', 'cant', 'cry', 'ev', 'desp', 'nee', 'body', 'nee', 'get', 'stress', 'out\\n\\nso', 'instead', 'iv', 'bottl', 'bad', 'emot', 'year', 'theyr', 'manifest', 'mindshat', 'pan', 'attacks\\n\\noh', 'joy'], ['constantly', 'bully', 'child', 'cry', 'nowadays', 'cant', 'cry', 'even', 'desperately', 'need', 'body', 'need', 'get', 'stress', 'out\\n\\nso', 'instead', 'ive', 'bottle', 'bad', 'emotions', 'years', 'theyre', 'manifest', 'mindshattering', 'panic', 'attacks\\n\\noh', 'joy'])\n",
      "original document: \n",
      "['Thank', 'you.', 'Idk', 'why', 'people', \"won't\", 'accept', 'this', 'as', 'an', 'answer', 'it', 'fucking', 'works.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'idk', 'peopl', 'wont', 'acceiv', 'answ', 'fuck', 'work'], ['thank', 'idk', 'people', 'wont', 'accept', 'answer', 'fuck', 'work'])\n",
      "original document: \n",
      "['Yeah', 'I', 'mean', 'we', 'are', 'talking', 'about', 'a', 'guy', 'who', 'jumps', 'and', 'squishes', 'people.', 'https://youtu.be/nfLS4nt5aQw']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'mean', 'talk', 'guy', 'jump', 'squ', 'peopl', 'httpsyoutubenfls4nt5aqw'], ['yeah', 'mean', 'talk', 'guy', 'jump', 'squish', 'people', 'httpsyoutubenfls4nt5aqw'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['\"It', 'worked', 'for', 'me', 'and', 'it', 'worked', 'for', 'Conor,', 'but', 'he', 'added', 'the', 'part', 'where', 'you', 'fight', 'in', 'a', 'completely', 'other', 'sport\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'work', 'con', 'ad', 'part', 'fight', 'complet', 'sport'], ['work', 'work', 'conor', 'add', 'part', 'fight', 'completely', 'sport'])\n",
      "original document: \n",
      "['&gt;', '', 'the', 'story', 'was', 'completely', 'over', 'my', 'expectations,', 'it', 'fluctuates', 'to', 'pretty', 'entertaining', 'to', 'burst-out-laughing', 'hilarious,', 'and', 'a', 'slightly-more-than-healthy', 'dose', 'of', 'cringy', 'one-liners\\n\\nRULUE', 'LOVES', 'THE', 'DARK', 'PRINCE.\\n\\nRULUE', 'LOVES', 'THE', 'DARK', 'PRINCE.\\n\\nRULUE', 'LOVES', 'THE', 'DARK', 'PRINCE.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'story', 'complet', 'expect', 'fluctu', 'pretty', 'entertain', 'burstoutlaugh', 'hil', 'slightlymorethanhealthy', 'dos', 'cringy', 'oneliners\\n\\nrulue', 'lov', 'dark', 'prince\\n\\nrulu', 'lov', 'dark', 'prince\\n\\nrulu', 'lov', 'dark', 'print'], ['gt', 'story', 'completely', 'expectations', 'fluctuate', 'pretty', 'entertain', 'burstoutlaughing', 'hilarious', 'slightlymorethanhealthy', 'dose', 'cringy', 'oneliners\\n\\nrulue', 'love', 'dark', 'prince\\n\\nrulue', 'love', 'dark', 'prince\\n\\nrulue', 'love', 'dark', 'prince'])\n",
      "original document: \n",
      "['Um', 'ok....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['um', 'ok'], ['um', 'ok'])\n",
      "original document: \n",
      "['Read', 'the', 'sidebar.', 'It', 'meets', 'none', 'of', 'the', 'criteria.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['read', 'sideb', 'meet', 'non', 'criter'], ['read', 'sidebar', 'meet', 'none', 'criteria'])\n",
      "original document: \n",
      "['GODS,', 'SHE', 'HAS', 'TANLINES', 'NOW.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'tanlin'], ['gods', 'tanlines'])\n",
      "original document: \n",
      "['Hows', 'he', 'doing?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['how'], ['hows'])\n",
      "original document: \n",
      "['Long', 'as', 'your', 'BAC', \"wasn't\", 'above', '.06', \"you're\", 'good.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['long', 'bac', 'wasnt', 'six', 'yo', 'good'], ['long', 'bac', 'wasnt', 'six', 'youre', 'good'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Id', 'recommend', 'that', 'you', 'go', '\"porn', 'style\"', 'and', 'pull', 'out', 'all', 'the', 'time', 'until', 'you', 'would', 'welcome', 'a', 'baby....why', 'add', 'that', 'stress', 'to', 'sex?', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'recommend', 'go', 'porn', 'styl', 'pul', 'tim', 'would', 'welcom', 'babywhy', 'ad', 'stress', 'sex'], ['id', 'recommend', 'go', 'porn', 'style', 'pull', 'time', 'would', 'welcome', 'babywhy', 'add', 'stress', 'sex'])\n",
      "original document: \n",
      "['And', 'we', 'always', 'see', 'it', 'coming', 'first.', 'We', 'are', 'the', 'canary', 'in', 'the', 'societal', 'coal', 'mine.', 'They', 'call', 'us', 'the', 'religious', 'right', 'for', 'a', 'reason', '-', \"we're\", 'usually', 'right.', '\\n\\nRemember', 'the', 'alleged', '*Gay', 'Agenda?*', 'A', 'conspiracy', 'to', 'queer', 'up', 'your', 'kids??', 'Lol,', 'not', 'lol', \":'(\", '\\n\\nBy', 'the', 'by...', 'ISLAM!!!!', 'AHHHH', 'THE', 'HORROR!!!!!', 'STOP', 'IT', 'NOW', 'BEFORE', \"IT'S\", 'TOO', 'LATE!!!!', '🐥', '😱💀\\n\\nEdit:', 'http://www.thegatewaypundit.com/2017/09/report-france-western-europe-majority-muslim-40-years/\\n\\nChoose', 'to', 'be', 'Muslim:', 'freedom', 'of', 'religion,', 'no', 'problem.\\n\\nChoose', 'to', 'inflict', 'Islam', 'on', 'others:', 'act', 'of', 'war,', 'terror,', 'and', 'genocide.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'see', 'com', 'first', 'can', 'societ', 'coal', 'min', 'cal', 'us', 'religy', 'right', 'reason', 'us', 'right', '\\n\\nremember', 'alleg', 'gay', 'agend', 'conspir', 'que', 'kid', 'lol', 'lol', '\\n\\nby', 'islam', 'ahhhh', 'hor', 'stop', 'lat', '\\n\\nedit', 'httpwwwthegatewaypunditcom201709reportfrancewesterneuropemajoritymuslim40years\\n\\nchoose', 'muslim', 'freedom', 'relig', 'problem\\n\\nchoose', 'inflict', 'islam', 'oth', 'act', 'war', 'ter', 'genocid'], ['always', 'see', 'come', 'first', 'canary', 'societal', 'coal', 'mine', 'call', 'us', 'religious', 'right', 'reason', 'usually', 'right', '\\n\\nremember', 'allege', 'gay', 'agenda', 'conspiracy', 'queer', 'kid', 'lol', 'lol', '\\n\\nby', 'islam', 'ahhhh', 'horror', 'stop', 'late', '\\n\\nedit', 'httpwwwthegatewaypunditcom201709reportfrancewesterneuropemajoritymuslim40years\\n\\nchoose', 'muslim', 'freedom', 'religion', 'problem\\n\\nchoose', 'inflict', 'islam', 'others', 'act', 'war', 'terror', 'genocide'])\n",
      "original document: \n",
      "['Yup,', 'I', 'read', 'the', 'whole', 'thing,', 'but', 'unlike', 'the', 'first', 'analysis,', 'the', 'whole', '\"crying', 'triggered', 'a', 'crying', 'triggering', 'another', 'feeling', 'of', 'being', 'defeated\"', 'is', 'kind', 'of...', 'odd', 'and', 'extremely', 'forced.', 'I', 'mean,', \"it's\", 'not', 'some', 'random', 'crying', 'but', \"Goten's,\", 'who', 'at', 'that', 'point', 'of', 'the', 'series', 'was', 'meant', 'to', 'be', 'a', '\"mini-Goku\"', 'is', 'the', 'one', 'awakening', 'Broly.', \"It's\", 'just', 'too', 'much', 'coincidence,', 'and', 'also', 'going', 'by', \"Occam's\", 'razor,', \"it's\", 'just', 'the', '\"more', 'fitting\"', 'explanation.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup', 'read', 'whol', 'thing', 'unlik', 'first', 'analys', 'whol', 'cry', 'trig', 'cry', 'trig', 'anoth', 'feel', 'def', 'kind', 'od', 'extrem', 'forc', 'mean', 'random', 'cry', 'got', 'point', 'sery', 'meant', 'minigoku', 'on', 'awak', 'bro', 'much', 'coincid', 'also', 'going', 'occam', 'raz', 'fit', 'expl'], ['yup', 'read', 'whole', 'thing', 'unlike', 'first', 'analysis', 'whole', 'cry', 'trigger', 'cry', 'trigger', 'another', 'feel', 'defeat', 'kind', 'odd', 'extremely', 'force', 'mean', 'random', 'cry', 'gotens', 'point', 'series', 'mean', 'minigoku', 'one', 'awaken', 'broly', 'much', 'coincidence', 'also', 'go', 'occams', 'razor', 'fit', 'explanation'])\n",
      "original document: \n",
      "['ppl', 'can', 'say', 'what', 'they', 'want', 'about', 'YES', 'announcers,', 'but', \"they're\", 'always', 'extremely', 'courteous', 'and', 'professional']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ppl', 'say', 'want', 'ye', 'annount', 'theyr', 'alway', 'extrem', 'court', 'profess'], ['ppl', 'say', 'want', 'yes', 'announcers', 'theyre', 'always', 'extremely', 'courteous', 'professional'])\n",
      "original document: \n",
      "['Hot', 'damn.', '(งツ)ว']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hot', 'damn'], ['hot', 'damn'])\n",
      "original document: \n",
      "['Yes,', 'the', 'iPad', '3', 'is', 'supported,', 'not', 'the', 'Mini', '3.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'ipad', 'three', 'support', 'min', 'three'], ['yes', 'ipad', 'three', 'support', 'mini', 'three'])\n",
      "original document: \n",
      "['pretty', 'much', 'same.....was', 'actually', 'just', 'putting', 'on', 'heavy', 'slayer', 'with', 'seraph', 'tears', 'on', 'it,,,,,,,,,but', 'I', \"don't\", 'understand', 'the', 'seaph', 'defence', 'and', 'seraph', 'pd.....I', 'can', 'only', 'refit', 'after', 'I', 'take', 'out', 'of', 'garage', 'and', 'dismnted', 'them', 'but', 'cant', 'equip', 'or', 'sell....and', 'their', 'description', 'is', 'just', '\"too', 'good\"', 'any', 'thoughts??\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'much', 'samewa', 'act', 'put', 'heavy', 'slay', 'seraph', 'tear', 'itbut', 'dont', 'understand', 'seaph', 'def', 'seraph', 'pdi', 'refit', 'tak', 'gar', 'dismnt', 'cant', 'equip', 'selland', 'describ', 'good', 'thoughts\\n'], ['pretty', 'much', 'samewas', 'actually', 'put', 'heavy', 'slayer', 'seraph', 'tear', 'itbut', 'dont', 'understand', 'seaph', 'defence', 'seraph', 'pdi', 'refit', 'take', 'garage', 'dismnted', 'cant', 'equip', 'selland', 'description', 'good', 'thoughts\\n'])\n",
      "original document: \n",
      "['Those', 'massive', 'parking', 'lots', 'are', 'packed', 'with', 'Schlitz', 'Park', 'commuters', 'Monday', 'through', 'Friday.', '', 'It', 'took', 'them', 'years', 'to', 'get', 'agreement', 'from', 'their', 'lessees', 'to', 'repave', 'that', 'big', 'lot.', 'Can', 'you', 'imagine', 'the', 'jockeying', 'required', 'to', 'shut', 'it', 'down', 'long', 'enough', 'to', 'build', 'a', 'parking', 'garage?', 'Plus', 'the', 'impact', 'to', 'both', 'Pleasant', 'and', 'Cherry', 'during', 'construction', 'would', 'be', 'incredibly', 'inconvenient', 'for', 'residents', 'and', 'commuting', 'employees', 'alike.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mass', 'park', 'lot', 'pack', 'schlitz', 'park', 'commut', 'monday', 'friday', 'took', 'year', 'get', 'agr', 'less', 'repav', 'big', 'lot', 'imagin', 'jockey', 'requir', 'shut', 'long', 'enough', 'build', 'park', 'gar', 'plu', 'impact', 'pleas', 'cherry', 'construct', 'would', 'incred', 'inconveny', 'resid', 'commut', 'employ', 'alik'], ['massive', 'park', 'lot', 'pack', 'schlitz', 'park', 'commuters', 'monday', 'friday', 'take', 'years', 'get', 'agreement', 'lessees', 'repave', 'big', 'lot', 'imagine', 'jockey', 'require', 'shut', 'long', 'enough', 'build', 'park', 'garage', 'plus', 'impact', 'pleasant', 'cherry', 'construction', 'would', 'incredibly', 'inconvenient', 'residents', 'commute', 'employees', 'alike'])\n",
      "original document: \n",
      "['First,', 'try', 'using', 'your', 'controller', 'in', 'another', 'game', 'and', 'try', 'using', 'another', 'controller,', 'this', 'will', 'determine', 'whether', 'this', 'is', 'a', 'problem', 'with', 'the', 'controller', 'or', 'the', 'game.', 'If', 'it’s', 'fine', 'then', 'try', 'resetting', 'your', 'controls', 'to', 'default', 'in', 'rocket', 'league.', 'Also', 'try', 'reinserting', 'your', 'controller', 'into', 'its', 'port', 'if', 'you’re', 'on', 'pc.', 'Honestly,', 'you', 'should', 'be', 'posting', 'this', 'onto', 'the', 'tomshardware', 'forums.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'try', 'us', 'control', 'anoth', 'gam', 'try', 'us', 'anoth', 'control', 'determin', 'wheth', 'problem', 'control', 'gam', 'fin', 'try', 'reset', 'control', 'default', 'rocket', 'leagu', 'also', 'try', 'reinsert', 'control', 'port', 'yo', 'pc', 'honest', 'post', 'onto', 'tomshardw', 'forums\\n'], ['first', 'try', 'use', 'controller', 'another', 'game', 'try', 'use', 'another', 'controller', 'determine', 'whether', 'problem', 'controller', 'game', 'fine', 'try', 'reset', 'control', 'default', 'rocket', 'league', 'also', 'try', 'reinserting', 'controller', 'port', 'youre', 'pc', 'honestly', 'post', 'onto', 'tomshardware', 'forums\\n'])\n",
      "original document: \n",
      "['Paediatric', 'intubation', 'is', '*much*', 'harder', 'because', 'of', 'their', 'differing', 'anatomy,', 'the', 'risk', 'for', 'barotrauma', 'is', 'higher', 'in', 'kids,', 'their', 'soft', 'tissues', 'are', 'more', 'easily', 'traumatised', 'and', \"they're\", 'easier', 'to', 'manage', 'with', 'less', 'invasive', 'methods', 'anyway.', 'Unless', 'you', 'need', 'the', 'tube', 'because', \"it's\", 'an', 'asthmatic', 'arrest,', 'or', 'because', 'of', 'developing', 'airway', 'oedema,', 'ETT', 'should', 'be', 'a', 'very', 'low', 'priority', 'in', 'children.', 'I', 'would', 'argue', 'that', 'paramedics', 'should', 'almost', 'never', 'really', 'be', 'considering', 'paediatric', 'intubation', 'if', 'other', 'methods', 'are', 'available,', 'and', 'never', 'electively', 'outside', 'of', 'airway', 'oedema.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['paedy', 'intub', 'much', 'hard', 'diff', 'anatom', 'risk', 'barotraum', 'high', 'kid', 'soft', 'tissu', 'easy', 'traum', 'theyr', 'easy', 'man', 'less', 'invas', 'method', 'anyway', 'unless', 'nee', 'tub', 'asthm', 'arrest', 'develop', 'airway', 'oedem', 'et', 'low', 'pri', 'childr', 'would', 'argu', 'param', 'almost', 'nev', 'real', 'consid', 'paedy', 'intub', 'method', 'avail', 'nev', 'elect', 'outsid', 'airway', 'oedem'], ['paediatric', 'intubation', 'much', 'harder', 'differ', 'anatomy', 'risk', 'barotrauma', 'higher', 'kid', 'soft', 'tissue', 'easily', 'traumatise', 'theyre', 'easier', 'manage', 'less', 'invasive', 'methods', 'anyway', 'unless', 'need', 'tube', 'asthmatic', 'arrest', 'develop', 'airway', 'oedema', 'ett', 'low', 'priority', 'children', 'would', 'argue', 'paramedics', 'almost', 'never', 'really', 'consider', 'paediatric', 'intubation', 'methods', 'available', 'never', 'electively', 'outside', 'airway', 'oedema'])\n",
      "original document: \n",
      "['As', 'others', 'have', 'suggested', 'i', 'would', 'say', 'ranger.', 'Probably', 'hunter', 'because', 'the', 'beastmaster', 'in', 'my', 'opinion,', 'still', 'after', 'the', 'rework,', 'leves', 'alot', 'to', 'be', 'desired.', 'Also', 'with', 'a', 'few', 'levels', 'of', 'Druid', 'for', 'shapechange', 'and', 'spells.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oth', 'suggest', 'would', 'say', 'rang', 'prob', 'hunt', 'beastmast', 'opin', 'stil', 'rework', 'lev', 'alot', 'desir', 'also', 'level', 'druid', 'shapechang', 'spel'], ['others', 'suggest', 'would', 'say', 'ranger', 'probably', 'hunter', 'beastmaster', 'opinion', 'still', 'rework', 'leves', 'alot', 'desire', 'also', 'level', 'druid', 'shapechange', 'spell'])\n",
      "original document: \n",
      "['Hallelujah.', 'Some', 'sense!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hallelujah', 'sens'], ['hallelujah', 'sense'])\n",
      "original document: \n",
      "['The', 'title,', 'it', 'is', 'backwards.', 'You', 'say', '\"Other', 'people', 'say', 's2x', 'will', 'cripple', 'bitcoin', 'core\"', 'while', 'these', 'people', 'say', 'that', '/r/bitcoin.', 'You', 'say', '\"This', 'subreddit', 'says', 'the', 'opposite\"', 'While', 'other', 'people', 'say', 'the', 'opposite', '/r/btc.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['titl', 'backward', 'say', 'peopl', 'say', 's2x', 'crippl', 'bitcoin', 'cor', 'peopl', 'say', 'rbitcoin', 'say', 'subreddit', 'say', 'opposit', 'peopl', 'say', 'opposit', 'rbtc'], ['title', 'backwards', 'say', 'people', 'say', 's2x', 'cripple', 'bitcoin', 'core', 'people', 'say', 'rbitcoin', 'say', 'subreddit', 'say', 'opposite', 'people', 'say', 'opposite', 'rbtc'])\n",
      "original document: \n",
      "['Well', 'if', 'you', 'like', 'it', \"you're\", 'not', 'part', 'of', 'a', '\"problem\".', 'Buy', 'what', 'you', 'enjoy,', 'just', 'because', 'I', 'find', 'it', 'a', 'little', 'boring', \"doesn't\", 'mean', 'I', 'think', 'its', 'a', 'problem', 'if', 'others', 'like', 'it', 'and', 'buy', 'it.', 'The', 'bigger', 'issue', 'is', 'when', 'people', 'who', \"don't\", 'like', 'it,', 'or', 'actively', 'hate', 'it,', 'continue', 'buying', 'it.', 'Because', 'fuck,', 'thats', 'not', 'good', 'for', 'anyone']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'lik', 'yo', 'part', 'problem', 'buy', 'enjoy', 'find', 'littl', 'bor', 'doesnt', 'mean', 'think', 'problem', 'oth', 'lik', 'buy', 'big', 'issu', 'peopl', 'dont', 'lik', 'act', 'hat', 'continu', 'buy', 'fuck', 'that', 'good', 'anyon'], ['well', 'like', 'youre', 'part', 'problem', 'buy', 'enjoy', 'find', 'little', 'bore', 'doesnt', 'mean', 'think', 'problem', 'others', 'like', 'buy', 'bigger', 'issue', 'people', 'dont', 'like', 'actively', 'hate', 'continue', 'buy', 'fuck', 'thats', 'good', 'anyone'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['If', 'you', 'have', 'a', 'melee', 'build,', 'just', 'run', 'under', 'the', 'dragon', 'and', 'put', 'yourself', 'behind', 'his', 'hind', 'legs.', 'He', \"can't\", 'do', 'the', 'stomp', 'attack', 'backwards', 'and', 'if', 'you', \"aren't\", 'between', 'his', 'legs', 'he', \"can't\", 'hit', 'you', 'with', 'the', 'backward', 'fire', 'attack.', 'Make', 'sure', 'you', 'have', 'something', 'with', 'stamina', 'regen', 'on', 'and', 'save', 'some', 'up', 'as', 'well.', 'He', 'will', 'take', 'to', 'the', 'air,', 'and', 'you', 'have', 'to', 'sprint', 'towards', 'his', 'tail,', 'as', 'when', 'is', 'done', 'with', 'his', 'AOE,', 'he', 'will', 'dro0', 'straight', 'down.', 'If', 'you', 'ran', 'far', 'enough', 'you', 'will', 'be', 'behind', 'his', 'back', 'legs', 'again.', 'Repeat', 'until', 'desired', 'results', 'come.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mel', 'build', 'run', 'dragon', 'put', 'behind', 'hind', 'leg', 'cant', 'stomp', 'attack', 'backward', 'ar', 'leg', 'cant', 'hit', 'backward', 'fir', 'attack', 'mak', 'sur', 'someth', 'stamin', 'reg', 'sav', 'wel', 'tak', 'air', 'sprint', 'toward', 'tail', 'don', 'ao', 'dro0', 'straight', 'ran', 'far', 'enough', 'behind', 'back', 'leg', 'rep', 'desir', 'result', 'com'], ['melee', 'build', 'run', 'dragon', 'put', 'behind', 'hind', 'legs', 'cant', 'stomp', 'attack', 'backwards', 'arent', 'legs', 'cant', 'hit', 'backward', 'fire', 'attack', 'make', 'sure', 'something', 'stamina', 'regen', 'save', 'well', 'take', 'air', 'sprint', 'towards', 'tail', 'do', 'aoe', 'dro0', 'straight', 'run', 'far', 'enough', 'behind', 'back', 'legs', 'repeat', 'desire', 'result', 'come'])\n",
      "original document: \n",
      "['Do', 'you', 'run', 'some', 'special', 'rune', 'setup', 'to', 'secure.targon', 'kills?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['run', 'spec', 'run', 'setup', 'securetargon', 'kil'], ['run', 'special', 'rune', 'setup', 'securetargon', 'kill'])\n",
      "original document: \n",
      "['I', 'have', 'crimson', 'halo', '+', 'hexed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['crimson', 'halo', 'hex'], ['crimson', 'halo', 'hex'])\n",
      "original document: \n",
      "['Nice', 'pull!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'pul'], ['nice', 'pull'])\n",
      "original document: \n",
      "['So', 'you', 'know,', \"'Do\", 'your', 'own', \"research'\", 'is', 'a', 'tagline', 'of', 'flattards', 'everywhere.\\n\\nGuy', 'can', 'ball', 'though!', '', 'Needs', 'a', 'new', 'agent', 'to', 'encourage', 'him', 'to', 'keep', 'his', \"'woke'\", 'in', 'check.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'research', 'taglin', 'flattard', 'everywhere\\n\\nguy', 'bal', 'though', 'nee', 'new', 'ag', 'enco', 'keep', 'wok', 'check'], ['know', 'research', 'tagline', 'flattards', 'everywhere\\n\\nguy', 'ball', 'though', 'need', 'new', 'agent', 'encourage', 'keep', 'wake', 'check'])\n",
      "original document: \n",
      "['It', 'makes', 'you', 'seem', 'more', 'assertive.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'seem', 'assert'], ['make', 'seem', 'assertive'])\n",
      "original document: \n",
      "['hm?', 'my', 'budget', 'is?', 'what', 'do', 'you', 'mean', 'like', 'my', 'budget', 'isnt', 'enough', 'or', 'something', 'like', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hm', 'budget', 'mean', 'lik', 'budget', 'isnt', 'enough', 'someth', 'lik'], ['hm', 'budget', 'mean', 'like', 'budget', 'isnt', 'enough', 'something', 'like'])\n",
      "original document: \n",
      "['I', 'just', 'pretend', 'that', 'he', 'most', 'likely', 'failed', 'at', 'knocking', 'the', 'last', 'guy', 'off,', 'and', 'got', 'knocked', 'off', 'himself.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretend', 'lik', 'fail', 'knock', 'last', 'guy', 'got', 'knock'], ['pretend', 'likely', 'fail', 'knock', 'last', 'guy', 'get', 'knock'])\n",
      "original document: \n",
      "['', 'To', 'become', 'a', 'true', 'Yanomamo', 'warrior,', 'you', 'must', 'prove', 'that', 'you', 'can', 'withstand', '**tits', 'so', 'small', \"they're\", 'concave**', 'without', 'crying', 'out.\\n\\nI&amp;apos;m', 'sorry,', 'sir,', 'but', 'we', \"don't\", 'allow', '**tits', 'so', 'small', \"they're\", 'concave**', 'at', 'the', 'country', 'club.\\n\\nAre', 'you', 'thinking', 'what', 'I&amp;apos;m', 'thinking,', 'B1?', 'I', 'think', 'I', 'am,', 'B2:', 'it&amp;apos;s', '**tits', 'so', 'small', \"they're\", 'concave**', 'time!\\n\\nDuring', 'his', 'midlife', 'crisis,', 'my', 'dad', 'got', 'really', 'into', '**tits', 'so', 'small', \"they're\", 'concave**.\\n\\nChannel', '5&amp;apos;s', 'new', 'reality', 'show', 'features', 'eight', 'washed-up', 'celebrities', 'living', 'with', '**tits', 'so', 'small', \"they're\", 'concave**.\\n\\nThis', 'holiday', 'season,', 'Tim', 'Allen', 'must', 'overcome', 'his', 'fear', 'of', '**tits', 'so', 'small', \"they're\", 'concave**', 'to', 'save', 'Christmas.\\n\\nIF', 'you', 'like', '**tits', 'so', 'small', \"they're\", 'concave**,', 'YOU', 'MIGHT', 'BE', 'A', 'REDNECK.\\n\\nHe', 'who', 'controls', '**tits', 'so', 'small', \"they're\", 'concave**', 'controls', 'the', 'world.\\n\\nWhen', 'all', 'else', 'fails,', 'I', 'can', 'always', 'masturbate', 'to', '**tits', 'so', 'small', \"they're\", 'concave**.\\n\\nHere', 'at', 'the', 'Academy', 'for', 'Gifted', 'Children,', 'we', 'all', 'students', 'to', 'explore', '**tits', 'so', 'small', \"they're\", 'concave**', 'at', 'their', 'own', 'pace.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['becom', 'tru', 'yanomamo', 'warry', 'must', 'prov', 'withstand', 'tit', 'smal', 'theyr', 'concav', 'without', 'cry', 'out\\n\\niampaposm', 'sorry', 'sir', 'dont', 'allow', 'tit', 'smal', 'theyr', 'concav', 'country', 'club\\n\\nare', 'think', 'iampaposm', 'think', 'b1', 'think', 'b2', 'itampaposs', 'tit', 'smal', 'theyr', 'concav', 'time\\n\\nduring', 'midl', 'cris', 'dad', 'got', 'real', 'tit', 'smal', 'theyr', 'concave\\n\\nchannel', '5ampaposs', 'new', 'real', 'show', 'feat', 'eight', 'washedup', 'celebr', 'liv', 'tit', 'smal', 'theyr', 'concave\\n\\nthis', 'holiday', 'season', 'tim', 'al', 'must', 'overcom', 'fear', 'tit', 'smal', 'theyr', 'concav', 'sav', 'christmas\\n\\nif', 'lik', 'tit', 'smal', 'theyr', 'concav', 'might', 'redneck\\n\\nhe', 'control', 'tit', 'smal', 'theyr', 'concav', 'control', 'world\\n\\nwhen', 'els', 'fail', 'alway', 'masturb', 'tit', 'smal', 'theyr', 'concave\\n\\nher', 'academy', 'gift', 'childr', 'stud', 'expl', 'tit', 'smal', 'theyr', 'concav', 'pac'], ['become', 'true', 'yanomamo', 'warrior', 'must', 'prove', 'withstand', 'tits', 'small', 'theyre', 'concave', 'without', 'cry', 'out\\n\\niampaposm', 'sorry', 'sir', 'dont', 'allow', 'tits', 'small', 'theyre', 'concave', 'country', 'club\\n\\nare', 'think', 'iampaposm', 'think', 'b1', 'think', 'b2', 'itampaposs', 'tits', 'small', 'theyre', 'concave', 'time\\n\\nduring', 'midlife', 'crisis', 'dad', 'get', 'really', 'tits', 'small', 'theyre', 'concave\\n\\nchannel', '5ampaposs', 'new', 'reality', 'show', 'feature', 'eight', 'washedup', 'celebrities', 'live', 'tits', 'small', 'theyre', 'concave\\n\\nthis', 'holiday', 'season', 'tim', 'allen', 'must', 'overcome', 'fear', 'tits', 'small', 'theyre', 'concave', 'save', 'christmas\\n\\nif', 'like', 'tits', 'small', 'theyre', 'concave', 'might', 'redneck\\n\\nhe', 'control', 'tits', 'small', 'theyre', 'concave', 'control', 'world\\n\\nwhen', 'else', 'fail', 'always', 'masturbate', 'tits', 'small', 'theyre', 'concave\\n\\nhere', 'academy', 'gift', 'children', 'students', 'explore', 'tits', 'small', 'theyre', 'concave', 'pace'])\n",
      "original document: \n",
      "[\"I'm\", 'sorry', 'if', 'thats', 'how', 'this', 'came', 'across', 'but', 'i', 'was', 'just', 'trying', 'to', 'figure', 'out', 'how', 'my', 'grandparents', 'can', 'prevent', 'losing', 'everything', 'to', 'pay', 'for', 'care', 'as', 'they', 'are', 'not', 'wealthy', 'at', 'all.', 'They', 'have', 'paid', 'taxes', 'all', 'their', 'life', 'and', 'will', 'continue', 'to', 'do', 'so,', 'i', 'was', 'just', 'wondering', 'whats', 'the', 'best', 'way', 'to', 'go', 'into', 'care/set', 'up', 'a', 'trust', 'fund', 'to', 'try', 'and', 'protect', 'some', 'of', 'their', 'assets,', 'i', 'was', 'not', 'asking', 'how', 'to', 'avoid', 'paying', 'anything', 'and', 'make', 'the', 'government', 'pay', 'it', 'all']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'that', 'cam', 'across', 'try', 'fig', 'grandp', 'prev', 'los', 'everyth', 'pay', 'car', 'wealthy', 'paid', 'tax', 'lif', 'continu', 'wond', 'what', 'best', 'way', 'go', 'careset', 'trust', 'fund', 'try', 'protect', 'asset', 'ask', 'avoid', 'pay', 'anyth', 'mak', 'govern', 'pay'], ['im', 'sorry', 'thats', 'come', 'across', 'try', 'figure', 'grandparents', 'prevent', 'lose', 'everything', 'pay', 'care', 'wealthy', 'pay', 'tax', 'life', 'continue', 'wonder', 'whats', 'best', 'way', 'go', 'careset', 'trust', 'fund', 'try', 'protect', 'assets', 'ask', 'avoid', 'pay', 'anything', 'make', 'government', 'pay'])\n",
      "original document: \n",
      "['Goddamn.', '', 'The', 'front', 'fell', 'off.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goddamn', 'front', 'fel'], ['goddamn', 'front', 'fell'])\n",
      "original document: \n",
      "['will', 'fix', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fix'], ['fix'])\n",
      "original document: \n",
      "['*Cho', 'went', 'over', 'the', 'injuries.', 'The', 'knife', 'wounds', 'were', 'easy', 'enough', 'to', 'fix,', 'and', 'luckily', 'it', 'seemed', 'the', 'blast', 'had', 'missed', 'anything', 'important.', 'An', 'hour', 'later', 'she', 'was', 'ready', 'to', 'wake', 'Kin', 'up.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cho', 'went', 'injury', 'knif', 'wound', 'easy', 'enough', 'fix', 'lucky', 'seem', 'blast', 'miss', 'anyth', 'import', 'hour', 'lat', 'ready', 'wak', 'kin'], ['cho', 'go', 'injuries', 'knife', 'wound', 'easy', 'enough', 'fix', 'luckily', 'seem', 'blast', 'miss', 'anything', 'important', 'hour', 'later', 'ready', 'wake', 'kin'])\n",
      "original document: \n",
      "['Roto/8Cat/10Teams', '\\nMason', 'Plumlee', 'or', 'Nikola', 'Mirotic?', '\\n\\nThoughts', 'now', 'that', 'Wade', 'left', 'the', 'bulls?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['roto8cat10teams', '\\nmason', 'pluml', 'nikol', 'mirot', '\\n\\nthoughts', 'wad', 'left', 'bul'], ['roto8cat10teams', '\\nmason', 'plumlee', 'nikola', 'mirotic', '\\n\\nthoughts', 'wade', 'leave', 'bull'])\n",
      "original document: \n",
      "['I', 'member']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['memb'], ['member'])\n",
      "original document: \n",
      "['Yeah', 'I', 'believe', 'so']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'believ'], ['yeah', 'believe'])\n",
      "original document: \n",
      "['https://twitter.com/xxbufsiz/status/913022619900796928']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpstwittercomxxbufsizstatus913022619900796928'], ['httpstwittercomxxbufsizstatus913022619900796928'])\n",
      "original document: \n",
      "['Totally', 'agreed.', \"He's\", 'looking', 'a', 'little', 'stiff', 'in', 'this', 'colder', 'weather.', 'We', \"don't\", 'need', 'another', 'injury.', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized and lemmatized document: \n",
      "(['tot', 'agree', 'hes', 'look', 'littl', 'stiff', 'cold', 'weath', 'dont', 'nee', 'anoth', 'injury'], ['totally', 'agree', 'hes', 'look', 'little', 'stiff', 'colder', 'weather', 'dont', 'need', 'another', 'injury'])\n",
      "original document: \n",
      "['I', 'mean,', 'it', 'is', 'a', 'beta', 'so', 'anything', 'can', 'happen,', 'but', 'it', 'worked', 'for', 'me..', 'I', 'know', 'this', 'is', 'bad', 'advice,', 'but', 'I', \"don't\", 'think', 'a', 'lot', 'of', 'bad', 'things', 'can', 'really', 'happen', 'unless', \"you're\", 'really', 'unlucky.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'bet', 'anyth', 'hap', 'work', 'know', 'bad', 'adv', 'dont', 'think', 'lot', 'bad', 'thing', 'real', 'hap', 'unless', 'yo', 'real', 'unlucky'], ['mean', 'beta', 'anything', 'happen', 'work', 'know', 'bad', 'advice', 'dont', 'think', 'lot', 'bad', 'things', 'really', 'happen', 'unless', 'youre', 'really', 'unlucky'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['143413618|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413486\\n&gt;2016==&gt;', 'Trump', '(and', 'loving', 'the', 'shitstorm)\\n\\n#justdeplorablethings\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, six hundred and eighteen', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413486\\ngt2016gt', 'trump', 'lov', 'shitstorm\\n\\njustdeplorablethings\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, six hundred and eighteen', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413486\\ngt2016gt', 'trump', 'love', 'shitstorm\\n\\njustdeplorablethings\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"It's\", 'supposed', 'to', 'mimic', 'this', 'https://cdn.inquisitr.com/wp-content/uploads/2017/05/KKK-Torches-Charlottesville.jpg\\n\\nAdditionally,', \"there's\", 'not', 'a', 'lot', 'of', 'ambiguity', 'around', 'chanting', '\"Blood', 'and', 'soil\"', 'along', 'with', 'other', 'Nazi', 'slogans.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suppos', 'mim', 'httpscdninquisitrcomwpcontentuploads201705kkktorchescharlottesvillejpg\\n\\nadditionally', 'ther', 'lot', 'ambigu', 'around', 'chant', 'blood', 'soil', 'along', 'naz', 'slog'], ['suppose', 'mimic', 'httpscdninquisitrcomwpcontentuploads201705kkktorchescharlottesvillejpg\\n\\nadditionally', 'theres', 'lot', 'ambiguity', 'around', 'chant', 'blood', 'soil', 'along', 'nazi', 'slogans'])\n",
      "original document: \n",
      "[\"That's\", 'something', 'I', 'considered', '-', 'he', 'was', 'in', 'his', '70s', 'and', \"didn't\", 'have', 'many', 'reviews', '(though', 'the', '2', 'he', 'had', 'were', 'favorable).', '\\n\\nBut', 'I', 'mean...you', \"don't\", 'pay', 'Motel', '6', 'price', 'and', 'expect', 'the', 'Hilton.', 'Haha.', '\\n\\nETA:', 'Of', 'course,', 'I', \"wouldn't\", 'mind', 'honest', 'and', 'in', 'depth', 'reviews', 'if', 'the', 'AirBnb', 'algorithms', \"didn't\", 'fuck', 'you', 'over', 'for', 'having', 'anything', 'less', 'that', 'a', 'perfect', '5.0.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'someth', 'consid', '70s', 'didnt', 'many', 'review', 'though', 'two', 'fav', '\\n\\nbut', 'meanyou', 'dont', 'pay', 'motel', 'six', 'pric', 'expect', 'hilton', 'hah', '\\n\\neta', 'cours', 'wouldnt', 'mind', 'honest', 'dep', 'review', 'airbnb', 'algorithm', 'didnt', 'fuck', 'anyth', 'less', 'perfect', 'fifty'], ['thats', 'something', 'consider', '70s', 'didnt', 'many', 'review', 'though', 'two', 'favorable', '\\n\\nbut', 'meanyou', 'dont', 'pay', 'motel', 'six', 'price', 'expect', 'hilton', 'haha', '\\n\\neta', 'course', 'wouldnt', 'mind', 'honest', 'depth', 'review', 'airbnb', 'algorithms', 'didnt', 'fuck', 'anything', 'less', 'perfect', 'fifty'])\n",
      "original document: \n",
      "['Buttery', 'males']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['buttery', 'mal'], ['buttery', 'males'])\n",
      "original document: \n",
      "['LOL']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "[\"That's\", 'the', 'pot', 'calling', 'the', 'kettle', 'black.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'pot', 'cal', 'kettl', 'black'], ['thats', 'pot', 'call', 'kettle', 'black'])\n",
      "original document: \n",
      "['Nice', '10k', 'pc', 'you', 'got', 'there....', 'cant', 'even', 'produce', 'its', 'own', 'power']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', '10k', 'pc', 'got', 'cant', 'ev', 'produc', 'pow'], ['nice', '10k', 'pc', 'get', 'cant', 'even', 'produce', 'power'])\n",
      "original document: \n",
      "['I', 'was', 'you,', 'then', 'after', 'getting', 'hammered', 'one', 'too', 'many', 'times', '4v3', 'or', '4v2', 'and', 'watching', 'my', 'stats', 'take', 'a', 'big', 'dip', 'I', 'now', 'leave', 'as', 'soon', 'as', 'someone', 'drops']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'ham', 'on', 'many', 'tim', '4v3', '4v2', 'watch', 'stat', 'tak', 'big', 'dip', 'leav', 'soon', 'someon', 'drop'], ['get', 'hammer', 'one', 'many', 'time', '4v3', '4v2', 'watch', 'stats', 'take', 'big', 'dip', 'leave', 'soon', 'someone', 'drop'])\n",
      "original document: \n",
      "['I', 'only', 'like', 'the', 'Rockies', 'when', 'they', 'wear', 'purple']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'rocky', 'wear', 'purpl'], ['like', 'rockies', 'wear', 'purple'])\n",
      "original document: \n",
      "['What’s', 'brown', 'and', 'sticky?\\n\\nA', 'stick.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'brown', 'sticky\\n\\na', 'stick'], ['whats', 'brown', 'sticky\\n\\na', 'stick'])\n",
      "original document: \n",
      "['Shaven', 'Haven', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shav'], ['shave'])\n",
      "original document: \n",
      "['It', 'is', 'racist.', 'Liberals', 'are', 'too', 'dumb', 'to', 'understand.', 'She', 'doesnt', 'want', 'to', 'date', 'a', 'white', 'male', 'because', 'of', 'the', 'white', 'male', 'patriarchy', 'she', 'claims', 'exists.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rac', 'lib', 'dumb', 'understand', 'doesnt', 'want', 'dat', 'whit', 'mal', 'whit', 'mal', 'patriarchy', 'claim', 'ex'], ['racist', 'liberals', 'dumb', 'understand', 'doesnt', 'want', 'date', 'white', 'male', 'white', 'male', 'patriarchy', 'claim', 'exist'])\n",
      "original document: \n",
      "['Performance', 'is', 'balanced,', 'you', 'can', 'take', 'any', 'car', 'and', 'succeeed', 'probably.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['perform', 'bal', 'tak', 'car', 'succeee', 'prob'], ['performance', 'balance', 'take', 'car', 'succeeed', 'probably'])\n",
      "original document: \n",
      "['Man,', 'just', 'wait', 'for', 'them', 'to', 'play', 'and', 'show', 'themselves,', 'same', 'for', 'every', 'team.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['man', 'wait', 'play', 'show', 'every', 'team'], ['man', 'wait', 'play', 'show', 'every', 'team'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['What', 'am', 'I', 'supposed', 'to', 'watch', 'now?', ':(', 'someone', 'please', 'suggest', 'something.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suppos', 'watch', 'someon', 'pleas', 'suggest', 'someth'], ['suppose', 'watch', 'someone', 'please', 'suggest', 'something'])\n",
      "original document: \n",
      "['You’re', '19,', 'calm', 'the', 'fuck', 'down.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'nineteen', 'calm', 'fuck'], ['youre', 'nineteen', 'calm', 'fuck'])\n",
      "original document: \n",
      "['Yeah', 'pretty', 'much.', 'Not', 'proud', 'of', 'my', 'reaction', 'but', 'the', 'sheer', 'effort', 'put', 'into', 'telling', 'me', 'to', 'fuck', 'off', 'was', 'crazy.\\n\\nNote:', 'this', 'was', 'my', 'ex-girlfriend', 'from', 'when', 'I', 'was', 'in', 'my', 'early', '20s,', 'not', 'my', 'ex-wife', '-', 'she', 'certainly', 'made', 'mistakes', 'but', 'nothing', 'like', '*that*.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'pretty', 'much', 'proud', 'react', 'she', 'effort', 'put', 'tel', 'fuck', 'crazy\\n\\nnote', 'exgirlfriend', 'ear', '20s', 'exw', 'certain', 'mad', 'mistak', 'noth', 'lik'], ['yeah', 'pretty', 'much', 'proud', 'reaction', 'sheer', 'effort', 'put', 'tell', 'fuck', 'crazy\\n\\nnote', 'exgirlfriend', 'early', '20s', 'exwife', 'certainly', 'make', 'mistake', 'nothing', 'like'])\n",
      "original document: \n",
      "['10', 'Team,', '1', 'PPR,\\n\\n-', 'I', 'have', 'a', 'very', 'young', 'team,', 'I', 'am', 'also', 'not', 'competitive', 'this', 'year.', '(Almost', 'guaranteed', 'to', 'start', '0-4', 'after', 'my', 'opponents', 'Rodgers/Nelson', 'stack', 'on', 'Thursday).\\n\\n-', '**Trade:**', 'Travis', 'Kelce,', 'Rawls,', '2019', '2nd', '(not', 'mine,', 'probably', 'a', 'mid)', '\\n\\n-', '**Receive:**', 'OJ', 'Howard,', 'David', 'Njoku,', '2018', 'First', '(Likely', 'mid).\\n\\n&amp;nbsp;\\n\\n-', 'Rawls', 'is', 'just', 'being', 'throw', 'in', 'since', 'I', 'would', 'drop', 'him', 'anyway.', \"It's\", 'either', 'him', 'or', 'Clement', 'and', \"I'm\", 'probably', 'dropping', 'Clement', 'for', 'Marlon', 'Mack.\\n\\n-', 'My', \"TE's\", 'after', 'this', 'trade', 'would', 'be:', 'Njoku,', 'Jonnu', 'Smith,', 'and', 'OJ', 'Howard\\n\\n-', 'It', 'would', 'also', 'be', 'my', '**fifth**', '2018', 'first', 'round', 'pick.', '(Mine', 'which', 'will', 'be', '1.01', 'or', '1.02', 'at', 'the', 'worst,', 'two', 'probable', 'mids,', 'and', 'two', 'probable', 'late).\\n\\n&amp;nbsp;\\n\\n-', 'I', 'realize', 'Kelce', 'is', 'a', 'great', 'asset.', 'I', 'do', 'have', 'Mahomes,', 'which', 'would', 'make', 'for', 'a', 'great', 'stack', 'in', 'the', 'future.', 'But', 'Kelce', 'will', 'probably', 'be', 'at', 'the', 'end', 'of', 'his', 'prime', 'by', 'the', 'time', 'I', 'am', 'truly', 'competitive.', 'Kelce', 'is', 'my', 'second', 'oldest', 'player.', '(Luck', 'is', 'my', 'oldest).', 'Kelce', 'has', 'also', 'only', 'had', '1', 'elite', 'year', 'as', 'a', 'TE', 'in', 'fantasy.', '2014/2015', 'were', 'decent,', 'but', 'nothing', 'like', 'last', 'year.', 'They', 'also', \"didn't\", 'have', 'Kareem', 'Hunt', 'last', 'year', 'and', 'Hill', 'was', 'a', '\"gadget\"', 'rookie.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'team', 'on', 'ppr\\n\\n', 'young', 'team', 'also', 'competit', 'year', 'almost', 'guarantee', 'start', 'four', 'oppon', 'rodgersnelson', 'stack', 'thursday\\n\\n', 'trad', 'trav', 'kelc', 'rawl', 'two thousand and nineteen', '2nd', 'min', 'prob', 'mid', '\\n\\n', 'receiv', 'oj', 'howard', 'david', 'njoku', 'two thousand and eighteen', 'first', 'lik', 'mid\\n\\nampnbsp\\n\\n', 'rawl', 'throw', 'sint', 'would', 'drop', 'anyway', 'eith', 'cle', 'im', 'prob', 'drop', 'cle', 'marlon', 'mack\\n\\n', 'tes', 'trad', 'would', 'njoku', 'jonnu', 'smi', 'oj', 'howard\\n\\n', 'would', 'also', 'fif', 'two thousand and eighteen', 'first', 'round', 'pick', 'min', 'one hundred and on', 'one hundred and two', 'worst', 'two', 'prob', 'mid', 'two', 'prob', 'late\\n\\nampnbsp\\n\\n', 'real', 'kelc', 'gre', 'asset', 'mahom', 'would', 'mak', 'gre', 'stack', 'fut', 'kelc', 'prob', 'end', 'prim', 'tim', 'tru', 'competit', 'kelc', 'second', 'oldest', 'play', 'luck', 'oldest', 'kelc', 'also', 'on', 'elit', 'year', 'te', 'fantasy', 'twenty million, one hundred and forty-two thousand and fifteen', 'dec', 'noth', 'lik', 'last', 'year', 'also', 'didnt', 'kareem', 'hunt', 'last', 'year', 'hil', 'gadget', 'rooky'], ['ten', 'team', 'one', 'ppr\\n\\n', 'young', 'team', 'also', 'competitive', 'year', 'almost', 'guarantee', 'start', 'four', 'opponents', 'rodgersnelson', 'stack', 'thursday\\n\\n', 'trade', 'travis', 'kelce', 'rawls', 'two thousand and nineteen', '2nd', 'mine', 'probably', 'mid', '\\n\\n', 'receive', 'oj', 'howard', 'david', 'njoku', 'two thousand and eighteen', 'first', 'likely', 'mid\\n\\nampnbsp\\n\\n', 'rawls', 'throw', 'since', 'would', 'drop', 'anyway', 'either', 'clement', 'im', 'probably', 'drop', 'clement', 'marlon', 'mack\\n\\n', 'tes', 'trade', 'would', 'njoku', 'jonnu', 'smith', 'oj', 'howard\\n\\n', 'would', 'also', 'fifth', 'two thousand and eighteen', 'first', 'round', 'pick', 'mine', 'one hundred and one', 'one hundred and two', 'worst', 'two', 'probable', 'mids', 'two', 'probable', 'late\\n\\nampnbsp\\n\\n', 'realize', 'kelce', 'great', 'asset', 'mahomes', 'would', 'make', 'great', 'stack', 'future', 'kelce', 'probably', 'end', 'prime', 'time', 'truly', 'competitive', 'kelce', 'second', 'oldest', 'player', 'luck', 'oldest', 'kelce', 'also', 'one', 'elite', 'year', 'te', 'fantasy', 'twenty million, one hundred and forty-two thousand and fifteen', 'decent', 'nothing', 'like', 'last', 'year', 'also', 'didnt', 'kareem', 'hunt', 'last', 'year', 'hill', 'gadget', 'rookie'])\n",
      "original document: \n",
      "['A', 'lot', 'of', 'these', 'guys', \"shouldn't\", 'be', 'calling', 'out', 'Tony', 'but', 'the', 'guys', 'Tony', 'should', 'be', 'fighting', 'are', 'AFK...', 'Conor,', 'Nate,', 'Khabib,', 'Eddie,', 'etc.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lot', 'guy', 'shouldnt', 'cal', 'tony', 'guy', 'tony', 'fight', 'afk', 'con', 'nat', 'khabib', 'eddy', 'etc'], ['lot', 'guy', 'shouldnt', 'call', 'tony', 'guy', 'tony', 'fight', 'afk', 'conor', 'nate', 'khabib', 'eddie', 'etc'])\n",
      "original document: \n",
      "[\"it's\", 'already', 'open.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'op'], ['already', 'open'])\n",
      "original document: \n",
      "['Bent', 'over', 'I', 'see.', 'Pin', 'you', 'against', 'that', 'wall.', 'Squeeze', 'them', 'tit', 'from', 'behind.', 'Slide', 'one', 'hand', 'down', 'and', 'rub', 'you', 'till', 'you', 'start', 'getting', 'wet.', 'Slide', 'the', 'panty', 'to', 'the', 'side', 'and', 'push', 'myself', 'in.', 'Just', 'the', 'beginning']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bent', 'see', 'pin', 'wal', 'squeez', 'tit', 'behind', 'slid', 'on', 'hand', 'rub', 'til', 'start', 'get', 'wet', 'slid', 'panty', 'sid', 'push', 'begin'], ['bend', 'see', 'pin', 'wall', 'squeeze', 'tit', 'behind', 'slide', 'one', 'hand', 'rub', 'till', 'start', 'get', 'wet', 'slide', 'panty', 'side', 'push', 'begin'])\n",
      "original document: \n",
      "['&gt;**I', \"can't\", 'help', 'but', 'feel', 'like', 'the', 'principle', 'is', 'inspired', 'by', 'him', 'in', 'some', 'way**\\n\\nYeah', 'I', 'can', 'read.', 'Apparently', 'you', \"can't.\", \"Don't\", 'worry,', \"it's\", 'ok', 'that', 'you', 'are', 'a', 'little', 'bit', 'behind', 'for', 'your', 'age.', \"I'm\", 'sure', \"you'll\", 'catch', 'up', 'one', 'day.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gti', 'cant', 'help', 'feel', 'lik', 'principl', 'inspir', 'way\\n\\nyeah', 'read', 'app', 'cant', 'dont', 'worry', 'ok', 'littl', 'bit', 'behind', 'ag', 'im', 'sur', 'youl', 'catch', 'on', 'day'], ['gti', 'cant', 'help', 'feel', 'like', 'principle', 'inspire', 'way\\n\\nyeah', 'read', 'apparently', 'cant', 'dont', 'worry', 'ok', 'little', 'bite', 'behind', 'age', 'im', 'sure', 'youll', 'catch', 'one', 'day'])\n",
      "original document: \n",
      "['Power', 'cords', 'across', 'the', 'floor,', 'cushioned', 'chair,', 'feet', 'up...', \"couldn't\", 'be', 'at', 'a', 'salon...', \"couldn't\", 'be.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pow', 'cord', 'across', 'flo', 'cush', 'chair', 'feet', 'couldnt', 'salon', 'couldnt'], ['power', 'cord', 'across', 'floor', 'cushion', 'chair', 'feet', 'couldnt', 'salon', 'couldnt'])\n",
      "original document: \n",
      "['Ill', 'give', 'a', 'try', 'to', 'Moda', 'and', 'phenyl', 'soon,', 'Ive', 'been', 'recommended', 'for', 'these.', 'Others', 'can', 'be', 'Adrafinil', 'or', 'Theacrine,', 'but', 'got', 'no', 'experience', 'with', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'giv', 'try', 'mod', 'phenyl', 'soon', 'iv', 'recommend', 'oth', 'adrafinil', 'theacrin', 'got', 'expery'], ['ill', 'give', 'try', 'moda', 'phenyl', 'soon', 'ive', 'recommend', 'others', 'adrafinil', 'theacrine', 'get', 'experience'])\n",
      "original document: \n",
      "['Alwady.\\n\\n***\\n\\n^(Bleep-bloop,', \"I'm\", 'a', 'bot.', 'This', ')^[portmanteau](https://en.wikipedia.org/wiki/Portmanteau)', '^(', 'was', 'created', 'from', 'the', 'phrase', \"'Always\", \"ready!'.)\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alwady\\n\\n\\n\\nbleepbloop', 'im', 'bot', 'portmanteauhttpsenwikipediaorgwikiportmanteau', 'cre', 'phrase', 'alway', 'ready'], ['alwady\\n\\n\\n\\nbleepbloop', 'im', 'bot', 'portmanteauhttpsenwikipediaorgwikiportmanteau', 'create', 'phrase', 'always', 'ready'])\n",
      "original document: \n",
      "['Were', 'you', 'winning?', 'Could', 'make', 'sense', 'then', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['win', 'could', 'mak', 'sens'], ['win', 'could', 'make', 'sense'])\n",
      "original document: \n",
      "['Still', 'close', 'to', 'a', 'friend', 'I', 'made', 'in', '5th', 'grade,', 'but', 'she', 'lives', 'in', 'another', 'state.', 'Still', 'closest', 'to', 'someone', 'I', 'have', 'known', 'for', '30', 'years,', 'but', 'I', 'moved', 'to', 'another', 'state.', 'Drove', 'back', 'when', 'she', 'was', 'in', 'an', 'accident', 'and', 'stayed', 'with', 'her', 'for', '4', 'weeks', 'till', 'she', 'was', 'better.', 'Locally,', 'some', 'one', 'I', 'have', 'known', 'for', 'seven', 'years', 'is', 'very', 'close', 'to', 'me.', 'A', 'neighbor', 'I', 'have', 'known', 'for', 'three', 'years.', 'Not', 'sure', 'which', 'one', 'of', 'these', 'woman', 'are', 'my', \"'best'\", 'friends,', 'each', 'of', 'them', 'had', 'their', 'strong', 'points.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'clos', 'friend', 'mad', '5th', 'grad', 'liv', 'anoth', 'stat', 'stil', 'closest', 'someon', 'known', 'thirty', 'year', 'mov', 'anoth', 'stat', 'drov', 'back', 'accid', 'stay', 'four', 'week', 'til', 'bet', 'loc', 'on', 'known', 'sev', 'year', 'clos', 'neighb', 'known', 'three', 'year', 'sur', 'on', 'wom', 'best', 'friend', 'strong', 'point'], ['still', 'close', 'friend', 'make', '5th', 'grade', 'live', 'another', 'state', 'still', 'closest', 'someone', 'know', 'thirty', 'years', 'move', 'another', 'state', 'drive', 'back', 'accident', 'stay', 'four', 'weeks', 'till', 'better', 'locally', 'one', 'know', 'seven', 'years', 'close', 'neighbor', 'know', 'three', 'years', 'sure', 'one', 'woman', 'best', 'friends', 'strong', 'point'])\n",
      "original document: \n",
      "['$100', 'says', 'none', 'of', 'them', 'have', 'any', 'strong', 'male', 'characters', 'in', 'their', 'lives.', 'I', 'smell', \"'daddy\", \"issues'\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred', 'say', 'non', 'strong', 'mal', 'charact', 'liv', 'smel', 'daddy', 'issu'], ['one hundred', 'say', 'none', 'strong', 'male', 'character', 'live', 'smell', 'daddy', 'issue'])\n",
      "original document: \n",
      "['sold', 'google', 'pixel', 'to', '/u/yarudl']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sold', 'googl', 'pixel', 'uyarudl'], ['sell', 'google', 'pixel', 'uyarudl'])\n",
      "original document: \n",
      "['What', 'type', 'of', 'equipment', 'here?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['typ', 'equip'], ['type', 'equipment'])\n",
      "original document: \n",
      "['/u/amazingpikachu_qw']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uamazingpikachu_qw'], ['uamazingpikachu_qw'])\n",
      "original document: \n",
      "['&gt;', 'East', 'Tea', 'Can\\nInteresting,', 'I', 'shop', 'at', 'the', 'nearby', 'Longos', 'and', 'just', 'noticed', 'this', 'place.', 'Will', 'definitely', 'try', 'it', '-', 'the', 'menu', 'looks', 'great!\\nETA', '-', 'Thank', 'you!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'east', 'tea', 'can\\ninteresting', 'shop', 'nearby', 'longo', 'not', 'plac', 'definit', 'try', 'menu', 'look', 'great\\neta', 'thank'], ['gt', 'east', 'tea', 'can\\ninteresting', 'shop', 'nearby', 'longos', 'notice', 'place', 'definitely', 'try', 'menu', 'look', 'great\\neta', 'thank'])\n",
      "original document: \n",
      "['Is', 'that', 'a', 'view', 'of', 'philly?', 'Looks', 'like', 'one', 'of', 'the', 'Liberty', 'Towers.', 'I', 'love', 'the', 'monitors', 'too.', 'I’d', 'love', 'that', 'setup', 'boss.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['view', 'phil', 'look', 'lik', 'on', 'liberty', 'tow', 'lov', 'monit', 'id', 'lov', 'setup', 'boss'], ['view', 'philly', 'look', 'like', 'one', 'liberty', 'tower', 'love', 'monitor', 'id', 'love', 'setup', 'boss'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Might', 'you', 'be', 'able', 'to', 'create', 'a', 'zip', 'file', 'with', 'those?', '', 'This', 'is', 'a', 'good', 'strategy!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['might', 'abl', 'cre', 'zip', 'fil', 'good', 'strategy'], ['might', 'able', 'create', 'zip', 'file', 'good', 'strategy'])\n",
      "original document: \n",
      "['Yes.', 'As', 'long', 'as', 'you', 'have', 'a', 'key', 'you', 'can', 'open', 'the', 'chest.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'long', 'key', 'op', 'chest'], ['yes', 'long', 'key', 'open', 'chest'])\n",
      "original document: \n",
      "['I', 'found', 'you', 'can', 'say', '\"Hey', 'Google,', 'sleep\".', 'Then', 'the', 'SHIELD', 'sleeps.', 'You', 'can', 'wake', 'it', 'up', 'if', 'you', 'have', 'enabled', 'always', 'listening.', 'Then', 'you', 'can', 'say', '\"Hey', 'Google,', 'launch', 'YouTube\".', 'The', 'SHIELD', 'will', 'turn', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['found', 'say', 'hey', 'googl', 'sleep', 'shield', 'sleep', 'wak', 'en', 'alway', 'list', 'say', 'hey', 'googl', 'launch', 'youtub', 'shield', 'turn'], ['find', 'say', 'hey', 'google', 'sleep', 'shield', 'sleep', 'wake', 'enable', 'always', 'listen', 'say', 'hey', 'google', 'launch', 'youtube', 'shield', 'turn'])\n",
      "original document: \n",
      "['Tbh,', 'with', 'a', 'quality', 'loss', 'in', 'Pullman,', 'I', 'think', 'he', 'has', 'it', 'locked', 'up.', '', 'The', 'dude', 'just', 'comes', 'alive', 'in', 'the', 'final', '2', 'minutes', 'of', 'a', 'game.', '', 'Seriously.', '', 'The', 'dude', 'is', 'money', 'for', '2', 'minutes', 'out', 'of', '60', 'in', '80%', 'of', 'games', 'this', 'year,', 'how', 'do', 'you', 'deny', 'him', 'the', 'Heisman?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tbh', 'qual', 'loss', 'pullm', 'think', 'lock', 'dud', 'com', 'al', 'fin', 'two', 'minut', 'gam', 'sery', 'dud', 'money', 'two', 'minut', 'sixty', 'eighty', 'gam', 'year', 'deny', 'heism'], ['tbh', 'quality', 'loss', 'pullman', 'think', 'lock', 'dude', 'come', 'alive', 'final', 'two', 'minutes', 'game', 'seriously', 'dude', 'money', 'two', 'minutes', 'sixty', 'eighty', 'game', 'year', 'deny', 'heisman'])\n",
      "original document: \n",
      "['oh', 'fuck', 'i', \"didn't\", 'realise', 'that', 'was', 'laura', 'palmer', 'and', 'thought', 'it', 'was', 'a', 'mirrored', 'steel', 'toaster', 'reflecting', \"OP's\", 'smile']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'fuck', 'didnt', 'real', 'laur', 'palm', 'thought', 'mir', 'steel', 'toast', 'reflect', 'op', 'smil'], ['oh', 'fuck', 'didnt', 'realise', 'laura', 'palmer', 'think', 'mirror', 'steel', 'toaster', 'reflect', 'ops', 'smile'])\n",
      "original document: \n",
      "['I', 'think', \"you've\", 'got', 'NV', 'turned', 'on', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'youv', 'got', 'nv', 'turn'], ['think', 'youve', 'get', 'nv', 'turn'])\n",
      "original document: \n",
      "['The', 'word', '“dragon”', 'comes', 'from', 'the', 'Greek', 'word', '“draconta,”', 'which', 'means', '“to', 'watch.”', 'The', 'Greeks', 'saw', 'dragons', 'as', 'beasts', 'that', 'guarded', 'valuable', 'items.', 'In', 'fact,', 'many', 'cultures', 'depict', 'dragons', 'as', 'hoarding', 'treasure.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['word', 'dragon', 'com', 'greek', 'word', 'dracont', 'mean', 'watch', 'greek', 'saw', 'dragon', 'beast', 'guard', 'valu', 'item', 'fact', 'many', 'cult', 'depict', 'dragon', 'hoard', 'treas'], ['word', 'dragon', 'come', 'greek', 'word', 'draconta', 'mean', 'watch', 'greeks', 'saw', 'dragons', 'beasts', 'guard', 'valuable', 'items', 'fact', 'many', 'culture', 'depict', 'dragons', 'hoard', 'treasure'])\n",
      "original document: \n",
      "['Have', '2,', '299', 'titan,', '302', 'warlock', 'GT:', 'Slenderstalker1']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'two hundred and ninety-nine', 'tit', 'three hundred and two', 'warlock', 'gt', 'slenderstalker1'], ['two', 'two hundred and ninety-nine', 'titan', 'three hundred and two', 'warlock', 'gt', 'slenderstalker1'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/KarmaCourt/comments/70ooah/the_people_of_numerous_subs_vs_unotkennyloggins/?st=J85RS0J8&amp;sh=cd30a395']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrkarmacourtcomments70ooahthe_people_of_numerous_subs_vs_unotkennylogginsstj85rs0j8ampshcd30a395'], ['httpswwwredditcomrkarmacourtcomments70ooahthe_people_of_numerous_subs_vs_unotkennylogginsstj85rs0j8ampshcd30a395'])\n",
      "original document: \n",
      "['Phone', 'screens', 'still', 'do', 'scratch.', 'Glass', 'has', 'a', '5.5', 'mohs', 'rating', 'and', 'is', 'made', 'from', 'sand.', 'Sand', 'can', 'and', 'will', 'scratch', 'your', 'screen.', \"I've\", 'seen', 'plenty', 'of', 'screens', 'with', 'scratched', 'edges', 'and', 'fronts,', 'even', 'if', 'the', 'phones', 'have', 'cases.', \"It's\", 'impossible', 'to', 'avoid', 'scratches', 'without', 'a', 'screen', 'protector.\\n\\nThe', 'reason', 'you', 'listed', \"isn't\", 'a', 'reason.', 'Screen', 'protectors', 'are', 'super', 'easy', 'to', 'replace', 'and', 'they', 'protect', 'your', 'screen.', 'I', 'always', 'have', 'and', 'always', 'will', 'use', 'a', 'screen', 'protector.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['phon', 'screens', 'stil', 'scratch', 'glass', 'fifty-five', 'moh', 'rat', 'mad', 'sand', 'sand', 'scratch', 'screen', 'iv', 'seen', 'plenty', 'screens', 'scratched', 'edg', 'front', 'ev', 'phon', 'cas', 'imposs', 'avoid', 'scratches', 'without', 'screen', 'protector\\n\\nthe', 'reason', 'list', 'isnt', 'reason', 'screen', 'protect', 'sup', 'easy', 'replac', 'protect', 'screen', 'alway', 'alway', 'us', 'screen', 'protect'], ['phone', 'screen', 'still', 'scratch', 'glass', 'fifty-five', 'mohs', 'rat', 'make', 'sand', 'sand', 'scratch', 'screen', 'ive', 'see', 'plenty', 'screen', 'scratch', 'edge', 'front', 'even', 'phone', 'case', 'impossible', 'avoid', 'scratch', 'without', 'screen', 'protector\\n\\nthe', 'reason', 'list', 'isnt', 'reason', 'screen', 'protectors', 'super', 'easy', 'replace', 'protect', 'screen', 'always', 'always', 'use', 'screen', 'protector'])\n",
      "original document: \n",
      "['The', 'room', 'moves', 'when', 'I', 'close', 'my', 'eyes', 'and', 'makes', 'me', 'dizzy']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['room', 'mov', 'clos', 'ey', 'mak', 'dizzy'], ['room', 'move', 'close', 'eye', 'make', 'dizzy'])\n",
      "original document: \n",
      "['Classic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['class'], ['classic'])\n",
      "original document: \n",
      "['\"Hey', 'man,', \"I'm\", 'looking', 'to', 'pick', 'up', 'within', 'x', 'days.', 'Are', 'you', 'in', 'stock', 'yet', 'or', 'should', 'I', 'hit', 'someone', 'else', 'up?\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'man', 'im', 'look', 'pick', 'within', 'x', 'day', 'stock', 'yet', 'hit', 'someon', 'els'], ['hey', 'man', 'im', 'look', 'pick', 'within', 'x', 'days', 'stock', 'yet', 'hit', 'someone', 'else'])\n",
      "original document: \n",
      "['I', 'know', \"there's\", 'a', 'French', 'vanilla', 'flavoured', 'coffee', 'that', 'is', 'available', 'in', 'fine', 'grind', 'and', 'k-cups', '(not', 'sure', 'if', \"it's\", 'in', 'Tassimo', 'but', 'if', 'anyone', 'knows', 'that', 'please', 'let', 'me', 'know!)', 'that', 'looks', 'pretty', 'similar', 'to', 'the', 'other', 'coffee', 'packages', 'but', 'has', 'a', 'blue', 'box', 'behind', 'the', 'writing', 'where', 'the', 'normal', 'coffee', 'has', 'a', 'red', 'box', 'on', 'it.', 'I', 'can', 'tell', 'you', 'with', 'absolute', 'certainty', 'that', 'that', 'is', 'not', 'what', \"you're\", 'looking', 'for.', 'The', 'French', 'vanilla', 'cappuccino', 'sounds', 'like', 'it', 'should', 'be', 'what', \"you're\", 'looking', 'for,', 'but', 'I', \"haven't\", 'seen', 'it', 'so', 'I', \"wouldn't\", 'be', 'able', 'to', 'say', 'for', 'sure.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'ther', 'french', 'vanill', 'flavo', 'coff', 'avail', 'fin', 'grind', 'kcup', 'sur', 'tassimo', 'anyon', 'know', 'pleas', 'let', 'know', 'look', 'pretty', 'simil', 'coff', 'pack', 'blu', 'box', 'behind', 'writ', 'norm', 'coff', 'red', 'box', 'tel', 'absolv', 'certainty', 'yo', 'look', 'french', 'vanill', 'cappuccino', 'sound', 'lik', 'yo', 'look', 'hav', 'seen', 'wouldnt', 'abl', 'say', 'sur'], ['know', 'theres', 'french', 'vanilla', 'flavour', 'coffee', 'available', 'fine', 'grind', 'kcups', 'sure', 'tassimo', 'anyone', 'know', 'please', 'let', 'know', 'look', 'pretty', 'similar', 'coffee', 'package', 'blue', 'box', 'behind', 'write', 'normal', 'coffee', 'red', 'box', 'tell', 'absolute', 'certainty', 'youre', 'look', 'french', 'vanilla', 'cappuccino', 'sound', 'like', 'youre', 'look', 'havent', 'see', 'wouldnt', 'able', 'say', 'sure'])\n",
      "original document: \n",
      "['4', 'is', 'not', 'more', 'absurd', 'than', '5.', 'It', 'had', 'dark,', 'foreboding', 'Spanish', 'villages,', 'dark', 'lightning,', 'forests,', 'moody', 'castles', 'and', 'dim', 'lighting.', 'Only', 'a', 'segment', 'on', 'the', 'Island', 'towards', 'the', 'last', 'leg', 'of', 'the', 'game', 'does', 'it', 'begin', 'to', 'twist', 'into', 'a', 'more', 'RE5/6', 'action-esque', 'chapter.', '\\n\\n5', 'however,', 'from', 'the', 'getgo,', 'is', 'action', 'packed', 'and', 'horror', 'lite.', 'Crowds', 'of', 'enemies', 'in', 'broad', 'daylight', 'in', 'the', '', 'African', 'heat,', 'followed', 'by', 'silly', 'tentacle', 'bosses', 'like', 'Excella', 'or', 'dragon', 'sea', 'monsters', 'like', 'Irving,', 'huge', 'minigun', 'fights', 'on', 'a', 'boat,', 'plus', 'other', 'things.', 'A', 'co-op', 'partner', 'in', 'the', 'form', 'of', 'Sheva', 'also', 'helps', 'eliminate', 'any', 'pure', 'traces', 'of', 'survival', 'horror.\\n\\nDon’t', 'get', 'me', 'wrong,', 'I', 'like', 'RE4', 'and', 'RE5', '', 'a', 'lot.', 'Both', 'of', 'them', 'are', 'great', 'games,', 'I', 'actually', 'prefer', '5', 'over', '4,', 'but', 'you', 'can’t', 'deny.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['four', 'absurd', 'fiv', 'dark', 'forebod', 'span', 'vil', 'dark', 'lightn', 'forest', 'moody', 'castl', 'dim', 'light', 'seg', 'island', 'toward', 'last', 'leg', 'gam', 'begin', 'twist', 're56', 'actionesqu', 'chapt', '\\n\\n5', 'howev', 'getgo', 'act', 'pack', 'hor', 'lit', 'crowd', 'enemy', 'broad', 'daylight', 'afr', 'heat', 'follow', 'sil', 'tentac', 'boss', 'lik', 'excell', 'dragon', 'sea', 'monst', 'lik', 'irv', 'hug', 'minigun', 'fight', 'boat', 'plu', 'thing', 'coop', 'partn', 'form', 'shev', 'also', 'help', 'elimin', 'pur', 'trac', 'surv', 'horror\\n\\ndont', 'get', 'wrong', 'lik', 're4', 're5', 'lot', 'gre', 'gam', 'act', 'pref', 'fiv', 'four', 'cant', 'deny'], ['four', 'absurd', 'five', 'dark', 'forebode', 'spanish', 'villages', 'dark', 'lightning', 'forest', 'moody', 'castle', 'dim', 'light', 'segment', 'island', 'towards', 'last', 'leg', 'game', 'begin', 'twist', 're56', 'actionesque', 'chapter', '\\n\\n5', 'however', 'getgo', 'action', 'pack', 'horror', 'lite', 'crowd', 'enemies', 'broad', 'daylight', 'african', 'heat', 'follow', 'silly', 'tentacle', 'boss', 'like', 'excella', 'dragon', 'sea', 'monsters', 'like', 'irving', 'huge', 'minigun', 'fight', 'boat', 'plus', 'things', 'coop', 'partner', 'form', 'sheva', 'also', 'help', 'eliminate', 'pure', 'trace', 'survival', 'horror\\n\\ndont', 'get', 'wrong', 'like', 're4', 're5', 'lot', 'great', 'game', 'actually', 'prefer', 'five', 'four', 'cant', 'deny'])\n",
      "original document: \n",
      "['Not', 'really.', 'He', 'may', 'have', 'been', 'a', 'little', 'dismissive', 'of', 'Schrab,', 'but', 'Schrab', 'thrives', 'on', 'playing', 'the', 'heel,', 'so', 'I', \"didn't\", 'feel', 'it', 'was', 'an', 'issue.', 'Dan', 'verbally', 'abusing', 'the', 'audience,', 'however...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'may', 'littl', 'dismit', 'schrab', 'schrab', 'thrives', 'play', 'heel', 'didnt', 'feel', 'issu', 'dan', 'verb', 'abus', 'audy', 'howev'], ['really', 'may', 'little', 'dismissive', 'schrab', 'schrab', 'thrive', 'play', 'heel', 'didnt', 'feel', 'issue', 'dan', 'verbally', 'abuse', 'audience', 'however'])\n",
      "original document: \n",
      "['Do', 'good', 'christians', 'sexually', 'assault', 'girls', 'at', 'Disneyland?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'christians', 'sex', 'assault', 'girl', 'disneyland'], ['good', 'christians', 'sexually', 'assault', 'girls', 'disneyland'])\n",
      "original document: \n",
      "['How', 'about', 'durability?', 'Mac', 'air', 'came', 'in', 'aluminum', 'body.', 'I', 'never', 'dropped', 'it', 'or', 'anything,', 'but', 'even', 'though', 'it', 'had', 'several', 'minor', 'crash', 'on', 'to', 'wall,', 'this', 'laptop', 'just', 'never', 'failed', 'on', 'any', 'thing.', 'No', 'virus,', 'no', 'display', 'failure,', 'no', 'internal', 'problems,', 'nothing.', 'Do', 'you', 'think', 'Aero', '15X', 'can', 'last', 'long', 'without', 'severe', 'problem?\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dur', 'mac', 'air', 'cam', 'alumin', 'body', 'nev', 'drop', 'anyth', 'ev', 'though', 'sev', 'min', 'crash', 'wal', 'laptop', 'nev', 'fail', 'thing', 'vir', 'display', 'fail', 'intern', 'problem', 'noth', 'think', 'aero', '15x', 'last', 'long', 'without', 'sev', 'problem\\n'], ['durability', 'mac', 'air', 'come', 'aluminum', 'body', 'never', 'drop', 'anything', 'even', 'though', 'several', 'minor', 'crash', 'wall', 'laptop', 'never', 'fail', 'thing', 'virus', 'display', 'failure', 'internal', 'problems', 'nothing', 'think', 'aero', '15x', 'last', 'long', 'without', 'severe', 'problem\\n'])\n",
      "original document: \n",
      "['16,186']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sixteen thousand, one hundred and eighty-six'], ['sixteen thousand, one hundred and eighty-six'])\n",
      "original document: \n",
      "['It', 'probably', \"doesn't\", 'do', 'much', 'to', 'assuage', 'your', 'worries', 'about', 'the', 'way', 'that', 'you', 'represent', 'veganism', '(even', 'though', 'I', \"don't\", 'think', 'you', 'need', 'to', 'worry', 'about', 'that),', 'but', 'if', 'someone', 'says', 'something', 'along', 'those', 'lines', 'to', 'you,', 'you', 'can', 'just', 'point', 'out', 'that', 'veganism', \"didn't\", 'give', 'you', 'an', 'autoimmune', 'disease.', 'And', 'by', 'caring', 'about', 'your', 'own', 'health,', 'about', 'animals', 'and', 'about', 'the', 'environment,', 'you', '*are*', 'being', 'a', 'good', 'example.', \"You're\", 'doing', 'what', 'you', 'can', 'with', 'what', 'you', 'have,', 'which', 'is', 'all', 'that', 'can', 'be', 'asked', 'of', 'anyone.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'doesnt', 'much', 'assu', 'worry', 'way', 'repres', 'veg', 'ev', 'though', 'dont', 'think', 'nee', 'worry', 'someon', 'say', 'someth', 'along', 'lin', 'point', 'veg', 'didnt', 'giv', 'autoimmun', 'diseas', 'car', 'heal', 'anim', 'environ', 'good', 'exampl', 'yo', 'ask', 'anyon'], ['probably', 'doesnt', 'much', 'assuage', 'worry', 'way', 'represent', 'veganism', 'even', 'though', 'dont', 'think', 'need', 'worry', 'someone', 'say', 'something', 'along', 'line', 'point', 'veganism', 'didnt', 'give', 'autoimmune', 'disease', 'care', 'health', 'animals', 'environment', 'good', 'example', 'youre', 'ask', 'anyone'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Cool.', 'You', 'should', 'focus', 'on', 'the', 'fact', 'that', 'your', 'dad', 'loves', 'you', 'dispite', 'him', 'being', 'ignorant', 'of', 'modern', 'gender/sexuality', 'expression.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cool', 'foc', 'fact', 'dad', 'lov', 'dispit', 'ign', 'modern', 'gendersex', 'express'], ['cool', 'focus', 'fact', 'dad', 'love', 'dispite', 'ignorant', 'modern', 'gendersexuality', 'expression'])\n",
      "original document: \n",
      "['I', 'agree,', \"it's\", 'just', 'that', 'ethically', 'speaking,', 'this', 'is', 'bs']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'eth', 'speak', 'bs'], ['agree', 'ethically', 'speak', 'bs'])\n",
      "original document: \n",
      "['Omg', 'what', 'a', 'cutie', ':D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['omg', 'cuty'], ['omg', 'cutie'])\n",
      "original document: \n",
      "['Maybe', \"you'd\", 'be', 'more', 'artistic', 'if', 'you', \"didn't\", 'spend', 'all', 'your', 'time', 'playing', 'Rocket', 'League.', '/s']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'youd', 'art', 'didnt', 'spend', 'tim', 'play', 'rocket', 'leagu'], ['maybe', 'youd', 'artistic', 'didnt', 'spend', 'time', 'play', 'rocket', 'league'])\n",
      "original document: \n",
      "[\"###[Here's\", 'your', 'Reddit', 'Silver,', 'Grimey_ass_nigga!](http://i.imgur.com/x0jw93q.png', '\"Reddit', 'Silver\")', '\\n***\\n/u/Grimey_ass_nigga', 'has', 'received', 'silver', '1', 'time.', '(given', 'by', '/u/lemmelickurcucumber)', '__[info](http://reddit.com/r/RedditSilverRobot)__']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['her', 'reddit', 'silv', 'grimey_ass_niggahttpiimgurcomx0jw93qpng', 'reddit', 'silv', '\\n\\nugrimey_ass_nigga', 'receiv', 'silv', 'on', 'tim', 'giv', 'ulemmelickurcucumb', '__infohttpredditcomrredditsilverrobot__'], ['heres', 'reddit', 'silver', 'grimey_ass_niggahttpiimgurcomx0jw93qpng', 'reddit', 'silver', '\\n\\nugrimey_ass_nigga', 'receive', 'silver', 'one', 'time', 'give', 'ulemmelickurcucumber', '__infohttpredditcomrredditsilverrobot__'])\n",
      "original document: \n",
      "['And', 'the', '\"mainland\"', 'probably', 'refer', 'to', 'England,', 'Wales,', '&amp;', 'Scotland,', 'but', 'not', 'North', 'Ireland,', 'the', 'Island', 'of', 'Man,', 'or', 'all', 'these', 'other', 'enclaves', 'and', 'islands']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mainland', 'prob', 'ref', 'england', 'wal', 'amp', 'scotland', 'nor', 'ireland', 'island', 'man', 'enclav', 'island'], ['mainland', 'probably', 'refer', 'england', 'wales', 'amp', 'scotland', 'north', 'ireland', 'island', 'man', 'enclaves', 'islands'])\n",
      "original document: \n",
      "['Sweet', 'sweet', 'release']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sweet', 'sweet', 'releas'], ['sweet', 'sweet', 'release'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['\\n[I', 'live', 'near', 'them', '](http://www.achtypistours.gr/sites/default/files/imagecache/galleryformatter_slide/12/08/pyramids_and_sphinx.jpg)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\ni', 'liv', 'near', 'httpwwwachtypistoursgrsitesdefaultfilesimagecachegalleryformatter_slide1208pyramids_and_sphinxjpg'], ['\\ni', 'live', 'near', 'httpwwwachtypistoursgrsitesdefaultfilesimagecachegalleryformatter_slide1208pyramids_and_sphinxjpg'])\n",
      "original document: \n",
      "['They', 'actually', 'do', 'pay', 'federal', 'taxes.', 'Personal', 'income', 'tax', 'is', 'the', 'one', 'major', 'exception.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'pay', 'fed', 'tax', 'person', 'incom', 'tax', 'on', 'maj', 'exception\\n\\n'], ['actually', 'pay', 'federal', 'tax', 'personal', 'income', 'tax', 'one', 'major', 'exception\\n\\n'])\n",
      "original document: \n",
      "[\"I'm\", 'usually', 'cynical', 'as', 'fuck', 'about', 'everything', 'I', 'read,', 'but', 'this', 'just', 'screams', 'corruption.', \"She's\", 'probably', 'getting', '', 'other', 'forms', 'of', '\"aid\"', 'delivered', 'straight', 'into', 'her', 'personal', 'bank', 'account,', 'or', \"she's\", 'just', 'trying', 'to', 'save', 'her', 'own', 'ass', 'by', 'blaming', 'it', 'all', 'on', 'Trump.', 'The', 'man', 'sent', '10k', 'people', 'and', 'flooded', 'their', 'ports', 'with', 'supplies,', 'what', 'the', 'fuck', 'else', 'do', 'you', 'want']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'us', 'cyn', 'fuck', 'everyth', 'read', 'screams', 'corrupt', 'she', 'prob', 'get', 'form', 'aid', 'del', 'straight', 'person', 'bank', 'account', 'she', 'try', 'sav', 'ass', 'blam', 'trump', 'man', 'sent', '10k', 'peopl', 'flood', 'port', 'supply', 'fuck', 'els', 'want'], ['im', 'usually', 'cynical', 'fuck', 'everything', 'read', 'scream', 'corruption', 'shes', 'probably', 'get', 'form', 'aid', 'deliver', 'straight', 'personal', 'bank', 'account', 'shes', 'try', 'save', 'ass', 'blame', 'trump', 'man', 'send', '10k', 'people', 'flood', 'port', 'supply', 'fuck', 'else', 'want'])\n",
      "original document: \n",
      "['Yep', 'I', 'just', \"don't\", 'see', 'all', 'of', 'the', 'hype', 'around', 'Rahm', 'when', 'he', 'really', \"hasn't\", 'done', 'anything', 'of', 'note']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'dont', 'see', 'hyp', 'around', 'rahm', 'real', 'hasnt', 'don', 'anyth', 'not'], ['yep', 'dont', 'see', 'hype', 'around', 'rahm', 'really', 'hasnt', 'do', 'anything', 'note'])\n",
      "original document: \n",
      "['Thats', 'rough']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'rough'], ['thats', 'rough'])\n",
      "original document: \n",
      "['Yes,', 'I', 'was', 'very', 'confused', 'at', 'first,', 'and', 'not', 'really', 'interested', 'in', 'it,', 'but', 'the', 'shades', 'of', 'green', 'were', 'perfect', 'for', 'my', 'room,', 'and', 'Michaels', 'had', 'them', 'on', 'sale,', 'so', 'I', 'figured', \"I'd\", 'give', 'it', 'a', 'shot.', 'I', 'used', 'a', 'P', 'hook,', 'mesh', 'stitch,', 'and', 'it', 'was', 'pretty', 'easy', 'to', 'work', 'with!', \"I'm\", 'pleased', 'with', 'the', 'overall', 'result.', \"It's\", 'cozy,', 'sorta', 'hippie-ish.', '😃', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'confus', 'first', 'real', 'interest', 'shad', 'green', 'perfect', 'room', 'michael', 'sal', 'fig', 'id', 'giv', 'shot', 'us', 'p', 'hook', 'mesh', 'stitch', 'pretty', 'easy', 'work', 'im', 'pleas', 'overal', 'result', 'cozy', 'sort', 'hippy'], ['yes', 'confuse', 'first', 'really', 'interest', 'shade', 'green', 'perfect', 'room', 'michaels', 'sale', 'figure', 'id', 'give', 'shoot', 'use', 'p', 'hook', 'mesh', 'stitch', 'pretty', 'easy', 'work', 'im', 'please', 'overall', 'result', 'cozy', 'sorta', 'hippieish'])\n",
      "original document: \n",
      "['Are', 'you', 'still', 'married?', '', 'Are', 'you', 'still', 'being', 'abused?', '', 'Your', 'post', 'made', 'it', 'sound', 'that', 'way.', '', '\\n\\n&gt;Over', 'the', 'years,', \"he's\", 'convinced', 'them', \"I'm\", 'the', 'abuser', 'and', 'that', 'an', 'example', 'of', 'my', 'abuse', 'is', 'calling', 'the', 'police\\n\\nAbusers', 'often', 'accuse', 'their', 'victims', 'of', 'being', 'abusive.', '', 'Your', 'children', 'are', 'adults.', '', 'At', 'this', 'point', 'in', 'their', 'lives,', 'they', 'should', 'recognize', 'his', 'bullshit', 'for', 'what', 'it', 'is.', '', 'If', 'they', 'really', 'think', 'that', 'you', 'are', 'the', 'problem,', 'then', 'it', 'sounds', 'like', 'they', 'take', 'after', 'their', 'father.', '', \"I'm\", 'sorry', 'to', 'say', 'that.', '', '\\n\\nIs', 'that', 'the', 'reason', 'you', \"don't\", 'want', 'to', 'go', 'against', 'him?', '', 'Because', \"you're\", 'worried', \"you'll\", 'alienate', 'your', 'children?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'marry', 'stil', 'abus', 'post', 'mad', 'sound', 'way', '\\n\\ngtover', 'year', 'hes', 'convint', 'im', 'abus', 'exampl', 'abus', 'cal', 'police\\n\\nabusers', 'oft', 'accus', 'victim', 'abud', 'childr', 'adult', 'point', 'liv', 'recogn', 'bullshit', 'real', 'think', 'problem', 'sound', 'lik', 'tak', 'fath', 'im', 'sorry', 'say', '\\n\\nis', 'reason', 'dont', 'want', 'go', 'yo', 'worry', 'youl', 'aly', 'childr'], ['still', 'marry', 'still', 'abuse', 'post', 'make', 'sound', 'way', '\\n\\ngtover', 'years', 'hes', 'convince', 'im', 'abuser', 'example', 'abuse', 'call', 'police\\n\\nabusers', 'often', 'accuse', 'victims', 'abusive', 'children', 'adults', 'point', 'live', 'recognize', 'bullshit', 'really', 'think', 'problem', 'sound', 'like', 'take', 'father', 'im', 'sorry', 'say', '\\n\\nis', 'reason', 'dont', 'want', 'go', 'youre', 'worry', 'youll', 'alienate', 'children'])\n",
      "original document: \n",
      "['For', 'when', 'they', 'masturbate.', 'Most', 'people', \"don't\", 'want', 'to', 'clean', 'up', 'with', 'there', 'under', 'garments', 'when', 'they', 'still', 'want', 'to', 'use', 'them', 'that', 'day.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['masturb', 'peopl', 'dont', 'want', 'cle', 'gar', 'stil', 'want', 'us', 'day'], ['masturbate', 'people', 'dont', 'want', 'clean', 'garment', 'still', 'want', 'use', 'day'])\n",
      "original document: \n",
      "['If', 'it', \"doesn't\", 'have', 'screen', 'on', 'the', 'lid', 'with', 'which', 'I', 'can', 'stream', 'all', 'my', 'favorite', 'Geek', 'Franchises', 'from', 'beyond', 'the', 'grave,', 'then', 'they', 'can', 'FUCK', 'RIGHT', 'OFF.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'screen', 'lid', 'stream', 'favorit', 'geek', 'franch', 'beyond', 'grav', 'fuck', 'right'], ['doesnt', 'screen', 'lid', 'stream', 'favorite', 'geek', 'franchise', 'beyond', 'grave', 'fuck', 'right'])\n",
      "original document: \n",
      "['One', 'random']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random'], ['one', 'random'])\n",
      "original document: \n",
      "['I', 'love', 'your', 'post!!', '\\n\"It\\'s', 'messed', 'up', 'that', 'alcohol', 'is', 'the', 'problem', 'and', 'it', 'also', 'convinces', 'us', \"it's\", 'the', 'solution.\"\\nTHIS.', 'Sooo', 'true.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'post', '\\nits', 'mess', 'alcohol', 'problem', 'also', 'convint', 'us', 'solution\\nthis', 'sooo', 'tru', '\\n'], ['love', 'post', '\\nits', 'mess', 'alcohol', 'problem', 'also', 'convince', 'us', 'solution\\nthis', 'sooo', 'true', '\\n'])\n",
      "original document: \n",
      "['I', 'use', 'genned', 'Pokémon', 'for', 'Masuda', 'then', 'relates', 'after', 'I', 'get', 'my', 'shiny', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'gen', 'pokemon', 'masud', 'rel', 'get', 'shiny'], ['use', 'genned', 'pokemon', 'masuda', 'relate', 'get', 'shiny'])\n",
      "original document: \n",
      "['I', 'bought', 'the', 'same', 'set.', 'I', 'love', 'them!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bought', 'set', 'lov'], ['buy', 'set', 'love'])\n",
      "original document: \n",
      "['My', 'company', 'matches', 'dollar', 'for', 'dollar', 'up', 'to', '6%', 'of', 'my', 'gross', 'pay.', '', 'They', 'also', 'throw', 'an', 'additional', '3%', 'for', 'free', 'because', \"we're\", 'a', 'non-union', 'site.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['company', 'match', 'doll', 'doll', 'six', 'gross', 'pay', 'also', 'throw', 'addit', 'three', 'fre', 'nonun', 'sit'], ['company', 'match', 'dollar', 'dollar', 'six', 'gross', 'pay', 'also', 'throw', 'additional', 'three', 'free', 'nonunion', 'site'])\n",
      "original document: \n",
      "[\"I'd\", 'say', 'june', 'at', 'the', 'latest', 'without', 'having', 'a', 'real', 'impact']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'say', 'jun', 'latest', 'without', 'real', 'impact'], ['id', 'say', 'june', 'latest', 'without', 'real', 'impact'])\n",
      "original document: \n",
      "['You', 'need', 'to', 'increase', 'the', 'frequency', 'of', 'your', 'DC', 'power.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'increas', 'frequ', 'dc', 'pow'], ['need', 'increase', 'frequency', 'dc', 'power'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['What', 'problems', 'are', 'you', 'speaking', 'of,', 'specifically?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'speak', 'spec'], ['problems', 'speak', 'specifically'])\n",
      "original document: \n",
      "['It', 'is', 'all', 'on', 'Hulu.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hulu'], ['hulu'])\n",
      "original document: \n",
      "['This', 'was', 'sparked', 'out', 'of', 'conversation', 'with', 'a', 'friend', 'about', 'the', 'current', 'mindset', 'of', 'the', 'average', 'American.', 'The', 'general', 'distrust', 'of', 'our', 'government,', 'growing', 'inequality(including', 'the', 'rise', 'of', 'AI', 'and', 'the', 'inevitable', 'massive', 'loss', 'of', 'jobs),', 'mass', 'surveillance', 'and', 'consistent', 'nuclear', 'threat', 'by', 'delusional', 'leaders', 'being', 'the', 'main', 'catalysts.', 'And', 'by', 'media', 'portrayal', 'and', 'tribe', 'mentality', 'among', 'social', 'media,', 'these', 'issues', 'and', 'the', 'likelihood', 'of', 'revolution', 'seem', 'to', 'be', 'growing', 'everyday.', '', 'It', 'seems', 'this', 'form', 'of', 'corrupt', 'government', 'certainly', 'has', 'a', 'limited', 'life-span.', 'It', 'also', 'seems', 'relatively', 'certain', 'there', 'lies', 'a', 'tipping', 'point', 'at', 'which', 'the', 'majority', 'of', 'Americans', 'will', 'be', 'so', 'fed', 'up', 'with', 'the', 'failing', 'system', 'that', 'unification', 'to', 'overthrow', 'the', 'gov', 'is', 'inevitable.', 'Do', 'you', 'think', \"it's\", 'likely,', 'or', 'even', 'possible?', 'Is', 'it', 'preventable', 'or', 'inevitable?', 'What', 'do', 'you', 'think', 'this', 'could', 'look', 'like?', 'A', 'lot', 'of', 'big,', 'unanswerable', 'questions', 'here.', 'Just', 'curious', 'what', 'your', 'thoughts', 'are!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spark', 'convers', 'friend', 'cur', 'mindset', 'av', 'am', 'gen', 'distrust', 'govern', 'grow', 'inequalityinclud', 'ris', 'ai', 'inevit', 'mass', 'loss', 'job', 'mass', 'surveil', 'consist', 'nuclear', 'threat', 'delud', 'lead', 'main', 'catalyst', 'med', 'portray', 'trib', 'ment', 'among', 'soc', 'med', 'issu', 'lik', 'revolv', 'seem', 'grow', 'everyday', 'seem', 'form', 'corrupt', 'govern', 'certain', 'limit', 'lifesp', 'also', 'seem', 'rel', 'certain', 'lie', 'tip', 'point', 'maj', 'am', 'fed', 'fail', 'system', 'un', 'overthrow', 'gov', 'inevit', 'think', 'lik', 'ev', 'poss', 'prev', 'inevit', 'think', 'could', 'look', 'lik', 'lot', 'big', 'unansw', 'quest', 'cury', 'thought'], ['spark', 'conversation', 'friend', 'current', 'mindset', 'average', 'american', 'general', 'distrust', 'government', 'grow', 'inequalityincluding', 'rise', 'ai', 'inevitable', 'massive', 'loss', 'job', 'mass', 'surveillance', 'consistent', 'nuclear', 'threat', 'delusional', 'leaders', 'main', 'catalysts', 'media', 'portrayal', 'tribe', 'mentality', 'among', 'social', 'media', 'issue', 'likelihood', 'revolution', 'seem', 'grow', 'everyday', 'seem', 'form', 'corrupt', 'government', 'certainly', 'limit', 'lifespan', 'also', 'seem', 'relatively', 'certain', 'lie', 'tip', 'point', 'majority', 'americans', 'feed', 'fail', 'system', 'unification', 'overthrow', 'gov', 'inevitable', 'think', 'likely', 'even', 'possible', 'preventable', 'inevitable', 'think', 'could', 'look', 'like', 'lot', 'big', 'unanswerable', 'question', 'curious', 'thoughts'])\n",
      "original document: \n",
      "['I', 'would', 'be', 'careful', 'with', 'advising', 'the', 'p47m', 'model', 'for', 'the', 'US.', 'Its', 'flight', 'model', 'gets', 'borked', 'every', 'other', 'patch', 'and', 'this', 'patch', 'it', 'seems', 'the', 'M', 'is', 'not', 'very', 'good', 'at', 'the', 'moment.', 'Better', 'off', 'putting', 'a', 'tali', 'on', 'the', 'p47N', 'or', 'like', 'you', 'advised', 'the', 'US', 'spit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'car', 'adv', 'p47m', 'model', 'us', 'flight', 'model', 'get', 'bork', 'every', 'patch', 'patch', 'seem', 'good', 'mom', 'bet', 'put', 'tal', 'p47n', 'lik', 'adv', 'us', 'spit'], ['would', 'careful', 'advise', 'p47m', 'model', 'us', 'flight', 'model', 'get', 'borked', 'every', 'patch', 'patch', 'seem', 'good', 'moment', 'better', 'put', 'tali', 'p47n', 'like', 'advise', 'us', 'spit'])\n",
      "original document: \n",
      "['Very', 'pretty.', \"I've\", 'made', 'a', 'couple', 'hats', 'this', 'week', 'and', 'hated', 'how', 'they', 'came', 'out.', \"I'm\", 'jealous', 'of', 'you!', 'Ha', 'ha.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'iv', 'mad', 'coupl', 'hat', 'week', 'hat', 'cam', 'im', 'jeal', 'ha', 'ha'], ['pretty', 'ive', 'make', 'couple', 'hat', 'week', 'hat', 'come', 'im', 'jealous', 'ha', 'ha'])\n",
      "original document: \n",
      "['I', 'take', '2', 'trains', 'to', 'work', 'and', '2', 'trains', 'home.', '', \"That's\", '$7', 'a', 'day', 'for', '5', 'days', 'a', 'week', 'for', '$35.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'two', 'train', 'work', 'two', 'train', 'hom', 'that', 'sev', 'day', 'fiv', 'day', 'week', 'thirty-five'], ['take', 'two', 'train', 'work', 'two', 'train', 'home', 'thats', 'seven', 'day', 'five', 'days', 'week', 'thirty-five'])\n",
      "original document: \n",
      "['esoognoM-']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['esoognom'], ['esoognom'])\n",
      "original document: \n",
      "['What', 'a', 'battle.', 'Still', 'confused', 'by', 'several', 'of', 'the', 'officials', 'calls', 'but', \"I'll\", 'take', 'it.', '\\n\\nAlso,', 'we', 'hit', 'the', 'most', 'unlikely', '50', 'yard', 'field', 'goal', 'of', 'all', 'time.', 'I', \"don't\", 'even', 'trust', 'the', 'kickers', 'to', 'make', 'an', 'XP']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['battl', 'stil', 'confus', 'sev', 'off', 'cal', 'il', 'tak', '\\n\\nalso', 'hit', 'unlik', 'fifty', 'yard', 'field', 'goal', 'tim', 'dont', 'ev', 'trust', 'kick', 'mak', 'xp'], ['battle', 'still', 'confuse', 'several', 'officials', 'call', 'ill', 'take', '\\n\\nalso', 'hit', 'unlikely', 'fifty', 'yard', 'field', 'goal', 'time', 'dont', 'even', 'trust', 'kickers', 'make', 'xp'])\n",
      "original document: \n",
      "['She', 'owns', 'everything!', 'Everything', 'is', 'hers!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['own', 'everyth', 'everyth'], ['own', 'everything', 'everything'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Make', 'em', 'yourself.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'em'], ['make', 'em'])\n",
      "original document: \n",
      "['Fucking', 'awesome.', 'Unexpected', 'awesome.\\n\\nDefinitely', 'best', 'new', 'animated', 'show', 'since', 'rick', '&amp;', 'morty.', 'IMO', \"it's\", 'already', 'on', 'that', 'level.\\n\\nWhen', 'a', '12', 'yr', 'old', \"kid's\", 'hairy', 'hormone', 'monster', 'skull-fucked', 'a', 'decapitated', 'head,', 'I', 'was', 'sold.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'awesom', 'unexpect', 'awesome\\n\\ndefinitely', 'best', 'new', 'anim', 'show', 'sint', 'rick', 'amp', 'morty', 'imo', 'already', 'level\\n\\nwhen', 'twelv', 'yr', 'old', 'kid', 'hairy', 'hormon', 'monst', 'skullfuck', 'decapit', 'head', 'sold'], ['fuck', 'awesome', 'unexpected', 'awesome\\n\\ndefinitely', 'best', 'new', 'animate', 'show', 'since', 'rick', 'amp', 'morty', 'imo', 'already', 'level\\n\\nwhen', 'twelve', 'yr', 'old', 'kid', 'hairy', 'hormone', 'monster', 'skullfucked', 'decapitate', 'head', 'sell'])\n",
      "original document: \n",
      "['https://www.blackvue.com/shop/\\n\\nBlackVue', 'DR750S-2CH\\n\\nDual', 'Front', 'and', 'Rear', 'full', 'HD', 'dash', 'cam', 'with', 'Sony', 'STARVIS', 'Image', 'Sensor,', 'GPS,', 'Wi-Fi,', 'Cloud', 'connectivity,', 'impact', 'and', 'motion', 'detection.\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwblackvuecomshop\\n\\nblackvue', 'dr750s2ch\\n\\ndual', 'front', 'rear', 'ful', 'hd', 'dash', 'cam', 'sony', 'starv', 'im', 'sens', 'gps', 'wif', 'cloud', 'connect', 'impact', 'mot', 'detection\\n\\n\\n\\n'], ['httpswwwblackvuecomshop\\n\\nblackvue', 'dr750s2ch\\n\\ndual', 'front', 'rear', 'full', 'hd', 'dash', 'cam', 'sony', 'starvis', 'image', 'sensor', 'gps', 'wifi', 'cloud', 'connectivity', 'impact', 'motion', 'detection\\n\\n\\n\\n'])\n",
      "original document: \n",
      "['Ammonia', 'is', 'quite', 'water', 'soluble', 'and', 'the', 'amounts', 'we', 'measure', 'in', 'aquariums', 'are', 'pretty', 'small', 'so', 'you', 'may', 'actually', 'be', 'able', 'to', 'detect', 'ammonia', 'from', 'the', 'air', 'in', 'the', 'water.', '', '\\n\\nOf', 'course,', 'it', \"doesn't\", 'matter', 'where', 'it', 'comes', 'from', 'as', 'it', 'has', 'the', 'same', 'effect', 'on', 'fish.', 'But', 'if', 'your', 'tank', 'is', 'nowhere', 'near', 'your', 'litter', 'box', 'or', 'bird', 'cage', 'but', 'you', 'run', 'the', 'test', 'near', 'one,', 'it', 'could', 'affect', 'the', 'results.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ammon', 'quit', 'wat', 'solubl', 'amount', 'meas', 'aquar', 'pretty', 'smal', 'may', 'act', 'abl', 'detect', 'ammon', 'air', 'wat', '\\n\\nof', 'cours', 'doesnt', 'mat', 'com', 'effect', 'fish', 'tank', 'nowh', 'near', 'lit', 'box', 'bird', 'cag', 'run', 'test', 'near', 'on', 'could', 'affect', 'result'], ['ammonia', 'quite', 'water', 'soluble', 'amount', 'measure', 'aquariums', 'pretty', 'small', 'may', 'actually', 'able', 'detect', 'ammonia', 'air', 'water', '\\n\\nof', 'course', 'doesnt', 'matter', 'come', 'effect', 'fish', 'tank', 'nowhere', 'near', 'litter', 'box', 'bird', 'cage', 'run', 'test', 'near', 'one', 'could', 'affect', 'result'])\n",
      "original document: \n",
      "['Removed.', '', 'Video', 'deleted.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov', 'video', 'delet'], ['remove', 'video', 'delete'])\n",
      "original document: \n",
      "['BAARLEEY', 'PRIMOOOO']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['baarleey', 'primoooo'], ['baarleey', 'primoooo'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Fitness', 'pf', 'the', 'storm', 'were', 'bsicly', 'a', 'cho', 'gall', 'only', 'stream', 'and', 'had', 'around', '300', 'viewrs', 'alwys', 'so', 'yeah']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fit', 'pf', 'storm', 'bsic', 'cho', 'gal', 'stream', 'around', 'three hundred', 'viewr', 'alwy', 'yeah'], ['fitness', 'pf', 'storm', 'bsicly', 'cho', 'gall', 'stream', 'around', 'three hundred', 'viewrs', 'alwys', 'yeah'])\n",
      "original document: \n",
      "['&gt;He', 'said', 'he', 'supported', 'the', 'pair', 'protesting,', 'but', 'wanted', 'them', 'to', 'do', 'it', 'in', 'other', 'ways', '-', 'kneeling', 'after', 'a', 'touchdown', 'in', 'the', 'end', 'zone', 'or', 'writing', 'and', 'passing', 'out', 'a', 'paper', 'about', 'the', 'issues.\\n\\n\"I', 'support', 'a', 'protest,', 'but', 'only', 'when', 'and', 'how', 'I', 'say.\"', 'Just', 'sack', 'up', 'and', 'say', 'you', \"don't\", 'support', \"it--it's\", 'not', 'support', 'if', 'you', 'dictate', 'how', \"it's\", 'done.', '', \"I'd\", 'still', 'think', \"it's\", 'stupid,', 'but', 'could', 'respect', 'the', 'honesty', 'at', 'least.\\n\\nThat', 'said,', \"there's\", 'really', 'nothing', 'anyone', 'can', 'do', 'since', 'it', 'sounds', 'like', 'the', 'program', 'is', 'private.', '', 'Such', 'programs', 'can', 'set', 'whatever', 'standards', 'they', 'like', 'as', 'long', 'as', 'they', \"don't\", 'discriminate', 'against', 'a', 'protected', 'class.', 'A', 'public', 'school', 'would', 'be', 'a', 'different', 'story', 'I', 'think.\\n', '', '', '', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gthe', 'said', 'support', 'pair', 'protest', 'want', 'way', 'kneel', 'touchdown', 'end', 'zon', 'writ', 'pass', 'pap', 'issues\\n\\ni', 'support', 'protest', 'say', 'sack', 'say', 'dont', 'support', 'itit', 'support', 'dict', 'don', 'id', 'stil', 'think', 'stupid', 'could', 'respect', 'honesty', 'least\\n\\nth', 'said', 'ther', 'real', 'noth', 'anyon', 'sint', 'sound', 'lik', 'program', 'priv', 'program', 'set', 'whatev', 'standard', 'lik', 'long', 'dont', 'discrimin', 'protect', 'class', 'publ', 'school', 'would', 'diff', 'story', 'think\\n'], ['gthe', 'say', 'support', 'pair', 'protest', 'want', 'ways', 'kneel', 'touchdown', 'end', 'zone', 'write', 'pass', 'paper', 'issues\\n\\ni', 'support', 'protest', 'say', 'sack', 'say', 'dont', 'support', 'itits', 'support', 'dictate', 'do', 'id', 'still', 'think', 'stupid', 'could', 'respect', 'honesty', 'least\\n\\nthat', 'say', 'theres', 'really', 'nothing', 'anyone', 'since', 'sound', 'like', 'program', 'private', 'program', 'set', 'whatever', 'standards', 'like', 'long', 'dont', 'discriminate', 'protect', 'class', 'public', 'school', 'would', 'different', 'story', 'think\\n'])\n",
      "original document: \n",
      "[\"We're\", 'dying!', 'Please,', 'for', 'the', 'love', 'of', 'God,', 'SEND', 'CAN', 'OPENERS!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dying', 'pleas', 'lov', 'god', 'send', 'op'], ['die', 'please', 'love', 'god', 'send', 'openers'])\n",
      "original document: \n",
      "['Added']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad'], ['add'])\n",
      "original document: \n",
      "['Ms', 'st', 'course', 'to', 'ruining', 'this', 'for', 'you.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ms', 'st', 'cours', 'ruin'], ['ms', 'st', 'course', 'ruin'])\n",
      "original document: \n",
      "[\"That's\", 'beautiful!', 'I', 'always', 'loved', 'the', 'card,', 'but', 'the', 'art', 'never', 'did', 'a', 'damn', 'thing', 'for', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'beauty', 'alway', 'lov', 'card', 'art', 'nev', 'damn', 'thing'], ['thats', 'beautiful', 'always', 'love', 'card', 'art', 'never', 'damn', 'thing'])\n",
      "original document: \n",
      "['&gt;', 'This', 'makes', 'no', 'logical,', 'rhetorical,', 'or', 'grammatical', 'sense\\n\\nYes', 'it', 'does,', 'because', 'when', 'you', 'say', 'the', 'reasoning', 'behind', 'the', 'action', 'is', 'fair', 'but', 'the', 'action', 'as', 'taken', 'is', 'not', \"you're\", 'defending', 'the', 'intent', 'to', 'take', 'action,', 'not', 'the', 'specific', 'action', 'itself.', 'In', 'this', 'sense', 'you', 'defend', 'an', 'action', 'taken', 'in', 'response', 'to', 'their', 'thinking,', 'just', 'not', 'the', 'kind', 'of', 'action.', 'The', 'kind', 'of', 'thinking', 'that', 'leads', 'to', 'genocide', 'is', 'not', 'rational,', 'its', 'not', 'sensible,', 'and', 'its', 'not', 'fair.', 'There', 'is', 'no', 'just', 'motive', 'that', 'makes', 'people', 'believe', 'in', 'mass', 'rape.', 'That', 'is', 'hate.', 'Your', 'entire', 'premise', 'is', 'that', \"there's\", 'a', 'just', 'way', 'to', 'think', 'of', 'another', 'people', 'in', 'a', 'way', 'that', 'can', 'make', 'you', 'want', 'to', 'do', 'this.\\n\\n&gt;which', 'is', 'that', 'you', 'think', \"I'm\", 'excusing', 'rape\\n\\nOn', 'some', 'level', 'you', 'are', 'to', 'an', 'extent.', 'Its', 'like', 'if', 'someone', 'said', '\"so', 'and', 'so', 'got', 'raped', 'by', 'her', 'ex\"', 'and', 'someone', 'replied', '\"well', 'she', 'did', 'screw', 'him', 'over', 'hard', 'in', 'the', 'divorce\".', 'You', 'know', 'saying', 'that', 'is', 'fucked', 'up', 'and', 'you', 'know', 'the', 'impulse', 'to', 'consider', 'the', 'action', 'that', 'way', 'and', 'then', 'express', 'it', 'says', 'something', 'terrible', 'about', 'how', 'the', 'speaker', 'is', 'envisioning', 'the', 'dynamics', 'of', 'the', 'relationship.\\n\\nWhen', 'people', 'are', 'tried', 'for', 'crimes', 'the', 'things', \"you're\", 'saying', 'are', 'used', 'as', 'mitigation.', 'The', 'reasoning', 'behind', 'an', 'act', 'is', 'used', 'to', 'lessen', 'or', 'strengthen', 'the', 'condemnation', 'and', 'ensuing', 'punishment', 'for', 'the', 'act.', 'On', 'any', 'level', 'where', 'you', 'look', 'at', 'how', 'our', 'value', 'system', 'manifests', 'judgment', 'for', 'actions', 'taken', 'you', 'are', 'mitigating', 'it.\\n\\n&gt;The', 'problem', 'with', 'your', 'argument', 'is', 'that', 'nowhere', 'did', 'I', 'say', 'I', 'supported', 'the', 'how', 'of', 'the', 'Burmese', 'actions\\n\\nI', 'already', 'acknowledged', 'that.', 'I', 'said', 'that', \"doesn't\", 'matter', 'and', 'I', 'have', 'explained', 'why.', 'Your', 'prejudice', 'against', 'Islam', 'allows', 'you', 'to', 'justify', 'however', 'thinking', 'in', 'these', 'grotesque', 'terms', 'and', 'think', \"you're\", 'clean', 'in', 'doing', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'mak', 'log', 'rhet', 'gram', 'sense\\n\\nyes', 'say', 'reason', 'behind', 'act', 'fair', 'act', 'tak', 'yo', 'defend', 'int', 'tak', 'act', 'spec', 'act', 'sens', 'defend', 'act', 'tak', 'respons', 'think', 'kind', 'act', 'kind', 'think', 'lead', 'genocid', 'rat', 'sens', 'fair', 'mot', 'mak', 'peopl', 'believ', 'mass', 'rap', 'hat', 'entir', 'prem', 'ther', 'way', 'think', 'anoth', 'peopl', 'way', 'mak', 'want', 'this\\n\\ngtwhich', 'think', 'im', 'excus', 'rape\\n\\non', 'level', 'ext', 'lik', 'someon', 'said', 'got', 'rap', 'ex', 'someon', 'reply', 'wel', 'screw', 'hard', 'divorc', 'know', 'say', 'fuck', 'know', 'impuls', 'consid', 'act', 'way', 'express', 'say', 'someth', 'terr', 'speak', 'envid', 'dynam', 'relationship\\n\\nwhen', 'peopl', 'tri', 'crim', 'thing', 'yo', 'say', 'us', 'mitig', 'reason', 'behind', 'act', 'us', 'less', 'strengthen', 'condemn', 'ensu', 'pun', 'act', 'level', 'look', 'valu', 'system', 'manifest', 'judg', 'act', 'tak', 'mitig', 'it\\n\\ngtthe', 'problem', 'argu', 'nowh', 'say', 'support', 'burmes', 'actions\\n\\ni', 'already', 'acknowledg', 'said', 'doesnt', 'mat', 'explain', 'prejud', 'islam', 'allow', 'just', 'howev', 'think', 'grotesqu', 'term', 'think', 'yo', 'cle'], ['gt', 'make', 'logical', 'rhetorical', 'grammatical', 'sense\\n\\nyes', 'say', 'reason', 'behind', 'action', 'fair', 'action', 'take', 'youre', 'defend', 'intent', 'take', 'action', 'specific', 'action', 'sense', 'defend', 'action', 'take', 'response', 'think', 'kind', 'action', 'kind', 'think', 'lead', 'genocide', 'rational', 'sensible', 'fair', 'motive', 'make', 'people', 'believe', 'mass', 'rape', 'hate', 'entire', 'premise', 'theres', 'way', 'think', 'another', 'people', 'way', 'make', 'want', 'this\\n\\ngtwhich', 'think', 'im', 'excuse', 'rape\\n\\non', 'level', 'extent', 'like', 'someone', 'say', 'get', 'rap', 'ex', 'someone', 'reply', 'well', 'screw', 'hard', 'divorce', 'know', 'say', 'fuck', 'know', 'impulse', 'consider', 'action', 'way', 'express', 'say', 'something', 'terrible', 'speaker', 'envision', 'dynamics', 'relationship\\n\\nwhen', 'people', 'try', 'crimes', 'things', 'youre', 'say', 'use', 'mitigation', 'reason', 'behind', 'act', 'use', 'lessen', 'strengthen', 'condemnation', 'ensue', 'punishment', 'act', 'level', 'look', 'value', 'system', 'manifest', 'judgment', 'action', 'take', 'mitigate', 'it\\n\\ngtthe', 'problem', 'argument', 'nowhere', 'say', 'support', 'burmese', 'actions\\n\\ni', 'already', 'acknowledge', 'say', 'doesnt', 'matter', 'explain', 'prejudice', 'islam', 'allow', 'justify', 'however', 'think', 'grotesque', 'term', 'think', 'youre', 'clean'])\n",
      "original document: \n",
      "['added']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad'], ['add'])\n",
      "original document: \n",
      "['I', 'like', 'the', 'mood', 'you', 'have', 'with', 'this', 'beat.', 'The', 'bells', 'are', 'dope', 'at', 'creating', 'an', 'eerie', 'dark', 'feeling.', 'I', 'do', 'think', 'that', 'there', 'are', 'a', 'couple', 'of', 'mixing', 'issues', 'though.', 'Like', 'u/AnythingIsBad', 'said,', 'the', '808', 'and', 'kick', 'don’t', 'really', 'punch.', 'The', 'snare', 'also', 'seems', 'a', 'little', 'weak', 'as', 'well.', 'Other', 'than', 'that', 'I', 'think', 'this', 'is', 'great.', 'Keep', 'it', 'up!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'mood', 'beat', 'bel', 'dop', 'cre', 'eery', 'dark', 'feel', 'think', 'coupl', 'mix', 'issu', 'though', 'lik', 'uanythingisbad', 'said', 'eight hundred and eight', 'kick', 'dont', 'real', 'punch', 'snar', 'also', 'seem', 'littl', 'weak', 'wel', 'think', 'gre', 'keep'], ['like', 'mood', 'beat', 'bell', 'dope', 'create', 'eerie', 'dark', 'feel', 'think', 'couple', 'mix', 'issue', 'though', 'like', 'uanythingisbad', 'say', 'eight hundred and eight', 'kick', 'dont', 'really', 'punch', 'snare', 'also', 'seem', 'little', 'weak', 'well', 'think', 'great', 'keep'])\n",
      "original document: \n",
      "['It', 'depends', 'on', 'whether', 'the', 'toll', 'collection', 'authority', 'has', 'a', 'reciprocal', 'agreement', 'with', 'the', 'state/province.\\n\\nI', 'have', 'used', 'British', 'Columbia', 'plates', 'in', 'Washington', 'and', 'California,', 'and', 'successfully', 'evaded', 'the', 'tolls', 'because', 'while', 'my', 'license', 'plate', 'was', 'scanned,', 'no', 'agreement', 'was', 'in', 'place', 'to', 'actually', 'track', 'me', 'down', 'and', 'bill', 'me.', 'Same', 'for', 'using', 'California', 'plates', 'in', 'Ontario,', 'Canada.\\n\\nToll', 'collection', 'authorities', 'will', 'only', 'track', 'down', 'out-of-state', 'plates', 'that', 'they', 'expect', 'to', 'show', 'up', 'frequently.', 'For', 'example,', 'New', 'York', 'is', 'often', 'visited', 'by', 'drivers', 'from', 'Ontario', 'and', 'Massachusetts,', 'so', 'if', \"you're\", 'from', 'those', 'places', 'and', 'try', 'to', 'use', 'NY', 'tolled', 'roads', 'without', 'an', 'EZPass', 'you', 'will', 'get', 'nailed.', 'Likewise,', 'Ontario', 'will', 'track', 'drivers', 'from', 'Quebec,', 'Michigan,', 'and', 'NY', 'for', 'sure.', 'But', 'if', 'your', 'car', 'is', 'registered', 'in', 'California,', 'then', \"it's\", 'highly', 'unlikely', 'that', \"they'll\", 'bother', 'to', 'track', 'you', 'down.', 'It', 'costs', 'them', 'money', 'to', 'do', 'so,', 'and', 'California-plated', 'cars', 'are', 'a', 'rare', 'sight', 'in', 'that', 'region.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'wheth', 'tol', 'collect', 'auth', 'reciproc', 'agr', 'stateprovince\\n\\ni', 'us', 'brit', 'columb', 'plat', 'washington', 'californ', 'success', 'evad', 'tol', 'licens', 'plat', 'scan', 'agr', 'plac', 'act', 'track', 'bil', 'us', 'californ', 'plat', 'ontario', 'canada\\n\\ntoll', 'collect', 'auth', 'track', 'outofst', 'plat', 'expect', 'show', 'frequ', 'exampl', 'new', 'york', 'oft', 'visit', 'driv', 'ontario', 'massachuset', 'yo', 'plac', 'try', 'us', 'ny', 'tol', 'road', 'without', 'ezpass', 'get', 'nail', 'likew', 'ontario', 'track', 'driv', 'quebec', 'michig', 'ny', 'sur', 'car', 'reg', 'californ', 'high', 'unlik', 'theyl', 'both', 'track', 'cost', 'money', 'californiapl', 'car', 'rar', 'sight', 'reg'], ['depend', 'whether', 'toll', 'collection', 'authority', 'reciprocal', 'agreement', 'stateprovince\\n\\ni', 'use', 'british', 'columbia', 'plat', 'washington', 'california', 'successfully', 'evade', 'toll', 'license', 'plate', 'scan', 'agreement', 'place', 'actually', 'track', 'bill', 'use', 'california', 'plat', 'ontario', 'canada\\n\\ntoll', 'collection', 'authorities', 'track', 'outofstate', 'plat', 'expect', 'show', 'frequently', 'example', 'new', 'york', 'often', 'visit', 'drivers', 'ontario', 'massachusetts', 'youre', 'place', 'try', 'use', 'ny', 'toll', 'roads', 'without', 'ezpass', 'get', 'nail', 'likewise', 'ontario', 'track', 'drivers', 'quebec', 'michigan', 'ny', 'sure', 'car', 'register', 'california', 'highly', 'unlikely', 'theyll', 'bother', 'track', 'cost', 'money', 'californiaplated', 'cars', 'rare', 'sight', 'region'])\n",
      "original document: \n",
      "['Interested', 'in', 'Beauty', 'and', 'the', 'Beast,', 'or', 'Incredibles\\n\\nhttps://www.reddit.com/r/uvtrade/comments/72lsuf/offer_guardians_of_the_galaxy_2_diary_of_a_wimpy/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'beauty', 'beast', 'incredibles\\n\\nhttpswwwredditcomruvtradecomments72lsufoffer_guardians_of_the_galaxy_2_diary_of_a_wimpy'], ['interest', 'beauty', 'beast', 'incredibles\\n\\nhttpswwwredditcomruvtradecomments72lsufoffer_guardians_of_the_galaxy_2_diary_of_a_wimpy'])\n",
      "original document: \n",
      "['Fam', 'we', 'gon', 'organize.', '#teamcro']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fam', 'gon', 'org', 'teamcro'], ['fam', 'gon', 'organize', 'teamcro'])\n",
      "original document: \n",
      "['I', 'like', 'Arriva', 'as', 'they', 'have', 'charge', 'points', 'and', 'are', 'cheaper', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'arriv', 'charg', 'point', 'cheap'], ['like', 'arriva', 'charge', 'point', 'cheaper'])\n",
      "original document: \n",
      "['Git', 'ur', 'ass', 'over', 'to', 'r/tightywhities']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['git', 'ur', 'ass', 'rtightywh'], ['git', 'ur', 'ass', 'rtightywhities'])\n",
      "original document: \n",
      "['Yeah,', 'pretty', 'sure', 'that', 'was', 'against', 'the', 'wind', 'but', 'he', 'definitely', 'shanked', 'that', 'one', 'regardless']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'pretty', 'sur', 'wind', 'definit', 'shank', 'on', 'regardless'], ['yeah', 'pretty', 'sure', 'wind', 'definitely', 'shank', 'one', 'regardless'])\n",
      "original document: \n",
      "['*aboot\\nCanadians', 'where', 'right', 'the', 'whole', 'time']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aboot\\ncanadians', 'right', 'whol', 'tim'], ['aboot\\ncanadians', 'right', 'whole', 'time'])\n",
      "original document: \n",
      "['If', 'you', 'would', 'like', 'a', 'battle', 'run', 'for', 'this', 'conflict', 'post,', 'you', 'must', 'post', 'it', 'in', 'the', 'appropriate', '[MODPOST]', 'thread', 'designated', 'for', 'collecting', '[CONFLICT]', 'posts.\\n\\n#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/worldpowers)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lik', 'battl', 'run', 'conflict', 'post', 'must', 'post', 'appropry', 'modpost', 'thread', 'design', 'collect', 'conflict', 'posts\\n\\namp009\\n\\namp009\\n\\namp009\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorworldpow', 'quest', 'concern'], ['would', 'like', 'battle', 'run', 'conflict', 'post', 'must', 'post', 'appropriate', 'modpost', 'thread', 'designate', 'collect', 'conflict', 'posts\\n\\namp009\\n\\namp009\\n\\namp009\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorworldpowers', 'question', 'concern'])\n",
      "original document: \n",
      "['Big_d_dray', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['big_d_dray'], ['big_d_dray'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"He's\", 'more', 'important', 'to', 'God.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'import', 'god'], ['hes', 'important', 'god'])\n",
      "original document: \n",
      "['unisom', 'and', 'b6.', 'you', 'HAVE', 'to', 'take', 'them', 'together', 'for', 'it', 'to', 'work.', 'ask', 'your', 'doctor.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unisom', 'b6', 'tak', 'togeth', 'work', 'ask', 'doct'], ['unisom', 'b6', 'take', 'together', 'work', 'ask', 'doctor'])\n",
      "original document: \n",
      "[\"Doesn't\", 'surprise', 'me', 'at', 'all', 'I', \"wouldn't\", 'trust', 'that', 'lying', 'scum', 'to', 'tell', 'me', 'the', 'time', '.', 'If', 'his', 'mouth', 'is', 'moving', 'then', 'rest', 'assured', \"it's\", 'spreading', 'lies', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'surpr', 'wouldnt', 'trust', 'lying', 'scum', 'tel', 'tim', 'mou', 'mov', 'rest', 'ass', 'spreading', 'lie'], ['doesnt', 'surprise', 'wouldnt', 'trust', 'lie', 'scum', 'tell', 'time', 'mouth', 'move', 'rest', 'assure', 'spread', 'lie'])\n",
      "original document: \n",
      "['143416868|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'nN0OTd4c)\\n\\n&gt;&gt;143415951\\n\\nif', 'you', 'let', 'me', 'smoke', 'weed', 'and', 'leave', 'me', 'alone', \"i'll\", 'vote', 'for', 'you\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and sixteen thousand, eight hundred and sixty-eight', 'gt', 'unit', 'stat', 'anonym', 'id', 'nn0otd4c\\n\\ngtgt143415951\\n\\nif', 'let', 'smok', 'wee', 'leav', 'alon', 'il', 'vot', 'you\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and sixteen thousand, eight hundred and sixty-eight', 'gt', 'unite', 'state', 'anonymous', 'id', 'nn0otd4c\\n\\ngtgt143415951\\n\\nif', 'let', 'smoke', 'weed', 'leave', 'alone', 'ill', 'vote', 'you\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', \"we'll\", 'ever', 'get', 'We', 'Are', 'Legend.', 'Sentido', 'is', 'pretty', 'great', 'as', 'well.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'wel', 'ev', 'get', 'legend', 'sentido', 'pretty', 'gre', 'wel'], ['dont', 'think', 'well', 'ever', 'get', 'legend', 'sentido', 'pretty', 'great', 'well'])\n",
      "original document: \n",
      "['Chiefs', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chief'], ['chiefs'])\n",
      "original document: \n",
      "[\"Aaaand....they're\", 'probably', 'gonna', 'remember', 'it,', 'come', 'election', 'time.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aaaandtheyr', 'prob', 'gonn', 'rememb', 'com', 'elect', 'tim'], ['aaaandtheyre', 'probably', 'gonna', 'remember', 'come', 'election', 'time'])\n",
      "original document: \n",
      "['Fuck', 'sake,', 'thank', 'you', 'for', 'the', 'answer', 'bro.', \"\\n\\nI'm\", '£41', 'down', 'for', 'literally', 'NOTHING', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'sak', 'thank', 'answ', 'bro', '\\n\\nim', 'forty-one', 'lit', 'noth'], ['fuck', 'sake', 'thank', 'answer', 'bro', '\\n\\nim', 'forty-one', 'literally', 'nothing'])\n",
      "original document: \n",
      "[\"I've\", 'been', 'doing', 'even', '3rd', 'washes', 'and', 'stocking', 'those', 'in', 'my', 'fridge', 'and', 'freezer.', 'May', 'or', 'may', 'not', 'help', 'but', 'at', 'least', \"I'll\", 'have', 'it', 'if', 'I', 'run', 'out.', '\\n\\nGlad', \"you're\", 'feeling', 'better', 'too!!👏🏼🙂']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'ev', '3rd', 'wash', 'stock', 'fridg', 'freez', 'may', 'may', 'help', 'least', 'il', 'run', '\\n\\nglad', 'yo', 'feel', 'bet'], ['ive', 'even', '3rd', 'wash', 'stock', 'fridge', 'freezer', 'may', 'may', 'help', 'least', 'ill', 'run', '\\n\\nglad', 'youre', 'feel', 'better'])\n",
      "original document: \n",
      "['14,', '32,', 'and', 'three', 'randoms', 'please.', 'If', 'the', 'requested', 'spots', 'aren’t', 'available,', 'I’ll', 'take', 'randoms', 'instead.', 'Five', 'total', 'spots.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fourteen', 'thirty-two', 'three', 'random', 'pleas', 'request', 'spot', 'ar', 'avail', 'il', 'tak', 'random', 'instead', 'fiv', 'tot', 'spot'], ['fourteen', 'thirty-two', 'three', 'randoms', 'please', 'request', 'spot', 'arent', 'available', 'ill', 'take', 'randoms', 'instead', 'five', 'total', 'spot'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Srs', 'this', 'is', 'just', 'insane.', 'What', 'is', 'it', 'about', 'beauty', 'gurus', 'that', 'makes', 'people', 'froth', 'at', 'the', 'mouth?', 'I', 'just', 'want', 'to', 'enjoy', 'pretty', 'makeup', 'dammit.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['srs', 'ins', 'beauty', 'gur', 'mak', 'peopl', 'fro', 'mou', 'want', 'enjoy', 'pretty', 'makeup', 'dammit'], ['srs', 'insane', 'beauty', 'gurus', 'make', 'people', 'froth', 'mouth', 'want', 'enjoy', 'pretty', 'makeup', 'dammit'])\n",
      "original document: \n",
      "['Thanks!', 'That', 'was', 'interesting', 'to', 'read.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'interest', 'read'], ['thank', 'interest', 'read'])\n",
      "original document: \n",
      "['As', 'for', 'the', 'name?', 'I', 'doubt', 'he', 'was', 'the', 'first', 'to', 'pioneer', 'it', 'but', \"I've\", 'always', 'called', 'them', 'Wheeler', 'handles', 'or', 'Wheeler', 'knock', \"off's\", 'because', 'he', 'does', 'it', 'correctly', 'while', 'the', 'majority', 'fall', 'short.\\n\\nI', 'love', 'the', 'look', 'of', 'these', 'handles', 'in', \"photo's\", 'but', 'the', 'pinky', 'swell', 'is', 'almost', 'always', 'too', 'wide/fat', 'for', 'my', 'tastes.', 'Not', 'a', 'fan', 'of', 'having', 'my', 'index', 'grip', 'tighter', 'than', 'my', \"pinky's.\", 'I', 'prefer', 'an', 'equal', 'grip', 'across', 'the', 'handle', 'or', 'slight', 'tapering', 'towards', 'the', 'rear.\\n\\nI', 'understand', 'the', 'idea', 'behind', 'the', 'design', 'but', 'it', 'is', 'actually', 'counter', 'intuitive', 'for', 'such', 'a', 'small', 'knife.', 'The', 'flair', 'is', 'better', 'suited', 'on', 'a', 'chopper', 'above', '10\"', 'in', 'blade', 'length.', 'Think', 'Kukri', 'handles.\\n\\nIf', 'my', 'pinky', \"doesn't\", 'touch', 'or', 'nearly', 'touch', 'the', 'inside', 'of', 'my', 'palm', 'when', 'I', 'grip', 'it,', \"it's\", 'too', 'fat', 'and', 'diminish', 'the', 'overall', 'grip.', \"I'm\", 'not', 'a', 'big', 'guy', 'but', 'I', 'have', 'piano', 'fingers', 'and', 'if', 'the', 'blade', 'wont', 'fit', 'my', 'hand', 'it', 'wont', 'fit', 'a', \"6'\", '5\"', 'mans', 'hand', 'correctly', 'either', 'with', 'there', 'extra', 'body', 'fat.\\n\\nIf', 'you', 'want', 'a', 'great', 'example', 'of', 'these', 'in', 'slight', 'variations', 'look', 'too', 'Nick', 'Wheeler.', 'He', 'does', 'it', 'right.', \"You'll\", 'notice', 'the', 'pinky', 'swell', 'on', 'most', 'of', 'his', 'models', 'are', 'obviously', 'smaller', 'than', 'the', 'index', 'grip', 'unlike', 'the', 'one', 'in', 'the', 'linked', 'photo.', 'And', 'if', 'they', 'are', 'not', 'they', 'are', 'more', 'dramatically', 'shaped', 'at', 'the', 'rear.', \"Doesn't\", 'look', 'as', 'pretty', 'but', 'I', 'feel', \"it's\", 'more', \"practical.\\n\\nHere's\", 'a', 'great', '[example](https://www.instagram.com/p/BXwy6dKFuew/?taken-by=nick_wheeler_knives_and_newfs).', 'It', 'may', 'be', 'hard', 'to', 'tell', 'for', 'some', 'but', 'that', 'extra', 'meat', 'being', 'removed', 'will', 'make', 'a', 'world', 'of', 'difference', 'in', 'the', 'hand.\\n\\nQuick', 'test', 'for', 'people', 'at', 'home.', 'Hold', 'your', 'hand', 'up.', 'Is', 'your', 'pinky', 'as', 'long', 'as', 'your', 'index?', 'If', 'no', 'then', 'the', 'swell', 'should', 'not', 'be', 'thicker', 'than', 'your', 'index', 'unless', 'your', 'intention', 'is', 'to', 'make', 'a', 'chopper.', 'Equal', 'grip', 'across', 'the', 'boards', 'makes', 'for', 'a', 'more', 'versatile', 'grip', 'but', 'the', 'blades', 'shape', 'truly', 'dictates', 'the', 'knifes', 'usage', 'so', 'plan', 'accordingly.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nam', 'doubt', 'first', 'pion', 'iv', 'alway', 'cal', 'wheel', 'handl', 'wheel', 'knock', 'off', 'correct', 'maj', 'fal', 'short\\n\\ni', 'lov', 'look', 'handl', 'photo', 'pinky', 'swel', 'almost', 'alway', 'widef', 'tast', 'fan', 'index', 'grip', 'tight', 'pinky', 'pref', 'eq', 'grip', 'across', 'handl', 'slight', 'tap', 'toward', 'rear\\n\\ni', 'understand', 'ide', 'behind', 'design', 'act', 'count', 'intuit', 'smal', 'knif', 'flair', 'bet', 'suit', 'chop', 'ten', 'blad', 'leng', 'think', 'kukr', 'handles\\n\\nif', 'pinky', 'doesnt', 'touch', 'near', 'touch', 'insid', 'palm', 'grip', 'fat', 'dimin', 'overal', 'grip', 'im', 'big', 'guy', 'piano', 'fing', 'blad', 'wont', 'fit', 'hand', 'wont', 'fit', 'six', 'fiv', 'man', 'hand', 'correct', 'eith', 'extr', 'body', 'fat\\n\\nif', 'want', 'gre', 'exampl', 'slight', 'vary', 'look', 'nick', 'wheel', 'right', 'youl', 'not', 'pinky', 'swel', 'model', 'obvy', 'smal', 'index', 'grip', 'unlik', 'on', 'link', 'photo', 'dram', 'shap', 'rear', 'doesnt', 'look', 'pretty', 'feel', 'practical\\n\\nheres', 'gre', 'examplehttpswwwinstagramcompbxwy6dkfuewtakenbynick_wheeler_knives_and_newfs', 'may', 'hard', 'tel', 'extr', 'meat', 'remov', 'mak', 'world', 'diff', 'hand\\n\\nquick', 'test', 'peopl', 'hom', 'hold', 'hand', 'pinky', 'long', 'index', 'swel', 'thick', 'index', 'unless', 'int', 'mak', 'chop', 'eq', 'grip', 'across', 'board', 'mak', 'versatil', 'grip', 'blad', 'shap', 'tru', 'dict', 'knif', 'us', 'plan', 'accord'], ['name', 'doubt', 'first', 'pioneer', 'ive', 'always', 'call', 'wheeler', 'handle', 'wheeler', 'knock', 'off', 'correctly', 'majority', 'fall', 'short\\n\\ni', 'love', 'look', 'handle', 'photos', 'pinky', 'swell', 'almost', 'always', 'widefat', 'taste', 'fan', 'index', 'grip', 'tighter', 'pinkys', 'prefer', 'equal', 'grip', 'across', 'handle', 'slight', 'taper', 'towards', 'rear\\n\\ni', 'understand', 'idea', 'behind', 'design', 'actually', 'counter', 'intuitive', 'small', 'knife', 'flair', 'better', 'suit', 'chopper', 'ten', 'blade', 'length', 'think', 'kukri', 'handles\\n\\nif', 'pinky', 'doesnt', 'touch', 'nearly', 'touch', 'inside', 'palm', 'grip', 'fat', 'diminish', 'overall', 'grip', 'im', 'big', 'guy', 'piano', 'finger', 'blade', 'wont', 'fit', 'hand', 'wont', 'fit', 'six', 'five', 'man', 'hand', 'correctly', 'either', 'extra', 'body', 'fat\\n\\nif', 'want', 'great', 'example', 'slight', 'variations', 'look', 'nick', 'wheeler', 'right', 'youll', 'notice', 'pinky', 'swell', 'model', 'obviously', 'smaller', 'index', 'grip', 'unlike', 'one', 'link', 'photo', 'dramatically', 'shape', 'rear', 'doesnt', 'look', 'pretty', 'feel', 'practical\\n\\nheres', 'great', 'examplehttpswwwinstagramcompbxwy6dkfuewtakenbynick_wheeler_knives_and_newfs', 'may', 'hard', 'tell', 'extra', 'meat', 'remove', 'make', 'world', 'difference', 'hand\\n\\nquick', 'test', 'people', 'home', 'hold', 'hand', 'pinky', 'long', 'index', 'swell', 'thicker', 'index', 'unless', 'intention', 'make', 'chopper', 'equal', 'grip', 'across', 'board', 'make', 'versatile', 'grip', 'blades', 'shape', 'truly', 'dictate', 'knife', 'usage', 'plan', 'accordingly'])\n",
      "original document: \n",
      "['Yeah.', 'His', 'plan', 'to', 'replace', 'Cersei', 'with', 'Margaery', \"wouldn't\", 'work', 'if', 'he', \"didn't\", 'know.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'plan', 'replac', 'cerse', 'margaery', 'wouldnt', 'work', 'didnt', 'know'], ['yeah', 'plan', 'replace', 'cersei', 'margaery', 'wouldnt', 'work', 'didnt', 'know'])\n",
      "original document: \n",
      "['Waiting', 'on', 'entry', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'entry', '\\n'], ['wait', 'entry', '\\n'])\n",
      "original document: \n",
      "['I', 'agree.', 'Shinji', 'is', 'somewhat', 'of', 'a', 'coward', 'but', 'not', 'nearly', 'as', 'cowardly', 'as', 'people', 'make', 'him', 'out', 'to', 'be.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'shinj', 'somewh', 'coward', 'near', 'coward', 'peopl', 'mak'], ['agree', 'shinji', 'somewhat', 'coward', 'nearly', 'cowardly', 'people', 'make'])\n",
      "original document: \n",
      "['Jasmine', 'masters*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jasmin', 'mast'], ['jasmine', 'master'])\n",
      "original document: \n",
      "['Mond', 'probably', \"shouldn't\", 'pass', 'tonight.', 'It', 'will', 'only', 'hurt', 'A&amp;Ms', 'offense.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mond', 'prob', 'shouldnt', 'pass', 'tonight', 'hurt', 'aampm', 'offens'], ['mond', 'probably', 'shouldnt', 'pass', 'tonight', 'hurt', 'aampms', 'offense'])\n",
      "original document: \n",
      "['Wow', 'they', 'didnt', 'know', 'what', 'they', 'had', 'at', 'all.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'didnt', 'know'], ['wow', 'didnt', 'know'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'would', 'take', 'out', 'a', 'loan', 'for', 'that', 'sheep', 'if', 'they', 'made', 'her', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'tak', 'loan', 'sheep', 'mad'], ['would', 'take', 'loan', 'sheep', 'make'])\n",
      "original document: \n",
      "['Poor', 'guy']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poor', 'guy'], ['poor', 'guy'])\n",
      "original document: \n",
      "['I', 'understand', 'the', 'impact', 'of', 'wireless', 'vs', 'wired.', '\\n\\nBut', \"I'm\", 'taking', 'wired', 'out', 'of', 'the', 'question,', 'and', 'asking...', 'the', 'difference', 'between', 'a', '$50', 'wireless', 'adapter', 'and', '$100', 'adapter.\\n\\nSorry', 'if', 'that', \"wasn't\", 'clear.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['understand', 'impact', 'wireless', 'vs', 'wir', '\\n\\nbut', 'im', 'tak', 'wir', 'quest', 'ask', 'diff', 'fifty', 'wireless', 'adapt', 'one hundred', 'adapter\\n\\nsorry', 'wasnt', 'clear'], ['understand', 'impact', 'wireless', 'vs', 'wire', '\\n\\nbut', 'im', 'take', 'wire', 'question', 'ask', 'difference', 'fifty', 'wireless', 'adapter', 'one hundred', 'adapter\\n\\nsorry', 'wasnt', 'clear'])\n",
      "original document: \n",
      "[\"It's\", 'really', 'hard', 'to', 'know', 'why', 'Roger', 'has', 'such', 'poor', 'conversion', 'rate', 'relative', 'to', 'what', \"you'd\", 'expect', 'given', 'his', 'normal', 'performance.', \"It's\", 'hard', 'to', 'say', \"he's\", 'choking,', 'because', \"he's\", 'extremely', 'good', 'on', 'tie', 'breaks.', '\\n\\nPerhaps', \"he's\", 'overly', 'conservative', 'on', 'breaks?', \"I'm\", 'not', 'sure,', 'I', 'feel', 'like', \"he's\", 'often', 'too', 'concerned', 'with', 'just', 'landing', 'the', 'return', 'in', 'on', 'breaking', 'points', 'and', 'not', 'risking', 'it', 'enough.', 'Not', 'always,', 'of', 'course,', 'but', 'often.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'hard', 'know', 'rog', 'poor', 'convert', 'rat', 'rel', 'youd', 'expect', 'giv', 'norm', 'perform', 'hard', 'say', 'hes', 'chok', 'hes', 'extrem', 'good', 'tie', 'break', '\\n\\nperhaps', 'hes', 'ov', 'conserv', 'break', 'im', 'sur', 'feel', 'lik', 'hes', 'oft', 'concern', 'land', 'return', 'break', 'point', 'risk', 'enough', 'alway', 'cours', 'oft'], ['really', 'hard', 'know', 'roger', 'poor', 'conversion', 'rate', 'relative', 'youd', 'expect', 'give', 'normal', 'performance', 'hard', 'say', 'hes', 'choke', 'hes', 'extremely', 'good', 'tie', 'break', '\\n\\nperhaps', 'hes', 'overly', 'conservative', 'break', 'im', 'sure', 'feel', 'like', 'hes', 'often', 'concern', 'land', 'return', 'break', 'point', 'risk', 'enough', 'always', 'course', 'often'])\n",
      "original document: \n",
      "[\"That's\", 'just', 'a', 'taste', 'of', 'what', 'FSU', 'fans', 'deal', 'with', 'all', 'season', 'long.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'tast', 'fsu', 'fan', 'deal', 'season', 'long'], ['thats', 'taste', 'fsu', 'fan', 'deal', 'season', 'long'])\n",
      "original document: \n",
      "['I', 'really', \"didn't\", 'expect', 'to', 'get', 'in', 'like', 'this.', 'I', \"don't\", 'know', 'what', 'to', 'do.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'didnt', 'expect', 'get', 'lik', 'dont', 'know'], ['really', 'didnt', 'expect', 'get', 'like', 'dont', 'know'])\n",
      "original document: \n",
      "['This!!!', 'Like', \"it's\", 'a', 'competition.', 'And', 'omg,', 'the', 'hate', 'Japril', 'fans', 'have', 'been', 'dishing', 'out', 'to', 'Maggie', 'since', 'the', 'season', '13', 'finale', 'is', 'alot', 'worse!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'competit', 'omg', 'hat', 'japril', 'fan', 'dish', 'maggy', 'sint', 'season', 'thirteen', 'fin', 'alot', 'wors'], ['like', 'competition', 'omg', 'hate', 'japril', 'fan', 'dish', 'maggie', 'since', 'season', 'thirteen', 'finale', 'alot', 'worse'])\n",
      "original document: \n",
      "['2', 'spots']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'spot'], ['two', 'spot'])\n",
      "original document: \n",
      "[\"How'd\", 'you', 'link', 'to', 'a', 'picture?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['howd', 'link', 'pict'], ['howd', 'link', 'picture'])\n",
      "original document: \n",
      "['Not', 'at', 'all,', \"we'll\", 'be', 'better', 'next', 'year', 'im', 'sure', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'bet', 'next', 'year', 'im', 'sur'], ['well', 'better', 'next', 'year', 'im', 'sure'])\n",
      "original document: \n",
      "['Hey', 'how', 'did', 'you', 'resolve', 'this', 'issue?', 'New', 'account?', 'Change', 'payment', 'methods?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'resolv', 'issu', 'new', 'account', 'chang', 'pay', 'method'], ['hey', 'resolve', 'issue', 'new', 'account', 'change', 'payment', 'methods'])\n",
      "original document: \n",
      "['Great', 'ass']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'ass'], ['great', 'ass'])\n",
      "original document: \n",
      "['Haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah'], ['haha'])\n",
      "original document: \n",
      "['I’ve', 'wondered-', 'what', 'does', 'it', 'mean', 'to', '“keep', 'your', 'mind', 'in', 'hell', 'and', 'don’t', 'despair”?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'wond', 'mean', 'keep', 'mind', 'hel', 'dont', 'despair'], ['ive', 'wonder', 'mean', 'keep', 'mind', 'hell', 'dont', 'despair'])\n",
      "original document: \n",
      "['How', 'the', 'fuck', 'is', 'that', 'a', 'sandwich', '?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'sandwich'], ['fuck', 'sandwich'])\n",
      "original document: \n",
      "['just', \"don't\", 'write', 'the', 'brand', 'name,', 'when', \"it's\", 'a', 'Supreme', 'tee', 'you', 'can', 'just', 'put', '\"graphic', 'tee\"', 'same', 'with', 'other', 'apparel,', 'if', \"it's\", 'a', 'grey', 'FOG', 'hoodie', 'you', 'just', 'put', 'grey', 'hoodie', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'writ', 'brand', 'nam', 'suprem', 'tee', 'put', 'graph', 'tee', 'apparel', 'grey', 'fog', 'hoody', 'put', 'grey', 'hoody'], ['dont', 'write', 'brand', 'name', 'supreme', 'tee', 'put', 'graphic', 'tee', 'apparel', 'grey', 'fog', 'hoodie', 'put', 'grey', 'hoodie'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', \"don't\", 'agree', 'with', 'them', 'breaking', 'up', 'with', 'you', 'over', 'it', 'but', 'still', 'just', 'be', 'careful', 'so', 'you', \"don't\", 'get', 'hurt...', 'might', 'come', 'across', 'the', 'wrong', 'kind', 'of', 'person', 'one', 'day', ':(', '\\n\\nBut', 'you', 'did', 'awesome.', 'Bet', 'little', 'ol', 'bitty', 'bitch', \"didn't\", 'know', 'what', 'hit', 'her', '😂']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'agr', 'break', 'stil', 'car', 'dont', 'get', 'hurt', 'might', 'com', 'across', 'wrong', 'kind', 'person', 'on', 'day', '\\n\\nbut', 'awesom', 'bet', 'littl', 'ol', 'bitty', 'bitch', 'didnt', 'know', 'hit'], ['dont', 'agree', 'break', 'still', 'careful', 'dont', 'get', 'hurt', 'might', 'come', 'across', 'wrong', 'kind', 'person', 'one', 'day', '\\n\\nbut', 'awesome', 'bet', 'little', 'ol', 'bitty', 'bitch', 'didnt', 'know', 'hit'])\n",
      "original document: \n",
      "['**VERY', 'VERY', 'BTA**', 'dont', 'you', 'dare', 'say', 'its', 'only', 'bta']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bta', 'dont', 'dar', 'say', 'bta'], ['bta', 'dont', 'dare', 'say', 'bta'])\n",
      "original document: \n",
      "['Thanks!', '', 'I', \"can't\", 'believe', 'it', 'either.', '', \"It's\", 'crazy', 'how', 'things', 'turn', 'out!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'cant', 'believ', 'eith', 'crazy', 'thing', 'turn'], ['thank', 'cant', 'believe', 'either', 'crazy', 'things', 'turn'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'Melvin', 'Manhoef', 'ever', 'gave', 'a', 'shit', 'about', 'weight.', 'He', 'was', 'pretty', 'muscular', 'himself,', 'trained', 'to', 'fight', 'like', 'a', 'killer,', 'and', 'was', 'confident', 'as', 'hell', 'early', 'in', 'his', 'career.\\n', '\\nThe', 'guy', \"KO'd\", 'Mark', 'Hunt', 'with', 'pretty', 'much', '1', 'punch.', 'https://www.youtube.com/watch?v=Cf9U0quIrB8']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'melvin', 'manhoef', 'ev', 'gav', 'shit', 'weight', 'pretty', 'muscul', 'train', 'fight', 'lik', 'kil', 'confid', 'hel', 'ear', 'career\\n', '\\nthe', 'guy', 'kod', 'mark', 'hunt', 'pretty', 'much', 'on', 'punch', 'httpswwwyoutubecomwatchvcf9u0quirb8'], ['dont', 'think', 'melvin', 'manhoef', 'ever', 'give', 'shit', 'weight', 'pretty', 'muscular', 'train', 'fight', 'like', 'killer', 'confident', 'hell', 'early', 'career\\n', '\\nthe', 'guy', 'kod', 'mark', 'hunt', 'pretty', 'much', 'one', 'punch', 'httpswwwyoutubecomwatchvcf9u0quirb8'])\n",
      "original document: \n",
      "['Can', 'you', 'lend', 'me', 'a', 'jar', 'of', 'love?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lend', 'jar', 'lov'], ['lend', 'jar', 'love'])\n",
      "original document: \n",
      "['/u/TOP_708']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['utop_708'], ['utop_708'])\n",
      "original document: \n",
      "['&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lt3'], ['lt3'])\n",
      "original document: \n",
      "['Spots', '17', 'and', '31', 'please.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spot', 'seventeen', 'thirty-one', 'pleas'], ['spot', 'seventeen', 'thirty-one', 'please'])\n",
      "original document: \n",
      "['What', 'do', 'I', 'put', 'on', 'it', 'to', 'make', 'it', 'one?\\nI', 'put', '“this', 'is', 'a', 'poll”', 'for', 'now', '\\nDone']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['put', 'mak', 'one\\ni', 'put', 'pol', '\\ndone'], ['put', 'make', 'one\\ni', 'put', 'poll', '\\ndone'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'think', \"that's\", 'Chinese', \"you're\", 'hearing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'that', 'chines', 'yo', 'hear'], ['think', 'thats', 'chinese', 'youre', 'hear'])\n",
      "original document: \n",
      "['Yep,', 'just', 'did', 'the', 'same', 'thing', 'at', 'the', 'same', 'time.', 'Its', 'getting', 'too', 'tight', 'to', 'get', 'called', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'thing', 'tim', 'get', 'tight', 'get', 'cal'], ['yep', 'thing', 'time', 'get', 'tight', 'get', 'call'])\n",
      "original document: \n",
      "['SemiAmusingBot', 'says:', 'I', 'am', 'now', 'installed', 'on', 'a', 'VPS.', 'reply', '#3', 'test']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['semiamusingbot', 'say', 'instal', 'vps', 'reply', 'three', 'test'], ['semiamusingbot', 'say', 'instal', 'vps', 'reply', 'three', 'test'])\n",
      "original document: \n",
      "['E']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['e'], ['e'])\n",
      "original document: \n",
      "['LOL', 'imagine', 'a', 'battle', 'royale', 'in', 'destiny.', 'I', \"don't\", 'think', 'bluehole', 'has', 'the', 'guts', 'to', 'sue', 'bungie', ':p']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'imagin', 'battl', 'roy', 'destiny', 'dont', 'think', 'bluehol', 'gut', 'sue', 'bungy', 'p'], ['lol', 'imagine', 'battle', 'royale', 'destiny', 'dont', 'think', 'bluehole', 'gut', 'sue', 'bungie', 'p'])\n",
      "original document: \n",
      "['When', 'trading', 'use', 'Warframe', 'Market', 'because', 'if', 'you', \"don't\", 'you', \"don't\", 'know', 'if', 'your', 'getting', 'a', 'good', 'or', 'bad', 'deal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trad', 'us', 'warfram', 'market', 'dont', 'dont', 'know', 'get', 'good', 'bad', 'deal'], ['trade', 'use', 'warframe', 'market', 'dont', 'dont', 'know', 'get', 'good', 'bad', 'deal'])\n",
      "original document: \n",
      "['100%', 'the', '\"fighting', 'for', 'our', 'freedom\"', 'line', 'as', 'a', 'recruitment', 'tool', 'is', 'nothing', 'but', 'nonsense.', '', '', 'If', 'it', 'was', 'all', 'about', 'the', 'pay', 'check', 'or', 'the', 'education', 'there', 'are', 'better', 'and', 'safer', 'ways', 'of', 'getting', 'those', 'things.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred', 'fight', 'freedom', 'lin', 'recruit', 'tool', 'noth', 'nonsens', 'pay', 'check', 'educ', 'bet', 'saf', 'way', 'get', 'thing'], ['one hundred', 'fight', 'freedom', 'line', 'recruitment', 'tool', 'nothing', 'nonsense', 'pay', 'check', 'education', 'better', 'safer', 'ways', 'get', 'things'])\n",
      "original document: \n",
      "['You', 'want', 'too', 'experience', 'a', 'different', 'culture', 'way', 'of', 'life', 'want', 'to', 'see', 'the', 'world/your', 'country', 'from', 'their', 'point', 'of', 'view,Their', 'history', '&amp;', 'food', 'Also', 'what', 'country', 'do', 'you', 'want', 'to', 'study', 'in?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'expery', 'diff', 'cult', 'way', 'lif', 'want', 'see', 'worldyo', 'country', 'point', 'viewtheir', 'hist', 'amp', 'food', 'also', 'country', 'want', 'study'], ['want', 'experience', 'different', 'culture', 'way', 'life', 'want', 'see', 'worldyour', 'country', 'point', 'viewtheir', 'history', 'amp', 'food', 'also', 'country', 'want', 'study'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['This', 'has', 'been', 'posted', 'like', '3', 'or', '4', 'times', 'in', 'the', 'last', 'few', 'days.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'lik', 'three', 'four', 'tim', 'last', 'day'], ['post', 'like', 'three', 'four', 'time', 'last', 'days'])\n",
      "original document: \n",
      "['I', 'agree.', 'But', 'damn,', 'someone', 'made', 'a', 'huge', 'error.', 'We', 'just', 'moved', 'a', 'house', 'across', 'a', 'street,', 'remodeled', 'it,', 'and', 'moved', 'it', 'back', 'with', 'less', 'damage.', '(Moved', 'out', 'of', 'a', 'flood', 'zone', 'for', 'FEMA', 'rule', 'work', 'around', ')']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'damn', 'someon', 'mad', 'hug', 'er', 'mov', 'hous', 'across', 'street', 'remodel', 'mov', 'back', 'less', 'dam', 'mov', 'flood', 'zon', 'fem', 'rul', 'work', 'around'], ['agree', 'damn', 'someone', 'make', 'huge', 'error', 'move', 'house', 'across', 'street', 'remodel', 'move', 'back', 'less', 'damage', 'move', 'flood', 'zone', 'fema', 'rule', 'work', 'around'])\n",
      "original document: \n",
      "['On', 'buzzfeed', 'in', 'a', 'week', 'and', 'your', \"uncle's\", 'facebook', 'in', 'three.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['buzzfee', 'week', 'unc', 'facebook', 'three'], ['buzzfeed', 'week', 'uncles', 'facebook', 'three'])\n",
      "original document: \n",
      "['En', 'resumen:', 'que', 'te', 'hinchen', 'los', 'huevos', 'a', 'niveles', 'de', 'explosión']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['en', 'resum', 'que', 'te', 'hinch', 'los', 'huevo', 'nivel', 'de', 'explod'], ['en', 'resumen', 'que', 'te', 'hinchen', 'los', 'huevos', 'niveles', 'de', 'explosion'])\n",
      "original document: \n",
      "['I', 'read', 'this', 'with', \"Rick's\", 'voice', 'from', '\"Rick', 'and', 'Morty\"', 'and', 'that', 'was', 'absolutely', 'glorious', 'lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['read', 'rick', 'voic', 'rick', 'morty', 'absolv', 'glory', 'lol'], ['read', 'rick', 'voice', 'rick', 'morty', 'absolutely', 'glorious', 'lol'])\n",
      "original document: \n",
      "['so', 'many', 'African', 'guys', 'so', 'obviously', 'well', 'off', 'with', 'their', 'designer', 'clothes', 'and', 'accessories', 'hustled', 'me', 'for', 'money', 'handouts', 'that', 'i', \"don't\", 'know', 'what', 'to', 'think', 'of', 'it', 'anymore,', 'really', 'perplexing']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'afr', 'guy', 'obvy', 'wel', 'design', 'cloth', 'access', 'hustl', 'money', 'handout', 'dont', 'know', 'think', 'anym', 'real', 'perplex'], ['many', 'african', 'guy', 'obviously', 'well', 'designer', 'clothe', 'accessories', 'hustle', 'money', 'handouts', 'dont', 'know', 'think', 'anymore', 'really', 'perplex'])\n",
      "original document: \n",
      "['one', 'random', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"It's\", 'free', 'inhabitants', 'all', 'the', 'way', 'down.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fre', 'inhabit', 'way'], ['free', 'inhabitants', 'way'])\n",
      "original document: \n",
      "['#FriendOfThePod']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['friendofthepod'], ['friendofthepod'])\n",
      "original document: \n",
      "['143417405|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'w371oNMY)\\n\\n2008:', 'Bob', 'Barr', 'and', 'I', 'think', 'democratic', 'downticket.\\n2012:', 'Aleppo', 'and', 'I', 'think', 'republican', 'downticket.\\n2016:', 'Trump', 'and', 'all', 'republican', \"downticket.\\n\\nI'll\", 'vote', 'democrat', 'if', \"it's\", 'a', 'Jim', 'Webb', 'style', 'of', 'DINO', 'or', 'blue', 'dog', 'democrat.', 'In', 'other', 'words', 'I', 'will', 'never', 'be', 'voting', 'democrat.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, four hundred and fiv', 'gt', 'unit', 'stat', 'anonym', 'id', 'w371onmy\\n\\n2008', 'bob', 'bar', 'think', 'democr', 'downticket\\n2012', 'aleppo', 'think', 'republ', 'downticket\\n2016', 'trump', 'republ', 'downticket\\n\\nill', 'vot', 'democr', 'jim', 'web', 'styl', 'dino', 'blu', 'dog', 'democr', 'word', 'nev', 'vot', 'democrat\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, four hundred and five', 'gt', 'unite', 'state', 'anonymous', 'id', 'w371onmy\\n\\n2008', 'bob', 'barr', 'think', 'democratic', 'downticket\\n2012', 'aleppo', 'think', 'republican', 'downticket\\n2016', 'trump', 'republican', 'downticket\\n\\nill', 'vote', 'democrat', 'jim', 'webb', 'style', 'dino', 'blue', 'dog', 'democrat', 'word', 'never', 'vote', 'democrat\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"Wasn't\", 'saying', 'it', 'was', 'that', 'way', 'for', 'everyone,', 'I', 'specifically', 'avoided', 'saying', '\"all\"', 'for', 'that', 'reason.', '', '/shrug', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wasnt', 'say', 'way', 'everyon', 'spec', 'avoid', 'say', 'reason', 'shrug', '\\n'], ['wasnt', 'say', 'way', 'everyone', 'specifically', 'avoid', 'say', 'reason', 'shrug', '\\n'])\n",
      "original document: \n",
      "['I', 'think', 'you', 'guys', 'are', 'pretty', 'solid', 'honestly.', \"You've\", 'really', 'only', 'struggled', 'in', 'our', 'game,', 'as', 'far', 'as', 'I', 'can', 'tell.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'guy', 'pretty', 'solid', 'honest', 'youv', 'real', 'struggled', 'gam', 'far', 'tel'], ['think', 'guy', 'pretty', 'solid', 'honestly', 'youve', 'really', 'struggle', 'game', 'far', 'tell'])\n",
      "original document: \n",
      "['Which', 'they', 'have', 'in', 'multitudes', 'of', 'other', 'call', 'of', 'duties', 'its', 'fun', 'to', 'go', 'back', 'to', 'ww2', 'gunplay', 'and', 'no', 'exo', 'movements.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['multitud', 'cal', 'duty', 'fun', 'go', 'back', 'ww2', 'gunplay', 'exo', 'mov'], ['multitudes', 'call', 'duties', 'fun', 'go', 'back', 'ww2', 'gunplay', 'exo', 'movements'])\n",
      "original document: \n",
      "['Pilot', 'who', 'is', 'scared', 'of', 'heights', 'reporting', 'in.', 'Yes', \"it's\", 'a', 'thing,', 'and', \"I've\", 'got', 'several', 'friends', 'who', 'are', 'also', 'pilots', 'and', 'also', \"don't\", 'like', 'heights.', 'For', 'me', 'at', 'least,', \"it's\", 'not', 'heights', 'themselves,', \"it's\", 'what', \"I'm\", 'on/in.', 'On', 'cliffs,', 'tall', 'bridges,', 'buildings,', 'ect.', 'I', 'loose', 'my', 'shit.', 'Put', 'me', 'in', 'an', 'airplane,', 'and', \"I'm\", 'chill', 'as', 'fuck.', \"I'm\", 'not', 'sure', 'why,', 'but', 'it', 'might', 'have', 'something', 'to', 'do', 'with', 'being', 'a', 'bit', 'of', 'a', 'control', 'freak.', 'If', \"I'm\", 'in', 'an', 'airplane,', 'in', 'my', 'mind,', \"there's\", 'a', 'whole', 'lot', 'less', 'that', 'can', 'go', 'wrong,', 'because', \"I've\", 'done', 'my', 'preflight,', 'and', 'followed', 'all', 'the', 'checklists,', 'and', 'if', \"I'm\", 'not', 'PIC,', 'then', 'I', 'know', 'that', 'whoever', 'is', 'flying', 'knows', 'what', \"they're\", 'doing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pilot', 'scar', 'height', 'report', 'ye', 'thing', 'iv', 'got', 'sev', 'friend', 'also', 'pilot', 'also', 'dont', 'lik', 'height', 'least', 'height', 'im', 'onin', 'cliff', 'tal', 'bridg', 'build', 'ect', 'loos', 'shit', 'put', 'airpl', 'im', 'chil', 'fuck', 'im', 'sur', 'might', 'someth', 'bit', 'control', 'freak', 'im', 'airpl', 'mind', 'ther', 'whol', 'lot', 'less', 'go', 'wrong', 'iv', 'don', 'preflight', 'follow', 'checkl', 'im', 'pic', 'know', 'whoev', 'fly', 'know', 'theyr'], ['pilot', 'scar', 'heights', 'report', 'yes', 'thing', 'ive', 'get', 'several', 'friends', 'also', 'pilot', 'also', 'dont', 'like', 'heights', 'least', 'heights', 'im', 'onin', 'cliffs', 'tall', 'bridge', 'build', 'ect', 'loose', 'shit', 'put', 'airplane', 'im', 'chill', 'fuck', 'im', 'sure', 'might', 'something', 'bite', 'control', 'freak', 'im', 'airplane', 'mind', 'theres', 'whole', 'lot', 'less', 'go', 'wrong', 'ive', 'do', 'preflight', 'follow', 'checklists', 'im', 'pic', 'know', 'whoever', 'fly', 'know', 'theyre'])\n",
      "original document: \n",
      "[\"I'm\", 'happy', 'for', 'you.', '', 'I', 'was', 'watching', 'Stef', 'Sanjati', 'FFS', 'video', 'and', 'hers', 'was', 'paid', 'for', 'by', 'a', 'gofundme', 'campaign.', '', 'I', 'think', 'it', 'was', 'around', '$37,000', 'for', 'forehead,', 'upper', 'lip,', 'trachea', 'shave,', 'jaw', 'and', 'chin.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'happy', 'watch', 'stef', 'sanjat', 'ffs', 'video', 'paid', 'gofundm', 'campaign', 'think', 'around', 'thirty-seven thousand', 'forehead', 'up', 'lip', 'trache', 'shav', 'jaw', 'chin'], ['im', 'happy', 'watch', 'stef', 'sanjati', 'ffs', 'video', 'pay', 'gofundme', 'campaign', 'think', 'around', 'thirty-seven thousand', 'forehead', 'upper', 'lip', 'trachea', 'shave', 'jaw', 'chin'])\n",
      "original document: \n",
      "['I', 'think', 'you', 'misunderstood', 'my', 'comment.', 'I', 'agree', 'with', 'you', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'misunderstood', 'com', 'agr', 'though'], ['think', 'misunderstand', 'comment', 'agree', 'though'])\n",
      "original document: \n",
      "['FKM']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fkm'], ['fkm'])\n",
      "original document: \n",
      "[\"Wouldn't\", 'be', 'surprised', 'if', 'Garza', 'is', 'actually', 'hurting.', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'surpr', 'garz', 'act', 'hurt'], ['wouldnt', 'surprise', 'garza', 'actually', 'hurt'])\n",
      "original document: \n",
      "['&gt;Definitely', 'defective?', 'The', 'chip', 'could', 'be', 'fine;', 'the', 'only', 'way', 'to', 'know', 'for', 'sure', 'is', 'to', 'test.\\n\\nIt', 'was', 'a', 'fabrication', 'issue(not', 'a', 'bug).', '', \"It's\", 'been', 'replicated', 'at', 'level1techs', 'as', 'well.', '', 'So', 'yes,', \"it's\", 'definitely', 'defective', 'if', 'you', 'use', 'compilers', 'on', 'Linux(which', 'most', 'people', \"don't\", 'do', 'in', 'benchmarks).', '', 'Pre-week', '25', 'has', 'the', 'compiler', 'issue.', '', 'Post-25', \"doesn't.\\n\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtdefinitely', 'defect', 'chip', 'could', 'fin', 'way', 'know', 'sur', 'test\\n\\nit', 'fabr', 'issuenot', 'bug', 'reply', 'level1techs', 'wel', 'ye', 'definit', 'defect', 'us', 'compil', 'linuxwhich', 'peopl', 'dont', 'benchmark', 'preweek', 'twenty-five', 'compil', 'issu', 'post25', 'doesnt\\n'], ['gtdefinitely', 'defective', 'chip', 'could', 'fine', 'way', 'know', 'sure', 'test\\n\\nit', 'fabrication', 'issuenot', 'bug', 'replicate', 'level1techs', 'well', 'yes', 'definitely', 'defective', 'use', 'compilers', 'linuxwhich', 'people', 'dont', 'benchmarks', 'preweek', 'twenty-five', 'compiler', 'issue', 'post25', 'doesnt\\n'])\n",
      "original document: \n",
      "['Got', 'it.', 'I', \"don't\", 'have', 'a', 'particular', 'attachment', 'to', 'either', 'so', 'I', 'guess', \"I'll\", 'be', 'putting', 'off', 'rerolling', 'until', 'another', 'event.', 'Thanks', 'for', 'the', 'explanation', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'dont', 'particul', 'attach', 'eith', 'guess', 'il', 'put', 'rerol', 'anoth', 'ev', 'thank', 'expl', 'though'], ['get', 'dont', 'particular', 'attachment', 'either', 'guess', 'ill', 'put', 'rerolling', 'another', 'event', 'thank', 'explanation', 'though'])\n",
      "original document: \n",
      "['Question', 'from', 'the', 'woodwork:', 'is', 'it', 'okay', 'to', 'come', 'out?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quest', 'woodwork', 'okay', 'com'], ['question', 'woodwork', 'okay', 'come'])\n",
      "original document: \n",
      "['Thanks', 'for', 'listening!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'list'], ['thank', 'listen'])\n",
      "original document: \n",
      "['That', 'concentration', 'tho', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'tho'], ['concentration', 'tho'])\n",
      "original document: \n",
      "['The', 'reasons', 'for', 'them', 'not', 'using', 'it', 'are', 'vauge,', 'we', \"don't\", 'know', 'exactly', 'why,', 'even', 'in', 'Gods', 'of', 'Mars', 'they', \"don't\", 'specify', 'EXACTLY...since', \"that's\", 'the', 'freaking', 'mantra', 'of', 'most', 'black', 'library', 'books,', 'they', 'never', 'want', 'to', 'be', 'specific', 'unless', 'it', 'leads', 'to', 'a', 'plot', 'which', 'IS', 'ambiguous.', '\\n\\nbut', 'as', 'for', 'it', 'being', 'part', 'of', 'necrontyr', 'tech.\\n\\n&gt;Kotov', 'sighed', 'and', 'nodded', 'as', 'if', 'Roboute', 'had', 'passed', 'some', 'kind', 'of', 'test.‘Very', 'well,', 'Mister', 'Surcouf,', 'I', 'believe', 'you', 'may', 'be', 'correct.', 'Perhaps', 'some', 'aspect', 'of', 'necrontyr', 'technology', 'does', 'lie', 'at', 'the', 'heart', 'of', 'the', 'Breath', 'of', 'the', 'Gods,', 'and', 'if', 'that', 'is', 'the', 'case,', 'then', 'it', 'is', 'doubly', 'imperative', 'we', 'prevent', 'Telok', 'from', 'leaving', 'this', 'world.’\\n‘Why?’', 'said', 'Anders,', '‘I', 'mean,', 'besides', 'the', 'obvious?’\\n‘Because', 'if', 'there', 'is', 'any', 'truth', 'to', 'the', 'old', 'legends,', 'then', 'it', 'is', 'entirely', 'possible', 'that', 'a', 'vast', 'shard', 'of', 'one', 'of', 'the', 'ancient', 'necrontyr', 'gods', 'lies', 'entombed', 'within', 'the', 'Noctis', 'Labyrinthus.’\\nAnd', 'suddenly', 'it', 'all', 'made', 'a', 'twisted', 'kind', 'of', 'sense', 'to', 'Roboute.', 'He', 'turned', 'to', 'Bielanna,', 'who', 'appeared', 'to', 'be', 'studiously', 'ignoring', 'their', 'conversation.\\n‘You', 'knew,', 'didn’t', 'you?’', 'he', 'said.', '‘You', 'said', 'as', 'much', 'back', 'in', 'the', 'cavern.', 'What', 'did', 'you', 'call', 'it?', '“The', 'infernal', 'engine', 'of', 'the', 'Yngir?”', 'I’m', 'going', 'to', 'assume', 'that’s', 'your', 'word', 'for', 'the', 'necrontyr', 'gods.’\\nBielanna', 'nodded', 'slowly.\\n‘Now', 'you', 'see', 'why', 'we', 'fought', 'so', 'hard', 'to', 'stop', 'you,’', 'she', 'said.', '‘And', 'why', 'we', 'now', 'spill', 'our', 'blood', 'to', 'help', 'you.’']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'us', 'vaug', 'dont', 'know', 'exact', 'ev', 'god', 'mar', 'dont', 'spec', 'exactlysint', 'that', 'freak', 'mantr', 'black', 'libr', 'book', 'nev', 'want', 'spec', 'unless', 'lead', 'plot', 'ambigu', '\\n\\nbut', 'part', 'necrontyr', 'tech\\n\\ngtkotov', 'sigh', 'nod', 'robout', 'pass', 'kind', 'testvery', 'wel', 'mist', 'surcouf', 'believ', 'may', 'correct', 'perhap', 'aspect', 'necrontyr', 'technolog', 'lie', 'heart', 'brea', 'god', 'cas', 'doubl', 'imp', 'prev', 'telok', 'leav', 'world\\nwhy', 'said', 'and', 'mean', 'besid', 'obvious\\nbecause', 'tru', 'old', 'legend', 'entir', 'poss', 'vast', 'shard', 'on', 'ant', 'necrontyr', 'god', 'lie', 'entomb', 'within', 'noct', 'labyrinthus\\nand', 'sud', 'mad', 'twist', 'kind', 'sens', 'robout', 'turn', 'bielann', 'appear', 'study', 'ign', 'conversation\\nyou', 'knew', 'didnt', 'said', 'said', 'much', 'back', 'cavern', 'cal', 'infern', 'engin', 'yngir', 'im', 'going', 'assum', 'that', 'word', 'necrontyr', 'gods\\nbielanna', 'nod', 'slowly\\nnow', 'see', 'fought', 'hard', 'stop', 'said', 'spil', 'blood', 'help'], ['reason', 'use', 'vauge', 'dont', 'know', 'exactly', 'even', 'gods', 'mar', 'dont', 'specify', 'exactlysince', 'thats', 'freak', 'mantra', 'black', 'library', 'book', 'never', 'want', 'specific', 'unless', 'lead', 'plot', 'ambiguous', '\\n\\nbut', 'part', 'necrontyr', 'tech\\n\\ngtkotov', 'sigh', 'nod', 'roboute', 'pass', 'kind', 'testvery', 'well', 'mister', 'surcouf', 'believe', 'may', 'correct', 'perhaps', 'aspect', 'necrontyr', 'technology', 'lie', 'heart', 'breath', 'gods', 'case', 'doubly', 'imperative', 'prevent', 'telok', 'leave', 'world\\nwhy', 'say', 'anders', 'mean', 'besides', 'obvious\\nbecause', 'truth', 'old', 'legends', 'entirely', 'possible', 'vast', 'shard', 'one', 'ancient', 'necrontyr', 'gods', 'lie', 'entomb', 'within', 'noctis', 'labyrinthus\\nand', 'suddenly', 'make', 'twist', 'kind', 'sense', 'roboute', 'turn', 'bielanna', 'appear', 'studiously', 'ignore', 'conversation\\nyou', 'know', 'didnt', 'say', 'say', 'much', 'back', 'cavern', 'call', 'infernal', 'engine', 'yngir', 'im', 'go', 'assume', 'thats', 'word', 'necrontyr', 'gods\\nbielanna', 'nod', 'slowly\\nnow', 'see', 'fight', 'hard', 'stop', 'say', 'spill', 'blood', 'help'])\n",
      "original document: \n",
      "['He', 'had', 'it...', 'but', 'over', 'slid', 'the', 'bag...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['slid', 'bag'], ['slide', 'bag'])\n",
      "original document: \n",
      "['a', 'puppy']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['puppy'], ['puppy'])\n",
      "original document: \n",
      "['Your', '[**post**](https://www.reddit.com/r/twinpeaks/comments/73ig94/can_someone_translate_this_interview_with_monica/)', 'has', 'been', 'automatically', 'removed', 'because', 'every', 'post', 'title', 'Must', 'Begin', 'With', 'a', '**written**', '[bracketed', 'tag]', 'of', 'what', 'it', 'has', 'spoilers', 'for.', '**Pick', 'only', 'one!**', 'If', 'your', 'title', 'does', 'not', 'contain', 'one', 'of', 'the', '**exact', 'tags', 'from', 'this', 'list**', 'it', 'will', 'be', 'removed!', 'The', 'list', 'of', 'tags', 'is', 'as', 'followed.\\n\\n*', '[No', 'Spoilers]', '-', '(Your', 'post', 'has', 'no', 'spoilers', 'and', \"**won't\", 'generate', 'spoilery', 'comments**)\\n*', '[Original', 'Run]', '-(Your', 'post', 'has', 'spoilers', 'related', 'to', 'anything', 'prior', 'to', 'season', '3', 'including', 'the', 'show,', 'movie,', 'and', 'books)\\n*', '[All]', '-(Your', 'post', 'has', 'spoilers', 'for', 'season', '3', '*and', 'possibly', 'all', 'prior', 'content*)\\n\\n\\nPlease', '[resubmit', 'your', 'content.](https://www.reddit.com/r/twinpeaks/submit?title=%5BReplace%20This%20With%20Proper%20Tag%5D&amp;selftext=true)', 'with', 'the', 'proper', 'tag.\\n\\n***\\n\\n[Submission', 'Rules](https://www.reddit.com/r/twinpeaks/wiki/subredditrules#wiki_submission_rules)', '|', '[Spoiler', 'Guide](https://www.reddit.com/r/twinpeaks/wiki/spoilerpolicy)', '|', '[FAQ](https://www.reddit.com/r/twinpeaks/wiki/faq)\\n\\n\\n\\n', '***\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/twinpeaks)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['posthttpswwwredditcomrtwinpeakscomments73ig94can_someone_translate_this_interview_with_monica', 'autom', 'remov', 'every', 'post', 'titl', 'must', 'begin', 'writ', 'bracket', 'tag', 'spoil', 'pick', 'on', 'titl', 'contain', 'on', 'exact', 'tag', 'list', 'remov', 'list', 'tag', 'followed\\n\\n', 'spoil', 'post', 'spoil', 'wont', 'gen', 'spoilery', 'comments\\n', 'origin', 'run', 'post', 'spoil', 'rel', 'anyth', 'pri', 'season', 'three', 'includ', 'show', 'movy', 'books\\n', 'post', 'spoil', 'season', 'three', 'poss', 'pri', 'content\\n\\n\\nplease', 'resubmit', 'contenthttpswwwredditcomrtwinpeakssubmittitle5breplace20this20with20proper20tag5dampselftexttru', 'prop', 'tag\\n\\n\\n\\nsubmission', 'ruleshttpswwwredditcomrtwinpeakswikisubredditruleswiki_submission_rules', 'spoil', 'guidehttpswwwredditcomrtwinpeakswikispoilerpolicy', 'faqhttpswwwredditcomrtwinpeakswikifaq\\n\\n\\n\\n', '\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetortwinpeak', 'quest', 'concern'], ['posthttpswwwredditcomrtwinpeakscomments73ig94can_someone_translate_this_interview_with_monica', 'automatically', 'remove', 'every', 'post', 'title', 'must', 'begin', 'write', 'bracket', 'tag', 'spoilers', 'pick', 'one', 'title', 'contain', 'one', 'exact', 'tag', 'list', 'remove', 'list', 'tag', 'followed\\n\\n', 'spoilers', 'post', 'spoilers', 'wont', 'generate', 'spoilery', 'comments\\n', 'original', 'run', 'post', 'spoilers', 'relate', 'anything', 'prior', 'season', 'three', 'include', 'show', 'movie', 'books\\n', 'post', 'spoilers', 'season', 'three', 'possibly', 'prior', 'content\\n\\n\\nplease', 'resubmit', 'contenthttpswwwredditcomrtwinpeakssubmittitle5breplace20this20with20proper20tag5dampselftexttrue', 'proper', 'tag\\n\\n\\n\\nsubmission', 'ruleshttpswwwredditcomrtwinpeakswikisubredditruleswiki_submission_rules', 'spoiler', 'guidehttpswwwredditcomrtwinpeakswikispoilerpolicy', 'faqhttpswwwredditcomrtwinpeakswikifaq\\n\\n\\n\\n', '\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetortwinpeaks', 'question', 'concern'])\n",
      "original document: \n",
      "['Yes.\\n\\nhttps://alexandria-library.space/files/Ebooks/WorldTracker/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yes\\n\\nhttpsalexandrialibraryspacefilesebooksworldtracker'], ['yes\\n\\nhttpsalexandrialibraryspacefilesebooksworldtracker'])\n",
      "original document: \n",
      "['Cooking', 'Mama']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cook', 'mam'], ['cook', 'mama'])\n",
      "original document: \n",
      "['His', 'humor', 'has', 'always', 'benefitted', 'from', 'bombastic', 'and', 'spontaneous', 'energy', '-', 'in', 'this', 'he', 'looked', 'like', 'he', 'could', 'barely', 'contain', 'himself', 'from', 'saying', 'something', 'ridiculous', 'or', 'breaking', 'out', 'in', 'fits', 'of', 'laughter.', 'Or', 'madness.', 'Or', 'both.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hum', 'alway', 'benefit', 'bombast', 'spont', 'energy', 'look', 'lik', 'could', 'bar', 'contain', 'say', 'someth', 'ridic', 'break', 'fit', 'laught', 'mad'], ['humor', 'always', 'benefit', 'bombastic', 'spontaneous', 'energy', 'look', 'like', 'could', 'barely', 'contain', 'say', 'something', 'ridiculous', 'break', 'fit', 'laughter', 'madness'])\n",
      "original document: \n",
      "['Probably', 'the', 'latest', 'ATH?', '🤔']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'latest', 'ath'], ['probably', 'latest', 'ath'])\n",
      "original document: \n",
      "['Mega', 'big', 'ass', 'fries...you', 'have', 'been', 'deemed', 'an', 'unfit', 'monther.', 'Thanks', 'for', 'choosing', 'Carl’s', 'JR.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meg', 'big', 'ass', 'friesyou', 'deem', 'unfit', 'month', 'thank', 'choos', 'carl', 'jr'], ['mega', 'big', 'ass', 'friesyou', 'deem', 'unfit', 'monther', 'thank', 'choose', 'carls', 'jr'])\n",
      "original document: \n",
      "['Would', 'like', 'to', 'know', 'if', 'I', 'messed', 'up', 'on', 'making', 'this', 'map', 'too', 'horribly.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lik', 'know', 'mess', 'mak', 'map', 'horr'], ['would', 'like', 'know', 'mess', 'make', 'map', 'horribly'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"I'll\", 'take', 'one!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'tak', 'on'], ['ill', 'take', 'one'])\n",
      "original document: \n",
      "['Now', 'I', \"don't\", 'care.', 'I', 'actually', 'prefer', 'Doom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'car', 'act', 'pref', 'doom'], ['dont', 'care', 'actually', 'prefer', 'doom'])\n",
      "original document: \n",
      "['Depending', 'on', 'the', 'size', 'of', 'the', 'lens', \"you're\", 'using,', 'you', 'can', 'sometimes', 'get', 'decent', 'results', 'by', 'getting', 'right', 'up', 'to', 'a', 'chain', 'link', 'fence', 'and', 'shooting', '\"through\"', 'it.', '\\n\\nAs', 'it', 'stands,', \"there's\", 'no', 'real', 'subject', 'here.', 'The', \"viewers'\", 'eyes', 'just', \"aren't\", 'drawn', 'to', 'anything', 'in', 'particular.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'siz', 'len', 'yo', 'us', 'sometim', 'get', 'dec', 'result', 'get', 'right', 'chain', 'link', 'fent', 'shoot', '\\n\\nas', 'stand', 'ther', 'real', 'subject', 'view', 'ey', 'ar', 'drawn', 'anyth', 'particul'], ['depend', 'size', 'lens', 'youre', 'use', 'sometimes', 'get', 'decent', 'result', 'get', 'right', 'chain', 'link', 'fence', 'shoot', '\\n\\nas', 'stand', 'theres', 'real', 'subject', 'viewers', 'eye', 'arent', 'draw', 'anything', 'particular'])\n",
      "original document: \n",
      "['Wooooo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wooooo'], ['wooooo'])\n",
      "original document: \n",
      "['Come', 'on', 'Truex', 'I', 'want', 'you', 'to', 'win', 'really', 'badly.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['com', 'truex', 'want', 'win', 'real', 'bad'], ['come', 'truex', 'want', 'win', 'really', 'badly'])\n",
      "original document: \n",
      "['Will', 'have', 'to', 'disagree', 'considering', 'they', 'aint.', 'LOL', 'people', 'thumb', 'down', 'like', 'total', 'bots.', '', 'Both', 'are', 'offensive', 'abilities', 'and', 'one', 'is', 'Doomfist', 'primary', 'weapon', 'and', 'allows', 'him', 'to', 'take', 'range', 'started', 'attack.', 'The', 'other', 'is', 'a', 'long', 'range', 'attack', 'for', 'a', 'melee.', 'They', 'both', 'offensive', 'and', 'kill', 'people', 'and', 'do', 'damage', 'but', 'Doomfist', 'attack', 'only', 'hits', 'one', 'target', 'with', 'damage', 'whilst', 'Firestrike', 'goes', 'through', 'all', 'targets.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['disagr', 'consid', 'aint', 'lol', 'peopl', 'thumb', 'lik', 'tot', 'bot', 'offend', 'abl', 'on', 'doomf', 'prim', 'weapon', 'allow', 'tak', 'rang', 'start', 'attack', 'long', 'rang', 'attack', 'mel', 'offend', 'kil', 'peopl', 'dam', 'doomf', 'attack', 'hit', 'on', 'target', 'dam', 'whilst', 'firestrik', 'goe', 'target'], ['disagree', 'consider', 'aint', 'lol', 'people', 'thumb', 'like', 'total', 'bots', 'offensive', 'abilities', 'one', 'doomfist', 'primary', 'weapon', 'allow', 'take', 'range', 'start', 'attack', 'long', 'range', 'attack', 'melee', 'offensive', 'kill', 'people', 'damage', 'doomfist', 'attack', 'hit', 'one', 'target', 'damage', 'whilst', 'firestrike', 'go', 'target'])\n",
      "original document: \n",
      "['Sad', 'but', 'true']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'tru'], ['sad', 'true'])\n",
      "original document: \n",
      "['I', 'love', 'this.', 'When', 'my', 'mother', 'was', 'in', 'the', 'hospital', 'after', 'being', 'diagnosed', 'with', \"non-Hodgkin's\", 'lymphoma,', 'her', '(very', 'healthy', 'looking)', 'doctor', 'told', 'her', 'that', 'she', 'had', 'beat', 'the', 'same', 'cancer', 'herself', 'many', 'years', 'ago.', 'It', 'was', 'an', 'incredible', 'ray', 'of', 'hope', 'for', 'my', 'mom.', 'No', 'doubt', 'the', 'kids', 'that', 'this', 'girl', 'treats', 'will', 'feel', 'as', 'hopeful', 'when', 'they', 'find', 'out', 'she', 'beat', 'cancer', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'moth', 'hospit', 'diagnos', 'nonhodgkin', 'lymphom', 'healthy', 'look', 'doct', 'told', 'beat', 'cant', 'many', 'year', 'ago', 'incred', 'ray', 'hop', 'mom', 'doubt', 'kid', 'girl', 'tre', 'feel', 'hop', 'find', 'beat', 'cant'], ['love', 'mother', 'hospital', 'diagnose', 'nonhodgkins', 'lymphoma', 'healthy', 'look', 'doctor', 'tell', 'beat', 'cancer', 'many', 'years', 'ago', 'incredible', 'ray', 'hope', 'mom', 'doubt', 'kid', 'girl', 'treat', 'feel', 'hopeful', 'find', 'beat', 'cancer'])\n",
      "original document: \n",
      "['Yeah,', 'but', 'has', 'he', 'knocked', 'out', 'George', 'Groves', 'in', 'front', 'of', '80,000', 'fans', 'at', 'Wembley', 'Stadium?', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'knock', 'georg', 'grov', 'front', 'eighty thousand', 'fan', 'wembley', 'stad'], ['yeah', 'knock', 'george', 'groves', 'front', 'eighty thousand', 'fan', 'wembley', 'stadium'])\n",
      "original document: \n",
      "['They', 'like', 'Hitler,', 'hate', 'Jews', 'and', 'deny', 'the', 'holocaust', 'happened,', 'you', 'can', 'determine', 'where', 'they', 'fit', 'in', 'from', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'hitl', 'hat', 'jew', 'deny', 'holocaust', 'hap', 'determin', 'fit'], ['like', 'hitler', 'hate', 'jews', 'deny', 'holocaust', 'happen', 'determine', 'fit'])\n",
      "original document: \n",
      "['No', 'worries,', 'I', 'knew', 'that,', 'just', 'having', 'a', 'bit', 'of', 'conversation.', 'I', \"wasn't\", 'at', 'all', 'offended.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['worry', 'knew', 'bit', 'convers', 'wasnt', 'offend'], ['worry', 'know', 'bite', 'conversation', 'wasnt', 'offend'])\n",
      "original document: \n",
      "['surely', 'not', 'when', 'waiting', 'for', 'any', 'switch', 'game', 'that', 'takes', 'forever', 'lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'wait', 'switch', 'gam', 'tak', 'forev', 'lol'], ['surely', 'wait', 'switch', 'game', 'take', 'forever', 'lol'])\n",
      "original document: \n",
      "['Od', 'crates.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['od', 'crat'], ['od', 'crate'])\n",
      "original document: \n",
      "['&gt;does', 'Blackbeard', 'already', 'found', 'out', 'about', 'poneglyphs?\\n\\nHe', 'better', 'have.', \"He's\", 'already', 'been', 'established', 'as', 'someone', 'who', 'does', 'his', 'research', 'on', 'things', 'of', 'interest', 'and', 'the', 'pieces', 'that', 'fit', 'into', 'his', 'plan', 'of', 'becoming', 'Pirate', 'King.', 'Also,', 'if', 'he', \"didn't\", 'know', 'before,', 'his', 'status', 'as', 'a', 'Yonko,', 'acquisition', 'of', 'a', 'large', 'crew', 'and', 'islands', \"should've\", 'provided', 'the', 'opportunity', 'for', 'him', 'to', 'have', 'discovered', 'that', 'at', 'some', 'point.\\n\\n&gt;and', 'his', 'whole', 'crew', 'gang', 'up', 'on', 'her', 'its', 'likely', \"he'll\", 'take', 'her', 'down\\n\\nIf', 'Blackbeard', 'still', 'needs', 'his', 'entire', 'crew', 'to', 'take', 'down', 'one', 'weakened', 'Yonko,', \"he's\", 'weak,', 'and', 'I', 'hope', 'that', \"isn't\", 'the', 'case.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtdoes', 'blackbeard', 'already', 'found', 'poneglyphs\\n\\nhe', 'bet', 'hes', 'already', 'est', 'someon', 'research', 'thing', 'interest', 'piec', 'fit', 'plan', 'becom', 'pir', 'king', 'also', 'didnt', 'know', 'stat', 'yonko', 'acquisit', 'larg', 'crew', 'island', 'shouldv', 'provid', 'opportun', 'discov', 'point\\n\\ngtand', 'whol', 'crew', 'gang', 'lik', 'hel', 'tak', 'down\\n\\nif', 'blackbeard', 'stil', 'nee', 'entir', 'crew', 'tak', 'on', 'weak', 'yonko', 'hes', 'weak', 'hop', 'isnt', 'cas'], ['gtdoes', 'blackbeard', 'already', 'find', 'poneglyphs\\n\\nhe', 'better', 'hes', 'already', 'establish', 'someone', 'research', 'things', 'interest', 'piece', 'fit', 'plan', 'become', 'pirate', 'king', 'also', 'didnt', 'know', 'status', 'yonko', 'acquisition', 'large', 'crew', 'islands', 'shouldve', 'provide', 'opportunity', 'discover', 'point\\n\\ngtand', 'whole', 'crew', 'gang', 'likely', 'hell', 'take', 'down\\n\\nif', 'blackbeard', 'still', 'need', 'entire', 'crew', 'take', 'one', 'weaken', 'yonko', 'hes', 'weak', 'hope', 'isnt', 'case'])\n",
      "original document: \n",
      "['Oh', 'sorry,', 'missed', 'what', 'you', 'were', 'trying', 'to', 'say.', \"I'm\", 'honestly', 'not', 'sure', 'then.', \"That's\", 'super', 'weird.', 'Canadian', 'is', 'like', '70', 'cents', 'on', 'the', 'US', 'dollar', 'right', 'now,', 'yeah?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sorry', 'miss', 'try', 'say', 'im', 'honest', 'sur', 'that', 'sup', 'weird', 'canad', 'lik', 'seventy', 'cent', 'us', 'doll', 'right', 'yeah'], ['oh', 'sorry', 'miss', 'try', 'say', 'im', 'honestly', 'sure', 'thats', 'super', 'weird', 'canadian', 'like', 'seventy', 'cents', 'us', 'dollar', 'right', 'yeah'])\n",
      "original document: \n",
      "['Well,', 'getting', 'naked', 'would', 'only', 'work', 'IF', 'Kiyo', 'sees', 'you', 'as', 'Anchin.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'get', 'nak', 'would', 'work', 'kiyo', 'see', 'anchin'], ['well', 'get', 'naked', 'would', 'work', 'kiyo', 'see', 'anchin'])\n",
      "original document: \n",
      "['I', 'like', 'the', 'whole', 'aesthetic', 'and', 'mood', 'of', 'it.', 'It', 'rained', 'pretty', 'hard', 'last', 'night', 'and', 'i', 'live', 'in', 'a', 'city', 'without', 'led', 'street', 'lights,', 'so', 'the', 'amber', 'glow', 'in', 'the', 'puddles', 'looked', 'pretty', 'nice.', 'I', 'had', 'to', 'bike', 'home', 'though.', 'That', 'sucked.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'whol', 'aesthet', 'mood', 'rain', 'pretty', 'hard', 'last', 'night', 'liv', 'city', 'without', 'led', 'street', 'light', 'amb', 'glow', 'puddl', 'look', 'pretty', 'nic', 'bik', 'hom', 'though', 'suck'], ['like', 'whole', 'aesthetic', 'mood', 'rain', 'pretty', 'hard', 'last', 'night', 'live', 'city', 'without', 'lead', 'street', 'light', 'amber', 'glow', 'puddle', 'look', 'pretty', 'nice', 'bike', 'home', 'though', 'suck'])\n",
      "original document: \n",
      "['What', 'was', 'that?\\nDid', 'I', 'hear', 'a', 'running', 'game?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that\\ndid', 'hear', 'run', 'gam'], ['that\\ndid', 'hear', 'run', 'game'])\n",
      "original document: \n",
      "['\"Fuck', 'Dee', 'Gordon\"', '-', 'Rio', 'Ruiz']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'dee', 'gordon', 'rio', 'ruiz'], ['fuck', 'dee', 'gordon', 'rio', 'ruiz'])\n",
      "original document: \n",
      "['I', 'played', 'it', 'a', 'few', 'times', 'when', 'I', 'first', 'started.', 'Then', 'I', 'honestly', 'forgot', 'it', 'was', 'even', 'part', 'of', 'the', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'tim', 'first', 'start', 'honest', 'forgot', 'ev', 'part', 'gam'], ['play', 'time', 'first', 'start', 'honestly', 'forget', 'even', 'part', 'game'])\n",
      "original document: \n",
      "['I', 'done', 'a', 'bamboozle.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['don', 'bamboozl'], ['do', 'bamboozle'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"What's\", 'spectrum?\\n\\nMore', 'specifically', \"where's\", 'spectrum.', '\\n\\nGFAQs', 'is', 'also', 'yelling', 'about', 'DCs', 'from', 'people', 'who', 'I', 'believe', 'are', 'in', 'the', 'UK.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'spectrum\\n\\nmore', 'spec', 'wher', 'spectr', '\\n\\ngfaqs', 'also', 'yel', 'dcs', 'peopl', 'believ', 'uk'], ['whats', 'spectrum\\n\\nmore', 'specifically', 'wheres', 'spectrum', '\\n\\ngfaqs', 'also', 'yell', 'dcs', 'people', 'believe', 'uk'])\n",
      "original document: \n",
      "['Hello,', 'this', 'is', 'to', 'let', 'you', 'know', 'that', 'your', 'post', 'is', 'up', 'and', 'running.\\n\\nSince', 'you', 'are', 'looking', 'for', 'a', 'something', 'in', 'particular,', 'please', 'make', 'sure', 'that', 'you', '[read', 'all', 'rules](https://www.reddit.com/r/sneakermarket/about/rules/)', 'to', 'ensure', 'that', 'your', 'post', \"isn't\", 'removed', 'by', 'a', 'moderator.\\n\\nA', 'few', 'reminders:\\n\\n1.', 'Sneakers', 'only!', 'You', 'are', 'only', 'allowed', 'to', 'look', 'for', 'sneakers', 'on', 'this', 'sub.', 'Anything', 'else', '(bots,', 'services,', 'other', 'clothing', 'items)', 'will', 'be', 'removed', '**without', 'warning**.\\n\\n2.', 'If', \"you're\", 'looking', 'for', 'something', 'Deadstock/New,', 'name', 'the', 'price', 'you', 'are', 'willing', 'to', 'pay.', 'Otherwise,', 'your', 'post', 'will', 'be', 'removed', 'by', 'a', 'moderator', '**without', 'warning**.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/sneakermarket)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hello', 'let', 'know', 'post', 'running\\n\\nsince', 'look', 'someth', 'particul', 'pleas', 'mak', 'sur', 'read', 'ruleshttpswwwredditcomrsneakermarketaboutr', 'ens', 'post', 'isnt', 'remov', 'moderator\\n\\na', 'reminders\\n\\n1', 'sneak', 'allow', 'look', 'sneak', 'sub', 'anyth', 'els', 'bot', 'serv', 'cloth', 'item', 'remov', 'without', 'warning\\n\\n2', 'yo', 'look', 'someth', 'deadstocknew', 'nam', 'pric', 'wil', 'pay', 'otherw', 'post', 'remov', 'mod', 'without', 'warning\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorsneakermarket', 'quest', 'concern'], ['hello', 'let', 'know', 'post', 'running\\n\\nsince', 'look', 'something', 'particular', 'please', 'make', 'sure', 'read', 'ruleshttpswwwredditcomrsneakermarketaboutrules', 'ensure', 'post', 'isnt', 'remove', 'moderator\\n\\na', 'reminders\\n\\n1', 'sneakers', 'allow', 'look', 'sneakers', 'sub', 'anything', 'else', 'bots', 'service', 'clothe', 'items', 'remove', 'without', 'warning\\n\\n2', 'youre', 'look', 'something', 'deadstocknew', 'name', 'price', 'will', 'pay', 'otherwise', 'post', 'remove', 'moderator', 'without', 'warning\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorsneakermarket', 'question', 'concern'])\n",
      "original document: \n",
      "['yeah,', 'Davion', 'Hall', 'is', 'not', 'great,', 'sadly,', 'Lynch', 'seems', 'to', 'be', 'doing', 'ok', 'though']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'dav', 'hal', 'gre', 'sad', 'lynch', 'seem', 'ok', 'though'], ['yeah', 'davion', 'hall', 'great', 'sadly', 'lynch', 'seem', 'ok', 'though'])\n",
      "original document: \n",
      "['I', \"don't\", 'even', 'know', 'what', 'happened', 'exactly.', '', 'Ugh', 'paying', '99', 'dollar', 'deductible', 'to', 'replace', 'is', 'gonna', 'hurt.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'ev', 'know', 'hap', 'exact', 'ugh', 'pay', 'ninety-nine', 'doll', 'deduct', 'replac', 'gonn', 'hurt'], ['dont', 'even', 'know', 'happen', 'exactly', 'ugh', 'pay', 'ninety-nine', 'dollar', 'deductible', 'replace', 'gonna', 'hurt'])\n",
      "original document: \n",
      "['Sorry', 'about', 'my', 'limitations.', \"I'm\", 'doing', 'the', 'best', 'I', 'can!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'limit', 'im', 'best'], ['sorry', 'limitations', 'im', 'best'])\n",
      "original document: \n",
      "['And', 'I', 'lost', 'an', 'Arena', 'game', 'thinking', 'Blade', 'Furry', 'with', 'a', 'LS', 'weapon', 'would', 'AOE', 'heal', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lost', 'aren', 'gam', 'think', 'blad', 'furry', 'ls', 'weapon', 'would', 'ao', 'heal'], ['lose', 'arena', 'game', 'think', 'blade', 'furry', 'ls', 'weapon', 'would', 'aoe', 'heal'])\n",
      "original document: \n",
      "['16,397']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sixteen thousand, three hundred and ninety-sev'], ['sixteen thousand, three hundred and ninety-seven'])\n",
      "original document: \n",
      "['SemiAmusingBot', 'says:', 'I', 'am', 'now', 'installed', 'on', 'a', 'VPS.', 'reply', '#3', 'test']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['semiamusingbot', 'say', 'instal', 'vps', 'reply', 'three', 'test'], ['semiamusingbot', 'say', 'instal', 'vps', 'reply', 'three', 'test'])\n",
      "original document: \n",
      "['Good', 'job', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'job'], ['good', 'job'])\n",
      "original document: \n",
      "['Max', \"McCormick's\", 'just', 'updated', 'his', 'title', 'on', 'LinkedIn', 'from', '\"Incoming', 'NHL', 'Regular\"', 'to', '\"Incoming', 'AHL', 'Regular\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['max', 'mccormicks', 'upd', 'titl', 'linkedin', 'incom', 'nhl', 'regul', 'incom', 'ahl', 'regul'], ['max', 'mccormicks', 'update', 'title', 'linkedin', 'incoming', 'nhl', 'regular', 'incoming', 'ahl', 'regular'])\n",
      "original document: \n",
      "['Yeah.', 'A', 'non', 'contact', 'voltage', 'tester', 'would', 'be', 'easier.', 'Check', 'and', 'see', 'if', 'the', 'wires', 'have', 'voltage', 'in', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'non', 'contact', 'volt', 'test', 'would', 'easy', 'check', 'see', 'wir', 'volt'], ['yeah', 'non', 'contact', 'voltage', 'tester', 'would', 'easier', 'check', 'see', 'wire', 'voltage'])\n",
      "original document: \n",
      "['Nah,', 'Turner', 'is', 'an', 'idiot', 'but', 'not', 'an', 'edgelord', 'idiot.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah', 'turn', 'idiot', 'edgelord', 'idiot'], ['nah', 'turner', 'idiot', 'edgelord', 'idiot'])\n",
      "original document: \n",
      "['Well', \"I'm\", 'trying', 'to', 'keep', 'it', 'discreet.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'im', 'try', 'keep', 'discreet'], ['well', 'im', 'try', 'keep', 'discreet'])\n",
      "original document: \n",
      "[\"Id's\", 'this', 'from', 'a', 'a', 'WikiHow', 'on', 'how', 'to', 'die', 'from', 'incurable', 'disease?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'wikihow', 'die', 'int', 'diseas'], ['ids', 'wikihow', 'die', 'incurable', 'disease'])\n",
      "original document: \n",
      "['Idk', 'what', 'ephylone', 'feels', 'like', 'but', 'if', 'its', 'like', 'meth', 'it', 'aint', 'that', 'bad,', 'now', 'if', 'it', 'burns', 'like', '2C-I', 'then', 'you', 'have', 'a', 'reason', 'to', 'complain\\n\\nYou', 'could', 'always', 'put', 'a', 'little', 'lidocaine', 'in', 'your', 'nose', 'first']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['idk', 'ephylon', 'feel', 'lik', 'lik', 'meth', 'aint', 'bad', 'burn', 'lik', '2ci', 'reason', 'complain\\n\\nyou', 'could', 'alway', 'put', 'littl', 'lidocain', 'nos', 'first'], ['idk', 'ephylone', 'feel', 'like', 'like', 'meth', 'aint', 'bad', 'burn', 'like', '2ci', 'reason', 'complain\\n\\nyou', 'could', 'always', 'put', 'little', 'lidocaine', 'nose', 'first'])\n",
      "original document: \n",
      "['/r/bosozoku', '', 'is', 'leaking', 'again.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rbosozoku', 'leak'], ['rbosozoku', 'leak'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Yeah.', 'In', 'theory', 'you', 'could', 'visit', 'a', 'half', 'dozen', 'countries', 'scattered', 'around', 'the', 'globe', 'in', 'a', 'single', 'day.\\n\\nWake', 'up', 'in', 'New', 'York,', 'have', 'breakfast', 'in', 'London,', 'a', 'business', 'meeting', 'in', 'Japan,', 'then', 'lunch', 'in', 'San', 'Francisco,', 'then', 'another', 'meeting', 'in', 'Sydney,', 'dinner', 'in', 'Shanghai', 'and', 'then', 'go', 'to', 'bed', 'in', 'Rome.\\n\\nI', 'dunno', 'why', \"you'd\", 'fly', 'to', 'another', 'country', 'for', 'meals', 'before', 'moving', 'on', 'to', 'another', 'for', 'the', 'next', 'meeting,', 'but', 'you', 'could', 'do', 'it.', '\\n\\nI', 'could', 'see', 'some', 'rich', 'guy', 'trying', 'to', 'see', 'just', 'how', 'many', 'non-neighboring', 'countries', 'he', 'could', 'get', 'through', 'in', 'the', 'space', 'of', '24', 'hours,', 'just', 'for', '', 'fun.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'the', 'could', 'visit', 'half', 'doz', 'country', 'scat', 'around', 'glob', 'singl', 'day\\n\\nwake', 'new', 'york', 'breakfast', 'london', 'busy', 'meet', 'jap', 'lunch', 'san', 'francisco', 'anoth', 'meet', 'sydney', 'din', 'shangha', 'go', 'bed', 'rome\\n\\ni', 'dunno', 'youd', 'fly', 'anoth', 'country', 'meal', 'mov', 'anoth', 'next', 'meet', 'could', '\\n\\ni', 'could', 'see', 'rich', 'guy', 'try', 'see', 'many', 'nonneighb', 'country', 'could', 'get', 'spac', 'twenty-four', 'hour', 'fun'], ['yeah', 'theory', 'could', 'visit', 'half', 'dozen', 'countries', 'scatter', 'around', 'globe', 'single', 'day\\n\\nwake', 'new', 'york', 'breakfast', 'london', 'business', 'meet', 'japan', 'lunch', 'san', 'francisco', 'another', 'meet', 'sydney', 'dinner', 'shanghai', 'go', 'bed', 'rome\\n\\ni', 'dunno', 'youd', 'fly', 'another', 'country', 'meals', 'move', 'another', 'next', 'meet', 'could', '\\n\\ni', 'could', 'see', 'rich', 'guy', 'try', 'see', 'many', 'nonneighboring', 'countries', 'could', 'get', 'space', 'twenty-four', 'hours', 'fun'])\n",
      "original document: \n",
      "['Who', 'is', 'downvoting', 'these', 'comments?', \"We're\", 'just', 'stating', 'our', 'results.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['downvot', 'com', 'stat', 'result'], ['downvoting', 'comment', 'state', 'result'])\n",
      "original document: \n",
      "['My', 'thought', 'was', 'Djoos', 'and', 'Bowey', 'would', 'get', 'more', 'ice', 'time', 'if', 'brooks', 'and', 'Carlson', 'were', 'split', 'up', 'and', 'paired', 'with', 'one', 'or', 'the', 'other.', 'My', 'fear', 'is', 'trotz', 'is', 'going', 'to', 'overplay', 'Orpik/Carlson', 'at', 'the', 'expense', 'of', 'ice', 'time', 'for', 'the', 'young', 'guys.', 'We’ll', 'see', 'maybe', 'if', 'Djoos', 'and', 'Bowery', 'develop', 'chemistry', 'together', 'that’ll', 'work', 'out', 'idk']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'djoo', 'bowey', 'would', 'get', 'ic', 'tim', 'brook', 'carlson', 'split', 'pair', 'on', 'fear', 'trotz', 'going', 'overplay', 'orpikcarlson', 'expens', 'ic', 'tim', 'young', 'guy', 'wel', 'see', 'mayb', 'djoo', 'bowery', 'develop', 'chem', 'togeth', 'thatl', 'work', 'idk'], ['think', 'djoos', 'bowey', 'would', 'get', 'ice', 'time', 'brook', 'carlson', 'split', 'pair', 'one', 'fear', 'trotz', 'go', 'overplay', 'orpikcarlson', 'expense', 'ice', 'time', 'young', 'guy', 'well', 'see', 'maybe', 'djoos', 'bowery', 'develop', 'chemistry', 'together', 'thatll', 'work', 'idk'])\n",
      "original document: \n",
      "['To', 'be', 'fair,', 'this', 'single', 'moment', \"isn't\", 'most', 'of', 'the', 'time.', 'Just', 'punch', 'me', 'right', 'in', 'the', 'dick,', 'man.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'singl', 'mom', 'isnt', 'tim', 'punch', 'right', 'dick', 'man'], ['fair', 'single', 'moment', 'isnt', 'time', 'punch', 'right', 'dick', 'man'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Agreed.', 'He', 'busted', 'long', 'runs', 'juking', 'Jabroni', 'Pepperoni', 'two', 'years', 'in', 'a', 'row.', \"Everybody's\", 'a', 'hater.', \"He's\", 'not', 'super', 'fast,', 'but', \"he's\", 'damn', 'shifty.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'bust', 'long', 'run', 'juk', 'jabron', 'pepperon', 'two', 'year', 'row', 'everybody', 'hat', 'hes', 'sup', 'fast', 'hes', 'damn', 'shifty'], ['agree', 'bust', 'long', 'run', 'juking', 'jabroni', 'pepperoni', 'two', 'years', 'row', 'everybodys', 'hater', 'hes', 'super', 'fast', 'hes', 'damn', 'shifty'])\n",
      "original document: \n",
      "['You', 'can', 'also', 'sacrifice', 'allies', 'to', 'Satan', 'mid-battle', 'to', 'boost', 'his', 'stats.', 'Unfortunately,', 'this', 'deletes', 'the', 'ally', 'from', 'your', 'roster,', 'and', '-', 'more', 'importantly', '-', 'lessens', 'your', 'arena', 'score.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'sacr', 'al', 'sat', 'midbattl', 'boost', 'stat', 'unfortun', 'delet', 'al', 'rost', 'import', 'less', 'aren', 'scor'], ['also', 'sacrifice', 'ally', 'satan', 'midbattle', 'boost', 'stats', 'unfortunately', 'delete', 'ally', 'roster', 'importantly', 'lessen', 'arena', 'score'])\n",
      "original document: \n",
      "['Or', 'Rick', 'and', 'Morty', 'if', 'you', 'really', 'want', 'to', 'get', 'intelligent.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rick', 'morty', 'real', 'want', 'get', 'intellig'], ['rick', 'morty', 'really', 'want', 'get', 'intelligent'])\n",
      "original document: \n",
      "['One', 'Random', 'Please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['#SELLER']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sel'], ['seller'])\n",
      "original document: \n",
      "['Stop', 'giving', 'Killins', 'the', 'ball', 'every', 'play.', 'Where', 'the', 'hell', 'is', 'Trequan??']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'giv', 'killin', 'bal', 'every', 'play', 'hel', 'trequ'], ['stop', 'give', 'killins', 'ball', 'every', 'play', 'hell', 'trequan'])\n",
      "original document: \n",
      "['i', 'wiped', 'my', 'cookies', 'for', 'reddit-stream', 'and', 'reddit', 'then', 'went', 'directly', 'to', 'it', 'by', 'changing', 'the', 'url', 'from', 'the', 'actual', 'game', 'thread.', 'that', 'worked.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wip', 'cooky', 'redditstream', 'reddit', 'went', 'direct', 'chang', 'url', 'act', 'gam', 'thread', 'work'], ['wipe', 'cookies', 'redditstream', 'reddit', 'go', 'directly', 'change', 'url', 'actual', 'game', 'thread', 'work'])\n",
      "original document: \n",
      "['So', 'did', 'you', 'like', 'repair', 'all', 'the', 'stuff', 'you', 'broke?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'repair', 'stuff', 'brok'], ['like', 'repair', 'stuff', 'break'])\n",
      "original document: \n",
      "['I', 'hope', \"there's\", 'a', 'way', 'to', 'opt', 'out.', '', 'I', 'do', 'not', 'want', 'to', 'be', 'bothered', 'to', 'need', 'to', 'remember', 'another', 'PIN', 'and', 'enter', 'it', 'every', 'damn', 'time.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'ther', 'way', 'opt', 'want', 'both', 'nee', 'rememb', 'anoth', 'pin', 'ent', 'every', 'damn', 'tim'], ['hope', 'theres', 'way', 'opt', 'want', 'bother', 'need', 'remember', 'another', 'pin', 'enter', 'every', 'damn', 'time'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'll\", 'take', '2', 'spots', 'please.', 'Any', 'of', 'the', 'following', 'in', 'this', 'order:', '#11,', '#22,', '#32,', '#23.', 'Random', 'ok', 'if', 'all', 'taken.\\n\\nThanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'tak', 'two', 'spot', 'pleas', 'follow', 'ord', 'elev', 'twenty-two', 'thirty-two', 'twenty-three', 'random', 'ok', 'taken\\n\\nthanks'], ['ill', 'take', 'two', 'spot', 'please', 'follow', 'order', 'eleven', 'twenty-two', 'thirty-two', 'twenty-three', 'random', 'ok', 'taken\\n\\nthanks'])\n",
      "original document: \n",
      "['mods', 'already', 'stack,', 'just', 'with', 'diminishing', 'returns.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mod', 'already', 'stack', 'dimin', 'return'], ['mods', 'already', 'stack', 'diminish', 'return'])\n",
      "original document: \n",
      "['Wow,', 'forgot', 'the', 'match', 'was', 'on', 'this', 'time']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'forgot', 'match', 'tim'], ['wow', 'forget', 'match', 'time'])\n",
      "original document: \n",
      "['Meh,', 'Danteh', 'is', 'more', 'mechanically', 'skilled', 'than', 'Esca', 'is,', 'and', \"it's\", 'not', 'like', 'his', 'gamesense', 'is', 'lacking.', 'Individually', 'I', \"don't\", 'think', 'you', 'can', 'really', 'say', 'Esca', 'is', 'significantly', 'better,', 'but', 'at', 'the', 'same', 'time', 'if', 'you', 'replaced', 'Esca', 'with', 'Danteh', 'LH', 'may', 'not', 'do', 'as', 'well', 'because', 'I', 'gather', 'Esca', 'is', 'a', 'very', 'important', 'personality', 'to', 'have', 'on', 'the', 'team.', '\\n\\nHard', 'to', 'judge', 'without', 'POV,', 'obviously,', 'or', 'at', 'least', 'good', 'stats', 'over', 'reasonable', 'sample', 'sizes.', '\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meh', 'danteh', 'mech', 'skil', 'esc', 'lik', 'gamesens', 'lack', 'individ', 'dont', 'think', 'real', 'say', 'esc', 'sign', 'bet', 'tim', 'replac', 'esc', 'danteh', 'lh', 'may', 'wel', 'gath', 'esc', 'import', 'person', 'team', '\\n\\nhard', 'judg', 'without', 'pov', 'obvy', 'least', 'good', 'stat', 'reason', 'sampl', 'siz', '\\n\\n'], ['meh', 'danteh', 'mechanically', 'skilled', 'esca', 'like', 'gamesense', 'lack', 'individually', 'dont', 'think', 'really', 'say', 'esca', 'significantly', 'better', 'time', 'replace', 'esca', 'danteh', 'lh', 'may', 'well', 'gather', 'esca', 'important', 'personality', 'team', '\\n\\nhard', 'judge', 'without', 'pov', 'obviously', 'least', 'good', 'stats', 'reasonable', 'sample', 'size', '\\n\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['You', 'do', 'not', 'have', 'enough', 'karma,', 'please', 'message', 'the', 'moderators', 'with', 'proof', 'of', 'trade,', 'please', 'check', 'the', 'OP', 'for', 'the', 'requirments.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['enough', 'karm', 'pleas', 'mess', 'mod', 'proof', 'trad', 'pleas', 'check', 'op', 'requir'], ['enough', 'karma', 'please', 'message', 'moderators', 'proof', 'trade', 'please', 'check', 'op', 'requirments'])\n",
      "original document: \n",
      "['How?', 'All', 'I', 'said', 'was', 'that', 'what', \"you're\", 'probably', 'classifying', 'as', 'a', 'skilled', 'PK', '(someone', 'who', \"doesn't\", 'use', 'lights', 'and', 'zone', 'much,)', 'is', 'someone', \"that's\", 'probably', 'losing', 'a', 'lot,', 'because', 'zone', 'and', 'lights', 'attack', 'are', 'the', 'best', 'and', 'kinda', 'only', 'good', 'parts', 'of', 'her', 'kit.', 'Timesnap', 'makes', 'them', 'better,', 'sure,', 'but', 'idk', 'why', \"you're\", 'saying', \"it's\", 'my', 'crutch.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'yo', 'prob', 'class', 'skil', 'pk', 'someon', 'doesnt', 'us', 'light', 'zon', 'much', 'someon', 'that', 'prob', 'los', 'lot', 'zon', 'light', 'attack', 'best', 'kind', 'good', 'part', 'kit', 'timesnap', 'mak', 'bet', 'sur', 'idk', 'yo', 'say', 'crutch'], ['say', 'youre', 'probably', 'classify', 'skilled', 'pk', 'someone', 'doesnt', 'use', 'light', 'zone', 'much', 'someone', 'thats', 'probably', 'lose', 'lot', 'zone', 'light', 'attack', 'best', 'kinda', 'good', 'part', 'kit', 'timesnap', 'make', 'better', 'sure', 'idk', 'youre', 'say', 'crutch'])\n",
      "original document: \n",
      "['You', 'have', 'to', 'be', 'staring', 'like', 'directly', 'at', 'it', 'basically.\\n\\nI', 'only', 'use', 'them', 'occasionally', 'for', 'the', 'deafening', 'effect.', 'If', 'they', \"can't\", 'hear', 'where', 'you', 'are', 'in', 'their', 'building,', 'they', 'are', 'less', 'prepared', 'when', 'you', 'come', 'around', 'the', 'corner', 'and', 'beat', 'dat', 'ass.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['star', 'lik', 'direct', 'basically\\n\\ni', 'us', 'occas', 'deaf', 'effect', 'cant', 'hear', 'build', 'less', 'prep', 'com', 'around', 'corn', 'beat', 'dat', 'ass\\n'], ['star', 'like', 'directly', 'basically\\n\\ni', 'use', 'occasionally', 'deafen', 'effect', 'cant', 'hear', 'build', 'less', 'prepare', 'come', 'around', 'corner', 'beat', 'dat', 'ass\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['How', 'do', 'access', 'the', 'monks', 'toaster', 'without', 'them', 'starting', 'a', 'war', 'with', 'you?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['access', 'monk', 'toast', 'without', 'start', 'war'], ['access', 'monks', 'toaster', 'without', 'start', 'war'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'had', 'just', 'started', 'the', 'movie', 'when', 'I', 'wrote', 'this,', 'and', 'despite', 'seeing', 'Star', 'Wars', 'give', 'or', 'take', '500x', 'I', \"didn't\", 'remember', 'that', 'part.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'movy', 'wrot', 'despit', 'see', 'star', 'war', 'giv', 'tak', '500x', 'didnt', 'rememb', 'part'], ['start', 'movie', 'write', 'despite', 'see', 'star', 'war', 'give', 'take', '500x', 'didnt', 'remember', 'part'])\n",
      "original document: \n",
      "['Ahh,', \"I'll\", 'have', 'to', 'pass', '-', 'not', 'big', 'on', 'certs', 'and', \"I'm\", 'good', 'on', 'boosts', 'right', 'now']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahh', 'il', 'pass', 'big', 'cert', 'im', 'good', 'boost', 'right'], ['ahh', 'ill', 'pass', 'big', 'certs', 'im', 'good', 'boost', 'right'])\n",
      "original document: \n",
      "['Cheeky', 'little', 'cunt', 'Danny.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cheeky', 'littl', 'cunt', 'danny'], ['cheeky', 'little', 'cunt', 'danny'])\n",
      "original document: \n",
      "[\"You're\", 'either', 'doing', 'him', 'a', 'favor,', 'or', 'cock', 'blocking', 'him.', '\\n\\n...Not', 'sure', 'I', 'wanna', 'know', 'which']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'eith', 'fav', 'cock', 'block', '\\n\\nnot', 'sur', 'wann', 'know'], ['youre', 'either', 'favor', 'cock', 'block', '\\n\\nnot', 'sure', 'wanna', 'know'])\n",
      "original document: \n",
      "['number', '32', 'and', 'one', 'random', 'please,', 'if', '32', 'is', 'taken', 'two', 'randoms', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['numb', 'thirty-two', 'on', 'random', 'pleas', 'thirty-two', 'tak', 'two', 'random', 'pleas'], ['number', 'thirty-two', 'one', 'random', 'please', 'thirty-two', 'take', 'two', 'randoms', 'please'])\n",
      "original document: \n",
      "['I', 'think', \"you're\", 'right,', 'they', '[have', 'a', 'page', 'here](https://support.google.com/youtube/answer/2853702?hl=en)', 'saying', 'to', 'use', 'h.264', 'but', \"doesn't\", 'mention', 'h.265', 'in', 'the', 'supported', 'formats.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'yo', 'right', 'pag', 'herehttpssupportgooglecomyoutubeanswer2853702hlen', 'say', 'us', 'h264', 'doesnt', 'ment', 'h265', 'support', 'form'], ['think', 'youre', 'right', 'page', 'herehttpssupportgooglecomyoutubeanswer2853702hlen', 'say', 'use', 'h264', 'doesnt', 'mention', 'h265', 'support', 'format'])\n",
      "original document: \n",
      "['143413429|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413280\\nNo', 'they', \"don't.\", 'I', 'wish', 'they', 'did', 'lol.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, four hundred and twenty-nin', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413280\\nno', 'dont', 'wish', 'lol\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, four hundred and twenty-nine', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413280\\nno', 'dont', 'wish', 'lol\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['He', 'actually', 'had', 'good', 'intentions', 'to', 'go', 'to', 'Iraq.', 'Alas,', '\"but', 'Saddam', 'is', 'a', 'bad', 'guy\"', 'is', 'not', 'a', 'good', 'excuse', 'to', 'overthrow', 'him.', 'For', 'a', 'politician,', \"he's\", 'weirdly', 'naive', 'on', 'how', 'complex', 'international', 'affairs', 'is.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'good', 'int', 'go', 'iraq', 'ala', 'saddam', 'bad', 'guy', 'good', 'excus', 'overthrow', 'polit', 'hes', 'weird', 'naiv', 'complex', 'intern', 'affair'], ['actually', 'good', 'intentions', 'go', 'iraq', 'alas', 'saddam', 'bad', 'guy', 'good', 'excuse', 'overthrow', 'politician', 'hes', 'weirdly', 'naive', 'complex', 'international', 'affairs'])\n",
      "original document: \n",
      "['I’m', 'not', 'your', 'friend,', 'pal.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'friend', 'pal'], ['im', 'friend', 'pal'])\n",
      "original document: \n",
      "['----------&gt;', '/r/DHExchange']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'rdhexchange'], ['gt', 'rdhexchange'])\n",
      "original document: \n",
      "['So', 'that', 'went', 'from', ':', \"'\", 'I', 'was', 'calling', 'the', 'Sens', 'dumbasses.', 'Calm', 'your', 'tits.', \"'\", '\\n\\nTo', ':', \"'\", 'I', 'was', 'calling', 'the', 'Sens', 'dumbasses.', 'Calm', 'your', 'tits.', 'Typical', 'habs', 'fan.', \"'\", '\\n\\nNeat', '.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['went', 'cal', 'sen', 'dumbass', 'calm', 'tit', '\\n\\nto', 'cal', 'sen', 'dumbass', 'calm', 'tit', 'typ', 'hab', 'fan', '\\n\\nneat'], ['go', 'call', 'sens', 'dumbasses', 'calm', 'tits', '\\n\\nto', 'call', 'sens', 'dumbasses', 'calm', 'tits', 'typical', 'habs', 'fan', '\\n\\nneat'])\n",
      "original document: \n",
      "['I', 'think', 'so.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think'], ['think'])\n",
      "original document: \n",
      "['I', 'love', 'alpina', 'and', \"I've\", 'been', 'hearing', 'a', 'lot', 'about', 'longines!', \"I'll\", 'give', 'em', 'both', 'a', 'look!', 'Thank', 'you', 'thank', 'you!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'alpin', 'iv', 'hear', 'lot', 'longin', 'il', 'giv', 'em', 'look', 'thank', 'thank'], ['love', 'alpina', 'ive', 'hear', 'lot', 'longines', 'ill', 'give', 'em', 'look', 'thank', 'thank'])\n",
      "original document: \n",
      "['Nothing', 'changed', '\\n\\n\\ngarbage', 'teammates\\nflip', 'a', 'coin', 'bro!\\nsearch', 'up', 'a', 'pros', 'rune', 'and', 'items', 'and', 'wing', 'it', 'zz\\nplay', 'new', 'champions', 'and', 'dont', 'know', 'what', 'youre', 'doing', 'cause', 'if', 'you', \"don't\", 'know', 'youre', 'doing', 'the', 'enemy', 'wont\\nAlso', \"don't\", 'go', 'too', 'ham', 'you', 'might', 'get', 'reported', 'for', 'toxicity!!!1']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noth', 'chang', '\\n\\n\\ngarbage', 'teammates\\nflip', 'coin', 'bro\\nsearch', 'pro', 'run', 'item', 'wing', 'zz\\nplay', 'new', 'champ', 'dont', 'know', 'yo', 'caus', 'dont', 'know', 'yo', 'enemy', 'wont\\nalso', 'dont', 'go', 'ham', 'might', 'get', 'report', 'toxicity1'], ['nothing', 'change', '\\n\\n\\ngarbage', 'teammates\\nflip', 'coin', 'bro\\nsearch', 'pros', 'rune', 'items', 'wing', 'zz\\nplay', 'new', 'champion', 'dont', 'know', 'youre', 'cause', 'dont', 'know', 'youre', 'enemy', 'wont\\nalso', 'dont', 'go', 'ham', 'might', 'get', 'report', 'toxicity1'])\n",
      "original document: \n",
      "['&gt;', 'If', \"you're\", 'a', 'sponsor,', 'would', 'you', 'rather', 'have', '100k', 'watch', 'for', '1h', 'or', '200k', 'for', '30', 'minutes?', '(Made', 'up', 'numbers).', 'To', 'me,', 'the', 'clear', 'answer', 'is', '200k,', 'and', \"I'm\", 'pretty', 'confident', 'they', \"agree.\\n\\nThat's\", 'a', 'made', 'up', 'number', 'and', \"didn't\", 'really', 'portray', 'the', 'changes', 'though.', \"You're\", 'making', 'an', 'assumption', 'that', 'the', 'total', 'unique', 'viewers', 'are', 'double', 'for', 'half', 'the', 'time.', 'While', \"it's\", 'true', 'that', 'total', 'unique', 'viewers', 'might', 'increase,', 'there', 'is', 'nothing', 'to', 'support', 'the', 'idea', 'that', \"it'd\", 'double.', 'On', 'the', 'other', 'hand,', 'it', 'is', 'already', 'proven', 'that', 'the', 'total', 'number', 'games', '(time)', 'will', 'be', 'less', 'than', 'half', 'of', 'the', 'previous', 'year.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'yo', 'spons', 'would', 'rath', '100k', 'watch', '1h', '200k', 'thirty', 'minut', 'mad', 'numb', 'clear', 'answ', '200k', 'im', 'pretty', 'confid', 'agree\\n\\nthats', 'mad', 'numb', 'didnt', 'real', 'portray', 'chang', 'though', 'yo', 'mak', 'assum', 'tot', 'un', 'view', 'doubl', 'half', 'tim', 'tru', 'tot', 'un', 'view', 'might', 'increas', 'noth', 'support', 'ide', 'itd', 'doubl', 'hand', 'already', 'prov', 'tot', 'numb', 'gam', 'tim', 'less', 'half', 'prevy', 'year'], ['gt', 'youre', 'sponsor', 'would', 'rather', '100k', 'watch', '1h', '200k', 'thirty', 'minutes', 'make', 'number', 'clear', 'answer', '200k', 'im', 'pretty', 'confident', 'agree\\n\\nthats', 'make', 'number', 'didnt', 'really', 'portray', 'change', 'though', 'youre', 'make', 'assumption', 'total', 'unique', 'viewers', 'double', 'half', 'time', 'true', 'total', 'unique', 'viewers', 'might', 'increase', 'nothing', 'support', 'idea', 'itd', 'double', 'hand', 'already', 'prove', 'total', 'number', 'game', 'time', 'less', 'half', 'previous', 'year'])\n",
      "original document: \n",
      "['\"For', 'the', 'next', 'two', 'weeks', '100%', 'of', 'sales', 'from', 'these', 'two', '@IsleSeatPodcast', 'shirts', 'will', 'go', 'to', '@UNIDOSxPR.', '\\n\\n#Isles', '\\n\\nhttp://www.NewYorkBootleg.com', '\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['next', 'two', 'week', 'one hundred', 'sal', 'two', 'isleseatpodcast', 'shirt', 'go', 'unidosxpr', '\\n\\nisles', '\\n\\nhttpwwwnewyorkbootlegcom'], ['next', 'two', 'weeks', 'one hundred', 'sales', 'two', 'isleseatpodcast', 'shirt', 'go', 'unidosxpr', '\\n\\nisles', '\\n\\nhttpwwwnewyorkbootlegcom'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['One', 'random', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['&gt;Elaborate,', 'please.', 'Show', 'me', 'where', 'I', 'said', 'abandon', 'your', 'mortgage,', 'kids', 'and', 'wife.\\n\\nYou', 'stated', 'to', 'leave', 'the', 'premises,', 'by', 'virtue', 'of', 'American', 'law,', 'having', 'left', 'the', 'premises', 'is', 'considered', 'abandonment', 'of', 'the', 'home', 'and', 'the', 'children', 'in', 'the', 'home.', 'This', 'is', 'well', 'proven', 'through', 'the', 'amazingly', 'broken', 'family', 'court', 'system.', '\\n\\nIf', 'you', 'leave', 'the', 'home,', 'you', 'can', 'pretty', 'much', 'write', 'off', 'ever', 'seeing', 'your', 'children', 'again', 'unless', 'you', 'are', 'independently', 'wealthy', 'and', 'the', 'wife/girlfriend', 'does', 'not', 'have', 'access', 'to', 'the', 'accounts.', '\\n\\n&gt;I’ve', 'about', 'had', 'my', 'fill', 'of', 'Internet', 'warriors', 'who', 'put', 'words', 'in', 'people’s', 'mouths,', 'instead', 'of', 'owning', 'their', 'assumptions', 'and', 'having', 'an', 'adult', 'conversion.\\n\\nI', 'made', 'no', 'assumptions,', 'you', 'said', 'to', 'leave', 'the', 'premises,', 'then', 'you', 'said', 'you', \"didn't\", 'mean', 'it,', 'that', 'it', 'was', '\"figurative\".', '\\n\\nSay', 'what', 'you', 'mean', 'and', 'you', \"won't\", 'have', 'all', 'of', 'these', '\"internet', 'warriors\"', 'putting', 'words', 'in', 'your', 'mouth.', 'In', 'fact,', 'if', 'it', 'happens', 'so', 'often,', 'perhaps', 'you', 'should', 'stop', 'bitching', 'about', 'it,', 'and', 'start', 'asking', 'why', 'it', 'is', 'that', 'your', 'statements', 'are', 'so', 'often', 'incorrectly', 'read', 'differently', 'than', 'you', 'meant', 'them.', '\\n\\n&gt;So,', 'please,', 'fucking', 'show', 'me', 'how', 'I', 'minced', 'words.\\n\\nOK.\\n\\nYou', 'said.', '\\n\\n&gt;Woman', 'hits', 'you,', 'you', 'leave', 'the', 'premise', 'one', 'way', 'or', 'another.\\n\\n\\nThen', 'you', 'stated', 'what', 'that', 'meant', 'was:\\n\\n&gt;', 'I', 'didn’t', 'mean,', '“leave', 'right', 'now', 'and', 'never', 'look', 'back”.', 'More', 'like,', 'walk', 'out', 'of', 'the', 'room', 'or', 'building', 'to', 'let', 'things', 'simmer', 'down', 'a', 'bit.\\n\\n\\nTo', 'which', 'I', 'reminded', 'you', 'that', 'in', 'the', 'American', 'legal', 'system,', 'doing', 'so', 'is', 'seen', 'as', 'willful', 'abandonment', 'and', 'will', 'count', 'against', 'you', 'in', 'court', 'the', 'majority', 'of', 'the', 'time.', '\\n\\nSo', 'first', 'you', 'said', 'leave', 'the', 'premises,', 'then', 'said', 'you', \"didn't\", 'mean', 'to', 'literally', 'leave,', 'just', 'go', 'to', 'another', 'room.', 'Cause', 'women', 'can', 'only', 'hit', 'you', 'standing', 'in', 'one', 'spot,', 'they', 'could', 'never', 'possibly', 'follow', 'you.', \"It's\", 'their', 'one', 'weakness', 'right.', '\\n\\nAs', 'an', 'aside,', 'you', 'have', 'made', 'the', 'mistake', 'numerous', 'times', 'now,', 'it', 'is', '\"premises\",', 'not', '\"premise\".', 'THE', 'MORE', 'YOU', 'KNOW', '彡☆', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtelab', 'pleas', 'show', 'said', 'abandon', 'mortg', 'kid', 'wife\\n\\nyou', 'stat', 'leav', 'prem', 'virtu', 'am', 'law', 'left', 'prem', 'consid', 'abandon', 'hom', 'childr', 'hom', 'wel', 'prov', 'amaz', 'brok', 'famy', 'court', 'system', '\\n\\nif', 'leav', 'hom', 'pretty', 'much', 'writ', 'ev', 'see', 'childr', 'unless', 'independ', 'wealthy', 'wifegirlfriend', 'access', 'account', '\\n\\ngtive', 'fil', 'internet', 'warry', 'put', 'word', 'peopl', 'mouth', 'instead', 'own', 'assum', 'adult', 'conversion\\n\\ni', 'mad', 'assum', 'said', 'leav', 'prem', 'said', 'didnt', 'mean', 'fig', '\\n\\nsay', 'mean', 'wont', 'internet', 'warry', 'put', 'word', 'mou', 'fact', 'hap', 'oft', 'perhap', 'stop', 'bitch', 'start', 'ask', 'stat', 'oft', 'incorrect', 'read', 'diff', 'meant', '\\n\\ngtso', 'pleas', 'fuck', 'show', 'mint', 'words\\n\\nok\\n\\nyou', 'said', '\\n\\ngtwoman', 'hit', 'leav', 'prem', 'on', 'way', 'another\\n\\n\\nthen', 'stat', 'meant', 'was\\n\\ngt', 'didnt', 'mean', 'leav', 'right', 'nev', 'look', 'back', 'lik', 'walk', 'room', 'build', 'let', 'thing', 'sim', 'bit\\n\\n\\nto', 'remind', 'am', 'leg', 'system', 'seen', 'wil', 'abandon', 'count', 'court', 'maj', 'tim', '\\n\\nso', 'first', 'said', 'leav', 'prem', 'said', 'didnt', 'mean', 'lit', 'leav', 'go', 'anoth', 'room', 'caus', 'wom', 'hit', 'stand', 'on', 'spot', 'could', 'nev', 'poss', 'follow', 'on', 'weak', 'right', '\\n\\nas', 'asid', 'mad', 'mistak', 'num', 'tim', 'prem', 'prem', 'know'], ['gtelaborate', 'please', 'show', 'say', 'abandon', 'mortgage', 'kid', 'wife\\n\\nyou', 'state', 'leave', 'premise', 'virtue', 'american', 'law', 'leave', 'premise', 'consider', 'abandonment', 'home', 'children', 'home', 'well', 'prove', 'amazingly', 'break', 'family', 'court', 'system', '\\n\\nif', 'leave', 'home', 'pretty', 'much', 'write', 'ever', 'see', 'children', 'unless', 'independently', 'wealthy', 'wifegirlfriend', 'access', 'account', '\\n\\ngtive', 'fill', 'internet', 'warriors', 'put', 'word', 'people', 'mouth', 'instead', 'own', 'assumptions', 'adult', 'conversion\\n\\ni', 'make', 'assumptions', 'say', 'leave', 'premise', 'say', 'didnt', 'mean', 'figurative', '\\n\\nsay', 'mean', 'wont', 'internet', 'warriors', 'put', 'word', 'mouth', 'fact', 'happen', 'often', 'perhaps', 'stop', 'bitch', 'start', 'ask', 'statements', 'often', 'incorrectly', 'read', 'differently', 'mean', '\\n\\ngtso', 'please', 'fuck', 'show', 'mince', 'words\\n\\nok\\n\\nyou', 'say', '\\n\\ngtwoman', 'hit', 'leave', 'premise', 'one', 'way', 'another\\n\\n\\nthen', 'state', 'mean', 'was\\n\\ngt', 'didnt', 'mean', 'leave', 'right', 'never', 'look', 'back', 'like', 'walk', 'room', 'build', 'let', 'things', 'simmer', 'bit\\n\\n\\nto', 'remind', 'american', 'legal', 'system', 'see', 'willful', 'abandonment', 'count', 'court', 'majority', 'time', '\\n\\nso', 'first', 'say', 'leave', 'premise', 'say', 'didnt', 'mean', 'literally', 'leave', 'go', 'another', 'room', 'cause', 'women', 'hit', 'stand', 'one', 'spot', 'could', 'never', 'possibly', 'follow', 'one', 'weakness', 'right', '\\n\\nas', 'aside', 'make', 'mistake', 'numerous', 'time', 'premise', 'premise', 'know'])\n",
      "original document: \n",
      "['No', 'worries.', 'It', 'got', 'me', 'out', 'of', 'bed', 'at', 'a', 'decent', 'time', 'for', 'once,', 'and', 'there', 'were', 'other', 'posts', 'giving', 'the', 'same', 'info.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['worry', 'got', 'bed', 'dec', 'tim', 'post', 'giv', 'info'], ['worry', 'get', 'bed', 'decent', 'time', 'post', 'give', 'info'])\n",
      "original document: \n",
      "['It', 'kinda', 'sucks', 'though.', 'Mine', 'has', 'been', 'collecting', 'dust', 'for', 'a', 'little', 'while', 'by', 'this', 'point', '--', 'unless', 'you', 'are', 'a', 'Nintendo', 'fan', 'there', \"isn't\", 'much', 'available', 'on', 'it', 'that', 'one', 'would', 'want.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'suck', 'though', 'min', 'collect', 'dust', 'littl', 'point', 'unless', 'nintendo', 'fan', 'isnt', 'much', 'avail', 'on', 'would', 'want'], ['kinda', 'suck', 'though', 'mine', 'collect', 'dust', 'little', 'point', 'unless', 'nintendo', 'fan', 'isnt', 'much', 'available', 'one', 'would', 'want'])\n",
      "original document: \n",
      "['Seeing', 'the', 'patriots', 'logos', 'just', 'makes', 'my', 'blood', 'boil.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'patriot', 'logo', 'mak', 'blood', 'boil'], ['see', 'patriots', 'logos', 'make', 'blood', 'boil'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['What', 'am', 'i', 'looking', 'at', 'here?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look'], ['look'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Do', 'I', 'need', 'this', '\"horizons\"', 'thing(expansion', 'pass?', 'deluxe', 'edition?)', 'to', 'encounter', 'them?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'horizon', 'thingexpand', 'pass', 'delux', 'edit', 'encount'], ['need', 'horizons', 'thingexpansion', 'pass', 'deluxe', 'edition', 'encounter'])\n",
      "original document: \n",
      "['Oh!', 'fantastic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'fantast'], ['oh', 'fantastic'])\n",
      "original document: \n",
      "['Nty']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nty'], ['nty'])\n",
      "original document: \n",
      "['Know', 'your', '/r/politics', 'rules', '101']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'rpolit', 'rul', 'one hundred and on'], ['know', 'rpolitics', 'rule', 'one hundred and one'])\n",
      "original document: \n",
      "['afaik', \"it's\", 'mainly', 'to', 'do', 'with', 'modding', 'the', 'Multiplayer.', 'Mods', 'could', 'let', 'you', 'do', 'some', 'really', 'fucked', 'up', 'things', 'in', 'MP', 'so', 'they', 'made', 'it', 'bannable.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['afaik', 'main', 'mod', 'multiplay', 'mod', 'could', 'let', 'real', 'fuck', 'thing', 'mp', 'mad', 'ban'], ['afaik', 'mainly', 'modding', 'multiplayer', 'mods', 'could', 'let', 'really', 'fuck', 'things', 'mp', 'make', 'bannable'])\n",
      "original document: \n",
      "['Drones', 'should', 'be', 'kept', 'the', 'same', 'to', 'ensure', 'consistency.', 'If', 'skins', 'are', 'introduced', 'they', 'could', 'be', 'detected', 'by', 'defenders', 'more', 'easily', 'or', 'blend', 'into', 'the', 'environment', 'more.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dron', 'kept', 'ens', 'consist', 'skin', 'introduc', 'could', 'detect', 'defend', 'easy', 'blend', 'environ'], ['drone', 'keep', 'ensure', 'consistency', 'skin', 'introduce', 'could', 'detect', 'defenders', 'easily', 'blend', 'environment'])\n",
      "original document: \n",
      "['Well,', 'I', 'basically', 'followed', 'amazingribs.com', 'recipe', 'for', 'wet', 'curing', 'a', 'ham.', 'Smoked', 'this', 'Duroc', 'pork', 'ham', 'on', 'my', 'WSM', 'at', '325F', 'for', 'about', '3', 'hours.', 'Smoked', 'with', 'maple', 'wood,', 'taste', 'delicious!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'bas', 'follow', 'amazingribscom', 'recip', 'wet', 'cur', 'ham', 'smok', 'duroc', 'pork', 'ham', 'wsm', '325f', 'three', 'hour', 'smok', 'mapl', 'wood', 'tast', 'delicy'], ['well', 'basically', 'follow', 'amazingribscom', 'recipe', 'wet', 'cure', 'ham', 'smoke', 'duroc', 'pork', 'ham', 'wsm', '325f', 'three', 'hours', 'smoke', 'maple', 'wood', 'taste', 'delicious'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'can', 'see', 'why', 'he', 'was', 'so', 'worried', 'about', 'uploading', 'videos,', 'this', 'one', 'was', 'garbage']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'worry', 'upload', 'video', 'on', 'garb'], ['see', 'worry', 'upload', 'videos', 'one', 'garbage'])\n",
      "original document: \n",
      "['2', 'randoms', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'pleas'], ['two', 'randoms', 'please'])\n",
      "original document: \n",
      "['Whoosh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whoosh'], ['whoosh'])\n",
      "original document: \n",
      "['So', '11:59', 'on', 'October', '1st?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one thousand, one hundred and fifty-nin', 'octob', '1st'], ['one thousand, one hundred and fifty-nine', 'october', '1st'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'sure', 'why,', 'but', 'when', 'I', 'loaded', 'up', 'my', 'game', 'suddenly', 'everything', 'in', 'the', 'Duna', 'system', 'had', 'a', 'good,', 'solid', 'connection', 'to', 'Kerbin', 'and', 'was', 'able', 'to', 'transmit.', 'Glad', 'to', 'have', 'this', 'sorted', 'out', 'before', 'I', 'start', 'sending', 'craft', 'out', 'to', 'other', 'planets', ':D\\n\\nEdit', '-', 'The', \"relay's\", 'orbital', 'position...', 'of', 'course.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'load', 'gam', 'sud', 'everyth', 'dun', 'system', 'good', 'solid', 'connect', 'kerbin', 'abl', 'transmit', 'glad', 'sort', 'start', 'send', 'craft', 'planet', 'd\\n\\nedit', 'relay', 'orbit', 'posit', 'cours'], ['im', 'sure', 'load', 'game', 'suddenly', 'everything', 'duna', 'system', 'good', 'solid', 'connection', 'kerbin', 'able', 'transmit', 'glad', 'sort', 'start', 'send', 'craft', 'planets', 'd\\n\\nedit', 'relay', 'orbital', 'position', 'course'])\n",
      "original document: \n",
      "['one', 'random', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['You', 'should', 'take', 'better', 'care', 'of', 'your', 'phone', 'then.', 'Idk', 'how', 'people', 'are', 'having', 'such', 'a', 'hard', 'time', 'with', 'the', 'getting', 'your', 'phone', 'stolen', 'part', 'of', 'this,', 'it’s', 'not', 'like', 'it’s', 'making', 'it', 'more', 'likely', 'that', 'you’ll', 'get', 'your', 'phone', 'stolen.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'bet', 'car', 'phon', 'idk', 'peopl', 'hard', 'tim', 'get', 'phon', 'stol', 'part', 'lik', 'mak', 'lik', 'youl', 'get', 'phon', 'stol'], ['take', 'better', 'care', 'phone', 'idk', 'people', 'hard', 'time', 'get', 'phone', 'steal', 'part', 'like', 'make', 'likely', 'youll', 'get', 'phone', 'steal'])\n",
      "original document: \n",
      "['Best', 'comment.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'com'], ['best', 'comment'])\n",
      "original document: \n",
      "[\"That's\", 'because', 'paywave', '/is/', 'a', 'credit', 'card', 'transaction,', 'last', 'I', 'checked']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'paywav', 'credit', 'card', 'transact', 'last', 'check'], ['thats', 'paywave', 'credit', 'card', 'transaction', 'last', 'check'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Yeah,', 'I', 'do', 'want', 'to', 'see', 'how', 'he', 'rocks', 'his', 'socks!!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'want', 'see', 'rock', 'sock'], ['yeah', 'want', 'see', 'rock', 'sock'])\n",
      "original document: \n",
      "[\"I'm\", 'trying', 'to', 'imagine', 'running', 'Goldschlager', 'through', 'a', 'Brita.', \"It'd\", 'be', 'like', 'panning', 'for', 'gold!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'try', 'imagin', 'run', 'goldschl', 'brit', 'itd', 'lik', 'pan', 'gold'], ['im', 'try', 'imagine', 'run', 'goldschlager', 'brita', 'itd', 'like', 'pan', 'gold'])\n",
      "original document: \n",
      "['I', 'see', 'thanks', 'for', 'the', 'info']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'thank', 'info'], ['see', 'thank', 'info'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['With', 'rumchata?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rumchat'], ['rumchata'])\n",
      "original document: \n",
      "['One', 'random', 'please!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['And', 'then', 'proceeds', 'to', 'tell', 'the', 'rest', 'of', 'us', 'how', 'we', 'ought', 'to', 'walk', '(drunken', 'meanderings', 'being', 'explicitly', 'prohibited),', 'saying', 'God', 'told', 'him', 'to', 'do', 'that,', 'too.', 'And', \"we'll\", 'be', 'punished', 'if', 'we', \"don't\", 'listen.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['process', 'tel', 'rest', 'us', 'ought', 'walk', 'drunk', 'meand', 'explicit', 'prohibit', 'say', 'god', 'told', 'wel', 'pun', 'dont', 'list'], ['proceed', 'tell', 'rest', 'us', 'ought', 'walk', 'drunken', 'meander', 'explicitly', 'prohibit', 'say', 'god', 'tell', 'well', 'punish', 'dont', 'listen'])\n",
      "original document: \n",
      "['To', 'be', 'blunt,', \"it's\", '*really*', 'immature', 'to', 'just', 'say', '\"I\\'m', 'going', 'to', 'act', 'however', 'I', 'want', 'or', 'find', 'most', 'convenient', 'and', 'everyone', 'else', 'should', 'just', 'deal', 'with', 'it.\"', 'The', 'rest', 'of', 'the', 'world', \"doesn't\", 'revolve', 'around', 'you,', 'and', 'if', 'you', 'expect', 'everyone', 'else', 'to', 'change', '*for', 'you*', \"it's\", 'not', 'going', 'to', 'happen.', 'If', \"you're\", 'acting', 'clingy', 'and', 'insecure', 'and', \"that's\", 'turning', 'people', 'off,', 'no', 'one', 'cares', 'that', 'you', '\"can\\'t', 'control', 'it.\"', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['blunt', 'real', 'im', 'say', 'im', 'going', 'act', 'howev', 'want', 'find', 'conveny', 'everyon', 'els', 'deal', 'rest', 'world', 'doesnt', 'revolv', 'around', 'expect', 'everyon', 'els', 'chang', 'going', 'hap', 'yo', 'act', 'clingy', 'insec', 'that', 'turn', 'peopl', 'on', 'car', 'cant', 'control'], ['blunt', 'really', 'immature', 'say', 'im', 'go', 'act', 'however', 'want', 'find', 'convenient', 'everyone', 'else', 'deal', 'rest', 'world', 'doesnt', 'revolve', 'around', 'expect', 'everyone', 'else', 'change', 'go', 'happen', 'youre', 'act', 'clingy', 'insecure', 'thats', 'turn', 'people', 'one', 'care', 'cant', 'control'])\n",
      "original document: \n",
      "['I', 'was', 'about', 'to', 'say', '“Here', 'in', 'San', 'Diego', 'it', 'definitely', 'never', 'died.”', 'I', 'can’t', 'raid', 'much', 'since', 'I', 'work', 'a', 'lot,', 'but', 'managed', 'to', 'just', 'walk', 'in', 'to', 'a', 'bunch', 'of', 'raids.', 'Got', 'one', 'yesterday', 'after', 'seeing', 'a', 'group', 'of', 'about', '14', 'people', 'at', 'a', 'gym.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'san', 'diego', 'definit', 'nev', 'died', 'cant', 'raid', 'much', 'sint', 'work', 'lot', 'man', 'walk', 'bunch', 'raid', 'got', 'on', 'yesterday', 'see', 'group', 'fourteen', 'peopl', 'gym'], ['say', 'san', 'diego', 'definitely', 'never', 'die', 'cant', 'raid', 'much', 'since', 'work', 'lot', 'manage', 'walk', 'bunch', 'raid', 'get', 'one', 'yesterday', 'see', 'group', 'fourteen', 'people', 'gym'])\n",
      "original document: \n",
      "['I', 'like', 'to', 'think', 'of', 'it', 'like', 'scotch', 'on', 'the', 'rocks.', 'At', 'the', 'beginning,', 'at', 'its', 'freshest,', 'the', 'flavors', 'are', 'bold', 'and', 'really', 'pop,', 'things', 'are', 'exciting', 'and', 'a', 'great', 'opener', 'for', 'the', 'adventure', 'your', 'about', 'to', 'experience.', 'As', 'you', 'get', 'farther', 'along', 'and', 'things', 'soften', 'and', 'mellow,', 'you', 'notice', 'the', 'softer', 'notes', 'as', 'they', 'are', 'allowed', 'to', 'come', 'to', 'prominence.', 'And', 'at', 'the', 'very', 'end', 'you', 'are', 'gently', 'let', 'back', 'down', 'as', 'you', 'throw', 'back', 'that', 'last', 'little', 'cusp', 'of', 'flavor', 'and', 'the', 'experience', 'fades.', 'All', 'great', 'breakfast', 'cereal,', 'like', 'scotch,', 'is', 'a', 'journey.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'think', 'lik', 'scotch', 'rock', 'begin', 'freshest', 'flav', 'bold', 'real', 'pop', 'thing', 'excit', 'gre', 'op', 'adv', 'expery', 'get', 'farth', 'along', 'thing', 'soft', 'mellow', 'not', 'soft', 'not', 'allow', 'com', 'promin', 'end', 'gent', 'let', 'back', 'throw', 'back', 'last', 'littl', 'cusp', 'flav', 'expery', 'fad', 'gre', 'breakfast', 'cer', 'lik', 'scotch', 'journey'], ['like', 'think', 'like', 'scotch', 'rock', 'begin', 'freshest', 'flavor', 'bold', 'really', 'pop', 'things', 'excite', 'great', 'opener', 'adventure', 'experience', 'get', 'farther', 'along', 'things', 'soften', 'mellow', 'notice', 'softer', 'note', 'allow', 'come', 'prominence', 'end', 'gently', 'let', 'back', 'throw', 'back', 'last', 'little', 'cusp', 'flavor', 'experience', 'fade', 'great', 'breakfast', 'cereal', 'like', 'scotch', 'journey'])\n",
      "original document: \n",
      "['That’s', 'just', 'a', 'kick', 'in', 'the', 'balls.', '(The', 'Rangers', 'jersey)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'kick', 'bal', 'rang', 'jersey'], ['thats', 'kick', 'ball', 'rangers', 'jersey'])\n",
      "original document: \n",
      "[\"I'm\", 'super', 'picky', 'about', 'screens,', 'and', 'I', \"don't\", 'think', 'it', 'really', 'matters.', 'There', 'is', 'an', 'office', 'supply', 'store', 'near', 'me', 'that', 'still', 'has', '9.7', 'inch', 'iPad', 'Pros', 'on', 'display', 'as', 'well', 'as', 'the', 'new', 'iPad', '2017,', 'and', \"I'm\", 'not', 'kidding,', 'I', 'thought', 'they', 'mislabeled', 'the', 'demo', 'units.', 'I', 'was', 'so', 'convinced', 'they', 'had', 'swapped', 'the', 'labels', 'on', 'the', 'demo', 'units', 'that', 'I', 'had', 'to', 'put', 'the', 'serial', 'numbers', 'in', 'a', 'search', 'engine', 'to', 'verify', 'that', 'the', 'iPad', '2017', 'and', 'the', 'iPad', 'Pro', '9.7', 'were', 'properly', 'labeled.\\n\\nIf', 'you', 'know', 'what', 'to', 'look', 'for,', \"it's\", 'more', 'clear,', 'but', 'it', 'is', 'hard', 'to', 'tell', 'the', 'units', 'apart', 'if', \"you're\", 'doing', 'it', 'for', 'the', 'first', 'time.\\n\\nIncidentally,', 'the', 'screen', 'on', 'the', 'iPad', '2017', 'is', 'not', '\"bad\".', 'In', 'fact,', 'if', 'you', 'Google', 'the', 'Notebookcheck', 'measurements', 'of', 'the', 'screen,', \"it's\", 'actually', 'quite', 'accurate.', '(In', 'terms', 'of', 'measured', 'DeltaE', 'within', 'the', 'sRGB', 'gamut,', 'both', 'grey', 'and', 'color,', \"it's\", 'actually', 'better', 'than', 'the', 'measurements', 'Notebookcheck', 'got', 'for', 'the', '10.5', 'iPad', 'Pro', '--', 'though', 'individual', 'numbers', \"aren't\", 'everything', 'and', 'there', 'will', 'obviously', 'be', 'variation', 'between', 'panels.)\\n\\nTwo', 'good', 'things', 'about', 'the', 'iPad', '2017', 'that', \"I've\", 'noticed:', '\\n\\n*', 'There', 'is', 'less', 'color', 'variation', 'severely', 'off-angle', 'than', 'both', 'generations', 'of', 'the', 'iPad', 'Pros.', 'I', 'think', 'this', 'is', 'due', 'to', 'the', 'difference', 'in', 'coatings,', 'or', 'perhaps', 'the', 'presence', 'of', 'the', 'sensing', 'layer', 'for', 'the', 'Apple', 'Pencil.\\n\\n*', 'The', 'anti-fingerprint', 'oleophobic', 'coating', 'on', 'the', 'iPad', '2017', 'is', 'definitely', 'more', 'effective', 'than', 'on', 'the', 'Pros.', 'I', 'think', 'they', 'had', 'to', 'compromise', 'there', 'to', 'support', 'the', 'Apple', 'Pencil.\\n\\nThere', 'is', 'a', 'little', 'more', 'brightness', 'variation', 'on', 'the', 'iPad', '2017', 'in', 'comparison', 'to', 'the', 'Pros,', 'and', 'of', 'course', 'there', 'is', 'no', 'support', 'for', 'an', 'extended', 'gamut', 'or', '120', 'Hz.', 'But', 'on', 'the', 'positive', 'side,', 'I', 'find', 'the', 'iPad', '2017', 'is', 'easier', 'to', 'hold', 'in', 'the', 'hand', 'because', 'of', 'the', 'bezel', 'size', 'and', 'the', 'slightly', 'smaller', 'screen', 'is', 'better', 'for', 'reading', 'eBooks.\\n\\nI', \"wouldn't\", 'hesitate', 'to', 'buy', 'the', 'iPad', '2017.', 'I', 'think', \"it's\", 'one', 'of', 'the', 'best', 'deals', 'Apple', 'has', 'ever', 'offered.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sup', 'picky', 'screens', 'dont', 'think', 'real', 'mat', 'off', 'supply', 'stor', 'near', 'stil', 'ninety-seven', 'inch', 'ipad', 'pro', 'display', 'wel', 'new', 'ipad', 'two thousand and seventeen', 'im', 'kid', 'thought', 'mislabel', 'demo', 'unit', 'convint', 'swap', 'label', 'demo', 'unit', 'put', 'ser', 'numb', 'search', 'engin', 'ver', 'ipad', 'two thousand and seventeen', 'ipad', 'pro', 'ninety-seven', 'prop', 'labeled\\n\\nif', 'know', 'look', 'clear', 'hard', 'tel', 'unit', 'apart', 'yo', 'first', 'time\\n\\nincidentally', 'screen', 'ipad', 'two thousand and seventeen', 'bad', 'fact', 'googl', 'notebookcheck', 'meas', 'screen', 'act', 'quit', 'acc', 'term', 'meas', 'delta', 'within', 'srgb', 'gamut', 'grey', 'col', 'act', 'bet', 'meas', 'notebookcheck', 'got', 'one hundred and fiv', 'ipad', 'pro', 'though', 'individ', 'numb', 'ar', 'everyth', 'obvy', 'vary', 'panels\\n\\ntwo', 'good', 'thing', 'ipad', 'two thousand and seventeen', 'iv', 'not', '\\n\\n', 'less', 'col', 'vary', 'sev', 'offangl', 'gen', 'ipad', 'pro', 'think', 'due', 'diff', 'coat', 'perhap', 'pres', 'sens', 'lay', 'appl', 'pencil\\n\\n', 'antifingerprint', 'oleophob', 'coat', 'ipad', 'two thousand and seventeen', 'definit', 'effect', 'pro', 'think', 'comprom', 'support', 'appl', 'pencil\\n\\nthere', 'littl', 'bright', 'vary', 'ipad', 'two thousand and seventeen', 'comparison', 'pro', 'cours', 'support', 'extend', 'gamut', 'one hundred and twenty', 'hz', 'posit', 'sid', 'find', 'ipad', 'two thousand and seventeen', 'easy', 'hold', 'hand', 'bezel', 'siz', 'slight', 'smal', 'screen', 'bet', 'read', 'ebooks\\n\\ni', 'wouldnt', 'hesit', 'buy', 'ipad', 'two thousand and seventeen', 'think', 'on', 'best', 'deal', 'appl', 'ev', 'off'], ['im', 'super', 'picky', 'screen', 'dont', 'think', 'really', 'matter', 'office', 'supply', 'store', 'near', 'still', 'ninety-seven', 'inch', 'ipad', 'pros', 'display', 'well', 'new', 'ipad', 'two thousand and seventeen', 'im', 'kid', 'think', 'mislabeled', 'demo', 'units', 'convince', 'swap', 'label', 'demo', 'units', 'put', 'serial', 'number', 'search', 'engine', 'verify', 'ipad', 'two thousand and seventeen', 'ipad', 'pro', 'ninety-seven', 'properly', 'labeled\\n\\nif', 'know', 'look', 'clear', 'hard', 'tell', 'units', 'apart', 'youre', 'first', 'time\\n\\nincidentally', 'screen', 'ipad', 'two thousand and seventeen', 'bad', 'fact', 'google', 'notebookcheck', 'measurements', 'screen', 'actually', 'quite', 'accurate', 'term', 'measure', 'deltae', 'within', 'srgb', 'gamut', 'grey', 'color', 'actually', 'better', 'measurements', 'notebookcheck', 'get', 'one hundred and five', 'ipad', 'pro', 'though', 'individual', 'number', 'arent', 'everything', 'obviously', 'variation', 'panels\\n\\ntwo', 'good', 'things', 'ipad', 'two thousand and seventeen', 'ive', 'notice', '\\n\\n', 'less', 'color', 'variation', 'severely', 'offangle', 'generations', 'ipad', 'pros', 'think', 'due', 'difference', 'coat', 'perhaps', 'presence', 'sense', 'layer', 'apple', 'pencil\\n\\n', 'antifingerprint', 'oleophobic', 'coat', 'ipad', 'two thousand and seventeen', 'definitely', 'effective', 'pros', 'think', 'compromise', 'support', 'apple', 'pencil\\n\\nthere', 'little', 'brightness', 'variation', 'ipad', 'two thousand and seventeen', 'comparison', 'pros', 'course', 'support', 'extend', 'gamut', 'one hundred and twenty', 'hz', 'positive', 'side', 'find', 'ipad', 'two thousand and seventeen', 'easier', 'hold', 'hand', 'bezel', 'size', 'slightly', 'smaller', 'screen', 'better', 'read', 'ebooks\\n\\ni', 'wouldnt', 'hesitate', 'buy', 'ipad', 'two thousand and seventeen', 'think', 'one', 'best', 'deal', 'apple', 'ever', 'offer'])\n",
      "original document: \n",
      "['r/me_irl']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rme_irl'], ['rme_irl'])\n",
      "original document: \n",
      "['I', \"don't\", 'know,', 'I', 'think', 'the', 'xim4', 'is', 'a', 'pretty', 'good', 'one.', '', 'Once', 'you', 'get', 'the', 'mouse', 'curve', 'profile', 'tweaked.', '', \"It's\", 'better', 'than', 'the', 'controller', 'for', 'someone', 'used', 'to', 'the', 'kbm.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'think', 'xim4', 'pretty', 'good', 'on', 'get', 'mous', 'curv', 'profil', 'tweak', 'bet', 'control', 'someon', 'us', 'kbm'], ['dont', 'know', 'think', 'xim4', 'pretty', 'good', 'one', 'get', 'mouse', 'curve', 'profile', 'tweak', 'better', 'controller', 'someone', 'use', 'kbm'])\n",
      "original document: \n",
      "['Work']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work'], ['work'])\n",
      "original document: \n",
      "['They', 'track', 'and', 'spy', 'on', 'you', 'and', 'all', 'your', 'activity.', 'There', 'was', 'a', 'canary', 'clause', 'taken', 'down', 'about', '2', 'years', 'ago', 'indicating', 'that', 'they', 'have', 'been', 'commandeered', 'by', 'the', 'NSA', 'or', 'like.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['track', 'spy', 'act', 'can', 'claus', 'tak', 'two', 'year', 'ago', 'ind', 'command', 'nsa', 'lik'], ['track', 'spy', 'activity', 'canary', 'clause', 'take', 'two', 'years', 'ago', 'indicate', 'commandeer', 'nsa', 'like'])\n",
      "original document: \n",
      "['nonsense', \"that's\", 'the', 'fun', 'part!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nonsens', 'that', 'fun', 'part'], ['nonsense', 'thats', 'fun', 'part'])\n",
      "original document: \n",
      "['I', 'just', 'wanna', 'see', 'how', 'we', 'compare', 'to', 'Washington', 'through', 'transitive', 'tbh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wann', 'see', 'comp', 'washington', 'transit', 'tbh'], ['wanna', 'see', 'compare', 'washington', 'transitive', 'tbh'])\n",
      "original document: \n",
      "['\"He', 'works', 'too\"\\n\\n/news', 'posts', 'picture', 'of', 'trump', 'golfing', '\\n\\n\"He', 'was', 'taking', 'a', 'break', 'in', 'between', 'all', 'his', 'deals\"\\n\\nOr', 'something.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'too\\n\\nnews', 'post', 'pict', 'trump', 'golf', '\\n\\nhe', 'tak', 'break', 'deals\\n\\nor', 'someth'], ['work', 'too\\n\\nnews', 'post', 'picture', 'trump', 'golf', '\\n\\nhe', 'take', 'break', 'deals\\n\\nor', 'something'])\n",
      "original document: \n",
      "['Frugality', 'and', 'value', 'are', 'relative.', 'Some', 'people', 'like', 'to', 'look', 'for', 'sales', 'and', 'to', 'invest', 'on', 'higher', 'quality', 'products.', 'If', 'you', 'feel', 'this', 'product', 'does', 'not', 'fit', 'your', 'budget,', 'then', 'this', 'product', 'is', 'not', 'for', 'you.', 'There', 'are', 'cheaper', 'alternatives', 'out', 'there.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['frug', 'valu', 'rel', 'peopl', 'lik', 'look', 'sal', 'invest', 'high', 'qual', 'produc', 'feel', 'produc', 'fit', 'budget', 'produc', 'cheap', 'altern'], ['frugality', 'value', 'relative', 'people', 'like', 'look', 'sales', 'invest', 'higher', 'quality', 'products', 'feel', 'product', 'fit', 'budget', 'product', 'cheaper', 'alternatives'])\n",
      "original document: \n",
      "['What', \"I'm\", 'thinking', 'of', 'is', 'how', 'natives', 'and', 'first', 'nations', 'in', 'North', 'America', 'had', 'lands', 'and', 'property', 'taken', 'by', 'the', 'government.', 'As', 'the', 'government', 'passed', 'laws', 'and', 'executive', 'actions', 'to', 'allow', 'these', 'seizures', 'there', 'is', 'no', \"'legal'\", 'requirement', 'for', 'restitution,', 'but', 'there', 'is', 'obvious', 'moral', 'need', 'to', 'repay', 'what', 'was', 'taken.', 'Is', 'there', 'a', 'name', 'for', 'such', 'debts,', 'where', 'the', 'debtor', 'has', 'escaped', 'legal', 'obligations,', 'but', 'there', 'is', 'a', 'societal', 'and', 'moral', 'expectation', 'they', 'repay', 'the', 'debt?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'think', 'nat', 'first', 'nat', 'nor', 'americ', 'land', 'property', 'tak', 'govern', 'govern', 'pass', 'law', 'execut', 'act', 'allow', 'seiz', 'leg', 'requir', 'restitut', 'obvy', 'mor', 'nee', 'repay', 'tak', 'nam', 'debt', 'debt', 'escap', 'leg', 'oblig', 'societ', 'mor', 'expect', 'repay', 'debt'], ['im', 'think', 'natives', 'first', 'nations', 'north', 'america', 'land', 'property', 'take', 'government', 'government', 'pass', 'laws', 'executive', 'action', 'allow', 'seizures', 'legal', 'requirement', 'restitution', 'obvious', 'moral', 'need', 'repay', 'take', 'name', 'debts', 'debtor', 'escape', 'legal', 'obligations', 'societal', 'moral', 'expectation', 'repay', 'debt'])\n",
      "original document: \n",
      "['11564']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eleven thousand, five hundred and sixty-four'], ['eleven thousand, five hundred and sixty-four'])\n",
      "original document: \n",
      "['Yep,', \"that's\", 'what', \"I'd\", 'suggest', 'too.', '', 'I', 'run', 'Death,', 'Curses,', 'Reaper', 'on', 'my', 'Necro,', 'but', 'Blood', 'is', 'a', 'solid', 'alternate', 'to', 'Reaper', 'if', 'you', \"don't\", 'have', 'HoT.', '', 'That', 'in', 'a', 'condi', 'damage', 'build', 'works', 'nice', 'for', 'open', 'world', \"stuff.\\n\\nI've\", 'been', 'able', 'to', 'solo', 'most', 'champions', 'in', 'open', 'world', '(and', 'HoT', 'HP', 'champs)', 'with', 'it', 'just', 'fine,', 'usually', 'staying', 'near', 'max', 'health', 'while', 'my', 'minions', 'tank.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'that', 'id', 'suggest', 'run', 'dea', 'curs', 'reap', 'necro', 'blood', 'solid', 'altern', 'reap', 'dont', 'hot', 'cond', 'dam', 'build', 'work', 'nic', 'op', 'world', 'stuff\\n\\nive', 'abl', 'solo', 'champ', 'op', 'world', 'hot', 'hp', 'champ', 'fin', 'us', 'stay', 'near', 'max', 'heal', 'min', 'tank'], ['yep', 'thats', 'id', 'suggest', 'run', 'death', 'curse', 'reaper', 'necro', 'blood', 'solid', 'alternate', 'reaper', 'dont', 'hot', 'condi', 'damage', 'build', 'work', 'nice', 'open', 'world', 'stuff\\n\\nive', 'able', 'solo', 'champion', 'open', 'world', 'hot', 'hp', 'champ', 'fine', 'usually', 'stay', 'near', 'max', 'health', 'minions', 'tank'])\n",
      "original document: \n",
      "['Its', 'a', 'good', 'thing', 'D.Va', 'doesnt', 'have', 'any', 'other', 'way', 'of', 'making', 'Zarya', 'use', 'her', 'shield.\\n\\nNone', 'at', 'all.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'thing', 'dva', 'doesnt', 'way', 'mak', 'zary', 'us', 'shield\\n\\nnone'], ['good', 'thing', 'dva', 'doesnt', 'way', 'make', 'zarya', 'use', 'shield\\n\\nnone'])\n",
      "original document: \n",
      "['How', 'hard', 'is', 'it', 'to', 'take', 'apart?', 'My', 'DMS', 'Right', 'is', 'always', 'activated', 'so', 'I', 'was', 'gonna', 'see', 'if', 'I', 'could', 'unstick', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hard', 'tak', 'apart', 'dms', 'right', 'alway', 'act', 'gonn', 'see', 'could', 'unstick'], ['hard', 'take', 'apart', 'dms', 'right', 'always', 'activate', 'gonna', 'see', 'could', 'unstick'])\n",
      "original document: \n",
      "[\"We're\", 'not', 'the', 'target', 'audience.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['target', 'audy'], ['target', 'audience'])\n",
      "original document: \n",
      "['I,', 'too,', 'was', 'at', 'Ribfest.', 'A', 'rib', 'cooker', 'singles', 'out', 'some', 'young', 'guy', 'wearing', 'a', 'white', '\"Where\\'s', 'the', 'beef?\"', 'T-shirt,', 'standing', 'off', 'to', 'the', 'side.', 'He', 'says', '\"Hey', 'buddy!', 'That', 'T-shirt', \"ain't\", 'going', 'to', 'stay', 'white', 'for', 'long\"', 'as', 'he', 'points', 'to', 'his', 'immaculately', 'clean', 'white', 'apron,', 'confusing', 'those', 'watching', 'the', 'exchange.', 'The', 'young', 'guy', 'says', '\"oh,', 'I', \"won't\", 'be', '*eating*', 'any', 'ribs\",', 'and', 'honestly', 'his', 'emphasis', 'of', 'the', 'word', '*eating*', 'creeped', 'me', 'out', 'a', 'bit,', 'but', 'whatever.', 'So', 'the', 'cook', 'huddles', 'with', 'associates', 'and', 'seem', 'to', 'come', 'to', 'some', 'type', 'of', 'agreement', 'and', 'calls', 'back', 'over', 'to', 'the', 'young', 'guy', '\"Hey', 'Buddy!', 'Come', 'here', 'a', 'minute!\"', 'So', 'he', 'walks', 'over', 'and', 'the', 'cook', 'turns', 'on', 'a', 'loud', 'white', 'noise', 'machine', 'and', 'we', \"can't\", 'hear', 'what', \"they're\", 'saying.', 'After', 'a', 'lot', 'of', 'emphatic', 'hand', 'waving,', 'head', 'nodding', '&amp;', 'shaking,', 'and', 'some', 'frantic', 'scribbling', 'and', 'erasing', 'on', 'a', 'white', 'board,', 'the', 'young', 'man', 'walks', 'over', 'to', 'the', 'rib', 'line', '(which', 'is', 'pretty', 'long', 'at', 'this', 'point', 'since', 'no', \"one's\", 'been', 'serving', 'ribs).', 'Everyone', 'in', 'line', 'does', 'a', 'triple-take', 'as', 'the', 'man', 'takes', 'his', 'place', 'in', 'line,', 'and', 'the', 'whole', 'line', 'is', 'now', 'murmuring', 'to', 'each', 'other', 'trying', 'to', 'make', 'sense', 'of', 'it', 'all.', 'Someone', 'finally', 'incredulously', 'asks', '\"What', 'could', 'the', 'cook', '*possibly*', 'have', 'said', 'to', 'you', 'to', 'convince', 'you', 'to', 'eat', 'after', 'you', 'already', 'declined', 'one', 'single', 'time?!?\"', 'The', 'man', 'smiles', 'and', 'says', '\"Well', 'you', 'see,', \"I'm\", 'a', 'VEGAN', 'and', \"can't\", 'eat', 'meat,', 'so', 'the', 'cook', 'had', 'offered', 'to', 'clean', 'off', 'the', 'grill', 'and', 'make', 'me', 'some', 'scrambled', 'eggs', 'instead.\"', 'And', 'one', 'of', 'the', 'people', 'in', 'line', 'says', '\"I', \"didn't\", 'know', 'they', 'had', 'scrambled', 'eggs!', 'I', 'want', 'that\".', 'Others', 'joined', 'in', '\"Me', 'too!\"', '\"I\\'ll', 'pay', 'double!\"', '\"Triple!\"', '\"Money', 'is', 'no', 'object!\"', 'It', 'was', 'pandemonium.', 'Another', 'man', 'stands', 'in', 'line', 'with', 'very', 'puzzled', 'look', 'on', 'his', 'face.', 'The', 'line', 'continues', 'to', 'swell', 'as', 'more', 'people', 'hear', 'the', 'rumor', 'of', 'scramble', 'eggs', 'and', 'eventually', 'all', 'the', 'Vegans', 'at', 'Ribfest', 'make', 'their', 'way', 'to', 'the', 'line.', 'Finally,', 'the', 'puzzled', 'man', 'speaks', 'up,', '\"Excuse', 'me,', 'I', 'know', 'this', 'is', 'none', 'of', 'my', 'business,', 'but', 'do', 'you', 'mean', 'vegetarian?\"', 'The', 'original', 'white', 'shirt', 'guy', 'gives', 'his', 'own', 'puzzled', 'look', 'in', 'return.', 'So', 'the', 'puzzled', 'man', 'clarifies', '\"Well', \"it's\", 'just', 'that', 'you', 'said', \"you're\", 'a', 'vegan', 'but', \"you're\", 'about', 'to', 'eat', 'eggs,', 'which', 'are', 'an', 'animal', 'product', '-', 'plus', 'I', 'think', \"there's\", 'probably', 'milk', 'in', 'it', 'too', 'if', \"they're\", 'doing', 'it', 'right\"', 'Someone', 'else', 'shouts', '\"And', 'vanilla', 'extract!\"', 'And', 'the', 'puzzled', 'guy', 'says', '\"Yeah,', 'maybe.', 'But', \"that's\", 'not', 'an', 'animal', 'product.', 'Anyways,', 'I', 'think', 'maybe', 'you', 'meant', 'vegetarian', 'rather', 'than', 'vegan,', 'since', 'it', 'sounds', 'like', 'you', \"don't\", 'eat', 'meat', 'rather', 'than', 'all', 'animal', 'products?\"', 'White', 'shirt:', '\"Vegan,', 'vegetarian,', \"what's\", 'the', 'difference?\"', 'Puzzled', 'guy:', '\"Um,', 'well,', 'I', '*just*', 'explained', 'the', 'difference.', 'You', 'know', 'what?', 'Never', 'mind;', \"it's\", 'none', 'of', 'my', 'business', 'anyway.\"', 'Then', 'someone', 'pipes', 'up', '\"Won\\'t', 'the', 'egg', 'mixture', 'just', 'run', 'through', 'the', 'grill?\"', '\"Maybe', 'they', 'have', 'a', 'skillet?\"', 'someone', 'replied.', '\\n\\nAfter', 'all', 'the', 'eggs', 'were', 'handed', 'out', 'everyone', 'held', 'hands', 'and', 'ate', 'their', 'eggs', '(with', 'their', 'faces', 'pressed', 'into', 'the', 'paper', 'plates', 'since', 'they', \"didn't\", 'have', 'their', 'hands', 'free).', 'It', 'was', 'the', 'most', 'beautiful', 'thing', \"I've\", 'ever', 'witnessed;', 'people', 'of', 'all', 'genders', 'coming', 'together', 'and', 'eating', 'scramble', 'eggs', 'with', 'their', 'faces.', 'I', 'shed', 'a', 'single', 'tear', 'and', 'clapped,', 'a', 'slow', 'non-sarcastic', 'clap.', 'My', 'hands', 'were', 'free', 'because', 'I', \"couldn't\", 'eat', 'the', 'eggs', '-', \"I'm\", 'gluten', 'intolerant.', 'u/manic_eye']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ribfest', 'rib', 'cook', 'singl', 'young', 'guy', 'wear', 'whit', 'wher', 'beef', 'tshirt', 'stand', 'sid', 'say', 'hey', 'buddy', 'tshirt', 'aint', 'going', 'stay', 'whit', 'long', 'point', 'immac', 'cle', 'whit', 'apron', 'confus', 'watch', 'exchang', 'young', 'guy', 'say', 'oh', 'wont', 'eat', 'rib', 'honest', 'emphas', 'word', 'eat', 'creep', 'bit', 'whatev', 'cook', 'huddl', 'assocy', 'seem', 'com', 'typ', 'agr', 'cal', 'back', 'young', 'guy', 'hey', 'buddy', 'com', 'minut', 'walk', 'cook', 'turn', 'loud', 'whit', 'nois', 'machin', 'cant', 'hear', 'theyr', 'say', 'lot', 'emph', 'hand', 'wav', 'head', 'nod', 'amp', 'shak', 'frant', 'scribbling', 'eras', 'whit', 'board', 'young', 'man', 'walk', 'rib', 'lin', 'pretty', 'long', 'point', 'sint', 'on', 'serv', 'rib', 'everyon', 'lin', 'tripletak', 'man', 'tak', 'plac', 'lin', 'whol', 'lin', 'murm', 'try', 'mak', 'sens', 'someon', 'fin', 'incred', 'ask', 'could', 'cook', 'poss', 'said', 'convint', 'eat', 'already', 'declin', 'on', 'singl', 'tim', 'man', 'smil', 'say', 'wel', 'see', 'im', 'veg', 'cant', 'eat', 'meat', 'cook', 'off', 'cle', 'gril', 'mak', 'scrambled', 'eg', 'instead', 'on', 'peopl', 'lin', 'say', 'didnt', 'know', 'scrambled', 'eg', 'want', 'oth', 'join', 'il', 'pay', 'doubl', 'tripl', 'money', 'object', 'pandemon', 'anoth', 'man', 'stand', 'lin', 'puzzl', 'look', 'fac', 'lin', 'continu', 'swel', 'peopl', 'hear', 'rum', 'scramble', 'eg', 'ev', 'veg', 'ribfest', 'mak', 'way', 'lin', 'fin', 'puzzl', 'man', 'speak', 'excus', 'know', 'non', 'busy', 'mean', 'veget', 'origin', 'whit', 'shirt', 'guy', 'giv', 'puzzl', 'look', 'return', 'puzzl', 'man', 'clar', 'wel', 'said', 'yo', 'veg', 'yo', 'eat', 'eg', 'anim', 'produc', 'plu', 'think', 'ther', 'prob', 'milk', 'theyr', 'right', 'someon', 'els', 'shout', 'vanill', 'extract', 'puzzl', 'guy', 'say', 'yeah', 'mayb', 'that', 'anim', 'produc', 'anyway', 'think', 'mayb', 'meant', 'veget', 'rath', 'veg', 'sint', 'sound', 'lik', 'dont', 'eat', 'meat', 'rath', 'anim', 'produc', 'whit', 'shirt', 'veg', 'veget', 'what', 'diff', 'puzzl', 'guy', 'um', 'wel', 'explain', 'diff', 'know', 'nev', 'mind', 'non', 'busy', 'anyway', 'someon', 'pip', 'wont', 'eg', 'mixt', 'run', 'gril', 'mayb', 'skillet', 'someon', 'reply', '\\n\\nafter', 'eg', 'hand', 'everyon', 'held', 'hand', 'at', 'eg', 'fac', 'press', 'pap', 'plat', 'sint', 'didnt', 'hand', 'fre', 'beauty', 'thing', 'iv', 'ev', 'wit', 'peopl', 'gend', 'com', 'togeth', 'eat', 'scramble', 'eg', 'fac', 'shed', 'singl', 'tear', 'clap', 'slow', 'nonsarcast', 'clap', 'hand', 'fre', 'couldnt', 'eat', 'eg', 'im', 'glut', 'intol', 'umanic_eye'], ['ribfest', 'rib', 'cooker', 'single', 'young', 'guy', 'wear', 'white', 'wheres', 'beef', 'tshirt', 'stand', 'side', 'say', 'hey', 'buddy', 'tshirt', 'aint', 'go', 'stay', 'white', 'long', 'point', 'immaculately', 'clean', 'white', 'apron', 'confuse', 'watch', 'exchange', 'young', 'guy', 'say', 'oh', 'wont', 'eat', 'rib', 'honestly', 'emphasis', 'word', 'eat', 'creep', 'bite', 'whatever', 'cook', 'huddle', 'associate', 'seem', 'come', 'type', 'agreement', 'call', 'back', 'young', 'guy', 'hey', 'buddy', 'come', 'minute', 'walk', 'cook', 'turn', 'loud', 'white', 'noise', 'machine', 'cant', 'hear', 'theyre', 'say', 'lot', 'emphatic', 'hand', 'wave', 'head', 'nod', 'amp', 'shake', 'frantic', 'scribble', 'erase', 'white', 'board', 'young', 'man', 'walk', 'rib', 'line', 'pretty', 'long', 'point', 'since', 'ones', 'serve', 'rib', 'everyone', 'line', 'tripletake', 'man', 'take', 'place', 'line', 'whole', 'line', 'murmur', 'try', 'make', 'sense', 'someone', 'finally', 'incredulously', 'ask', 'could', 'cook', 'possibly', 'say', 'convince', 'eat', 'already', 'decline', 'one', 'single', 'time', 'man', 'smile', 'say', 'well', 'see', 'im', 'vegan', 'cant', 'eat', 'meat', 'cook', 'offer', 'clean', 'grill', 'make', 'scramble', 'egg', 'instead', 'one', 'people', 'line', 'say', 'didnt', 'know', 'scramble', 'egg', 'want', 'others', 'join', 'ill', 'pay', 'double', 'triple', 'money', 'object', 'pandemonium', 'another', 'man', 'stand', 'line', 'puzzle', 'look', 'face', 'line', 'continue', 'swell', 'people', 'hear', 'rumor', 'scramble', 'egg', 'eventually', 'vegans', 'ribfest', 'make', 'way', 'line', 'finally', 'puzzle', 'man', 'speak', 'excuse', 'know', 'none', 'business', 'mean', 'vegetarian', 'original', 'white', 'shirt', 'guy', 'give', 'puzzle', 'look', 'return', 'puzzle', 'man', 'clarify', 'well', 'say', 'youre', 'vegan', 'youre', 'eat', 'egg', 'animal', 'product', 'plus', 'think', 'theres', 'probably', 'milk', 'theyre', 'right', 'someone', 'else', 'shout', 'vanilla', 'extract', 'puzzle', 'guy', 'say', 'yeah', 'maybe', 'thats', 'animal', 'product', 'anyways', 'think', 'maybe', 'mean', 'vegetarian', 'rather', 'vegan', 'since', 'sound', 'like', 'dont', 'eat', 'meat', 'rather', 'animal', 'products', 'white', 'shirt', 'vegan', 'vegetarian', 'whats', 'difference', 'puzzle', 'guy', 'um', 'well', 'explain', 'difference', 'know', 'never', 'mind', 'none', 'business', 'anyway', 'someone', 'pip', 'wont', 'egg', 'mixture', 'run', 'grill', 'maybe', 'skillet', 'someone', 'reply', '\\n\\nafter', 'egg', 'hand', 'everyone', 'hold', 'hand', 'eat', 'egg', 'face', 'press', 'paper', 'plat', 'since', 'didnt', 'hand', 'free', 'beautiful', 'thing', 'ive', 'ever', 'witness', 'people', 'genders', 'come', 'together', 'eat', 'scramble', 'egg', 'face', 'shed', 'single', 'tear', 'clap', 'slow', 'nonsarcastic', 'clap', 'hand', 'free', 'couldnt', 'eat', 'egg', 'im', 'gluten', 'intolerant', 'umanic_eye'])\n",
      "original document: \n",
      "['I', 'mean,', 'if', 'I', 'was', 'nice', 'and', 'pulled', 'them', 'aside', 'or', 'messaged', 'them', 'anonymously', 'to', 'let', 'them', 'know', \"maybe.\\n\\nI'm\", 'talking', 'full', 'on', 'Spongebob', '\"bAriSta\"', '\"eXpReSsO\"', 'shit.', 'lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'nic', 'pul', 'asid', 'mess', 'anonym', 'let', 'know', 'maybe\\n\\nim', 'talk', 'ful', 'spongebob', 'barist', 'expresso', 'shit', 'lol'], ['mean', 'nice', 'pull', 'aside', 'message', 'anonymously', 'let', 'know', 'maybe\\n\\nim', 'talk', 'full', 'spongebob', 'barista', 'expresso', 'shit', 'lol'])\n",
      "original document: \n",
      "['Oh', 'my', 'god,', 'yes', '-', 'when', 'we', 'do', 'laundry,', 'we', 'tend', 'to', 'throw', 'the', 'clean', 'clothes', 'into', 'a', 'pile', 'on', 'the', 'floor', 'of', 'my', 'craft', 'room.', 'And', 'then', '*I*', 'separate', 'it', 'out,', 'leave', 'him', 'a', 'pile', 'of', 'his', 'clothes,', 'and', 'it', 'stays', 'there', 'for', 'days', 'until', 'I', 'literally', 'dump', 'it', 'on', 'his', 'side', 'of', 'the', 'bed', 'so', 'he', 'has', 'to', 'deal', 'with', 'it', 'before', 'he', 'goes', 'to', 'sleep.', \"Don't\", 'be', 'sorry,', \"I'm\", 'working', 'on', 'my', 'third', 'glass', 'of', 'wine,', 'because', \"SOMEONE's\", 'gotta', 'finish', 'off', 'that', 'Carlo', 'Rossi', 'so', 'we', 'can', 'have', 'a', 'change', 'jar.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'god', 'ye', 'laundry', 'tend', 'throw', 'cle', 'cloth', 'pil', 'flo', 'craft', 'room', 'sep', 'leav', 'pil', 'cloth', 'stay', 'day', 'lit', 'dump', 'sid', 'bed', 'deal', 'goe', 'sleep', 'dont', 'sorry', 'im', 'work', 'third', 'glass', 'win', 'someon', 'gott', 'fin', 'carlo', 'ross', 'chang', 'jar'], ['oh', 'god', 'yes', 'laundry', 'tend', 'throw', 'clean', 'clothe', 'pile', 'floor', 'craft', 'room', 'separate', 'leave', 'pile', 'clothe', 'stay', 'days', 'literally', 'dump', 'side', 'bed', 'deal', 'go', 'sleep', 'dont', 'sorry', 'im', 'work', 'third', 'glass', 'wine', 'someones', 'gotta', 'finish', 'carlo', 'rossi', 'change', 'jar'])\n",
      "original document: \n",
      "['Well', 'an', 'important', 'distinction', 'to', 'make', 'is', 'that', 'Puerto', 'Rico', \"isn't\", 'state.', 'Second,', 'around', '80%', 'speak', 'only', 'Spanish,', 'which', 'furthers', 'the', 'us', 'vs', 'them.', 'And', 'Third,', \"it's\", 'hard', 'to', 'go', 'to', 'a', 'place', \"that's\", 'been', 'leveled', 'to', 'the', 'ground', 'to', 'help', 'out.', 'Houston', 'still', 'had', 'some', 'infrastructure.', 'There', 'is', 'basically', 'nothing', 'in', 'Puerto', 'Rico', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'import', 'distinct', 'mak', 'puerto', 'rico', 'isnt', 'stat', 'second', 'around', 'eighty', 'speak', 'span', 'furth', 'us', 'vs', 'third', 'hard', 'go', 'plac', 'that', 'level', 'ground', 'help', 'houston', 'stil', 'infrastruct', 'bas', 'noth', 'puerto', 'rico'], ['well', 'important', 'distinction', 'make', 'puerto', 'rico', 'isnt', 'state', 'second', 'around', 'eighty', 'speak', 'spanish', 'further', 'us', 'vs', 'third', 'hard', 'go', 'place', 'thats', 'level', 'grind', 'help', 'houston', 'still', 'infrastructure', 'basically', 'nothing', 'puerto', 'rico'])\n",
      "original document: \n",
      "['Ghostfacewilly1']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ghostfacewilly1'], ['ghostfacewilly1'])\n",
      "original document: \n",
      "['Have', 'you', 'seen', 'the', 'videos?', 'Or', 'of', 'musical.ly', 'in', 'general?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seen', 'video', 'mus', 'gen'], ['see', 'videos', 'musically', 'general'])\n",
      "original document: \n",
      "['Oh,', 'okay', 'you’re', 'actually', 'not', 'an', 'asshole,', 'just', 'human.', 'Sorry', 'that', 'this', 'happened', 'to', 'you,', 'in', 'Steam', 'I', 'recommend', 'you', 'physically', 'back', 'up', 'your', 'saves', 'because', 'it’s', 'a', 'little', 'bad', 'too', 'with', 'it’s', 'cloud', 'saves', 'especially', 'if', 'you', 'mod', 'games.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'okay', 'yo', 'act', 'asshol', 'hum', 'sorry', 'hap', 'steam', 'recommend', 'phys', 'back', 'sav', 'littl', 'bad', 'cloud', 'sav', 'espec', 'mod', 'gam'], ['oh', 'okay', 'youre', 'actually', 'asshole', 'human', 'sorry', 'happen', 'steam', 'recommend', 'physically', 'back', 'save', 'little', 'bad', 'cloud', 'save', 'especially', 'mod', 'game'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Is', 'there', 'not', 'an', 'official', 'rules', 'set', 'made', 'for', 'the', 'anime', 'yet?', \"I'm\", 'really', 'surprised.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['off', 'rul', 'set', 'mad', 'anim', 'yet', 'im', 'real', 'surpr'], ['official', 'rule', 'set', 'make', 'anime', 'yet', 'im', 'really', 'surprise'])\n",
      "original document: \n",
      "['Search', \"doesn't\", 'work', 'most', 'the', 'time', 'on', 'mobile', 'and', 'most', 'reddit', 'users', 'are', 'mobile', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['search', 'doesnt', 'work', 'tim', 'mobl', 'reddit', 'us', 'mobl'], ['search', 'doesnt', 'work', 'time', 'mobile', 'reddit', 'users', 'mobile'])\n",
      "original document: \n",
      "['The', 'feathery', 'creatures', 'keep', 'rushing', 'past', 'you', 'in', 'this', 'tunnel,', 'it', 'is', 'actually', 'just', 'a', 'bit', 'higher', 'than', 'you', 'are', 'tall', 'and', 'about', '5', 'times', 'as', 'wide,', 'it', 'is', 'damp', 'and', 'dirty.', 'One', 'of', 'the', 'creatures', 'takes', 'your', 'hand', 'and', 'while', 'pulling', 'you', 'talks.\\n\\n[\"Keep', 'quiet', 'already,', 'or', 'they', 'will', 'find', 'us.\"](/white)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feathery', 'cre', 'keep', 'rush', 'past', 'tunnel', 'act', 'bit', 'high', 'tal', 'fiv', 'tim', 'wid', 'damp', 'dirty', 'on', 'cre', 'tak', 'hand', 'pul', 'talks\\n\\nkeep', 'quiet', 'already', 'find', 'uswhit'], ['feathery', 'creatures', 'keep', 'rush', 'past', 'tunnel', 'actually', 'bite', 'higher', 'tall', 'five', 'time', 'wide', 'damp', 'dirty', 'one', 'creatures', 'take', 'hand', 'pull', 'talks\\n\\nkeep', 'quiet', 'already', 'find', 'uswhite'])\n",
      "original document: \n",
      "['Learned', '*Logistics*', 'on', 'the', 'Twelfth', 'Moon,', '370.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['learn', 'log', 'twelf', 'moon', 'three hundred and seventy'], ['learn', 'logistics', 'twelfth', 'moon', 'three hundred and seventy'])\n",
      "original document: \n",
      "['Oh', 'my', 'goodness.', 'This', 'is', 'pure', 'adorableness.', 'She', 'looks', 'so', 'happy.', 'I', 'bet', \"she'll\", 'remember', 'this', 'forever.', \"You're\", 'wonderful.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'good', 'pur', 'ad', 'look', 'happy', 'bet', 'shel', 'rememb', 'forev', 'yo', 'wond'], ['oh', 'goodness', 'pure', 'adorableness', 'look', 'happy', 'bet', 'shell', 'remember', 'forever', 'youre', 'wonderful'])\n",
      "original document: \n",
      "['https://media.giphy.com/media/GcDtLf4RAdiRG/source.gif']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsmediagiphycommediagcdtlf4radirgsourcegif'], ['httpsmediagiphycommediagcdtlf4radirgsourcegif'])\n",
      "original document: \n",
      "['idk', 'my', 'moms', 'pretty', 'cool', 'for', 'the', 'most', 'part.', 'But', 'every', 'time', 'I', 'try', 'to', 'have', 'a', 'conversation', 'with', 'my', 'dad', 'about', 'anything', 'other', 'than', 'sports', 'or', 'politics', 'it', 'feels', 'kinda', 'awkward', 'and', 'I', 'don’t', 'interact', 'with', 'him', 'a', 'lot', 'at', 'home.', 'We', 'share', 'very', 'little', 'in', 'common,', 'compared', 'to', 'me', 'and', 'my', 'mom', 'who', 'actually', 'have', 'a', 'decent', 'amount', 'in', 'common', '(we', 'like', 'the', 'same', 'type', 'of', 'movies,', 'we', 'have', 'TV', 'shows', 'we', 'watch', 'together,', 'etc.)\\n\\nNot', 'to', 'say', 'I', 'don’t', 'love', 'my', 'dad.', 'He', 'works', 'hard', 'for', 'me', 'and', 'the', 'rest', 'of', 'our', 'family', 'and', 'he', 'genuinely', 'means', 'well.', 'He’s', 'a', 'bit', 'strict', 'and', 'sometimes', 'he', 'goes', 'overboard', 'with', 'it', 'but', 'for', 'the', 'most', 'part', 'my', 'parents', 'are', 'cool.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['idk', 'mom', 'pretty', 'cool', 'part', 'every', 'tim', 'try', 'convers', 'dad', 'anyth', 'sport', 'polit', 'feel', 'kind', 'awkward', 'dont', 'interact', 'lot', 'hom', 'shar', 'littl', 'common', 'comp', 'mom', 'act', 'dec', 'amount', 'common', 'lik', 'typ', 'movy', 'tv', 'show', 'watch', 'togeth', 'etc\\n\\nnot', 'say', 'dont', 'lov', 'dad', 'work', 'hard', 'rest', 'famy', 'genuin', 'mean', 'wel', 'hes', 'bit', 'strict', 'sometim', 'goe', 'overboard', 'part', 'par', 'cool'], ['idk', 'moms', 'pretty', 'cool', 'part', 'every', 'time', 'try', 'conversation', 'dad', 'anything', 'sport', 'politics', 'feel', 'kinda', 'awkward', 'dont', 'interact', 'lot', 'home', 'share', 'little', 'common', 'compare', 'mom', 'actually', 'decent', 'amount', 'common', 'like', 'type', 'movies', 'tv', 'show', 'watch', 'together', 'etc\\n\\nnot', 'say', 'dont', 'love', 'dad', 'work', 'hard', 'rest', 'family', 'genuinely', 'mean', 'well', 'hes', 'bite', 'strict', 'sometimes', 'go', 'overboard', 'part', 'parent', 'cool'])\n",
      "original document: \n",
      "['The', 'Hell', 'Express.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hel', 'express'], ['hell', 'express'])\n",
      "original document: \n",
      "['Absolutely', 'gorgeous']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['absolv', 'gorg'], ['absolutely', 'gorgeous'])\n",
      "original document: \n",
      "['143414467|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'eFvRVltr)\\n\\n&gt;&gt;143413729\\nThats', 'just', 'hurtful\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, four hundred and sixty-seven', 'gt', 'unit', 'stat', 'anonym', 'id', 'efvrvltr\\n\\ngtgt143413729\\nthats', 'hurtful\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, four hundred and sixty-seven', 'gt', 'unite', 'state', 'anonymous', 'id', 'efvrvltr\\n\\ngtgt143413729\\nthats', 'hurtful\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['GOOD', \"FRICKIN'\", 'GOD.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'frickin', 'god'], ['good', 'frickin', 'god'])\n",
      "original document: \n",
      "['We', 'so', 'bad.', 'So', 'bad...', ':*(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bad', 'bad'], ['bad', 'bad'])\n",
      "original document: \n",
      "[\"That's\", 'not', 'exactly', 'true']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'exact', 'tru'], ['thats', 'exactly', 'true'])\n",
      "original document: \n",
      "['Get', 'a', 'scan', 'gauge', 'before', 'shit', 'gets', 'weird.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'scan', 'gaug', 'shit', 'get', 'weird'], ['get', 'scan', 'gauge', 'shit', 'get', 'weird'])\n",
      "original document: \n",
      "['Ah,', 'a', 'micro', 'fetish!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'micro', 'fet'], ['ah', 'micro', 'fetish'])\n",
      "original document: \n",
      "['At', 'least', 'we', 'all', 'have', 'access', 'to', 'alcohol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'access', 'alcohol'], ['least', 'access', 'alcohol'])\n",
      "original document: \n",
      "['This', 'person', 'is', 'very', 'confused']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['person', 'confus'], ['person', 'confuse'])\n",
      "original document: \n",
      "['two', 'random', 'spots']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'spot'], ['two', 'random', 'spot'])\n",
      "original document: \n",
      "['Start', 'planning', 'then', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'plan'], ['start', 'plan'])\n",
      "original document: \n",
      "['On', 'an', 'updated', 'note,', 'Tennessee', 'got', 'shut', 'out', 'today.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['upd', 'not', 'ten', 'got', 'shut', 'today'], ['update', 'note', 'tennessee', 'get', 'shut', 'today'])\n",
      "original document: \n",
      "['At', 'first', 'I', 'thought', 'he', 'fucked', 'up', 'the', 'kick.', \"He's\", 'incredible.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'thought', 'fuck', 'kick', 'hes', 'incred'], ['first', 'think', 'fuck', 'kick', 'hes', 'incredible'])\n",
      "original document: \n",
      "['Like', 'Saudi', 'Arabia', 'and', 'the', 'USA', 'are', 'now', 'controlled', 'by', 'the', 'rich', 'and', 'for', 'the', 'rich.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'saud', 'arab', 'us', 'control', 'rich', 'rich'], ['like', 'saudi', 'arabia', 'usa', 'control', 'rich', 'rich'])\n",
      "original document: \n",
      "['I', 'bet', 'they', 'are.', 'I', 'hope', 'you', 'bought', 'multiple', 'pairs', 'in', 'different', 'colors', 'so', 'you', 'can', 'feel', 'amazing', 'all', 'week.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'hop', 'bought', 'multipl', 'pair', 'diff', 'col', 'feel', 'amaz', 'week'], ['bet', 'hope', 'buy', 'multiple', 'pair', 'different', 'color', 'feel', 'amaze', 'week'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['zoz']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zoz'], ['zoz'])\n",
      "original document: \n",
      "['Please,', 'stop', 'making', 'it', 'into', 'a', 'joke', 'and', 'call', 'it', 'what', 'it', 'is:\\n\\npropaganda.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'stop', 'mak', 'jok', 'cal', 'is\\n\\npropaganda'], ['please', 'stop', 'make', 'joke', 'call', 'is\\n\\npropaganda'])\n",
      "original document: \n",
      "['You', 'can', 'get', 'Frutopia', 'at', 'any', 'Subway', 'or', 'movie', 'theater', 'in', 'Canada.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'frutop', 'subway', 'movy', 'the', 'canad'], ['get', 'frutopia', 'subway', 'movie', 'theater', 'canada'])\n",
      "original document: \n",
      "['[Or](https://abimon.org/dr/busts/celeste/15.png#sprite)', 'perhaps', 'he', 'suffered', 'an', 'unfortunate', 'accident', 'when', 'you', 'came', 'into', 'the', 'room.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['orhttpsabimonorgdrbustsceleste15pngsprit', 'perhap', 'suff', 'unfortun', 'accid', 'cam', 'room'], ['orhttpsabimonorgdrbustsceleste15pngsprite', 'perhaps', 'suffer', 'unfortunate', 'accident', 'come', 'room'])\n",
      "original document: \n",
      "['Will', 'women', 'be', 'allowed', 'to', 'get', 'them', 'up', 'on', 'two', 'wheels?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wom', 'allow', 'get', 'two', 'wheel'], ['women', 'allow', 'get', 'two', 'wheel'])\n",
      "original document: \n",
      "['**[MIRROR:', 'Someone', 'in', 'chat', 'triggers', 'a', 'sfx', 'at', 'the', 'perfect', 'time.](https://livestreamfails.com/post/7082)**\\n\\n---\\nCredit', 'to', '[twitch.tv/ramentard](https://www.twitch.tv/ramentard)', 'for', 'the', 'content', 'and', '[reddit.com/u/Baylix](https://reddit.com/user/baylix)', 'for', 'the', 'clip.', '[[Streamable', 'Alternative]](https://streamable.com/a9yz5)', '[[Vidme', 'Alternative]](https://vid.me/7h1A5)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mir', 'someon', 'chat', 'trig', 'sfx', 'perfect', 'timehttpslivestreamfailscompost7082\\n\\n\\ncredit', 'twitchtvramentardhttpswwwtwitchtvramentard', 'cont', 'redditcomubaylixhttpsredditcomuserbaylix', 'clip', 'streamable', 'alternativehttpsstreamablecoma9yz5', 'vidm', 'alternativehttpsvidme7h1a5'], ['mirror', 'someone', 'chat', 'trigger', 'sfx', 'perfect', 'timehttpslivestreamfailscompost7082\\n\\n\\ncredit', 'twitchtvramentardhttpswwwtwitchtvramentard', 'content', 'redditcomubaylixhttpsredditcomuserbaylix', 'clip', 'streamable', 'alternativehttpsstreamablecoma9yz5', 'vidme', 'alternativehttpsvidme7h1a5'])\n",
      "original document: \n",
      "['&gt;No,', \"you're\", 'playing', 'word', 'games.', 'Either', 'enslavement', 'is', 'the', 'price', 'of', 'being', 'saved,', 'or', 'the', 'act', 'of', 'saving', 'and', 'the', 'act', 'of', 'exploiting', 'are', 'separate.', 'You', \"can't\", 'have', 'it', 'both', \"ways.\\n\\nThat's\", 'not', 'how', \"you've\", 'described', 'the', 'situation.', 'If', 'they', 'offered', 'him', 'safety', 'in', 'exchange', 'for', 'becoming', 'a', 'slave,', 'while', 'he', 'was', 'floating', 'through', 'space,', 'that', 'would', 'be', 'a', 'different', 'issue.', 'This', 'would', 'be', 'the', 'one', 'case', 'where', 'the', 'act', 'of', 'enslaving', 'actually', 'improves', 'the', 'situation.\\n\\n&gt;Here', 'you', 'are', 'not', 'comparing', 'exploitation', 'with', 'slavery,', 'you', 'are', 'comparing', 'selling', 'with', 'slavery.', 'Selling', 'someone', 'an', 'apple', 'is', 'not', 'exploitation.\\n\\nOK:', 'You', 'are', 'starving', 'and', 'I', 'sell', 'you', 'an', 'apple', 'for', '100', '000$.', '-', 'I', 'still', 'improve', 'your', 'situation.', '\\n\\n&gt;Maintaining', 'slavery', 'requires', 'constantly', 'feeding,', 'clothing', 'and', 'sheltering', 'your', 'slaves,', 'so', 'you', 'are', 'continuously', 'improving', 'their', 'position.', 'These', 'acts', 'are', 'necessary', 'to', 'slavery,', 'but', 'slavery', 'is', 'not', 'necessary', 'to', 'these', 'acts.\\n\\nAh,', 'but', \"here's\", 'the', 'rub!', 'The', 'only', 'person', 'who', 'can', 'judge', 'the', 'change', 'in', 'quality', 'of', 'the', 'situation', 'is', 'the', 'person', 'themselves.', 'If', 'the', 'upkeep', 'produced', 'an', 'actual', 'net', 'improvement', 'to', 'the', \"slave's\", 'situation,', 'they', 'would', 'not', 'need', 'to', 'be', 'a', 'slave', '-', 'they', 'would', 'work', 'voluntarily!', 'The', 'fact', 'that', 'a', 'threat', 'of', 'violence', 'is', 'necessary', 'to', 'keep', 'the', 'slave', 'working', 'implies', 'that', 'maintaining', 'slavery', 'actually', 'worsens', 'their', 'situation.\\n\\nFor', 'slavery', 'to', 'be', 'slavery', '(forced', 'actions),', 'a', 'threat', 'of', 'force', 'must', 'be', 'present.', 'And', 'this', 'means', 'that', 'the', 'slaver', 'necessarily', 'worsens', 'the', \"slave's\", 'situation', 'to', 'make', 'them', 'choose', 'to', 'work!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtno', 'yo', 'play', 'word', 'gam', 'eith', 'enslav', 'pric', 'sav', 'act', 'sav', 'act', 'exploit', 'sep', 'cant', 'ways\\n\\nthat', 'youv', 'describ', 'situ', 'off', 'saf', 'exchang', 'becom', 'slav', 'flo', 'spac', 'would', 'diff', 'issu', 'would', 'on', 'cas', 'act', 'enslav', 'act', 'improv', 'situation\\n\\ngthere', 'comp', 'exploit', 'slavery', 'comp', 'sel', 'slavery', 'sel', 'someon', 'appl', 'exploitation\\n\\nok', 'starv', 'sel', 'appl', 'one hundred', 'zero', 'stil', 'improv', 'situ', '\\n\\ngtmaintaining', 'slavery', 'requir', 'const', 'fee', 'cloth', 'shelt', 'slav', 'continu', 'improv', 'posit', 'act', 'necess', 'slavery', 'slavery', 'necess', 'acts\\n\\nah', 'her', 'rub', 'person', 'judg', 'chang', 'qual', 'situ', 'person', 'upkeep', 'produc', 'act', 'net', 'improv', 'slav', 'situ', 'would', 'nee', 'slav', 'would', 'work', 'volunt', 'fact', 'threat', 'viol', 'necess', 'keep', 'slav', 'work', 'imply', 'maintain', 'slavery', 'act', 'wors', 'situation\\n\\nfor', 'slavery', 'slavery', 'forc', 'act', 'threat', 'forc', 'must', 'pres', 'mean', 'slav', 'necess', 'wors', 'slav', 'situ', 'mak', 'choos', 'work'], ['gtno', 'youre', 'play', 'word', 'game', 'either', 'enslavement', 'price', 'save', 'act', 'save', 'act', 'exploit', 'separate', 'cant', 'ways\\n\\nthats', 'youve', 'describe', 'situation', 'offer', 'safety', 'exchange', 'become', 'slave', 'float', 'space', 'would', 'different', 'issue', 'would', 'one', 'case', 'act', 'enslave', 'actually', 'improve', 'situation\\n\\ngthere', 'compare', 'exploitation', 'slavery', 'compare', 'sell', 'slavery', 'sell', 'someone', 'apple', 'exploitation\\n\\nok', 'starve', 'sell', 'apple', 'one hundred', 'zero', 'still', 'improve', 'situation', '\\n\\ngtmaintaining', 'slavery', 'require', 'constantly', 'feed', 'clothe', 'shelter', 'slave', 'continuously', 'improve', 'position', 'act', 'necessary', 'slavery', 'slavery', 'necessary', 'acts\\n\\nah', 'heres', 'rub', 'person', 'judge', 'change', 'quality', 'situation', 'person', 'upkeep', 'produce', 'actual', 'net', 'improvement', 'slave', 'situation', 'would', 'need', 'slave', 'would', 'work', 'voluntarily', 'fact', 'threat', 'violence', 'necessary', 'keep', 'slave', 'work', 'imply', 'maintain', 'slavery', 'actually', 'worsen', 'situation\\n\\nfor', 'slavery', 'slavery', 'force', 'action', 'threat', 'force', 'must', 'present', 'mean', 'slaver', 'necessarily', 'worsen', 'slave', 'situation', 'make', 'choose', 'work'])\n",
      "original document: \n",
      "['The', 'Ello', \"isn't\", 'going', 'to', 'blow', 'your', 'mind', 'but', 'it', 'is', 'one', 'of', 'the', 'best', 'included', 'in', 'a', 'kit', 'with', 'very', 'dependable', 'coils.', 'The', 'only', 'other', 'kits', 'that', 'have', 'a', 'good', 'tank', 'are', 'the', 'Vaporesso', 'mods', 'as', 'far', 'as', 'I', 'know.\\n\\nThe', 'Vtwo', 'has', 'a', 'trash', 'tank,', 'kroma-a', 'is', 'meh,', 'Kanger', 'tanks', 'are', 'across', 'the', 'board', 'bad,', 'etc.\\n\\nEdit:', 'the', 'Ello', 'is', 'probably', 'disappointing', 'because', 'of', 'how', 'good', 'the', 'original', 'Pico', 'tank', 'was.', 'The', 'Mello.', 'That', \"doesn't\", 'mean', 'the', 'Ello', 'is', 'bad.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ello', 'isnt', 'going', 'blow', 'mind', 'on', 'best', 'includ', 'kit', 'depend', 'coil', 'kit', 'good', 'tank', 'vaporesso', 'mod', 'far', 'know\\n\\nthe', 'vtwo', 'trash', 'tank', 'kroma', 'meh', 'kang', 'tank', 'across', 'board', 'bad', 'etc\\n\\nedit', 'ello', 'prob', 'disappoint', 'good', 'origin', 'pico', 'tank', 'mello', 'doesnt', 'mean', 'ello', 'bad', '\\n'], ['ello', 'isnt', 'go', 'blow', 'mind', 'one', 'best', 'include', 'kit', 'dependable', 'coil', 'kit', 'good', 'tank', 'vaporesso', 'mods', 'far', 'know\\n\\nthe', 'vtwo', 'trash', 'tank', 'kromaa', 'meh', 'kanger', 'tank', 'across', 'board', 'bad', 'etc\\n\\nedit', 'ello', 'probably', 'disappoint', 'good', 'original', 'pico', 'tank', 'mello', 'doesnt', 'mean', 'ello', 'bad', '\\n'])\n",
      "original document: \n",
      "['Interesting', 'that', 'these', 'days', 'yellow', 'cards', 'are', 'basically', 'something', 'you', 'have', 'to', 'have', 'a', 'strategy', 'for.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'day', 'yellow', 'card', 'bas', 'someth', 'strategy'], ['interest', 'days', 'yellow', 'card', 'basically', 'something', 'strategy'])\n",
      "original document: \n",
      "['My', 'advice', 'is', 'not', 'to', 'think', 'too', 'much', 'about', 'the', 'numbers;', 'the', 'entire', 'war', 'in', 'this', 'series', 'is', 'being', 'fought', 'by', 'like', 'twelve', 'people.', 'Assume', 'the', 'rest', 'of', 'the', 'galaxy', 'is', 'having', 'their', 'own', 'dramatic', 'conflicts', 'just', 'offscreen.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['adv', 'think', 'much', 'numb', 'entir', 'war', 'sery', 'fought', 'lik', 'twelv', 'peopl', 'assum', 'rest', 'galaxy', 'dram', 'conflict', 'offscreen'], ['advice', 'think', 'much', 'number', 'entire', 'war', 'series', 'fight', 'like', 'twelve', 'people', 'assume', 'rest', 'galaxy', 'dramatic', 'conflict', 'offscreen'])\n",
      "original document: \n",
      "[\"I'd\", 'start', 'Gillislee', 'over', 'Carson', 'every', 'week', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'start', 'gillisl', 'carson', 'every', 'week', 'lol'], ['id', 'start', 'gillislee', 'carson', 'every', 'week', 'lol'])\n",
      "original document: \n",
      "['I', 'would', 'say', 'their', 'denim', 'is', 'the', 'best', 'bet.', \"I've\", 'bought', 'several', 'pairs', 'from', 'them', 'and', 'they', 'have', 'all', 'turned', 'out', 'well.', '\\n\\nThe', 'tees', 'on', 'the', 'other', 'hand', 'are', 'pretty', 'inconsistent.', 'I', 'have', 'some', 'that', 'fit', 'perfectly', 'and', 'others', 'that', 'fit', 'like', 'crap.', 'They', 'also', 'do', 'not', 'hold', 'up', 'too', 'well', 'over', 'time.', 'They', 'tend', 'to', 'really', 'lose', 'their', 'shape', 'in', 'the', 'wash', 'compared', 'to', 'my', 'Reigning', 'Champ', 'and', '3Sixteen', 'shirts,', 'but', \"that's\", 'a', 'bit', 'of', 'unfair', 'comparison.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'say', 'denim', 'best', 'bet', 'iv', 'bought', 'sev', 'pair', 'turn', 'wel', '\\n\\nthe', 'tee', 'hand', 'pretty', 'inconsist', 'fit', 'perfect', 'oth', 'fit', 'lik', 'crap', 'also', 'hold', 'wel', 'tim', 'tend', 'real', 'los', 'shap', 'wash', 'comp', 'reign', 'champ', '3sixteen', 'shirt', 'that', 'bit', 'unfair', 'comparison'], ['would', 'say', 'denim', 'best', 'bet', 'ive', 'buy', 'several', 'pair', 'turn', 'well', '\\n\\nthe', 'tee', 'hand', 'pretty', 'inconsistent', 'fit', 'perfectly', 'others', 'fit', 'like', 'crap', 'also', 'hold', 'well', 'time', 'tend', 'really', 'lose', 'shape', 'wash', 'compare', 'reign', 'champ', '3sixteen', 'shirt', 'thats', 'bite', 'unfair', 'comparison'])\n",
      "original document: \n",
      "['Fake', 'service', 'dogs', 'are', 'a', 'problem', 'but', \"it's\", 'usually', 'obvious', 'when', \"it's\", 'not', 'a', 'service', 'dog.', 'A', 'family', 'member', 'of', 'mine', 'had', 'a', 'terrible', 'woman', 'as', 'a', 'girlfriend', 'who', 'brought', 'her', 'dog', 'to', 'a', 'hotel', 'under', 'the', 'guise', 'of', 'being', 'a', 'service', 'dog.', 'It', 'barked', 'and', 'ran', 'around', 'her.', 'It', 'was', 'infuriating.', 'From', 'what', \"you've\", 'said,', 'I', \"don't\", 'understand', 'why', 'she', 'thought', 'your', 'dog', 'was', 'a', 'fraud.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fak', 'serv', 'dog', 'problem', 'us', 'obvy', 'serv', 'dog', 'famy', 'memb', 'min', 'terr', 'wom', 'girlfriend', 'brought', 'dog', 'hotel', 'guis', 'serv', 'dog', 'bark', 'ran', 'around', 'infury', 'youv', 'said', 'dont', 'understand', 'thought', 'dog', 'fraud'], ['fake', 'service', 'dog', 'problem', 'usually', 'obvious', 'service', 'dog', 'family', 'member', 'mine', 'terrible', 'woman', 'girlfriend', 'bring', 'dog', 'hotel', 'guise', 'service', 'dog', 'bark', 'run', 'around', 'infuriate', 'youve', 'say', 'dont', 'understand', 'think', 'dog', 'fraud'])\n",
      "original document: \n",
      "['Yellow', 'https://twitter.com/CityLinkMelb/status/914071763058696192']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yellow', 'httpstwittercomcitylinkmelbstatus914071763058696192'], ['yellow', 'httpstwittercomcitylinkmelbstatus914071763058696192'])\n",
      "original document: \n",
      "['A', 'month', 'into', 'the', 'school', 'year', 'and', 'LO', 'is', 'settled', 'in', 'nicely', 'to', 'the', 'routine!', \"She's\", 'in', 'a', 'one', 'year', 'old', 'room', \"(she's\", '19', 'months)', 'at', 'the', 'center', 'that', 'I', 'work', 'at.', '', 'Group', 'size', 'is', 'small', '(3', 'teachers', 'for', 'a', 'class', 'of', '6)', 'and', 'I', 'love', 'the', 'her', 'teachers.', \"She's\", '', 'helper', 'at', 'school', 'but', 'gets', 'whiny', 'if', \"she's\", 'hungry', 'and', \"it's\", 'not', 'snack', 'time', 'or', 'lunch', 'time', 'yet.', '', 'So', 'pretty', 'reasonable!', 'Everyone', 'loves', 'her.', '', 'Except', 'for', 'that', 'one', 'time', 'that', 'she', 'covered', 'herself', 'in', 'bird', 'poop', 'on', 'the', 'playground', 'somehow.', '', 'I', 'thought', 'it', 'was', 'hilarious.\\n\\nThe', 'worst', 'part', 'is', 'having', 'to', 'pre-make', 'food', 'for', 'her', 'every', 'single', 'day.', '', 'Breakfast,', 'AM', 'Snack,', 'Lunch,', 'and', 'PM', 'Snack.', 'So', 'tiring!', 'Luckily', \"we're\", 'getting', 'a', 'routine', 'down', 'for', 'that', 'too.\\n\\nNot', 'School', 'related-', 'My', 'husband', 'bought', 'me', 'a', 'piece', 'of', 'wood', 'so', 'that', 'I', 'can', 'make', 'a', 'height', 'ruler', 'chart', 'to', 'have', 'on', 'the', 'wall', 'in', 'our', 'apartment.', '', 'I', 'want', 'to', 'make', 'it', '(as', 'opposed', 'to', 'just', 'using', 'a', 'door', 'jamb)', 'so', 'that', 'when', 'we', 'get', 'a', 'house', 'of', 'our', 'own', 'we', 'can', 'bring', 'it', 'with', 'us!', '', \"I'm\", 'excited', ':)\\n\\nTLDR:', 'First', 'month', 'at', 'school', 'is', 'great!', 'Also,', \"I'm\", 'excited', 'to', 'start', 'a', 'new', 'project!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mon', 'school', 'year', 'lo', 'settl', 'nic', 'routin', 'she', 'on', 'year', 'old', 'room', 'she', 'nineteen', 'month', 'cent', 'work', 'group', 'siz', 'smal', 'three', 'teach', 'class', 'six', 'lov', 'teach', 'she', 'help', 'school', 'get', 'whiny', 'she', 'hungry', 'snack', 'tim', 'lunch', 'tim', 'yet', 'pretty', 'reason', 'everyon', 'lov', 'exceiv', 'on', 'tim', 'cov', 'bird', 'poop', 'playground', 'somehow', 'thought', 'hilarious\\n\\nthe', 'worst', 'part', 'premak', 'food', 'every', 'singl', 'day', 'breakfast', 'snack', 'lunch', 'pm', 'snack', 'tir', 'lucky', 'get', 'routin', 'too\\n\\nnot', 'school', 'rel', 'husband', 'bought', 'piec', 'wood', 'mak', 'height', 'rul', 'chart', 'wal', 'apart', 'want', 'mak', 'oppos', 'us', 'door', 'jamb', 'get', 'hous', 'bring', 'us', 'im', 'excit', '\\n\\ntldr', 'first', 'mon', 'school', 'gre', 'also', 'im', 'excit', 'start', 'new', 'project'], ['month', 'school', 'year', 'lo', 'settle', 'nicely', 'routine', 'shes', 'one', 'year', 'old', 'room', 'shes', 'nineteen', 'months', 'center', 'work', 'group', 'size', 'small', 'three', 'teachers', 'class', 'six', 'love', 'teachers', 'shes', 'helper', 'school', 'get', 'whiny', 'shes', 'hungry', 'snack', 'time', 'lunch', 'time', 'yet', 'pretty', 'reasonable', 'everyone', 'love', 'except', 'one', 'time', 'cover', 'bird', 'poop', 'playground', 'somehow', 'think', 'hilarious\\n\\nthe', 'worst', 'part', 'premake', 'food', 'every', 'single', 'day', 'breakfast', 'snack', 'lunch', 'pm', 'snack', 'tire', 'luckily', 'get', 'routine', 'too\\n\\nnot', 'school', 'relate', 'husband', 'buy', 'piece', 'wood', 'make', 'height', 'ruler', 'chart', 'wall', 'apartment', 'want', 'make', 'oppose', 'use', 'door', 'jamb', 'get', 'house', 'bring', 'us', 'im', 'excite', '\\n\\ntldr', 'first', 'month', 'school', 'great', 'also', 'im', 'excite', 'start', 'new', 'project'])\n",
      "original document: \n",
      "['What', 'material', 'does', 'one', 'store', 'HF', 'in?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mat', 'on', 'stor', 'hf'], ['material', 'one', 'store', 'hf'])\n",
      "original document: \n",
      "['And', 'I', 'am', 'you', 'and', 'what', 'I', 'see', 'is', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see'], ['see'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['At', 'the', 'zoo', 'that', 'I', 'go', 'to', 'they', 'have', 'one', 'of', 'these', 'but', 'there', 'are', 'vents', 'all', 'along', 'the', 'floor.', \"It's\", 'always', 'cold', 'and', 'very', 'cool', 'to', 'be', 'in.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zoo', 'go', 'on', 'vent', 'along', 'flo', 'alway', 'cold', 'cool'], ['zoo', 'go', 'one', 'vent', 'along', 'floor', 'always', 'cold', 'cool'])\n",
      "original document: \n",
      "['They', 'should', 'have', 'a', 'bunch', 'of', 'tasty', 'food', 'stands', 'everywhere', 'on', 'sunday', 'for', 'when', 'the', 'anorexia', 'comes', 'into', 'force.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bunch', 'tasty', 'food', 'stand', 'everywh', 'sunday', 'anorex', 'com', 'forc'], ['bunch', 'tasty', 'food', 'stand', 'everywhere', 'sunday', 'anorexia', 'come', 'force'])\n",
      "original document: \n",
      "['We', 'own', 'every', 'clip', 'she', 'ever', 'made', 'all', 'the', 'way', 'back', 'to', 'when', 'she', 'actually', 'stripped', 'naked.', 'We', 'do', 'not', 'buy', 'her', 'foot', 'fetish,', 'not', 'into', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['every', 'clip', 'ev', 'mad', 'way', 'back', 'act', 'stripped', 'nak', 'buy', 'foot', 'fet'], ['every', 'clip', 'ever', 'make', 'way', 'back', 'actually', 'strip', 'naked', 'buy', 'foot', 'fetish'])\n",
      "original document: \n",
      "[\"I've\", 'already', 'had', 'way', 'better', 'matches', 'on', 'VI', 'than', 'I', 'have', 'on', 'GoT.', 'I', 'like', 'that', \"it's\", 'not', 'snowbally.', 'I', 'really', 'expected', 'it', 'to', 'be', 'a', 'map', 'where', 'whoever', 'gets', 'the', 'first', 'objective', 'people', 'call', '\"GG\".', 'I', 'kinda', 'like', 'where', 'it', 'is', 'now', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'already', 'way', 'bet', 'match', 'vi', 'got', 'lik', 'snowb', 'real', 'expect', 'map', 'whoev', 'get', 'first', 'object', 'peopl', 'cal', 'gg', 'kind', 'lik'], ['ive', 'already', 'way', 'better', 'match', 'vi', 'get', 'like', 'snowbally', 'really', 'expect', 'map', 'whoever', 'get', 'first', 'objective', 'people', 'call', 'gg', 'kinda', 'like'])\n",
      "original document: \n",
      "['I’ve', 'seen', 'all', 'their', 'beers', 'being', 'sold', 'at', 'jacked', 'up', 'prices.', 'Maybe', 'it’s', 'something', 'they', 'are', 'doing', 'to', 'keep', 'brewery', 'sales', 'up?', 'I’m', 'not', 'really', 'sure', 'what’s', 'going', 'on.\\n\\nI', 'saw', 'the', '18', 'watt', 'couple', 'months', 'ago', 'for', 'like', 'close', 'to', '20', 'bucks', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'seen', 'beer', 'sold', 'jack', 'pric', 'mayb', 'someth', 'keep', 'brewery', 'sal', 'im', 'real', 'sur', 'what', 'going', 'on\\n\\ni', 'saw', 'eighteen', 'wat', 'coupl', 'month', 'ago', 'lik', 'clos', 'twenty', 'buck', 'wel'], ['ive', 'see', 'beers', 'sell', 'jack', 'price', 'maybe', 'something', 'keep', 'brewery', 'sales', 'im', 'really', 'sure', 'whats', 'go', 'on\\n\\ni', 'saw', 'eighteen', 'watt', 'couple', 'months', 'ago', 'like', 'close', 'twenty', 'buck', 'well'])\n",
      "original document: \n",
      "['Thanks', 'got', 'a', 'bit', 'excited', 'and', 'posted', 'this', 'one', 'slightly,', '(OK', 'very', 'too),', 'early', 'thus', 'the', 'mad', 'rash', 'of', 'edits', 'afterwards.', 'I', 'had', 'the', 'idea', 'and', 'just', 'had', 'to', 'post', 'lest', 'it', 'suffered', 'becoming', 'another', 'project', 'prevarication,', 'got', 'so', 'much', 'to', 'do', 'at', 'the', 'moment', 'but', 'so', 'little', 'wilpower', 'to', 'knuckle', 'under.\\n\\nEDIT', 'Curses,', 'one', 'more', 'edit', 'before', 'the', 'sun', 'sets', 'or', 'is', 'it', 'the', 'night', 'falls.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'got', 'bit', 'excit', 'post', 'on', 'slight', 'ok', 'ear', 'thu', 'mad', 'rash', 'edit', 'afterward', 'ide', 'post', 'lest', 'suff', 'becom', 'anoth', 'project', 'prev', 'got', 'much', 'mom', 'littl', 'wilpow', 'knuckl', 'under\\n\\nedit', 'curs', 'on', 'edit', 'sun', 'set', 'night', 'fal'], ['thank', 'get', 'bite', 'excite', 'post', 'one', 'slightly', 'ok', 'early', 'thus', 'mad', 'rash', 'edit', 'afterwards', 'idea', 'post', 'lest', 'suffer', 'become', 'another', 'project', 'prevarication', 'get', 'much', 'moment', 'little', 'wilpower', 'knuckle', 'under\\n\\nedit', 'curse', 'one', 'edit', 'sun', 'set', 'night', 'fall'])\n",
      "original document: \n",
      "['It’s', 'always', 'sunny', 'in', 'Philadelphia-', 'various', 'episodes', 'are', 'my', 'fav', 'but', 'one', 'that', 'sticks', 'out', 'the', 'most', 'is', 'season', '3', 'episode', '9,', 'it’s', 'the', 'first', 'day', 'man/night', 'man', 'episode.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'sunny', 'philadelph', 'vary', 'episod', 'fav', 'on', 'stick', 'season', 'three', 'episod', 'nin', 'first', 'day', 'mannight', 'man', 'episod'], ['always', 'sunny', 'philadelphia', 'various', 'episodes', 'fav', 'one', 'stick', 'season', 'three', 'episode', 'nine', 'first', 'day', 'mannight', 'man', 'episode'])\n",
      "original document: \n",
      "[\"I've\", 'said', 'all', 'along', 'that', \"we're\", 'going', 'to', 'experience', 'a', 'Tennessee', 'post-Fulmer', 'fall', 'from', 'grace.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'said', 'along', 'going', 'expery', 'ten', 'postfulm', 'fal', 'grac'], ['ive', 'say', 'along', 'go', 'experience', 'tennessee', 'postfulmer', 'fall', 'grace'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Itsy?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['itsy'], ['itsy'])\n",
      "original document: \n",
      "['And', \"I'm\", 'fine', 'with', 'that.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'fin'], ['im', 'fine'])\n",
      "original document: \n",
      "['Pm']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pm'], ['pm'])\n",
      "original document: \n",
      "['Fishing', 'and', 'Football']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fish', 'footbal'], ['fish', 'football'])\n",
      "original document: \n",
      "['The', 'organization', 'themselves', 'is', 'filing', 'the', 'suit.', 'From', 'the', 'article', 'it', 'seems', 'the', 'leader,', 'who', 'is', 'a', 'lawyer,', '', 'is', 'a', 'racist', 'piece', 'of', 'shit.', 'Filing', 'potentially', 'frivolous', 'law', 'suits', 'sounds', 'like', 'something', 'on', 'the', 'lesser', 'end', 'of', 'the', 'bad', 'shit', 'this', 'guy', 'does.', '\\n\\nHere', 'is', 'an', 'apt', 'quote', 'to', 'showcase', 'what', 'type', 'of', 'man', 'he', 'is\\n\\n&gt;', '[Kill', 'every', 'goddamn', 'Zionist', 'in', 'Israel!', 'Goddamn', 'little', 'babies,', 'goddamn', 'old', 'ladies!', 'Blow', 'up', 'Zionist', 'supermarkets!](https://www.splcenter.org/fighting-hate/extremist-files/individual/malik-zulu-shabazz)\\n\\n\\n\\n\\n\\nThat', 'said,', 'I', 'have', 'no', 'idea', 'if', 'this', 'lawsuit', 'has', 'merit', 'or', 'not', 'since', \"I'm\", 'not', 'a', 'lawyer', 'but', 'this', 'guy', 'being', 'associated', 'with', 'it', 'is', 'horrible', 'pr', 'for', 'trying', 'to', 'get', 'people', 'to', 'support', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['org', 'fil', 'suit', 'artic', 'seem', 'lead', 'lawy', 'rac', 'piec', 'shit', 'fil', 'pot', 'frivol', 'law', 'suit', 'sound', 'lik', 'someth', 'less', 'end', 'bad', 'shit', 'guy', '\\n\\nhere', 'apt', 'quot', 'showcas', 'typ', 'man', 'is\\n\\ngt', 'kil', 'every', 'goddamn', 'zion', 'israel', 'goddamn', 'littl', 'baby', 'goddamn', 'old', 'lady', 'blow', 'zion', 'supermarketshttpswwwsplcenterorgfightinghateextremistfilesindividualmalikzulushabazz\\n\\n\\n\\n\\n\\nthat', 'said', 'ide', 'lawsuit', 'merit', 'sint', 'im', 'lawy', 'guy', 'assocy', 'horr', 'pr', 'try', 'get', 'peopl', 'support'], ['organization', 'file', 'suit', 'article', 'seem', 'leader', 'lawyer', 'racist', 'piece', 'shit', 'file', 'potentially', 'frivolous', 'law', 'suit', 'sound', 'like', 'something', 'lesser', 'end', 'bad', 'shit', 'guy', '\\n\\nhere', 'apt', 'quote', 'showcase', 'type', 'man', 'is\\n\\ngt', 'kill', 'every', 'goddamn', 'zionist', 'israel', 'goddamn', 'little', 'baby', 'goddamn', 'old', 'ladies', 'blow', 'zionist', 'supermarketshttpswwwsplcenterorgfightinghateextremistfilesindividualmalikzulushabazz\\n\\n\\n\\n\\n\\nthat', 'say', 'idea', 'lawsuit', 'merit', 'since', 'im', 'lawyer', 'guy', 'associate', 'horrible', 'pr', 'try', 'get', 'people', 'support'])\n",
      "original document: \n",
      "['Rolling', 'with', 'Watson.', 'Need', 'a', 'game', 'before', 'I', 'trust', 'Amari', 'again', 'and', 'Crabtree', 'is', 'questionable.', 'Too', 'many', 'question', 'marks.', 'Fairly', 'confident', 'in', 'starting', 'Carr', 'ROS', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rol', 'watson', 'nee', 'gam', 'trust', 'amar', 'crabt', 'quest', 'many', 'quest', 'mark', 'fair', 'confid', 'start', 'car', 'ros', 'though'], ['roll', 'watson', 'need', 'game', 'trust', 'amari', 'crabtree', 'questionable', 'many', 'question', 'mark', 'fairly', 'confident', 'start', 'carr', 'ros', 'though'])\n",
      "original document: \n",
      "['&gt;That', \"doesn't\", 'logic', 'right.', 'There', 'may', 'be', '1000', 'types', 'of', 'Christianity', 'but', 'none', 'of', 'them', 'are', 'related', 'to', 'whether', 'Christianity', 'is', '\"correct,\"', 'again,', 'whatever', 'that', 'means.', '\\n\\nSure', 'it', 'does.', 'If', 'we', 'take', 'that', 'Christianity', 'is', 'the', 'correct', 'religion', '(', 'instead', 'of', 'atheism', 'or', '*insert', 'a', 'list', 'of', 'all', 'other', 'deistic', 'ideology', 'here*)', 'then', 'any', 'given', 'Christian', 'has', 'a', '1', 'in', '1000', 'shot', 'at', 'interpreting', 'the', 'dogma', 'in', 'the', 'correct', 'way', 'as', 'to', 'not', 'be', 'a', 'blasphemous', 'hell', 'bound', 'sinner.', '\\n\\n&gt;What', 'does', 'it', 'mean', 'to', 'be', 'correct', '\\n\\nIn', 'this', 'case', 'it', 'means', 'that', 'of', 'all', 'available', 'options,', 'both', 'past', 'and', 'present,', 'you', 'successfully', 'chose', 'the', 'one', 'that', 'ends', 'up', 'being', 'true.', '\\n\\n&gt;and', 'furthermore,', 'what', 'does', 'a', 'sect', 'of', 'Christianity', 'not', 'practicing', 'Christianity', 'correctly', 'mean', 'to', 'whether', 'Christianity', 'is', 'correct?', '\\n\\nNothing.', \"That's\", 'why', 'I', 'said', 'that', 'if', 'we', 'accept', 'the', '**premise**', 'that', 'Christianity', 'is', 'correct,', 'it', 'still', 'only', 'gives', 'any', 'single', 'practioner', 'a', 'one', 'in', '1000', 'shot', 'at', 'being', 'right.', 'It', 'does', 'not', 'bear', 'on', 'wether', 'or', 'not', 'Christians', 'are', 'right,', 'that', 'number', 'was', 'too', 'astronomical', 'to', 'calculate.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthat', 'doesnt', 'log', 'right', 'may', 'one thousand', 'typ', 'christianity', 'non', 'rel', 'wheth', 'christianity', 'correct', 'whatev', 'mean', '\\n\\nsure', 'tak', 'christianity', 'correct', 'relig', 'instead', 'ath', 'insert', 'list', 'deist', 'ideolog', 'giv', 'christian', 'on', 'one thousand', 'shot', 'interpret', 'dogm', 'correct', 'way', 'blasphem', 'hel', 'bound', 'sin', '\\n\\ngtwhat', 'mean', 'correct', '\\n\\nin', 'cas', 'mean', 'avail', 'opt', 'past', 'pres', 'success', 'chos', 'on', 'end', 'tru', '\\n\\ngtand', 'furtherm', 'sect', 'christianity', 'pract', 'christianity', 'correct', 'mean', 'wheth', 'christianity', 'correct', '\\n\\nnothing', 'that', 'said', 'acceiv', 'prem', 'christianity', 'correct', 'stil', 'giv', 'singl', 'pract', 'on', 'one thousand', 'shot', 'right', 'bear', 'weth', 'christians', 'right', 'numb', 'astronom', 'calc'], ['gtthat', 'doesnt', 'logic', 'right', 'may', 'one thousand', 'type', 'christianity', 'none', 'relate', 'whether', 'christianity', 'correct', 'whatever', 'mean', '\\n\\nsure', 'take', 'christianity', 'correct', 'religion', 'instead', 'atheism', 'insert', 'list', 'deistic', 'ideology', 'give', 'christian', 'one', 'one thousand', 'shoot', 'interpret', 'dogma', 'correct', 'way', 'blasphemous', 'hell', 'bind', 'sinner', '\\n\\ngtwhat', 'mean', 'correct', '\\n\\nin', 'case', 'mean', 'available', 'options', 'past', 'present', 'successfully', 'choose', 'one', 'end', 'true', '\\n\\ngtand', 'furthermore', 'sect', 'christianity', 'practice', 'christianity', 'correctly', 'mean', 'whether', 'christianity', 'correct', '\\n\\nnothing', 'thats', 'say', 'accept', 'premise', 'christianity', 'correct', 'still', 'give', 'single', 'practioner', 'one', 'one thousand', 'shoot', 'right', 'bear', 'wether', 'christians', 'right', 'number', 'astronomical', 'calculate'])\n",
      "original document: \n",
      "['Send', 'me', 'a', 'PM.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['send', 'pm'], ['send', 'pm'])\n",
      "original document: \n",
      "['Yeah,', 'this', 'is', 'why', \"I'm\", 'not', 'too', 'upset', 'about', 'it,', \"it's\", 'still', 'a', 'beta.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'im', 'upset', 'stil', 'bet'], ['yeah', 'im', 'upset', 'still', 'beta'])\n",
      "original document: \n",
      "['Which', 'makes', 'sense.', \"It's\", 'the', 'implication', 'of', 'being', 'on', 'a', 'boat', 'that', 'makes', 'it', 'dangerous,', 'not', 'being', 'on', 'a', 'boat', 'itself.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'sens', 'imply', 'boat', 'mak', 'dang', 'boat'], ['make', 'sense', 'implication', 'boat', 'make', 'dangerous', 'boat'])\n",
      "original document: \n",
      "['Teacher', 'as', 'in', 'English', 'teacher?', \"I'm\", 'guessing', 'it', \"must've\", 'been', 'a', 'struggle', 'with', 'the', 'grind,', 'props', 'on', 'where', 'you', 'made', 'it.', 'Any', 'pointers', 'for', 'someone', 'looking', 'to', 'make', 'the', 'same', 'plunge?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['teach', 'engl', 'teach', 'im', 'guess', 'mustv', 'struggle', 'grind', 'prop', 'mad', 'point', 'someon', 'look', 'mak', 'plung'], ['teacher', 'english', 'teacher', 'im', 'guess', 'mustve', 'struggle', 'grind', 'prop', 'make', 'pointers', 'someone', 'look', 'make', 'plunge'])\n",
      "original document: \n",
      "['I', 'mean', 'going', 'by', 'the', 'rules', 'I', 'could', 'see', 'that', 'happening,', 'but', 'given', 'the', 'relationship', 'that', 'that', 'Goku', 'has', 'with', 'Zeno', 'I', \"don't\", 'think', \"they'll\", 'beat', 'him', 'down', 'for', 'using', 'that', 'technique,', 'even', 'if', 'it', 'does', 'draw', 'energy', 'from', 'eliminated', 'fighters.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'going', 'rul', 'could', 'see', 'hap', 'giv', 'rel', 'goku', 'zeno', 'dont', 'think', 'theyl', 'beat', 'us', 'techn', 'ev', 'draw', 'energy', 'elimin', 'fight'], ['mean', 'go', 'rule', 'could', 'see', 'happen', 'give', 'relationship', 'goku', 'zeno', 'dont', 'think', 'theyll', 'beat', 'use', 'technique', 'even', 'draw', 'energy', 'eliminate', 'fighters'])\n",
      "original document: \n",
      "['zle']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zle'], ['zle'])\n",
      "original document: \n",
      "['Always', 'and', 'forever.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'forev'], ['always', 'forever'])\n",
      "original document: \n",
      "['How', 'are', 'you', 'spending', '$15', 'by', 'yourself', 'at', 'Five', 'Guys?', 'My', 'boyfriend', 'and', 'I', 'spend', 'that', 'between', 'the', 'two', 'of', 'us', '(maybe', 'a', 'dollar', 'or', 'two', 'more', 'but', \"that's\", 'it).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spend', 'fifteen', 'fiv', 'guy', 'boyfriend', 'spend', 'two', 'us', 'mayb', 'doll', 'two', 'that'], ['spend', 'fifteen', 'five', 'guy', 'boyfriend', 'spend', 'two', 'us', 'maybe', 'dollar', 'two', 'thats'])\n",
      "original document: \n",
      "['Yeah', 'Nissan', 'obviously', 'isn’t', 'stupid', 'since', 'they', 'suffered', 'so', 'many', 'problems', 'with', 'the', 'Gen1s', 'but', 'then', 'again', 'when', 'everyone', 'else', 'is', 'going', 'with', 'active', 'cooling', 'it', 'makes', 'you', 'wonder', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'niss', 'obvy', 'isnt', 'stupid', 'sint', 'suff', 'many', 'problem', 'gen1s', 'everyon', 'els', 'going', 'act', 'cool', 'mak', 'wond'], ['yeah', 'nissan', 'obviously', 'isnt', 'stupid', 'since', 'suffer', 'many', 'problems', 'gen1s', 'everyone', 'else', 'go', 'active', 'cool', 'make', 'wonder'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Congrats', 'on', '90', 'days', 'clean', '\\n\\nI', 'know', 'I', 'am', 'an', 'impulsive', 'person,', 'but', \"I'm\", 'fortunately', 'also', 'very', 'paranoid', 'and', 'I', 'only', 'use', 'when', 'I', 'know', 'I', 'have', 'no', 'engagements', 'for', 'the', 'day.', \"I'm\", 'quite', 'secretive', 'about', 'my', 'use...', 'nobody', 'knows', 'and', 'I', 'have', 'to', 'balance', 'these', 'two', 'sides', 'of', 'my', 'life.', 'So', 'far', 'so', 'good.', 'There', 'is', 'also', 'excitement', 'in', 'that', 'dichotomy', 'of', 'recklessness', 'and', 'caution.', 'Who', 'knows', 'tho...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congr', 'nin', 'day', 'cle', '\\n\\ni', 'know', 'impuls', 'person', 'im', 'fortun', 'also', 'paranoid', 'us', 'know', 'eng', 'day', 'im', 'quit', 'secret', 'us', 'nobody', 'know', 'bal', 'two', 'sid', 'lif', 'far', 'good', 'also', 'excit', 'dichotom', 'reckless', 'caut', 'know', 'tho'], ['congrats', 'ninety', 'days', 'clean', '\\n\\ni', 'know', 'impulsive', 'person', 'im', 'fortunately', 'also', 'paranoid', 'use', 'know', 'engagements', 'day', 'im', 'quite', 'secretive', 'use', 'nobody', 'know', 'balance', 'two', 'side', 'life', 'far', 'good', 'also', 'excitement', 'dichotomy', 'recklessness', 'caution', 'know', 'tho'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"They're\", 'learning', 'I', 'say', 'we', 'pack', 'up', 'and', 'ring', 'Elon', 'Musk', 'for', 'some', 'tickets', 'to', 'Mars...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['theyr', 'learn', 'say', 'pack', 'ring', 'elon', 'musk', 'ticket', 'mar'], ['theyre', 'learn', 'say', 'pack', 'ring', 'elon', 'musk', 'ticket', 'mar'])\n",
      "original document: \n",
      "['Maybe', 'it', 'has', 'to', 'do', 'with', 'the', 'fact', 'that', 'being', 'a', 'reservist', 'sucks', 'shit', 'these', 'days.\\n\\nI', 'left', 'a', 'while', 'ago,', 'but', 'comparing', 'what', 'we', 'got', 'to', 'do,', 'compared', 'to', 'what', 'the', 'militia', 'got', 'to', 'do', 'in', 'the', \"1980's,\", 'it', 'is', 'why', 'no', 'wonder', 'why', 'no', 'one', 'wants', 'to', 'stay', 'in', 'anymore.\\n\\nGuys', 'I', 'know', 'that', 'were', 'in', 'in', 'the', \"80's\", 'talk', 'about', 'thousands', 'of', 'reservists', 'being', 'on', 'exercises', 'in', 'the', 'summer', 'and', 'thousands', 'on', 'exercise', 'in', 'the', 'winter.', 'M113s,', 'Grizzly,', 'Cougar', 'and', 'Husky', 'armored', 'vehicles,', 'multiple', 'artillery', 'batteries,', 'Air', 'Force', 'helicopters', 'for', 'infantry', 'air', 'assaults,', 'air', 'defense', 'batteries,', 'thousands', 'of', 'mortar', 'rounds,', 'the', 'fucking', 'FN,', 'New', \"MLVW's,\", 'Rambo', 'tier', 'amounts', 'of', \"M72's,\", 'parade', 'squares', 'that', 'fit', '1500', 'soldiers', 'and', 'full', 'support', 'battalions', 'were', 'normal', 'things.', 'Sometimes', 'Reg', 'force', 'was', 'fully/partially', 'involved,', 'sometimes', 'they', \"weren't.\\n\\nCompared\", 'to', 'now.', '\\nMaybe', '300', 'or', '400', 'reservists', 'will', 'go', 'one', 'exercise', 'if', 'the', 'Land', 'Force', 'Area', 'can', 'afford', 'it', 'that', 'year.', 'No', \"M113's,\", 'no', \"LAV's,\", 'Shit', 'ass', 'LSVW/Milverado/G-Wagon/MLVW', 'vehicles.', 'No', 'artillery', 'except', 'the', '4', 'forty', 'year', 'old', 'field', 'guns', 'that', 'the', 'artillery', 'platoons', 'have', 'in', 'the', 'whole', 'province.', 'Maybe', 'the', 'Air', 'Force', 'will', 'be', 'able', 'to', 'afford', 'to', 'send', 'out', '1', 'or', '2', 'unarmed', 'helicopters', 'for', 'a', 'day', 'or', 'two.', 'Maybe', 'the', 'medics', 'will', 'set', 'up', 'a', 'tent', 'if', 'they', 'can', 'get', 'enough', 'members', 'to', 'go', 'on', 'exercise.', 'The', 'food', 'will', 'come', 'from', 'the', 'base', 'kitchen', 'or', 'will', 'be', \"MRE's\", 'because', 'the', 'service', 'battalions', 'are', 'incapable', 'of', 'getting', 'enough', 'people', 'or', 'equipment', 'to', 'provide', 'those', 'services.', 'Any', 'M72', 'training', 'will', 'be', '35mm', 'training', 'insert', 'on', 'a', 'reused', 'tube.', 'Your', 'unit', 'and', 'the', 'exerciser', 'run', 'out', 'of', 'training', 'rounds', 'so', 'you', 'literally', 'say', '\"Bang', 'Bang\"', 'when', '\"shooting\".', 'Soldiers', 'get', 'encouraged', 'to', 'stay', 'home', 'on', 'range', 'qualification', 'days', 'because', 'their', 'units', \"can't\", 'afford', 'to', 'qualify', 'everyone', 'on', 'even', 'the', 'C7', 'let', 'alone', 'the', 'machine', 'guns.\\n\\nThat', 'same', 'parade', 'square', 'has', 'crumbled', 'so', 'much', 'that', 'it', 'could', 'only', 'parade', 'two', 'under', 'strength', 'platoons.', 'The', 'military', \"can't\", 'afford', 'to', 'fix', 'it,', 'and', 'there', 'is', 'no', 'point', 'in', 'fixing', 'it', 'really.', 'The', 'reserves', 'totally', 'depend', 'on', 'the', 'Regular', 'force', 'as', 'well.\\n\\n\\nThat', 'and', 'the', 'older', 'aged', 'higher', 'ranking', \"NCO's\", 'that', 'are', 'just', 'riding', 'it', 'out', 'to', 'top', 'up', 'their', 'pension.\\n\\n\\nReservists', 'are', 'part', 'time.', 'If', 'you', 'are', 'going', 'to', 'attract', 'high', 'quality', 'and', 'talented', 'people', 'to', 'the', 'reserves,', 'it', 'has', 'to', 'be', 'fun.', 'Why', 'would', 'anyone', 'earn', 'very', 'little', 'money', 'one', 'night', 'a', 'week', 'and', 'do', 'basically', 'nothing', 'with', 'not', 'much', 'opportunity', 'stay?', 'The', '$1.25', 'beers', 'in', 'the', 'mess', 'only', 'make', 'up', 'for', 'doing', 'your', '3rd', 'BFT', 'of', 'the', 'month', 'for', 'so', 'long.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'fact', 'reserv', 'suck', 'shit', 'days\\n\\ni', 'left', 'ago', 'comp', 'got', 'comp', 'milit', 'got', '1980s', 'wond', 'on', 'want', 'stay', 'anymore\\n\\nguys', 'know', '80s', 'talk', 'thousand', 'reserv', 'exerc', 'sum', 'thousand', 'exerc', 'wint', 'm113s', 'grizz', 'coug', 'husky', 'arm', 'vehic', 'multipl', 'artillery', 'battery', 'air', 'forc', 'helicopt', 'infantry', 'air', 'assault', 'air', 'defens', 'battery', 'thousand', 'mort', 'round', 'fuck', 'fn', 'new', 'mlvws', 'rambo', 'tier', 'amount', 'm72s', 'parad', 'squ', 'fit', 'one thousand, five hundred', 'soldy', 'ful', 'support', 'bat', 'norm', 'thing', 'sometim', 'reg', 'forc', 'fullypart', 'involv', 'sometim', 'werent\\n\\ncompared', '\\nmaybe', 'three hundred', 'four hundred', 'reserv', 'go', 'on', 'exerc', 'land', 'forc', 'are', 'afford', 'year', 'm113s', 'lav', 'shit', 'ass', 'lsvwmilveradogwagonmlvw', 'vehic', 'artillery', 'exceiv', 'four', 'forty', 'year', 'old', 'field', 'gun', 'artillery', 'platoon', 'whol', 'provint', 'mayb', 'air', 'forc', 'abl', 'afford', 'send', 'on', 'two', 'unarm', 'helicopt', 'day', 'two', 'mayb', 'med', 'set', 'tent', 'get', 'enough', 'memb', 'go', 'exerc', 'food', 'com', 'bas', 'kitch', 'mre', 'serv', 'bat', 'incap', 'get', 'enough', 'peopl', 'equip', 'provid', 'serv', 'm72', 'train', '35mm', 'train', 'insert', 'reus', 'tub', 'unit', 'exerc', 'run', 'train', 'round', 'lit', 'say', 'bang', 'bang', 'shoot', 'soldy', 'get', 'enco', 'stay', 'hom', 'rang', 'qual', 'day', 'unit', 'cant', 'afford', 'qual', 'everyon', 'ev', 'c7', 'let', 'alon', 'machin', 'guns\\n\\nthat', 'parad', 'squ', 'crumbl', 'much', 'could', 'parad', 'two', 'strength', 'platoon', 'milit', 'cant', 'afford', 'fix', 'point', 'fix', 'real', 'reserv', 'tot', 'depend', 'regul', 'forc', 'well\\n\\n\\nthat', 'old', 'ag', 'high', 'rank', 'nco', 'rid', 'top', 'pension\\n\\n\\nreservists', 'part', 'tim', 'going', 'attract', 'high', 'qual', 'tal', 'peopl', 'reserv', 'fun', 'would', 'anyon', 'earn', 'littl', 'money', 'on', 'night', 'week', 'bas', 'noth', 'much', 'opportun', 'stay', 'one hundred and twenty-fiv', 'beer', 'mess', 'mak', '3rd', 'bft', 'mon', 'long'], ['maybe', 'fact', 'reservist', 'suck', 'shit', 'days\\n\\ni', 'leave', 'ago', 'compare', 'get', 'compare', 'militia', 'get', '1980s', 'wonder', 'one', 'want', 'stay', 'anymore\\n\\nguys', 'know', '80s', 'talk', 'thousands', 'reservists', 'exercise', 'summer', 'thousands', 'exercise', 'winter', 'm113s', 'grizzly', 'cougar', 'husky', 'armor', 'vehicles', 'multiple', 'artillery', 'batteries', 'air', 'force', 'helicopters', 'infantry', 'air', 'assault', 'air', 'defense', 'batteries', 'thousands', 'mortar', 'round', 'fuck', 'fn', 'new', 'mlvws', 'rambo', 'tier', 'amount', 'm72s', 'parade', 'square', 'fit', 'one thousand, five hundred', 'soldier', 'full', 'support', 'battalions', 'normal', 'things', 'sometimes', 'reg', 'force', 'fullypartially', 'involve', 'sometimes', 'werent\\n\\ncompared', '\\nmaybe', 'three hundred', 'four hundred', 'reservists', 'go', 'one', 'exercise', 'land', 'force', 'area', 'afford', 'year', 'm113s', 'lavs', 'shit', 'ass', 'lsvwmilveradogwagonmlvw', 'vehicles', 'artillery', 'except', 'four', 'forty', 'year', 'old', 'field', 'gun', 'artillery', 'platoons', 'whole', 'province', 'maybe', 'air', 'force', 'able', 'afford', 'send', 'one', 'two', 'unarm', 'helicopters', 'day', 'two', 'maybe', 'medics', 'set', 'tent', 'get', 'enough', 'members', 'go', 'exercise', 'food', 'come', 'base', 'kitchen', 'mres', 'service', 'battalions', 'incapable', 'get', 'enough', 'people', 'equipment', 'provide', 'service', 'm72', 'train', '35mm', 'train', 'insert', 'reuse', 'tube', 'unit', 'exerciser', 'run', 'train', 'round', 'literally', 'say', 'bang', 'bang', 'shoot', 'soldier', 'get', 'encourage', 'stay', 'home', 'range', 'qualification', 'days', 'units', 'cant', 'afford', 'qualify', 'everyone', 'even', 'c7', 'let', 'alone', 'machine', 'guns\\n\\nthat', 'parade', 'square', 'crumble', 'much', 'could', 'parade', 'two', 'strength', 'platoons', 'military', 'cant', 'afford', 'fix', 'point', 'fix', 'really', 'reserve', 'totally', 'depend', 'regular', 'force', 'well\\n\\n\\nthat', 'older', 'age', 'higher', 'rank', 'ncos', 'rid', 'top', 'pension\\n\\n\\nreservists', 'part', 'time', 'go', 'attract', 'high', 'quality', 'talented', 'people', 'reserve', 'fun', 'would', 'anyone', 'earn', 'little', 'money', 'one', 'night', 'week', 'basically', 'nothing', 'much', 'opportunity', 'stay', 'one hundred and twenty-five', 'beers', 'mess', 'make', '3rd', 'bft', 'month', 'long'])\n",
      "original document: \n",
      "['People', 'say', 'this', 'all', 'the', 'time,', 'but', 'one', 'of', 'my', 'party', 'plays', 'with', 'controller', 'on', 'PC,', 'and', 'I', \"didn't\", 'know', 'about', 'it', 'until', 'he', 'casually', 'told', 'me.', 'We', 'are', 'both', 'Gold', 'III', '(*strange', 'since', 'we', 'play', 'together*),', 'but', 'I', 'never', 'felt', 'he', 'was', 'carried,', \"he's\", 'not', 'the', 'best', 'nor', 'the', 'worst', 'of', 'the', 'squad,', 'he', 'has', 'a', '1.2', 'K/D', 'in', 'Ranked', 'which', \"I'd\", 'say', 'is', 'good.', 'Honestly', 'I', 'used', 'to', 'play', 'on', 'console,', 'and', 'I', 'notice', 'the', 'increased', 'accuracy', 'of', 'the', 'mouse', 'almost', 'only', 'on', 'long', 'range', 'gunfights,', 'which', 'are', 'quite', 'rare', 'in', 'R6S']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'say', 'tim', 'on', 'party', 'play', 'control', 'pc', 'didnt', 'know', 'cas', 'told', 'gold', 'ii', 'strange', 'sint', 'play', 'togeth', 'nev', 'felt', 'carry', 'hes', 'best', 'worst', 'squad', 'twelv', 'kd', 'rank', 'id', 'say', 'good', 'honest', 'us', 'play', 'consol', 'not', 'increas', 'acc', 'mous', 'almost', 'long', 'rang', 'gunfight', 'quit', 'rar', 'r6s'], ['people', 'say', 'time', 'one', 'party', 'play', 'controller', 'pc', 'didnt', 'know', 'casually', 'tell', 'gold', 'iii', 'strange', 'since', 'play', 'together', 'never', 'felt', 'carry', 'hes', 'best', 'worst', 'squad', 'twelve', 'kd', 'rank', 'id', 'say', 'good', 'honestly', 'use', 'play', 'console', 'notice', 'increase', 'accuracy', 'mouse', 'almost', 'long', 'range', 'gunfights', 'quite', 'rare', 'r6s'])\n",
      "original document: \n",
      "['*Phallus', 'hadriani*\\n\\n*P.', 'impudicus*', 'and', '*P.', 'hadriani*', 'differ', 'only', 'in', 'the', 'color', 'of', 'the', 'volva', '(white', 'in', 'the', 'former,', 'purplish', 'in', 'the', 'latter).', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['phall', 'hadriani\\n\\np', 'impudic', 'p', 'hadrian', 'diff', 'col', 'volv', 'whit', 'form', 'purpl', 'lat'], ['phallus', 'hadriani\\n\\np', 'impudicus', 'p', 'hadriani', 'differ', 'color', 'volva', 'white', 'former', 'purplish', 'latter'])\n",
      "original document: \n",
      "['I', 'love', 'Gerolsteiner', 'for', 'the', 'same', 'reasons.', '', 'It', 'is', 'the', 'strongest', 'seltzer', \"I've\", 'had.', '', 'And', 'I', 'used', 'to', 'drink', 'for', 'the', 'same', 'reasons,', 'too.', '', 'I', 'was', 'never', 'one', 'to', 'drown', 'my', 'sorrows.', '', 'But', 'I', \"couldn't\", 'stop', 'a', 'good', 'feeling', 'from', 'taking', 'the', 'party', 'way', 'too', 'far.\\n\\nA', 'good,', 'hard-hitting', 'seltzer', 'really', 'does', 'the', 'trick,', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'gerolstein', 'reason', 'strongest', 'seltz', 'iv', 'us', 'drink', 'reason', 'nev', 'on', 'drown', 'sorrow', 'couldnt', 'stop', 'good', 'feel', 'tak', 'party', 'way', 'far\\n\\na', 'good', 'hardhit', 'seltz', 'real', 'trick', 'though'], ['love', 'gerolsteiner', 'reason', 'strongest', 'seltzer', 'ive', 'use', 'drink', 'reason', 'never', 'one', 'drown', 'sorrow', 'couldnt', 'stop', 'good', 'feel', 'take', 'party', 'way', 'far\\n\\na', 'good', 'hardhitting', 'seltzer', 'really', 'trick', 'though'])\n",
      "original document: \n",
      "['Cats', 'are', 'fun', 'and', 'magical', 'when', 'you', \"can't\", 'smell', 'their', 'poop.', 'FRESH', 'STEP!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cat', 'fun', 'mag', 'cant', 'smel', 'poop', 'fresh', 'step'], ['cat', 'fun', 'magical', 'cant', 'smell', 'poop', 'fresh', 'step'])\n",
      "original document: \n",
      "['[these](https://imgur.com/a/YjZXx)', 'what', 'kind', 'of', 'adjustable', 'elastic', 'cuffs', 'are', 'these?', 'Is', 'there', 'a', 'specific', 'name?', 'How', 'would', 'I', 'go', 'about', 'doing', 'this', 'myself', 'to', 'a', 'pair', 'of', 'nylon', 'wind', 'pants?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thesehttpsimgurcomayjzxx', 'kind', 'adjust', 'elast', 'cuff', 'spec', 'nam', 'would', 'go', 'pair', 'nylon', 'wind', 'pant'], ['thesehttpsimgurcomayjzxx', 'kind', 'adjustable', 'elastic', 'cuff', 'specific', 'name', 'would', 'go', 'pair', 'nylon', 'wind', 'pant'])\n",
      "original document: \n",
      "['The', 'difference', 'between', 'the', 'Silver', 'photoshopper', 'and', 'the', 'Diamond', 'photoshopper.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['diff', 'silv', 'photoshop', 'diamond', 'photoshop'], ['difference', 'silver', 'photoshopper', 'diamond', 'photoshopper'])\n",
      "original document: \n",
      "['Oh', 'god', 'if', 'he', 'does', 'take', 'pictures', 'so', 'we', 'can', 'laugh', 'at', 'his', 'expense.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'god', 'tak', 'pict', 'laugh', 'expens'], ['oh', 'god', 'take', 'picture', 'laugh', 'expense'])\n",
      "original document: \n",
      "['I', 'seem', 'to', 'remember', 'having', 'Bitmoji', 'before', 'other', 'people.', 'Your', 'report', 'very', 'well', 'may', 'have', 'had', 'an', 'effect,', 'that', 'seems', 'to', 'be', 'the', 'biggest', 'use', 'of', 'Beta.', \"We're\", 'the', 'only', 'people', 'who', 'have', 'direct', 'access', 'to', 'the', 'devs', 'to', 'tell', 'them', 'our', 'problems', 'and,', 'if', \"they're\", 'small', 'enough,', 'get', 'them', 'fixed', 'fairly', 'quickly.', 'All', \"I'm\", 'saying', 'is,', 'though,', 'is', 'that', 'if', 'you', 'have', 'a', 'Beta', 'feature,', 'maybe', 'include', 'it', 'in', 'the', 'Beta', 'release?', 'I', 'mean,', 'sure', 'they', 'tested', 'the', 'multiple', 'videos', 'thing', 'but', \"it's\", 'not', 'like', \"they're\", 'going', 'to', 'get', 'bug', 'reports', 'on', 'it', 'for', 'as', 'long', 'as', \"it's\", 'disabled', 'so', 'how', 'do', 'they', 'know', 'if', 'the', 'edits', \"they're\", 'making', 'are', 'working', 'for', 'the', 'people', 'that', 'complained?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seem', 'rememb', 'bitmoj', 'peopl', 'report', 'wel', 'may', 'effect', 'seem', 'biggest', 'us', 'bet', 'peopl', 'direct', 'access', 'dev', 'tel', 'problem', 'theyr', 'smal', 'enough', 'get', 'fix', 'fair', 'quick', 'im', 'say', 'though', 'bet', 'feat', 'mayb', 'includ', 'bet', 'releas', 'mean', 'sur', 'test', 'multipl', 'video', 'thing', 'lik', 'theyr', 'going', 'get', 'bug', 'report', 'long', 'dis', 'know', 'edit', 'theyr', 'mak', 'work', 'peopl', 'complain'], ['seem', 'remember', 'bitmoji', 'people', 'report', 'well', 'may', 'effect', 'seem', 'biggest', 'use', 'beta', 'people', 'direct', 'access', 'devs', 'tell', 'problems', 'theyre', 'small', 'enough', 'get', 'fix', 'fairly', 'quickly', 'im', 'say', 'though', 'beta', 'feature', 'maybe', 'include', 'beta', 'release', 'mean', 'sure', 'test', 'multiple', 'videos', 'thing', 'like', 'theyre', 'go', 'get', 'bug', 'report', 'long', 'disable', 'know', 'edit', 'theyre', 'make', 'work', 'people', 'complain'])\n",
      "original document: \n",
      "['', 'You', 'could', 'just', 'ask', 'them', 'out', 'to', 'a', 'movie,', 'restaurant,', 'bar,', 'or', 'to', 'do', 'whatever', 'else', 'you', 'like', 'to', 'do', 'and', 'let', 'things', 'happen', 'more', 'organically.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'ask', 'movy', 'resta', 'bar', 'whatev', 'els', 'lik', 'let', 'thing', 'hap', 'org'], ['could', 'ask', 'movie', 'restaurant', 'bar', 'whatever', 'else', 'like', 'let', 'things', 'happen', 'organically'])\n",
      "original document: \n",
      "['Good.', '\\n\\nVote', 'for', 'them.', '\\n\\nYou', 'obviously', 'hate', 'the', 'Greens.', '\\nVote', 'for', 'the', 'Greens', 'if', 'you', 'like.', '\\n\\nWhy', \"don't\", 'you?\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', '\\n\\nvote', '\\n\\nyou', 'obvy', 'hat', 'green', '\\nvote', 'green', 'lik', '\\n\\nwhy', 'dont', 'you\\n'], ['good', '\\n\\nvote', '\\n\\nyou', 'obviously', 'hate', 'green', '\\nvote', 'green', 'like', '\\n\\nwhy', 'dont', 'you\\n'])\n",
      "original document: \n",
      "['No,', \"it's\", 'really', 'not.', 'Knowing', 'your', 'name', 'does', 'not', 'mean', 'you', 'can', 'be', 'assessed', 'for', 'capacity.', 'That', 'patient', 'is', 'having', 'a', 'life-threatening', 'respiratory', 'decompensation', 'and', 'a', 'psychiatrist', 'is', 'never', 'going', 'to', 'tell', 'you', '\"don\\'t', 'intubate', 'them', 'until', 'I', 'get', 'a', 'chance', 'to', 'do', 'my', '30-minute', 'competence', 'assessment.\"', \"That's\", 'why', 'none', 'of', 'the', 'physicians', 'in', 'this', 'thread', 'thought', 'a', 'psych', 'assessment', 'was', 'a', 'reasonable', 'suggestion,', 'and', \"it's\", 'why', \"you're\", 'getting', 'downvoted.', 'You', 'need', 'to', 'just', 'accept', 'that', 'you', \"didn't\", 'understand', 'this', 'clinical', 'scenario.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'know', 'nam', 'mean', 'assess', 'capac', 'paty', 'lifethr', 'respir', 'decompens', 'psychy', 'nev', 'going', 'tel', 'dont', 'intub', 'get', 'chant', '30minute', 'compet', 'assess', 'that', 'non', 'phys', 'thread', 'thought', 'psych', 'assess', 'reason', 'suggest', 'yo', 'get', 'downvot', 'nee', 'acceiv', 'didnt', 'understand', 'clin', 'scenario'], ['really', 'know', 'name', 'mean', 'assess', 'capacity', 'patient', 'lifethreatening', 'respiratory', 'decompensation', 'psychiatrist', 'never', 'go', 'tell', 'dont', 'intubate', 'get', 'chance', '30minute', 'competence', 'assessment', 'thats', 'none', 'physicians', 'thread', 'think', 'psych', 'assessment', 'reasonable', 'suggestion', 'youre', 'get', 'downvoted', 'need', 'accept', 'didnt', 'understand', 'clinical', 'scenario'])\n",
      "original document: \n",
      "['Terrible', 'snap', 'and', 'hold.', 'Not', 'on', 'the', 'kicker.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['terr', 'snap', 'hold', 'kick'], ['terrible', 'snap', 'hold', 'kicker'])\n",
      "original document: \n",
      "['To', 'be', 'honest,', 'vanilla', 'stalker', 'is', 'pretty', 'shit', 'compared', 'to', 'modded', 'stalker,', \"it's\", 'an', 'inferior', 'experience.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'vanill', 'stalk', 'pretty', 'shit', 'comp', 'mod', 'stalk', 'infery', 'expery'], ['honest', 'vanilla', 'stalker', 'pretty', 'shit', 'compare', 'modded', 'stalker', 'inferior', 'experience'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Now', 'all', 'you', 'have', 'to', 'do', 'is', 'figure', 'out', 'a', 'way', 'to', 'connect', 'paintball', 'guns', 'to', 'them', 'or', 'something', 'and', 'have', 'dog', 'fights']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fig', 'way', 'connect', 'paintbal', 'gun', 'someth', 'dog', 'fight'], ['figure', 'way', 'connect', 'paintball', 'gun', 'something', 'dog', 'fight'])\n",
      "original document: \n",
      "['This', 'literally', 'just', 'happened', 'to', 'me', 'an', 'hour', 'ago.', 'Her', 'name', 'came', 'up', 'under', \"'friend\", 'recommendations\"', 'and', 'I', 'saw', 'her', 'picture', 'and', 'it', 'was', 'of', 'her', 'and', 'my', 'ex....', 'looking', 'happy.', 'They', 'just', 'moved', 'in', 'together', 'after', 'only', 'two', 'months.', \"I've\", 'been', 'crying', 'hysterically', 'ever', 'since.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'hap', 'hour', 'ago', 'nam', 'cam', 'friend', 'recommend', 'saw', 'pict', 'ex', 'look', 'happy', 'mov', 'togeth', 'two', 'month', 'iv', 'cry', 'hyst', 'ev', 'sint'], ['literally', 'happen', 'hour', 'ago', 'name', 'come', 'friend', 'recommendations', 'saw', 'picture', 'ex', 'look', 'happy', 'move', 'together', 'two', 'months', 'ive', 'cry', 'hysterically', 'ever', 'since'])\n",
      "original document: \n",
      "['143417688|', '&gt;', 'Australia', 'Anonymous', '(ID:', 'XYEoqXRK)\\n\\n&gt;&gt;143412250', '(OP)\\nI', 'voted', 'NO', 'on', 'gay', 'marriage.', 'The', 'whole', 'campaign', 'has', 'been', 'a', 'fucking', 'joke', 'to', 'the', 'point', 'that', 'the', 'media', 'rarely', 'reports', 'on', 'it', 'anymore.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, six hundred and eighty-eight', 'gt', 'austral', 'anonym', 'id', 'xyeoqxrk\\n\\ngtgt143412250', 'op\\ni', 'vot', 'gay', 'marry', 'whol', 'campaign', 'fuck', 'jok', 'point', 'med', 'rar', 'report', 'anymore\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, six hundred and eighty-eight', 'gt', 'australia', 'anonymous', 'id', 'xyeoqxrk\\n\\ngtgt143412250', 'op\\ni', 'vote', 'gay', 'marriage', 'whole', 'campaign', 'fuck', 'joke', 'point', 'media', 'rarely', 'report', 'anymore\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['I', 'tried', 'tempting', 'him', 'with', 'my', 'twsbi', '+', 'diamine', 'red', 'dragon,', 'but', 'he', \"won't\", 'give', 'in', 'yet!', 'I', 'think', \"I'll\", 'try', 'working', 'on', 'his', 'mom', 'first', '(she', 'loves', 'colour', 'and', \"I'll\", 'gift', 'her', 'a', 'pen', 'tomorrow),', 'so', 'we', 'can', 'fight', 'him', 'on', 'two', 'fronts', 'xD', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tri', 'tempt', 'twsbi', 'diamin', 'red', 'dragon', 'wont', 'giv', 'yet', 'think', 'il', 'try', 'work', 'mom', 'first', 'lov', 'colo', 'il', 'gift', 'pen', 'tomorrow', 'fight', 'two', 'front', 'xd'], ['try', 'tempt', 'twsbi', 'diamine', 'red', 'dragon', 'wont', 'give', 'yet', 'think', 'ill', 'try', 'work', 'mom', 'first', 'love', 'colour', 'ill', 'gift', 'pen', 'tomorrow', 'fight', 'two', 'front', 'xd'])\n",
      "original document: \n",
      "['Lockheed', 'Martin', 'has', 'agreed', 'to', 'participate', 'with', 'El', 'Salvador,', 'and', 'will', 'committing', 'the', 'full', 'arsenal', 'of', 'its', 'resources', 'as', 'it', 'would', 'any', 'other', 'client.', 'Of', 'course', 'this', 'has', 'been', 'with', 'the', 'urging', 'of', 'the', 'U.S', 'State', 'Department', 'who', 'is', 'interested', 'in', 'increasing', 'its', 'positive', 'diplomatic', 'presence', 'in', 'the', 'Caribbean.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lockhee', 'martin', 'agree', 'particip', 'el', 'salvad', 'commit', 'ful', 'ars', 'resourc', 'would', 'cli', 'cours', 'urg', 'us', 'stat', 'depart', 'interest', 'increas', 'posit', 'diplom', 'pres', 'carib'], ['lockheed', 'martin', 'agree', 'participate', 'el', 'salvador', 'commit', 'full', 'arsenal', 'resources', 'would', 'client', 'course', 'urge', 'us', 'state', 'department', 'interest', 'increase', 'positive', 'diplomatic', 'presence', 'caribbean'])\n",
      "original document: \n",
      "['[Original', 'post](https://www.reddit.com/r/france/comments/73ig93/chamrousse_isère/)', 'by', '/u/wisi_eu', 'in', '/r/france\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#whitelist', '\"france\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'posthttpswwwredditcomrfrancecomments73ig93chamrousse_isere', 'uwisi_eu', 'rfrance\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nwhitelist', 'france\\n'], ['original', 'posthttpswwwredditcomrfrancecomments73ig93chamrousse_isere', 'uwisi_eu', 'rfrance\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nwhitelist', 'france\\n'])\n",
      "original document: \n",
      "['It', 'does', 'matter', 'for', 'us', 'in', 'the', 'sense', 'that', 'we', 'need', '1', 'more', 'win', 'to', 'clinch', 'HFA', 'in', 'the', 'world', 'series', '']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized and lemmatized document: \n",
      "(['mat', 'us', 'sens', 'nee', 'on', 'win', 'clinch', 'hfa', 'world', 'sery'], ['matter', 'us', 'sense', 'need', 'one', 'win', 'clinch', 'hfa', 'world', 'series'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Burn', 'crosses,', 'but', 'use', 'responsibly-sourced', 'wood', 'that', \"doesn't\", 'lead', 'to', 'deforestation?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['burn', 'cross', 'us', 'responsiblysourc', 'wood', 'doesnt', 'lead', 'deforest'], ['burn', 'cross', 'use', 'responsiblysourced', 'wood', 'doesnt', 'lead', 'deforestation'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"I'm\", 'gonna', 'start', 'wearing', 'heels', 'soon...', 'something', 'I', 'never', 'wanted', 'to', 'do', 'since', 'I', 'had', 'a', 'complex', 'about', 'being', 'too', 'tall.', '', 'But', 'now', 'that', \"I'm\", 'thinner,', \"it's\", 'stupid', 'but', 'I', 'feel', 'like', 'a', 'model', 'when', 'I', 'wear', 'heels']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'gonn', 'start', 'wear', 'heel', 'soon', 'someth', 'nev', 'want', 'sint', 'complex', 'tal', 'im', 'thin', 'stupid', 'feel', 'lik', 'model', 'wear', 'heel'], ['im', 'gonna', 'start', 'wear', 'heel', 'soon', 'something', 'never', 'want', 'since', 'complex', 'tall', 'im', 'thinner', 'stupid', 'feel', 'like', 'model', 'wear', 'heel'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Is', 'this', 'what', 'is', 'coming', 'from', 'the', 'discontinuation', 'of', 'the', 'sugar', 'free', 'syrups??']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['com', 'discontinu', 'sug', 'fre', 'syrup'], ['come', 'discontinuation', 'sugar', 'free', 'syrups'])\n",
      "original document: \n",
      "['Oh,', 'thank', 'you.', 'Yeah,', 'I', 'was', 'searching', 'for', 'a', 'little', 'while', 'but', \"couldn't\", 'find', 'the', 'OP.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'thank', 'yeah', 'search', 'littl', 'couldnt', 'find', 'op'], ['oh', 'thank', 'yeah', 'search', 'little', 'couldnt', 'find', 'op'])\n",
      "original document: \n",
      "['because', 'making', 'a', 'joke', 'about', 'a', 'regime', 'which', 'murdered', '40-60', 'million', 'people', 'is', '', 'lulz', 'funny.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'jok', 'regim', 'murd', 'four thousand and sixty', 'mil', 'peopl', 'lulz', 'funny'], ['make', 'joke', 'regime', 'murder', 'four thousand and sixty', 'million', 'people', 'lulz', 'funny'])\n",
      "original document: \n",
      "['Every', 'human', 'is', 'not', 'an', '\"unborn', 'clump', 'of', 'cells\"', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['every', 'hum', 'unborn', 'clump', 'cel', 'though'], ['every', 'human', 'unborn', 'clump', 'cells', 'though'])\n",
      "original document: \n",
      "['They', 'look', 'blue', 'to', 'me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'blu'], ['look', 'blue'])\n",
      "original document: \n",
      "['Bionic-Frog', '283', 'Titan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bionicfrog', 'two hundred and eighty-three', 'tit'], ['bionicfrog', 'two hundred and eighty-three', 'titan'])\n",
      "original document: \n",
      "['I', 'hope', 'this', 'goes', 'viral', 'and', 'the', 'winning', 'name', 'be', 'like', 'osama', 'bin', 'laden', 'or', 'something.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'goe', 'vir', 'win', 'nam', 'lik', 'osam', 'bin', 'lad', 'someth'], ['hope', 'go', 'viral', 'win', 'name', 'like', 'osama', 'bin', 'lade', 'something'])\n",
      "original document: \n",
      "['My', 'current', 'shop', 'head', 'once', 'got', 'a', 'muscle', 'detached', 'from', 'his', 'forearm', 'and', 'curled', 'up', 'into', 'his', 'elbow', 'because', 'someone', 'dropped', 'something', 'they', 'were', 'hoisting', 'up', 'into', 'the', 'fly', 'system.', 'He', \"didn't\", 'let', 'go', 'and', 'the', '500', 'lbs', 'of', 'weight', 'on', 'the', 'other', 'end', 'of', 'the', 'rope', 'won.', 'He', 'has', 'a', 'scar', 'up', 'his', 'whole', 'arm', 'because', 'he', 'needed', 'surgery', 'to', 'reattach', 'the', 'muscle.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cur', 'shop', 'head', 'got', 'musc', 'detach', 'forearm', 'curl', 'elbow', 'someon', 'drop', 'someth', 'hoist', 'fly', 'system', 'didnt', 'let', 'go', 'five hundred', 'lbs', 'weight', 'end', 'rop', 'scar', 'whol', 'arm', 'nee', 'surgery', 'reattach', 'musc'], ['current', 'shop', 'head', 'get', 'muscle', 'detach', 'forearm', 'curl', 'elbow', 'someone', 'drop', 'something', 'hoist', 'fly', 'system', 'didnt', 'let', 'go', 'five hundred', 'lbs', 'weight', 'end', 'rope', 'scar', 'whole', 'arm', 'need', 'surgery', 'reattach', 'muscle'])\n",
      "original document: \n",
      "['Hello', '/u/UR_JUST_TO_GREEDY\\n\\nYour', 'post', 'has', 'been', 'flagged', 'as', 'a', 'repost', 'and', 'has', 'been', 'removed.', 'In', '/r/giveaways,', 'reposts', 'are', 'allowed', 'once', '**48**', 'hours', 'pass.\\n\\n**To', 'prevent', 'reposts', 'in', 'the', 'future,', 'use', 'the', '[Giveaways', 'Repost', 'Finder', 'Tool](http://checkrepost.com/)**.\\n\\nHere', 'is', 'the', 'original', 'post:', '[Win', 'a', 'Luxury', 'Workbag', 'Collection', 'or', 'a', '$1000', 'Gift', 'Card', 'to', 'Jennifer', 'Hamley', 'England', '(9/30){US', 'UK},](http://reddit.com/r/giveaways/comments/73i211/win_a_luxury_workbag_collection_or_a_1000_gift/)', 'by', 'Derelictive.', 'You', 'can', 'repost', 'this', 'in', '**1d', '22h', '44m', '43s**.', '\\n\\nPlease', 'note', 'that', 'the', 'bot', 'will', 'never', 'make', 'a', 'mistake', 'about', 'calculating', '48h.', 'But', 'if', 'a', 'mistake', 'was', 'made', 'please', '[message', 'the', 'mods](https://www.reddit.com/message/compose?to=/r/giveaways&amp;subject=/r/giveaways+Incorrect+Post+Removal&amp;message=/r/giveaways/comments/73ig6r/luxury_workbag_collection_giveaway_the_entire/%0A%0A&lt;&lt;Enter', 'Reason', 'Here&gt;&gt;)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hello', 'uur_just_to_greedy\\n\\nyo', 'post', 'flag', 'repost', 'remov', 'rgiveaway', 'repost', 'allow', 'forty-eight', 'hour', 'pass\\n\\nto', 'prev', 'repost', 'fut', 'us', 'giveaway', 'repost', 'find', 'toolhttpcheckrepostcom\\n\\nhere', 'origin', 'post', 'win', 'luxury', 'workb', 'collect', 'one thousand', 'gift', 'card', 'jen', 'hamley', 'england', '930us', 'ukhttpredditcomrgiveawayscomments73i211win_a_luxury_workbag_collection_or_a_1000_gift', 'derelict', 'repost', '1d', '22h', '44m', '43s', '\\n\\nplease', 'not', 'bot', 'nev', 'mak', 'mistak', 'calc', '48h', 'mistak', 'mad', 'pleas', 'mess', 'modshttpswwwredditcommessagecomposetorgiveawaysampsubjectrgiveawaysincorrectpostremovalampmessagergiveawayscomments73ig6rluxury_workbag_collection_giveaway_the_entire0a0altltenter', 'reason', 'heregtgt'], ['hello', 'uur_just_to_greedy\\n\\nyour', 'post', 'flag', 'repost', 'remove', 'rgiveaways', 'reposts', 'allow', 'forty-eight', 'hours', 'pass\\n\\nto', 'prevent', 'reposts', 'future', 'use', 'giveaways', 'repost', 'finder', 'toolhttpcheckrepostcom\\n\\nhere', 'original', 'post', 'win', 'luxury', 'workbag', 'collection', 'one thousand', 'gift', 'card', 'jennifer', 'hamley', 'england', '930us', 'ukhttpredditcomrgiveawayscomments73i211win_a_luxury_workbag_collection_or_a_1000_gift', 'derelictive', 'repost', '1d', '22h', '44m', '43s', '\\n\\nplease', 'note', 'bot', 'never', 'make', 'mistake', 'calculate', '48h', 'mistake', 'make', 'please', 'message', 'modshttpswwwredditcommessagecomposetorgiveawaysampsubjectrgiveawaysincorrectpostremovalampmessagergiveawayscomments73ig6rluxury_workbag_collection_giveaway_the_entire0a0altltenter', 'reason', 'heregtgt'])\n",
      "original document: \n",
      "['Many', 'schools', 'would', 'only', 'want', 'SAT', 'subject', 'scores', 'if', 'you', 'are', 'sending', 'the', 'SAT,', 'but', 'in', 'your', 'case,', 'your', 'SAT', 'scores', 'are', 'much', 'better', 'than', 'your', 'ACT', 'scores', 'so', 'you', 'should', 'probably', 'send', 'those.', 'As', 'far', 'as', 'subject', 'tests,', 'that', 'is', 'a', 'huge', 'range', 'of', 'scores', 'for', 'the', 'Math', 'II', 'you', 'are', 'getting.', 'The', 'Lit', 'test', 'is', 'generally', 'considered', 'pretty', 'hard,', 'so', 'even', 'a', '650', \"isn't\", 'that', 'bad', 'of', 'a', 'score.', 'What', 'books', 'are', 'you', 'using', 'for', 'Math', 'II?', 'I', 'could', 'give', 'you', 'some', 'tips', 'on', 'how', 'I', 'crammed', 'the', 'week', 'before', 'the', 'test.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'schools', 'would', 'want', 'sat', 'subject', 'scor', 'send', 'sat', 'cas', 'sat', 'scor', 'much', 'bet', 'act', 'scor', 'prob', 'send', 'far', 'subject', 'test', 'hug', 'rang', 'scor', 'math', 'ii', 'get', 'lit', 'test', 'gen', 'consid', 'pretty', 'hard', 'ev', 'six hundred and fifty', 'isnt', 'bad', 'scor', 'book', 'us', 'math', 'ii', 'could', 'giv', 'tip', 'cram', 'week', 'test'], ['many', 'school', 'would', 'want', 'sit', 'subject', 'score', 'send', 'sit', 'case', 'sit', 'score', 'much', 'better', 'act', 'score', 'probably', 'send', 'far', 'subject', 'test', 'huge', 'range', 'score', 'math', 'ii', 'get', 'light', 'test', 'generally', 'consider', 'pretty', 'hard', 'even', 'six hundred and fifty', 'isnt', 'bad', 'score', 'book', 'use', 'math', 'ii', 'could', 'give', 'tip', 'cram', 'week', 'test'])\n",
      "original document: \n",
      "['Be', 'interesting', 'to', 'see', 'the', 'stats', 'for', 'pc', 'as', 'well.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'see', 'stat', 'pc', 'wel'], ['interest', 'see', 'stats', 'pc', 'well'])\n",
      "original document: \n",
      "['Disgusting.', 'Go', 'fuck', 'yourself.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['disgust', 'go', 'fuck'], ['disgust', 'go', 'fuck'])\n",
      "original document: \n",
      "['I', 'identify', 'with', 'this', 'on', 'a', 'deep,', 'emotional', 'level.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ident', 'deep', 'emot', 'level'], ['identify', 'deep', 'emotional', 'level'])\n",
      "original document: \n",
      "['Oh', \"we'd\", 'love', 'to', 'hear', 'your', 'mature', 'opinion', 'u/cannabis_detox']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'wed', 'lov', 'hear', 'mat', 'opin', 'ucannabis_detox'], ['oh', 'wed', 'love', 'hear', 'mature', 'opinion', 'ucannabis_detox'])\n",
      "original document: \n",
      "['Spot', '53', 'if', 'open', 'and', 'one', 'random', '(2', 'spots', 'total)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spot', 'fifty-three', 'op', 'on', 'random', 'two', 'spot', 'tot'], ['spot', 'fifty-three', 'open', 'one', 'random', 'two', 'spot', 'total'])\n",
      "original document: \n",
      "['zozzle']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zozzl'], ['zozzle'])\n",
      "original document: \n",
      "['Or', 'use', 'a', 'raspberry', 'pi', 'and', 'have', 'exactly', 'the', 'same', 'set', 'up', '/', 'small', 'sized', 'box', 'but', 'with', 'thousands', 'of', 'games', 'from', 'dozens', 'of', 'consoles', 'instead', 'of', '20', 'games', 'from', '1', 'console,', 'plus', '4+', 'controllers', 'of', 'any', 'console', 'you', 'want', 'instead', 'of', '2', 'with', 'a', 'funky', 'connector', 'you', 'can’t', 'use', 'with', 'anything', 'else.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'raspberry', 'pi', 'exact', 'set', 'smal', 'siz', 'box', 'thousand', 'gam', 'doz', 'consol', 'instead', 'twenty', 'gam', 'on', 'consol', 'plu', 'four', 'control', 'consol', 'want', 'instead', 'two', 'funky', 'connect', 'cant', 'us', 'anyth', 'els'], ['use', 'raspberry', 'pi', 'exactly', 'set', 'small', 'size', 'box', 'thousands', 'game', 'dozens', 'console', 'instead', 'twenty', 'game', 'one', 'console', 'plus', 'four', 'controllers', 'console', 'want', 'instead', 'two', 'funky', 'connector', 'cant', 'use', 'anything', 'else'])\n",
      "original document: \n",
      "['Hey', 'guys!', 'Sorry', \"I'm\", 'late', 'to', 'the', 'game.', \"\\n\\nI'm\", 'not', 'sure', 'if', 'you', 'remember', 'playing', 'at', 'Tricky', 'Falls', 'in', 'El', 'Paso', '2', 'years', 'ago', 'on', 'the', 'night', 'that', 'The', 'Offspring', 'was', 'playing', 'in', 'town,', 'but', 'I', 'had', 'a', 'blast', 'at', 'your', 'show!', 'There', \"weren't\", 'many', 'of', 'us', 'there,', 'but', 'the', 'time', 'you', 'guys', 'spent', 'hanging', 'out', 'while', 'playing', 'and', 'after', 'was', 'fucking', 'awesome.', 'I', 'heard', 'that', 'the', 'follow', 'day', 'your', 'van', 'was', 'stolen', 'with', 'all', 'of', 'your', 'equipment', 'in', 'San', 'Antonio', 'or', 'something...', 'Did', 'you', 'guys', 'ever', 'get', 'your', 'stuff', 'back?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'guy', 'sorry', 'im', 'lat', 'gam', '\\n\\nim', 'sur', 'rememb', 'play', 'tricky', 'fal', 'el', 'paso', 'two', 'year', 'ago', 'night', 'offspr', 'play', 'town', 'blast', 'show', 'wer', 'many', 'us', 'tim', 'guy', 'spent', 'hang', 'play', 'fuck', 'awesom', 'heard', 'follow', 'day', 'van', 'stol', 'equip', 'san', 'antonio', 'someth', 'guy', 'ev', 'get', 'stuff', 'back'], ['hey', 'guy', 'sorry', 'im', 'late', 'game', '\\n\\nim', 'sure', 'remember', 'play', 'tricky', 'fall', 'el', 'paso', 'two', 'years', 'ago', 'night', 'offspring', 'play', 'town', 'blast', 'show', 'werent', 'many', 'us', 'time', 'guy', 'spend', 'hang', 'play', 'fuck', 'awesome', 'hear', 'follow', 'day', 'van', 'steal', 'equipment', 'san', 'antonio', 'something', 'guy', 'ever', 'get', 'stuff', 'back'])\n",
      "original document: \n",
      "['the', 'bushido', 'plan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bushido', 'plan'], ['bushido', 'plan'])\n",
      "original document: \n",
      "['Your', 'fellow', 'countrymen', 'beg', 'to', 'differ;', 'always', 'taking', 'credit', 'for', \"other's\", 'accomplishments', 'in', 'order', 'to', 'make', 'themselves', 'feel', 'better/superior.', 'That', 'is', 'until', 'the', 'connotations', 'are', 'bad', 'like', 'the', 'guy', 'above', 'said.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fellow', 'countrym', 'beg', 'diff', 'alway', 'tak', 'credit', 'oth', 'accompl', 'ord', 'mak', 'feel', 'bettersupery', 'connot', 'bad', 'lik', 'guy', 'said'], ['fellow', 'countrymen', 'beg', 'differ', 'always', 'take', 'credit', 'others', 'accomplishments', 'order', 'make', 'feel', 'bettersuperior', 'connotations', 'bad', 'like', 'guy', 'say'])\n",
      "original document: \n",
      "['1', 'random', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['Yeah', 'I', 'hate', 'ashura', 'the', 'most...', \"I'm\", 'also', 'not', 'allowed', 'to', 'play', 'any', 'sort', 'of', 'video', 'games,', 'tv', 'or', 'anything', 'that', 'involves', 'fun', 'on', 'the', '9th-12th', 'day.', 'I', 'go', 'to', 'the', 'mosque', 'where', 'they', 'beat', 'their', 'chest', '(I', 'like', 'to', 'be', 'in', 'spots', 'with', 'a', 'lot', 'of', 'people', 'so', 'I', \"don't\", 'have', 'to', 'beat', 'it', 'hard', 'and', 'it', 'looks', 'like', 'I', 'just', \"can't\", 'with', 'the', 'guy', 'in', 'front', 'of', 'me', 'being', '&lt;a', 'feet', 'away.', 'I', 'have', 'to', 'do', 'zinjeel', 'where', 'I', 'hit', 'my', 'back', 'with', 'the', 'chains', '(been', 'doing', 'since', '5th', 'grade).', 'At', 'the', 'end', 'my', 'uncle', 'came', 'up', 'to', 'me', 'and', 'told', 'me', 'that', 'I', 'needed', 'to', 'hit', 'my', 'back', 'harder,', 'he', 'showed', 'me', 'a', 'video', 'of', 'my', 'fatass', 'where', 'I', 'looked', 'like', 'a', 'suicidal', 'kid.', \"I've\", 'gotta', 'admit', 'the', 'worst', 'part', 'is', 'sitting', 'on', 'the', 'ground', 'during', 'the', 'lectures', 'in', 'Arabic', 'which', 'I', \"don't\", 'understand', 'yet', 'I', 'am', 'supposed', 'to', 'attend', 'earlier', 'than', 'regular.', \"I'm\", '15', 'and', 'I', 'can', 'assure', 'you', 'this', 'entire', '\"commemoration\"', 'is', 'bullshit.', 'If', 'you', \"don't\", 'like', 'my', 'English,', 'hit', 'up', 'my', 'English', 'teacher', 'because', \"I'm\", 'not', 'writing', 'a', 'fucking', 'MLA', 'format', 'essay', 'for', 'you', 'guys.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'hat', 'ashur', 'im', 'also', 'allow', 'play', 'sort', 'video', 'gam', 'tv', 'anyth', 'involv', 'fun', '9th12th', 'day', 'go', 'mosqu', 'beat', 'chest', 'lik', 'spot', 'lot', 'peopl', 'dont', 'beat', 'hard', 'look', 'lik', 'cant', 'guy', 'front', 'lta', 'feet', 'away', 'zinjeel', 'hit', 'back', 'chain', 'sint', '5th', 'grad', 'end', 'unc', 'cam', 'told', 'nee', 'hit', 'back', 'hard', 'show', 'video', 'fatass', 'look', 'lik', 'suicid', 'kid', 'iv', 'gott', 'admit', 'worst', 'part', 'sit', 'ground', 'lect', 'arab', 'dont', 'understand', 'yet', 'suppos', 'attend', 'ear', 'regul', 'im', 'fifteen', 'ass', 'entir', 'commem', 'bullshit', 'dont', 'lik', 'engl', 'hit', 'engl', 'teach', 'im', 'writ', 'fuck', 'mla', 'form', 'essay', 'guy'], ['yeah', 'hate', 'ashura', 'im', 'also', 'allow', 'play', 'sort', 'video', 'game', 'tv', 'anything', 'involve', 'fun', '9th12th', 'day', 'go', 'mosque', 'beat', 'chest', 'like', 'spot', 'lot', 'people', 'dont', 'beat', 'hard', 'look', 'like', 'cant', 'guy', 'front', 'lta', 'feet', 'away', 'zinjeel', 'hit', 'back', 'chain', 'since', '5th', 'grade', 'end', 'uncle', 'come', 'tell', 'need', 'hit', 'back', 'harder', 'show', 'video', 'fatass', 'look', 'like', 'suicidal', 'kid', 'ive', 'gotta', 'admit', 'worst', 'part', 'sit', 'grind', 'lecture', 'arabic', 'dont', 'understand', 'yet', 'suppose', 'attend', 'earlier', 'regular', 'im', 'fifteen', 'assure', 'entire', 'commemoration', 'bullshit', 'dont', 'like', 'english', 'hit', 'english', 'teacher', 'im', 'write', 'fuck', 'mla', 'format', 'essay', 'guy'])\n",
      "original document: \n",
      "['Two', 'please', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'pleas'], ['two', 'please'])\n",
      "original document: \n",
      "['If', \"you're\", 'interested,', 'the', 'Band', 'plays', 'a', 'pregame', 'concert', 'with', 'the', 'halftime', 'music', 'of', 'the', 'week', 'one', 'hour', 'before', 'the', 'game', 'at', 'Kimball', 'Concert', 'hall.', 'Then', 'they', 'march', 'down', 'in', 'front', 'of', 'the', 'stadium.', \"It's\", 'a', 'pretty', 'good', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'interest', 'band', 'play', 'pregam', 'concert', 'halftim', 'mus', 'week', 'on', 'hour', 'gam', 'kimbal', 'concert', 'hal', 'march', 'front', 'stad', 'pretty', 'good', 'tim'], ['youre', 'interest', 'band', 'play', 'pregame', 'concert', 'halftime', 'music', 'week', 'one', 'hour', 'game', 'kimball', 'concert', 'hall', 'march', 'front', 'stadium', 'pretty', 'good', 'time'])\n",
      "original document: \n",
      "['Lol', \"isn't\", 'EG', 'qualified', 'for', 'the', 'exact', 'same', 'number', 'of', 'LANs?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'isnt', 'eg', 'qual', 'exact', 'numb', 'lan'], ['lol', 'isnt', 'eg', 'qualify', 'exact', 'number', 'lans'])\n",
      "original document: \n",
      "[\"It's\", 'sad', 'that', 'her', 'mother', 'is', 'going', 'to', 'try', 'to', 'use', 'this', 'to', 'influence', 'her.', '\\n\\nHowever', 'your', 'ex', 'might', 'see', 'through', 'it', 'this', 'time', 'and', 'you', 'were', 'more', 'than', 'in', 'the', 'right', 'to', 'do', 'this', 'so', \"don't\", 'worry', 'about', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'moth', 'going', 'try', 'us', 'influ', '\\n\\nhowever', 'ex', 'might', 'see', 'tim', 'right', 'dont', 'worry'], ['sad', 'mother', 'go', 'try', 'use', 'influence', '\\n\\nhowever', 'ex', 'might', 'see', 'time', 'right', 'dont', 'worry'])\n",
      "original document: \n",
      "['Schools', 'AND', 'parents.', 'Ex', 'gf', 'was', 'a', 'goddamn', 'neurotic', 'mess', 'from', 'it', 'all.', 'Ridiculous.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['schools', 'par', 'ex', 'gf', 'goddamn', 'neurot', 'mess', 'ridic'], ['school', 'parent', 'ex', 'gf', 'goddamn', 'neurotic', 'mess', 'ridiculous'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'simple', 'man.', 'I', 'see', 'Mughals,', 'I', 'upvote!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'simpl', 'man', 'see', 'mugh', 'upvot'], ['im', 'simple', 'man', 'see', 'mughals', 'upvote'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Get', 'higher', 'baby.', 'DONT', 'EVER', 'COME', 'DOWN!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'high', 'baby', 'dont', 'ev', 'com'], ['get', 'higher', 'baby', 'dont', 'ever', 'come'])\n",
      "original document: \n",
      "[\"I'm\", 'gonna', 'take', 'a', 'wild', 'guess', 'and', 'assume', 'that', 'this', 'game', \"didn't\", 'go', 'your', 'way']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'gonn', 'tak', 'wild', 'guess', 'assum', 'gam', 'didnt', 'go', 'way'], ['im', 'gonna', 'take', 'wild', 'guess', 'assume', 'game', 'didnt', 'go', 'way'])\n",
      "original document: \n",
      "['This', 'looks', 'better']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'bet'], ['look', 'better'])\n",
      "original document: \n",
      "['Um', 'actually', 'strimmer', \"that's\", 'the', 'personification', 'of', 'death', 'masquerading', 'a', 'cop', 'as', 'played', 'by', 'Bobert']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['um', 'act', 'strimmer', 'that', 'person', 'dea', 'masquerad', 'cop', 'play', 'bobert'], ['um', 'actually', 'strimmer', 'thats', 'personification', 'death', 'masquerade', 'cop', 'play', 'bobert'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Mazda', 'already', 'has', 'a', 'limited', 'partnership', 'with', 'toyota.', 'I', \"wouldn't\", 'be', 'surprised', 'if', 'they', 'merged', 'or', 'cooperated', 'much', 'more', 'closely.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mazd', 'already', 'limit', 'partn', 'toyot', 'wouldnt', 'surpr', 'merg', 'coop', 'much', 'clos'], ['mazda', 'already', 'limit', 'partnership', 'toyota', 'wouldnt', 'surprise', 'merge', 'cooperate', 'much', 'closely'])\n",
      "original document: \n",
      "['Essentially', 'what', 'auto', 'said.', 'Everything', 'is', 'about', 'the', 'moment,', 'there', 'is', 'nothing', 'but', 'the', 'moment.', 'Both', 'past', 'and', 'future', 'are', 'concepts', 'of', 'mind.', 'There', 'is', 'only', 'change,', 'every', 'thing', 'comes', 'and', 'goes.', 'Now,', 'what', 'you', 'do', 'in', 'the', 'moment', 'is', 'what', 'creates', 'your', 'life.', 'What', 'you', 'set', 'was', 'an', 'intention', 'to', 'do', 'something,', 'you', 'knew', 'what', 'you', 'had', 'to', 'do', 'in', 'the', 'moment', 'and', 'you', 'executed.', 'What', \"I'm\", 'saying', 'is,', \"there's\", 'no', 'need', 'to', 'worry', 'about', 'something', 'that', \"hasn't\", 'come', 'to', 'pass,', 'or', 'get', 'excited', 'and', 'create', 'delusions', 'of', 'grandeur.', 'Not', 'necessarily', 'that', 'I', 'say', \"don't\", 'plan', 'for', 'the', 'future', 'or', 'intend', 'to', 'direct', 'your', 'life', 'a', 'certain', 'way.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ess', 'auto', 'said', 'everyth', 'mom', 'noth', 'mom', 'past', 'fut', 'conceiv', 'mind', 'chang', 'every', 'thing', 'com', 'goe', 'mom', 'cre', 'lif', 'set', 'int', 'someth', 'knew', 'mom', 'execut', 'im', 'say', 'ther', 'nee', 'worry', 'someth', 'hasnt', 'com', 'pass', 'get', 'excit', 'cre', 'delud', 'grand', 'necess', 'say', 'dont', 'plan', 'fut', 'intend', 'direct', 'lif', 'certain', 'way'], ['essentially', 'auto', 'say', 'everything', 'moment', 'nothing', 'moment', 'past', 'future', 'concepts', 'mind', 'change', 'every', 'thing', 'come', 'go', 'moment', 'create', 'life', 'set', 'intention', 'something', 'know', 'moment', 'execute', 'im', 'say', 'theres', 'need', 'worry', 'something', 'hasnt', 'come', 'pass', 'get', 'excite', 'create', 'delusions', 'grandeur', 'necessarily', 'say', 'dont', 'plan', 'future', 'intend', 'direct', 'life', 'certain', 'way'])\n",
      "original document: \n",
      "['Maybe', \"it's\", 'just', 'me,', 'but', \"I'd\", 'give', 'up', 'a', 'late', 'first', 'and', 'cap', 'filler.', 'I', 'trust', 'Brad', 'to', 'get', 'everything', 'out', 'of', 'him', 'there', 'is.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'id', 'giv', 'lat', 'first', 'cap', 'fil', 'trust', 'brad', 'get', 'everyth'], ['maybe', 'id', 'give', 'late', 'first', 'cap', 'filler', 'trust', 'brad', 'get', 'everything'])\n",
      "original document: \n",
      "[\"What's\", 'that', 'mean?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'mean'], ['whats', 'mean'])\n",
      "original document: \n",
      "['You', 'know', 'that', 'some', 'women', 'brew', 'too,', 'right?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'wom', 'brew', 'right'], ['know', 'women', 'brew', 'right'])\n",
      "original document: \n",
      "['They', 'paid', 'Espn']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['paid', 'espn'], ['pay', 'espn'])\n",
      "original document: \n",
      "['Are', 'you', 'sure?', 'Cause', 'my', \"momma's\", \"didn't\", 'produce', '190', 'degree', 'breast', 'milk.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'caus', 'momma', 'didnt', 'produc', 'one hundred and ninety', 'degr', 'breast', 'milk'], ['sure', 'cause', 'mommas', 'didnt', 'produce', 'one hundred and ninety', 'degree', 'breast', 'milk'])\n",
      "original document: \n",
      "['Emailed.', 'But', 'in', 'support', 'of', 'keeping', 'daylight', 'savings', 'time.', 'Light', 'in', 'the', 'sky', 'at', '3:30am', 'in', 'summer', 'is', 'absolutely', 'ridiculous.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['email', 'support', 'keep', 'daylight', 'sav', 'tim', 'light', 'sky', '330am', 'sum', 'absolv', 'ridic'], ['email', 'support', 'keep', 'daylight', 'save', 'time', 'light', 'sky', '330am', 'summer', 'absolutely', 'ridiculous'])\n",
      "original document: \n",
      "['I', 'think', \"that's\", 'true', 'through', 'the', 'history', 'of', 'mankind', 'haha', 'women', 'have', 'always', 'been', 'more', 'self', 'aware', 'about', 'appearance', 'and', 'conscious', 'about', 'how', 'they', 'appear', 'to', 'the', 'world.', \"There's\", 'definitely', 'minimal', 'effort', 'on', 'the', 'male', 'side,', 'but', 'women', 'love', 'analyze', 'to', 'the', 'smallest', 'detail', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'that', 'tru', 'hist', 'mankind', 'hah', 'wom', 'alway', 'self', 'aw', 'appear', 'conscy', 'appear', 'world', 'ther', 'definit', 'minim', 'effort', 'mal', 'sid', 'wom', 'lov', 'analys', 'smallest', 'detail'], ['think', 'thats', 'true', 'history', 'mankind', 'haha', 'women', 'always', 'self', 'aware', 'appearance', 'conscious', 'appear', 'world', 'theres', 'definitely', 'minimal', 'effort', 'male', 'side', 'women', 'love', 'analyze', 'smallest', 'detail'])\n",
      "original document: \n",
      "['Best', 'bet', 'is', 'to', 'just', 'download', 'a', 'release', 'that', 'comes', 'with', 'all', 'the', 'dlc,', 'and', 'transfer', 'the', 'files', 'over', 'to', 'the', 'legit', 'steam', 'installation', 'folder.\\n\\nIdk', 'uf', 'it', 'will', 'actually', 'work,', 'im', 'sure', 'steam', 'checks', 'to', 'see', 'if', 'you', 'actually', 'bought', 'the', 'dlc.\\n\\nGood', 'luck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'bet', 'download', 'releas', 'com', 'dlc', 'transf', 'fil', 'legit', 'steam', 'instal', 'folder\\n\\nidk', 'uf', 'act', 'work', 'im', 'sur', 'steam', 'check', 'see', 'act', 'bought', 'dlc\\n\\ngood', 'luck'], ['best', 'bet', 'download', 'release', 'come', 'dlc', 'transfer', 'file', 'legit', 'steam', 'installation', 'folder\\n\\nidk', 'uf', 'actually', 'work', 'im', 'sure', 'steam', 'check', 'see', 'actually', 'buy', 'dlc\\n\\ngood', 'luck'])\n",
      "original document: \n",
      "['The', 'new', 'emotes', '+', 'post', 'match', 'was', 'confirmed', 'in', 'the', 'touchdown', 'tournament.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'emot', 'post', 'match', 'confirm', 'touchdown', 'tourna'], ['new', 'emote', 'post', 'match', 'confirm', 'touchdown', 'tournament'])\n",
      "original document: \n",
      "['...', \"it's\", 'not', 'disrespecting', 'the', 'country.', 'Nor', 'is', 'it', 'disrespecting', 'the', 'flag.', 'And', \"it's\", 'certainly', 'not', 'disrespecting', 'the', 'military.\\n\\nWhy', 'do', 'people', 'have', 'such', 'a', 'hard', 'fucking', 'time', 'understanding', \"what's\", 'happening?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['disrespect', 'country', 'disrespect', 'flag', 'certain', 'disrespect', 'military\\n\\nwhy', 'peopl', 'hard', 'fuck', 'tim', 'understand', 'what', 'hap'], ['disrespect', 'country', 'disrespect', 'flag', 'certainly', 'disrespect', 'military\\n\\nwhy', 'people', 'hard', 'fuck', 'time', 'understand', 'whats', 'happen'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['You’ll', 'be', 'lucky', 'to', 'land', 'a', 'job', 'at', 'McDonald’s', 'with', 'a', '3rd', 'grade', 'education', 'LM🅱️O']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youl', 'lucky', 'land', 'job', 'mcdonalds', '3rd', 'grad', 'educ', 'lmo'], ['youll', 'lucky', 'land', 'job', 'mcdonalds', '3rd', 'grade', 'education', 'lmo'])\n",
      "original document: \n",
      "['I', 'am', 'a', 'retired', 'teacher', 'and', 'volunteer', 'in', 'kindergarten.', 'I', \"don't\", 'think', 'ADHD', 'is', 'stigmatized,', 'but', 'that', 'is', 'based', 'on', 'my', 'experience', 'in', 'education.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['retir', 'teach', 'volunt', 'kindergart', 'dont', 'think', 'adhd', 'stigm', 'bas', 'expery', 'educ'], ['retire', 'teacher', 'volunteer', 'kindergarten', 'dont', 'think', 'adhd', 'stigmatize', 'base', 'experience', 'education'])\n",
      "original document: \n",
      "['Ya.', 'Dude', 'really', 'is', 'A', 'POS', 'if', 'you', 'ask', 'me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya', 'dud', 'real', 'pos', 'ask'], ['ya', 'dude', 'really', 'pos', 'ask'])\n",
      "original document: \n",
      "['This', 'is', 'an', 'in', 'game', 'tip', 'also', 'so', 'I', \"don't\", 'know', 'why', 'people', \"don't\", 'know', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gam', 'tip', 'also', 'dont', 'know', 'peopl', 'dont', 'know'], ['game', 'tip', 'also', 'dont', 'know', 'people', 'dont', 'know'])\n",
      "original document: \n",
      "['If', 'the', 'times', 'stay', 'the', 'same,', 'then', \"I'd\", 'only', 'be', 'able', 'to', 'have', 'Orgo', 'with', 'Tovar', '(unless', 'Falzone', 'decides', 'he', 'likes', 'sophomores', 'again)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tim', 'stay', 'id', 'abl', 'orgo', 'tov', 'unless', 'falzon', 'decid', 'lik', 'sophom'], ['time', 'stay', 'id', 'able', 'orgo', 'tovar', 'unless', 'falzone', 'decide', 'like', 'sophomores'])\n",
      "original document: \n",
      "['&gt;', 'one', 'or', 'none\\n\\nI', \"didn't\", 'forget']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'on', 'none\\n\\ni', 'didnt', 'forget'], ['gt', 'one', 'none\\n\\ni', 'didnt', 'forget'])\n",
      "original document: \n",
      "['Knowing', 'reddit', 'some', 'idiots', 'will', 'still', 'think', \"it's\", 'real']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'reddit', 'idiot', 'stil', 'think', 'real'], ['know', 'reddit', 'idiots', 'still', 'think', 'real'])\n",
      "original document: \n",
      "['Thank', 'you,', 'I', 'will', 'check', 'it', 'out!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'check'], ['thank', 'check'])\n",
      "original document: \n",
      "['This', 'sub', 'now', 'allows', 'reposts', 'from', '2007?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sub', 'allow', 'repost', 'two thousand and seven'], ['sub', 'allow', 'reposts', 'two thousand and seven'])\n",
      "original document: \n",
      "['how', 'to', 'ruin', 'an', 'evening', 'in', '30', 'seconds']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ruin', 'ev', 'thirty', 'second'], ['ruin', 'even', 'thirty', 'second'])\n",
      "original document: \n",
      "['Because', 'of', 'how', 'legendary', 'their', 'second', 'fight', 'was,', 'a', 'lot', 'of', 'people', 'have', 'forgotten', 'just', 'how', 'good', 'their', 'first', 'one', 'was.', 'It', 'was', 'my', 'favourite', 'kind', 'of', 'a', 'fight:', 'a', 'technical', 'barn-burned.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['legend', 'second', 'fight', 'lot', 'peopl', 'forgot', 'good', 'first', 'on', 'favourit', 'kind', 'fight', 'techn', 'barnburn'], ['legendary', 'second', 'fight', 'lot', 'people', 'forget', 'good', 'first', 'one', 'favourite', 'kind', 'fight', 'technical', 'barnburned'])\n",
      "original document: \n",
      "[\"I'm\", 'sorry', 'if', 'this', 'is', 'a', 'stupid', 'question,', 'I', 'just', 'got', 'into', 'LPOTL', 'a', 'few', 'months', 'ago,', 'but', 'what', 'all', 'do', 'they', 'do', 'at', 'their', 'live', 'shows?', 'Thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'stupid', 'quest', 'got', 'lpotl', 'month', 'ago', 'liv', 'show', 'thank'], ['im', 'sorry', 'stupid', 'question', 'get', 'lpotl', 'months', 'ago', 'live', 'show', 'thank'])\n",
      "original document: \n",
      "['Noob', 'here.', \"What's\", \"LTC's\", 'potential?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noob', 'what', 'ltcs', 'pot'], ['noob', 'whats', 'ltcs', 'potential'])\n",
      "original document: \n",
      "['He', 'was', 'on', 'the', 'opposite', 'side', 'of', 'the', 'line,', 'most', 'likely', 'not', 'getting', 'a', 'call', 'for', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['opposit', 'sid', 'lin', 'lik', 'get', 'cal'], ['opposite', 'side', 'line', 'likely', 'get', 'call'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['143415878|', '&gt;', 'None', 'Anonymous', '(ID:', '5Pd916Ww)\\n\\n&gt;&gt;143412250', '(OP)\\n2012:', 'obama\\n2016:', 'bernie', 'then', 'hillary\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fifteen thousand, eight hundred and seventy-eight', 'gt', 'non', 'anonym', 'id', '5pd916ww\\n\\ngtgt143412250', 'op\\n2012', 'obama\\n2016', 'berny', 'hillary\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fifteen thousand, eight hundred and seventy-eight', 'gt', 'none', 'anonymous', 'id', '5pd916ww\\n\\ngtgt143412250', 'op\\n2012', 'obama\\n2016', 'bernie', 'hillary\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['i', 'would', 'honestly', 'get', 'divorced', ',', 'just', 'because', 'HE', 'now', '', 'counts', 'as', 'Straight', 'instead', 'of', 'lesbian', 'doesnt', 'mean', 'you', 'have', 'to', 'change', 'your', 'sexual', 'orientation', ',', 'its', 'okay', 'to', 'still', 'love', 'him', 'for', 'who', 'he', 'is', ',', 'but', 'dont', 'let', 'him', 'tell', 'you', 'what', 'you', 'have', 'to', 'like']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'honest', 'get', 'divorc', 'count', 'straight', 'instead', 'lesb', 'doesnt', 'mean', 'chang', 'sex', 'ory', 'okay', 'stil', 'lov', 'dont', 'let', 'tel', 'lik'], ['would', 'honestly', 'get', 'divorce', 'count', 'straight', 'instead', 'lesbian', 'doesnt', 'mean', 'change', 'sexual', 'orientation', 'okay', 'still', 'love', 'dont', 'let', 'tell', 'like'])\n",
      "original document: \n",
      "['Danke', 'fuer', 'den', 'Lesestoff!', 'Brauche', 'gerade', 'was', 'zum', 'bei', 'der', 'monotonen', 'Arbeit', 'nebenher', 'machen.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dank', 'fuer', 'den', 'lesestoff', 'brauch', 'gerad', 'zum', 'bei', 'der', 'monoton', 'arbeit', 'nebenh', 'mach'], ['danke', 'fuer', 'den', 'lesestoff', 'brauche', 'gerade', 'zum', 'bei', 'der', 'monotonen', 'arbeit', 'nebenher', 'machen'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Why', 'not?', '', 'What', \"can't\", 'Ross', 'do', 'as', 'a', 'wr?', '', \"I'm\", 'not', 'saying', \"he's\", 'the', 'next', 'Antonio', 'Brown,', 'you', \"can't\", 'say', 'anyone', 'will', 'end', 'up', 'being', 'a', 'top', '3', 'wr,', '', 'but', 'he', 'can', 'be', 'the', 'next', 'Emmanuel', 'Sanders,', 'or', 'the', 'next', 'Greg', 'Jennings.', '', 'What', 'are', 'all', 'these', 'flaws', 'John', 'Ross', 'has', 'at', 'wr', 'that', 'makes', 'him', 'incapable', 'of', 'developing', 'into', 'a', 'good', 'wr?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'ross', 'wr', 'im', 'say', 'hes', 'next', 'antonio', 'brown', 'cant', 'say', 'anyon', 'end', 'top', 'three', 'wr', 'next', 'emmanuel', 'sand', 'next', 'greg', 'jen', 'flaw', 'john', 'ross', 'wr', 'mak', 'incap', 'develop', 'good', 'wr'], ['cant', 'ross', 'wr', 'im', 'say', 'hes', 'next', 'antonio', 'brown', 'cant', 'say', 'anyone', 'end', 'top', 'three', 'wr', 'next', 'emmanuel', 'sanders', 'next', 'greg', 'jennings', 'flaw', 'john', 'ross', 'wr', 'make', 'incapable', 'develop', 'good', 'wr'])\n",
      "original document: \n",
      "['Let', 'me', 'elaborate,', 'this', 'unofficial', 'app,', 'released', '4', 'weeks', 'after', 'the', 'launch', 'of', 'Destiny', '2,', 'hit', 'the', 'mark', 'on', 'how', 'the', 'vault', 'should', 'be', 'presented.', 'It', 'went', 'FURTHER', 'by', 'adding', 'sub', 'categories,', 'which', 'just', 'makes', 'it', 'look', 'even', 'cleaner.', '\\n\\nIf', 'I', 'wanted', 'to', 'find', 'a', 'specific', 'helmet', 'I', 'have', 'in', 'my', 'vault,', 'and', 'it’s', 'located', 'right', 'in', 'the', 'middle,', 'I’d', 'have', 'to', 'skim', 'over', 'weapons', 'first,', 'then', 'narrow', 'my', 'search', 'through', 'all', 'of', 'my', 'armour', 'to', 'find', 'one', 'piece.', 'If', 'they', 'had', 'tabs', 'for', 'armour', 'and', 'weapons,', 'I’d', 'go', 'to', 'the', 'armour', 'tab', 'to', 'look.', 'If', 'they', 'went', 'further', 'and', 'added', 'subcategories,', 'I’d', 'just', 'have', 'to', 'look', 'through', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'elab', 'unoff', 'ap', 'releas', 'four', 'week', 'launch', 'destiny', 'two', 'hit', 'mark', 'vault', 'pres', 'went', 'ad', 'sub', 'categ', 'mak', 'look', 'ev', 'cle', '\\n\\nif', 'want', 'find', 'spec', 'helmet', 'vault', 'loc', 'right', 'middl', 'id', 'skim', 'weapon', 'first', 'narrow', 'search', 'armo', 'find', 'on', 'piec', 'tab', 'armo', 'weapon', 'id', 'go', 'armo', 'tab', 'look', 'went', 'ad', 'subcateg', 'id', 'look'], ['let', 'elaborate', 'unofficial', 'app', 'release', 'four', 'weeks', 'launch', 'destiny', 'two', 'hit', 'mark', 'vault', 'present', 'go', 'add', 'sub', 'categories', 'make', 'look', 'even', 'cleaner', '\\n\\nif', 'want', 'find', 'specific', 'helmet', 'vault', 'locate', 'right', 'middle', 'id', 'skim', 'weapons', 'first', 'narrow', 'search', 'armour', 'find', 'one', 'piece', 'tabs', 'armour', 'weapons', 'id', 'go', 'armour', 'tab', 'look', 'go', 'add', 'subcategories', 'id', 'look'])\n",
      "original document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', 'said!', ':D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'said'], ['well', 'say'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Can', 'you', 'imagine', 'how', 'the', 'left', 'would', 'respond', 'if', 'this', 'man', 'had', 'been', 'a', 'Christian?\\n\\nAlso:\\n\\n&gt;', 'Businessman', 'Soruth', 'Ali,', '42,', 'had', 'been', 'previously', 'convicted', 'of', 'raping', 'a', 'girl', 'in', 'her', 'school', 'uniform.\\n\\nWhy', 'was', 'he', 'free', 'to', 'commit', 'this', 'brutal', 'assault,', 'then?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['imagin', 'left', 'would', 'respond', 'man', 'christian\\n\\nalso\\n\\ngt', 'businessm', 'soru', 'al', 'forty-two', 'prevy', 'convict', 'rap', 'girl', 'school', 'uniform\\n\\nwhy', 'fre', 'commit', 'brut', 'assault'], ['imagine', 'leave', 'would', 'respond', 'man', 'christian\\n\\nalso\\n\\ngt', 'businessman', 'soruth', 'ali', 'forty-two', 'previously', 'convict', 'rap', 'girl', 'school', 'uniform\\n\\nwhy', 'free', 'commit', 'brutal', 'assault'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['^(Hi,', \"I'm\", 'a', 'bot', 'for', 'linking', 'direct', 'images', 'of', 'albums', 'with', 'only', '1', 'image)\\n\\n**https://i.imgur.com/px1M2R7.gifv**\\n\\n^^[Source](https://github.com/AUTplayed/imguralbumbot)', '^^|', '^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md)', '^^|', '^^[Creator](https://np.reddit.com/user/AUTplayed/)', '^^|', '^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme)', '^^|', '^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dnqikk7)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'im', 'bot', 'link', 'direct', 'im', 'album', 'on', 'image\\n\\nhttpsiimgurcompx1m2r7gifv\\n\\nsourcehttpsgithubcomautplayedimguralbumbot', 'whyhttpsgithubcomautplayedimguralbumbotblobmasterreadmemd', 'creatorhttpsnpredditcomuserautplay', 'ignoremehttpsnpredditcommessagecomposetoimguralbumbotampsubjectignoremeampmessageignorem', 'deletthishttpsnpredditcommessagecomposetoimguralbumbotampsubjectdelet20thisampmessagedelet20this20dnqikk7'], ['hi', 'im', 'bot', 'link', 'direct', 'image', 'albums', 'one', 'image\\n\\nhttpsiimgurcompx1m2r7gifv\\n\\nsourcehttpsgithubcomautplayedimguralbumbot', 'whyhttpsgithubcomautplayedimguralbumbotblobmasterreadmemd', 'creatorhttpsnpredditcomuserautplayed', 'ignoremehttpsnpredditcommessagecomposetoimguralbumbotampsubjectignoremeampmessageignoreme', 'deletthishttpsnpredditcommessagecomposetoimguralbumbotampsubjectdelet20thisampmessagedelet20this20dnqikk7'])\n",
      "original document: \n",
      "['One', 'random', 'please', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['I', 'still', 'replay', 'Crysis', '1', 'and', 'Warhead', 'to', 'this', 'day...', 'But', '2-3', 'are', 'hardly', 'worth', 'another', 'visit.', 'Actually,', '3', 'was', 'a', 'pretty', 'good', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'replay', 'crys', 'on', 'warhead', 'day', 'twenty-three', 'hard', 'wor', 'anoth', 'visit', 'act', 'three', 'pretty', 'good', 'tim'], ['still', 'replay', 'crysis', 'one', 'warhead', 'day', 'twenty-three', 'hardly', 'worth', 'another', 'visit', 'actually', 'three', 'pretty', 'good', 'time'])\n",
      "original document: \n",
      "['I', 'was', 'running', 'into', 'issues', 'with', 'bad', 'muscle', 'memory', 'on', 'Listen', 'to', 'my', 'Heart', '(along', 'with', 'the', 'others)', 'causing', 'me', 'to', 'miss', 'just', 'a', 'note', 'or', 'two.', 'My', 'FC', 'was', 'a', 'fluke;', 'I', 'got', 'it', 'right', 'after', 'waking', 'up', 'and', 'rolling', 'over', 'to', 'put', 'my', 'phone', 'on', 'my', 'pillow.', 'Go', 'figure!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['run', 'issu', 'bad', 'musc', 'mem', 'list', 'heart', 'along', 'oth', 'caus', 'miss', 'not', 'two', 'fc', 'fluk', 'got', 'right', 'wak', 'rol', 'put', 'phon', 'pillow', 'go', 'fig'], ['run', 'issue', 'bad', 'muscle', 'memory', 'listen', 'heart', 'along', 'others', 'cause', 'miss', 'note', 'two', 'fc', 'fluke', 'get', 'right', 'wake', 'roll', 'put', 'phone', 'pillow', 'go', 'figure'])\n",
      "original document: \n",
      "['/u/WaterGuy12\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uwaterguy12\\n'], ['uwaterguy12\\n'])\n",
      "original document: \n",
      "['&gt;', 'This', 'was', 'President', \"Obama's\", 'response', 'to', 'the', 'disaster', 'in', 'Haiti:\\n\\nSee', 'above,', 'he', 'literally', 'lead', 'with', 'a', 'comparison', 'of', 'President', \"Obama's\", 'relief', 'efforts', 'to', 'President', \"Trump's.\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'presid', 'obama', 'respons', 'disast', 'haiti\\n\\nsee', 'lit', 'lead', 'comparison', 'presid', 'obama', 'reliev', 'effort', 'presid', 'trump'], ['gt', 'president', 'obamas', 'response', 'disaster', 'haiti\\n\\nsee', 'literally', 'lead', 'comparison', 'president', 'obamas', 'relief', 'efforts', 'president', 'trump'])\n",
      "original document: \n",
      "['/r/hmmm']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rhmmm'], ['rhmmm'])\n",
      "original document: \n",
      "[\"Aren't\", 'right', 'turns', 'on', 'red', 'permitted', 'in', 'the', 'US', 'except', 'for', 'New', 'York', 'or', 'whenever', 'signs', 'prohibit', 'it?', 'They', 'might', 'not', 'be', 'in', 'the', 'right', 'lane', 'for', 'it,', 'but', 'it', \"doesn't\", 'look', 'like', 'they', 'inconvenienced', 'anyone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ar', 'right', 'turn', 'red', 'permit', 'us', 'exceiv', 'new', 'york', 'whenev', 'sign', 'prohibit', 'might', 'right', 'lan', 'doesnt', 'look', 'lik', 'inconveny', 'anyon'], ['arent', 'right', 'turn', 'red', 'permit', 'us', 'except', 'new', 'york', 'whenever', 'sign', 'prohibit', 'might', 'right', 'lane', 'doesnt', 'look', 'like', 'inconvenience', 'anyone'])\n",
      "original document: \n",
      "['If', 'it', 'was', 'Heath', 'Bell', 'would', 'he', 'sprint', 'back', 'in?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hea', 'bel', 'would', 'sprint', 'back'], ['heath', 'bell', 'would', 'sprint', 'back'])\n",
      "original document: \n",
      "['Stop', 'touching', 'yourself.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'touch'], ['stop', 'touch'])\n",
      "original document: \n",
      "['Does', 'it', 'matter?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mat'], ['matter'])\n",
      "original document: \n",
      "['How', 'the', 'fuck']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck'], ['fuck'])\n",
      "original document: \n",
      "['So', 'the', 'bar', 'is', 'zero', 'dollars?', 'Answer', 'the', 'question.', 'How', 'much', 'are', 'the', 'Clintons', 'donating?', \"They're\", 'worth', 'hundreds', 'of', 'millions.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bar', 'zero', 'doll', 'answ', 'quest', 'much', 'clinton', 'don', 'theyr', 'wor', 'hundr', 'mil'], ['bar', 'zero', 'dollars', 'answer', 'question', 'much', 'clintons', 'donate', 'theyre', 'worth', 'hundreds', 'millions'])\n",
      "original document: \n",
      "['Rafis', 'I', 'hope', 'you', 'get', 'well', 'soon', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['raf', 'hop', 'get', 'wel', 'soon', 'lt3'], ['rafis', 'hope', 'get', 'well', 'soon', 'lt3'])\n",
      "original document: \n",
      "['Is', 'this', 'like', 'a', 'novelty', 'account', 'where', 'you', 'pretend', 'to', 'live', '100', 'years', 'ago', 'and', 'spout', 'racist', 'epithets', 'like', \"they're\", 'candy?', 'Cool.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'novel', 'account', 'pretend', 'liv', 'one hundred', 'year', 'ago', 'spout', 'rac', 'epithet', 'lik', 'theyr', 'candy', 'cool'], ['like', 'novelty', 'account', 'pretend', 'live', 'one hundred', 'years', 'ago', 'spout', 'racist', 'epithets', 'like', 'theyre', 'candy', 'cool'])\n",
      "original document: \n",
      "['Where’s', 'MySpace', 'mountain?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wher', 'myspac', 'mountain'], ['wheres', 'myspace', 'mountain'])\n",
      "original document: \n",
      "['Does', 'that', 'friskiness', 'include', 'a', 'picture', 'from', 'behind?', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['frisky', 'includ', 'pict', 'behind'], ['friskiness', 'include', 'picture', 'behind'])\n",
      "original document: \n",
      "['#DEAR', 'MOTHER', 'OF', 'KEK', 'DO', 'I', 'LOVE', 'THIS', 'PLACE!!!!!!!!!!!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dear', 'moth', 'kek', 'lov', 'plac'], ['dear', 'mother', 'kek', 'love', 'place'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Well', 'I', 'can', 'see', 'that', 'now', 'you', 'say', 'it!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'see', 'say'], ['well', 'see', 'say'])\n",
      "original document: \n",
      "['Two', 'randoms', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'pleas'], ['two', 'randoms', 'please'])\n",
      "original document: \n",
      "['&gt;I', 'mean,', 'would', 'you', 'pay', 'attention', 'to', 'a', 'pundit', 'who', 'is', 'a', 'young', 'earth', 'creationist?\\n\\nIf', 'their', 'area', 'of', 'expertise', 'were', 'sufficiently', 'removed', 'from', 'the', 'domain', 'of', 'creationism,', 'I', 'might.', 'There', 'have', 'been', 'many', 'brilliant', 'people', \"who've\", 'believed', 'in', 'crazy', 'and', 'irrational', 'things.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gti', 'mean', 'would', 'pay', 'at', 'pundit', 'young', 'ear', 'creationist\\n\\nif', 'are', 'expert', 'sufficy', 'remov', 'domain', 'cre', 'might', 'many', 'bril', 'peopl', 'whov', 'believ', 'crazy', 'ir', 'thing'], ['gti', 'mean', 'would', 'pay', 'attention', 'pundit', 'young', 'earth', 'creationist\\n\\nif', 'area', 'expertise', 'sufficiently', 'remove', 'domain', 'creationism', 'might', 'many', 'brilliant', 'people', 'whove', 'believe', 'crazy', 'irrational', 'things'])\n",
      "original document: \n",
      "['Not', 'really', 'anything', 'you', 'can', 'do', 'until', 'the', 'car', 'is', 'above', 'water', 'again.', '', 'Selling', 'it', 'or', 'refinancing', 'will', 'both', 'require', 'money', 'from', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'anyth', 'car', 'wat', 'sel', 'refin', 'requir', 'money'], ['really', 'anything', 'car', 'water', 'sell', 'refinance', 'require', 'money'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'sure', 'anymore.', 'I', 'only', 'get', 'on', 'long', 'enough', 'to', 'farm', 'up', 'a', 'forma', 'BP', 'and', 'start', 'building', 'it', 'anymore.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'anym', 'get', 'long', 'enough', 'farm', 'form', 'bp', 'start', 'build', 'anym'], ['im', 'sure', 'anymore', 'get', 'long', 'enough', 'farm', 'forma', 'bp', 'start', 'build', 'anymore'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Yeah,', 'I', 'think', \"I'll\", 'be.', 'If', 'not,', 'then', 'around', '7pm.', '\\n\\nA', 'quick', 'question,', 'do', 'you', 'have', 'summer,', 'fall,', 'and', 'winter', 'deerlings', 'with', 'HA?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'think', 'il', 'around', '7pm', '\\n\\na', 'quick', 'quest', 'sum', 'fal', 'wint', 'deerl', 'ha'], ['yeah', 'think', 'ill', 'around', '7pm', '\\n\\na', 'quick', 'question', 'summer', 'fall', 'winter', 'deerlings', 'ha'])\n",
      "original document: \n",
      "['Thanks', 'for', 'the', 'chance!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'chant'], ['thank', 'chance'])\n",
      "original document: \n",
      "['*the', 'door', 'opens,', 'and', 'a', 'radar', 'tech', 'walks', 'onto', 'the', 'command', 'deck*\\n\\n\"Captain.', '', \"We've\", 'found', 'something', 'on', 'radar.', '', 'It', 'appears', 'to', 'be', 'a', 'small,', 'unmanned', 'aerial', 'vehicle\"\\n\\n\"Tag', 'it\"\\n\\n*a', 'crew', 'fires', 'a', 'small', 'adhesive', 'RFID', 'tag', 'from', 'a', 'pneumatic', 'launcher', 'on', 'a', 'drone', 'of', 'their', 'own.', '', 'It', 'sticks', 'to', 'the', 'other', 'drone,', 'and', 'transmits', 'its', 'location', 'to', 'the', 'Lemminkainen*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['door', 'op', 'rad', 'tech', 'walk', 'onto', 'command', 'deck\\n\\ncaptain', 'wev', 'found', 'someth', 'rad', 'appear', 'smal', 'unman', 'aer', 'vehicle\\n\\ntag', 'it\\n\\na', 'crew', 'fir', 'smal', 'adher', 'rfid', 'tag', 'pneum', 'launch', 'dron', 'stick', 'dron', 'transmit', 'loc', 'lemminkain'], ['door', 'open', 'radar', 'tech', 'walk', 'onto', 'command', 'deck\\n\\ncaptain', 'weve', 'find', 'something', 'radar', 'appear', 'small', 'unman', 'aerial', 'vehicle\\n\\ntag', 'it\\n\\na', 'crew', 'fire', 'small', 'adhesive', 'rfid', 'tag', 'pneumatic', 'launcher', 'drone', 'stick', 'drone', 'transmit', 'location', 'lemminkainen'])\n",
      "original document: \n",
      "['[dude', 'that´s', 'great', 'man](https://i.ytimg.com/vi/SolmwnWnWW4/hqdefault.jpg)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dud', 'that s', 'gre', 'manhttpsiytimgcomvisolmwnwnww4hqdefaultjpg'], ['dude', 'that s', 'great', 'manhttpsiytimgcomvisolmwnwnww4hqdefaultjpg'])\n",
      "original document: \n",
      "['Arden', 'key', 'got', 'fat.', \"Don't\", 'hold', 'your', 'breath', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ard', 'key', 'got', 'fat', 'dont', 'hold', 'brea'], ['arden', 'key', 'get', 'fat', 'dont', 'hold', 'breath'])\n",
      "original document: \n",
      "['Look', 'at', 'you!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look'], ['look'])\n",
      "original document: \n",
      "['One', 'spot', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'spot'], ['one', 'spot'])\n",
      "original document: \n",
      "['Sprinting', 'definitely', 'feels', 'faster', 'with', 'higher', 'mobility.', 'I', 'notice', 'it', 'all', 'the', 'time', 'when', 'going', 'from', 'my', 'Warlock', 'with', '2', 'mobility', 'to', 'my', 'Hunter', 'with', '10', 'mobility.', 'Sprinting', 'feels', 'fast', 'as', 'hell.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sprinting', 'definit', 'feel', 'fast', 'high', 'mobl', 'not', 'tim', 'going', 'warlock', 'two', 'mobl', 'hunt', 'ten', 'mobl', 'sprinting', 'feel', 'fast', 'hel'], ['sprint', 'definitely', 'feel', 'faster', 'higher', 'mobility', 'notice', 'time', 'go', 'warlock', 'two', 'mobility', 'hunter', 'ten', 'mobility', 'sprint', 'feel', 'fast', 'hell'])\n",
      "original document: \n",
      "['spread', 'was', 'Georgia', '-10', 'but', 'thanks', 'I', 'guess']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spread', 'georg', 'ten', 'thank', 'guess'], ['spread', 'georgia', 'ten', 'thank', 'guess'])\n",
      "original document: \n",
      "['Normally', 'play', 'thatcher', 'as', 'I', 'only', 'solo', 'que', 'and', 'people', 'generally', \"don't\", 'pick', 'him', 'that', 'often', 'and', 'on', 'defence', 'majority', 'of', 'the', 'time', 'I', 'play', 'smoke', 'just', 'because', 'he', 'is', 'the', 'most', 'versatile', 'operator', 'in', 'my', 'opinion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['norm', 'play', 'thatch', 'solo', 'que', 'peopl', 'gen', 'dont', 'pick', 'oft', 'def', 'maj', 'tim', 'play', 'smok', 'versatil', 'op', 'opin'], ['normally', 'play', 'thatcher', 'solo', 'que', 'people', 'generally', 'dont', 'pick', 'often', 'defence', 'majority', 'time', 'play', 'smoke', 'versatile', 'operator', 'opinion'])\n",
      "original document: \n",
      "['LIS', 'had', 'me', 'enjoying', 'Chloe', 'as', 'a', 'character', 'but', 'irritated', 'in', 'some', 'respects.', 'BTS', 'has', 'made', 'her', 'my', 'favourite', 'character', 'and', 'justified', 'plenty', 'for', 'why', 'she', 'is', 'the', 'way', 'she', 'is', 'in', 'LIS.', \"\\n\\n\\nI'm\", 'looking', 'forward', 'and', 'not', 'looking', 'forward', 'to', 'finding', 'out', 'how', 'Rachel', 'ends', 'up', 'with', 'Frank.', 'Chloe', 'considers', 'her', 'a', 'close', 'connection', 'and', 'inevitably', 'at', 'some', 'point,', 'a', 'lover,', 'otherwise', 'she', \"wouldn't\", 'have', 'flipped', 'out', 'when', 'you', 'find', 'out', 'the', 'relationship', 'of', 'Rachel', 'and', 'Frank.', \"\\n\\n\\nRachel's\", 'someone', 'I', 'think', 'who', 'enjoys', 'playing', 'with', 'people', 'to', 'a', 'degree', 'and', 'is', 'almost', 'certainly', 'a', 'liar,', 'yet', 'is', 'the', 'thing', 'with', 'Frank', 'because', 'of', 'something', 'that', 'happens,', 'or', 'are', 'the', 'feelings', 'genuine,', 'and', 'what', 'possesses', 'her', 'to', 'betray', 'Chloe?', 'Or', 'was', 'it', 'all', 'misinterpreted', 'by', 'Frank', 'or', 'mis-sold', 'as', 'something', 'more', 'serious', 'than', 'it', 'really', 'was.', 'Hard', 'to', 'say.', '\\nSo', 'many', 'questions.', 'Team', 'Chloe', 'all', 'the', 'way', 'though.', '\\n\\n\\nHella', 'looking', 'forward', 'to', 'the', 'next', 'episode.', '\\n\\n\\n#hashtagNotmyDavid']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lis', 'enjoy', 'chloe', 'charact', 'irrit', 'respect', 'bts', 'mad', 'favourit', 'charact', 'just', 'plenty', 'way', 'lis', '\\n\\n\\nim', 'look', 'forward', 'look', 'forward', 'find', 'rachel', 'end', 'frank', 'chloe', 'consid', 'clos', 'connect', 'inevit', 'point', 'lov', 'otherw', 'wouldnt', 'flip', 'find', 'rel', 'rachel', 'frank', '\\n\\n\\nrachels', 'someon', 'think', 'enjoy', 'play', 'peopl', 'degr', 'almost', 'certain', 'liar', 'yet', 'thing', 'frank', 'someth', 'hap', 'feel', 'genuin', 'possess', 'betray', 'chloe', 'misinterpret', 'frank', 'missold', 'someth', 'sery', 'real', 'hard', 'say', '\\nso', 'many', 'quest', 'team', 'chloe', 'way', 'though', '\\n\\n\\nhella', 'look', 'forward', 'next', 'episod', '\\n\\n\\nhashtagnotmydavid'], ['lis', 'enjoy', 'chloe', 'character', 'irritate', 'respect', 'bts', 'make', 'favourite', 'character', 'justify', 'plenty', 'way', 'lis', '\\n\\n\\nim', 'look', 'forward', 'look', 'forward', 'find', 'rachel', 'end', 'frank', 'chloe', 'consider', 'close', 'connection', 'inevitably', 'point', 'lover', 'otherwise', 'wouldnt', 'flip', 'find', 'relationship', 'rachel', 'frank', '\\n\\n\\nrachels', 'someone', 'think', 'enjoy', 'play', 'people', 'degree', 'almost', 'certainly', 'liar', 'yet', 'thing', 'frank', 'something', 'happen', 'feel', 'genuine', 'possess', 'betray', 'chloe', 'misinterpret', 'frank', 'missold', 'something', 'serious', 'really', 'hard', 'say', '\\nso', 'many', 'question', 'team', 'chloe', 'way', 'though', '\\n\\n\\nhella', 'look', 'forward', 'next', 'episode', '\\n\\n\\nhashtagnotmydavid'])\n",
      "original document: \n",
      "['[removed]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Maybe', 'that', 'will', 'help', 'wit', 'me', 'drinking', 'problem.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'help', 'wit', 'drink', 'problem'], ['maybe', 'help', 'wit', 'drink', 'problem'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'this', 'is', 'Prince_Kropotkin', 'bait.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'prince_kropotkin', 'bait'], ['feel', 'like', 'prince_kropotkin', 'bait'])\n",
      "original document: \n",
      "['This', 'is', 'awesome,', 'which', 'goodwill?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['awesom', 'goodwil'], ['awesome', 'goodwill'])\n",
      "original document: \n",
      "['Remember', 'when', 'Obama', 'started', 'that', 'fraudulent', 'university', 'named', 'after', 'himself,', 'and', 'have', 'to', 'give', 'all', 'that', 'tuition', 'money', 'back', 'when', 'it', 'was', 'proven', 'to', 'be', 'a', 'fraud?\\n\\nRemember', 'when', 'Obama', 'sold', 'steaks', 'through', 'a', 'Sharper', 'Image,', 'also', 'using', 'his', 'game', 'to', 'brand', 'these', 'steaks?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'obam', 'start', 'fraud', 'univers', 'nam', 'giv', 'tuit', 'money', 'back', 'prov', 'fraud\\n\\nremember', 'obam', 'sold', 'steak', 'sharp', 'im', 'also', 'us', 'gam', 'brand', 'steak'], ['remember', 'obama', 'start', 'fraudulent', 'university', 'name', 'give', 'tuition', 'money', 'back', 'prove', 'fraud\\n\\nremember', 'obama', 'sell', 'steaks', 'sharper', 'image', 'also', 'use', 'game', 'brand', 'steaks'])\n",
      "original document: \n",
      "['', 'Straight', 'up', 'or', 'on', 'the', 'line?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['straight', 'lin'], ['straight', 'line'])\n",
      "original document: \n",
      "['Elton', 'John,', 'Rolling', 'Stones,', 'Otis', 'Redding,', 'Neil', 'Diamond,', 'Neil', 'Young,', 'Al', 'Green,', 'Nina', 'Simone,', 'Dire', 'Straits,', 'Donny', 'Hathaway,', 'The', 'Impressions,', 'Iggy', 'Pop,', 'Rod', 'Stewart,', 'Earth,', 'Wind', '&amp;', 'Fire,', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['elton', 'john', 'rol', 'ston', 'ot', 'red', 'neil', 'diamond', 'neil', 'young', 'al', 'green', 'nin', 'simon', 'dir', 'straits', 'donny', 'hathaway', 'impress', 'iggy', 'pop', 'rod', 'stewart', 'ear', 'wind', 'amp', 'fir'], ['elton', 'john', 'roll', 'stone', 'otis', 'redding', 'neil', 'diamond', 'neil', 'young', 'al', 'green', 'nina', 'simone', 'dire', 'straits', 'donny', 'hathaway', 'impressions', 'iggy', 'pop', 'rod', 'stewart', 'earth', 'wind', 'amp', 'fire'])\n",
      "original document: \n",
      "['I', \"hadn't\", 'spotted', 'her', 'new', 'one,', \"didn't\", 'she', 'have', '/u/GoddessJ', 'and', '/u/TheRealGJ', 'too?', '', \"They're\", 'both', 'gone', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hadnt', 'spot', 'new', 'on', 'didnt', 'ugoddesss', 'utherealgs', 'theyr', 'gon', 'wel'], ['hadnt', 'spot', 'new', 'one', 'didnt', 'ugoddessj', 'utherealgj', 'theyre', 'go', 'well'])\n",
      "original document: \n",
      "['did', 'those', 'today', 'for', 'the', 'first', 'time,', 'it', 'was', 'pretty', 'cool']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['today', 'first', 'tim', 'pretty', 'cool'], ['today', 'first', 'time', 'pretty', 'cool'])\n",
      "original document: \n",
      "['Those', 'jeans', 'look', 'like', 'shit.', 'Gal', 'however', 'never', 'ceases', 'to', 'amaze.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jean', 'look', 'lik', 'shit', 'gal', 'howev', 'nev', 'ceas', 'amaz'], ['jeans', 'look', 'like', 'shit', 'gal', 'however', 'never', 'cease', 'amaze'])\n",
      "original document: \n",
      "['This', \"isn't\", 'a', 'new', 'occurrence.', 'Maybe', 'you', \"shouldn't\", 'complain', 'about', 'something', 'changing', 'if', 'you', \"didn't\", 'watch', 'it', 'enough', 'to', 'know', 'if', 'it', 'changes', 'or', 'not', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isnt', 'new', 'occur', 'mayb', 'shouldnt', 'complain', 'someth', 'chang', 'didnt', 'watch', 'enough', 'know', 'chang'], ['isnt', 'new', 'occurrence', 'maybe', 'shouldnt', 'complain', 'something', 'change', 'didnt', 'watch', 'enough', 'know', 'change'])\n",
      "original document: \n",
      "['That', 'Republika', 'Srpska', 'will', 'give', 'up', 'on', 'Brcko,', 'meaning', 'that', 'it', \"won't\", 'even', 'be', 'contiguous?', 'That', 'if', 'remainder', 'of', 'BiH', '(Federation', 'of', 'BiH', '+', 'Brcko)', 'decides', 'to', 'close', 'the', 'border', 'with', 'RS,', 'then', 'the', 'western', 'half', 'of', 'RS', 'will', 'live', 'at', 'mercy', 'of', 'Croatia?\\n\\nNow', \"that'd\", 'be', 'interesting', 'to', 'watch.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['republik', 'srpska', 'giv', 'brcko', 'mean', 'wont', 'ev', 'contigu', 'remaind', 'bih', 'fed', 'bih', 'brcko', 'decid', 'clos', 'bord', 'rs', 'western', 'half', 'rs', 'liv', 'mercy', 'croatia\\n\\nnow', 'thatd', 'interest', 'watch'], ['republika', 'srpska', 'give', 'brcko', 'mean', 'wont', 'even', 'contiguous', 'remainder', 'bih', 'federation', 'bih', 'brcko', 'decide', 'close', 'border', 'rs', 'western', 'half', 'rs', 'live', 'mercy', 'croatia\\n\\nnow', 'thatd', 'interest', 'watch'])\n",
      "original document: \n",
      "['Got.', 'DAAAAYUMM!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'daaaayum'], ['get', 'daaaayumm'])\n",
      "original document: \n",
      "['Since', 'rape', 'is', 'a', 'legal', 'crime,', 'I', 'think', 'the', 'law', 'is', 'pertinent', 'here.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sint', 'rap', 'leg', 'crim', 'think', 'law', 'pertin'], ['since', 'rape', 'legal', 'crime', 'think', 'law', 'pertinent'])\n",
      "original document: \n",
      "['Gracias', 'por', 'la', 'aclaración,', 'no', 'podría', 'haberse', 'dicho', 'mejor.', '\\nAdmito', 'mi', 'total', 'ignorancia', 'y', 'prejuicio', 'en', 'el', 'tema,', 'salte', 'porque', 'me', 'pareció', 'choto', 'leer', 'varios', 'comentarios', 'de', 'ese', 'calibre.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gracia', 'por', 'la', 'aclarac', 'podr', 'habers', 'dicho', 'mej', '\\nadmito', 'mi', 'tot', 'ignoranc', 'prejuicio', 'en', 'el', 'tem', 'salt', 'porqu', 'parecio', 'choto', 'leer', 'vario', 'comentario', 'de', 'es', 'calibr'], ['gracias', 'por', 'la', 'aclaracion', 'podria', 'haberse', 'dicho', 'mejor', '\\nadmito', 'mi', 'total', 'ignorancia', 'prejuicio', 'en', 'el', 'tema', 'salte', 'porque', 'parecio', 'choto', 'leer', 'varios', 'comentarios', 'de', 'ese', 'calibre'])\n",
      "original document: \n",
      "['独自の文化が発達していておもしろいよね']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Eh,', 'you', 'guys', 'did', 'the', 'right', 'thing.', 'Maybe', 'just', 'post', 'more', 'guys', 'at', 'the', 'door?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eh', 'guy', 'right', 'thing', 'mayb', 'post', 'guy', 'door'], ['eh', 'guy', 'right', 'thing', 'maybe', 'post', 'guy', 'door'])\n",
      "original document: \n",
      "['Same', 'with', 'Four', 'Horsemen', 'and', 'Mechanix']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['four', 'horsem', 'mechanix'], ['four', 'horsemen', 'mechanix'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'everything', 'but', 'the', 'guns', 'separates', 'it', 'from', 'cod']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'everyth', 'gun', 'sep', 'cod'], ['feel', 'like', 'everything', 'gun', 'separate', 'cod'])\n",
      "original document: \n",
      "['I', 'drugged', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['drug'], ['drug'])\n",
      "original document: \n",
      "['see', 'ya', 'in', 'December', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'ya', 'decemb'], ['see', 'ya', 'december'])\n",
      "original document: \n",
      "['Moisés', 'Arias']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mois', 'aria'], ['moises', 'arias'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['who?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Sounds', 'like', 'you', 'gave', 'this', 'a', 'lot', 'of', 'thought.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'lik', 'gav', 'lot', 'thought'], ['sound', 'like', 'give', 'lot', 'think'])\n",
      "original document: \n",
      "['According', 'to', 'some', 'news', 'articles,', 'they', 'are', 'mentioning', 'pay-by-the-hour', 'prices.', 'I’ll', 'try', 'the', 'beta', 'but', 'if', 'it’s', 'true', 'you', 'have', 'to', 'pay', 'by', 'hour,', 'especially', 'at', 'those', 'prices,', 'I', 'probably', 'won’t', 'continue.', '\\n\\nEdit:', 'Never', 'mind.', 'The', 'official', 'website', 'states', 'SHIELD', 'is', '$7.99/mo.', 'So', 'I’ll', 'go', 'by', 'that.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['accord', 'new', 'artic', 'ment', 'paybytheho', 'pric', 'il', 'try', 'bet', 'tru', 'pay', 'hour', 'espec', 'pric', 'prob', 'wont', 'continu', '\\n\\nedit', 'nev', 'mind', 'off', 'websit', 'stat', 'shield', '799mo', 'il', 'go'], ['accord', 'news', 'article', 'mention', 'paybythehour', 'price', 'ill', 'try', 'beta', 'true', 'pay', 'hour', 'especially', 'price', 'probably', 'wont', 'continue', '\\n\\nedit', 'never', 'mind', 'official', 'website', 'state', 'shield', '799mo', 'ill', 'go'])\n",
      "original document: \n",
      "['Fuck', 'the', 'Steelers!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'steel'], ['fuck', 'steelers'])\n",
      "original document: \n",
      "['143412442|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'UwlHxiGn)\\n\\n&gt;&gt;143412250', '(OP)\\n2012:', 'Paul\\n2016:', 'Trump\\n\\nTurned', '18', 'in', '2011', 'but', 'I', 'prob', \"would've\", 'voted', 'for', 'Obama', 'since', 'McCain', 'was', 'worse.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, four hundred and forty-two', 'gt', 'unit', 'stat', 'anonym', 'id', 'uwlhxign\\n\\ngtgt143412250', 'op\\n2012', 'paul\\n2016', 'trump\\n\\nturned', 'eighteen', 'two thousand and eleven', 'prob', 'wouldv', 'vot', 'obam', 'sint', 'mccain', 'worse\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, four hundred and forty-two', 'gt', 'unite', 'state', 'anonymous', 'id', 'uwlhxign\\n\\ngtgt143412250', 'op\\n2012', 'paul\\n2016', 'trump\\n\\nturned', 'eighteen', 'two thousand and eleven', 'prob', 'wouldve', 'vote', 'obama', 'since', 'mccain', 'worse\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Exactly', 'my', 'thought.', 'I', 'had', 'a', 'professor', 'in', 'college', 'who', 'had', 'four', 'patents', 'and', '3', 'of', 'them', 'never', 'made', 'him', 'a', 'dime.', 'He', 'said', 'one', 'of', 'them', 'made', 'him', '\"several', 'hundred', 'dollars', 'a', 'year\".', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'thought', 'profess', 'colleg', 'four', 'pat', 'three', 'nev', 'mad', 'dim', 'said', 'on', 'mad', 'sev', 'hundr', 'doll', 'year'], ['exactly', 'think', 'professor', 'college', 'four', 'patent', 'three', 'never', 'make', 'dime', 'say', 'one', 'make', 'several', 'hundred', 'dollars', 'year'])\n",
      "original document: \n",
      "['Just', 'be', 'honest', 'with', 'them.', 'Expect', 'their', 'reactions', 'to', 'be', 'negative.', 'You', 'said', 'you', 'met', 'him', 'a', 'year', 'ago', '-', 'any', 'grown', 'man', 'who', 'pursues', 'a', 'minor', 'is', 'going', 'to', 'get', 'a', 'healthy', 'amount', 'of', 'criticism', 'for', 'doing', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'expect', 'react', 'neg', 'said', 'met', 'year', 'ago', 'grown', 'man', 'pursu', 'min', 'going', 'get', 'healthy', 'amount', 'crit'], ['honest', 'expect', 'reactions', 'negative', 'say', 'meet', 'year', 'ago', 'grow', 'man', 'pursue', 'minor', 'go', 'get', 'healthy', 'amount', 'criticism'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['AP', 'and', 'Reuters', 'twitter', 'are', 'both', 'bare', 'bones,', 'but', 'will', 'give', 'accurate,', 'factually', 'correct', 'information.', '\\n\\nThe', 'BBC', 'radio', 'service,', 'NPR,', 'Al', 'Jazeera,', '&amp;', 'PBS', 'Frontline', 'will', 'give', 'more', 'in-depth', 'reporting', 'with', 'context,', 'but', 'be', 'aware', 'that', 'each', 'of', 'them', 'will', 'frame', 'certain', 'stories', 'in', 'ways', 'that', 'correspond', 'with', 'their', 'political', 'supporters', '(I.E.', 'BBC', 'may', 'give', 'biased', 'Brexit', 'coverage,', 'NPR', 'is', 'very', 'pro-establishment', 'for', 'the', 'major', '2', 'parties', 'in', 'the', 'states,', 'Al', 'Jazeera', 'on', 'Israel/Palestine).', 'But', 'generally', 'anything', 'they', 'cover', 'in', 'world', 'news', 'will', 'be', 'pretty', 'in-depth', 'and', 'well', 'covered,', 'especially', 'if', 'you', 'cross', 'reference', 'with', 'other', 'news', 'agencies', 'to', 'get', 'various', 'viewpoints.', '\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ap', 'reut', 'twit', 'bar', 'bon', 'giv', 'acc', 'fact', 'correct', 'inform', '\\n\\nthe', 'bbc', 'radio', 'serv', 'npr', 'al', 'jazeer', 'amp', 'pbs', 'frontlin', 'giv', 'indep', 'report', 'context', 'aw', 'fram', 'certain', 'story', 'way', 'correspond', 'polit', 'support', 'ie', 'bbc', 'may', 'giv', 'bias', 'brexit', 'cov', 'npr', 'proest', 'maj', 'two', 'party', 'stat', 'al', 'jazeer', 'israelpalestin', 'gen', 'anyth', 'cov', 'world', 'new', 'pretty', 'indep', 'wel', 'cov', 'espec', 'cross', 'ref', 'new', 'ag', 'get', 'vary', 'viewpoint', '\\n\\n'], ['ap', 'reuters', 'twitter', 'bare', 'bone', 'give', 'accurate', 'factually', 'correct', 'information', '\\n\\nthe', 'bbc', 'radio', 'service', 'npr', 'al', 'jazeera', 'amp', 'pbs', 'frontline', 'give', 'indepth', 'report', 'context', 'aware', 'frame', 'certain', 'stories', 'ways', 'correspond', 'political', 'supporters', 'ie', 'bbc', 'may', 'give', 'bias', 'brexit', 'coverage', 'npr', 'proestablishment', 'major', 'two', 'party', 'state', 'al', 'jazeera', 'israelpalestine', 'generally', 'anything', 'cover', 'world', 'news', 'pretty', 'indepth', 'well', 'cover', 'especially', 'cross', 'reference', 'news', 'agencies', 'get', 'various', 'viewpoints', '\\n\\n'])\n",
      "original document: \n",
      "[\"It's\", 'cool', 'I', 'was', 'just', 'trolling.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cool', 'trol'], ['cool', 'troll'])\n",
      "original document: \n",
      "['They', 'pop', 'out', 'so', 'easy', 'if', 'you', 'use', 'two', 'sewing', 'needles', 'at', 'each', 'corner', 'of', 'the', 'pan', 'diagonally', '-', 'I', 'did', 'my', 'Naked', '2,', '3,', 'and', 'Ultimate', 'Basics', 'that', 'way.', 'No', 'heat/effort', 'and', 'took', 'like', '5', 'minutes.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pop', 'easy', 'us', 'two', 'sew', 'needl', 'corn', 'pan', 'diagon', 'nak', 'two', 'three', 'ultim', 'bas', 'way', 'heateffort', 'took', 'lik', 'fiv', 'minut'], ['pop', 'easy', 'use', 'two', 'sew', 'needle', 'corner', 'pan', 'diagonally', 'naked', 'two', 'three', 'ultimate', 'basics', 'way', 'heateffort', 'take', 'like', 'five', 'minutes'])\n",
      "original document: \n",
      "['what', 'do', 'you', 'mean?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean'], ['mean'])\n",
      "original document: \n",
      "['Bots', \"don't\", 'count.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bot', 'dont', 'count'], ['bots', 'dont', 'count'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'bills', 'throw', 'away', 'comic', 'aside', 'is', 'meant', 'to', 'be', 'taken', '100%', 'seriously', 'm8']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'bil', 'throw', 'away', 'com', 'asid', 'meant', 'tak', 'one hundred', 'sery', 'm8'], ['dont', 'think', 'bill', 'throw', 'away', 'comic', 'aside', 'mean', 'take', 'one hundred', 'seriously', 'm8'])\n",
      "original document: \n",
      "['#Player', 'ratings', 'in', 'the', 'local', 'press', '', '\\n[Ruhr', 'Nachrichten](http://www.ruhrnachrichten.de/sport/bvb/)', '|', '[Westdeutsche', 'Allgemeine', 'Zeitung](https://www.waz.de/sport/fussball/bvb/bvb-star-aubameyang-verschiesst-elfmeter-klaeglich-note-5-id212097293.html)', '', '', '', '', '', '', '\\n\\n-------------------------------------------------------', '', '\\n\\nPlayer', '|', 'RN', 'Rating', '|', 'WAZ', 'Rating', '|', 'RN', 'Reader', 'vote', '|', 'Avg', '', '\\n---|---|---|---|---', '', '\\nBurki', '|', '2', '|', '2.5', '|', '2.1', '|', '2.20', '', '', '\\nAndrey', '|', '2.5', '|', '2.5', '|', '2.2', '|', '2.40', '', '', '', '', '', '\\nKagawa', '|', '3', '|', '3', '|', '2.2', '|', '2.73', '', '\\nSokratis', '|', '2', '|', '3.5', '|', '2.7', '|', '2.73', '', '', '\\nBartra', '|', '2.5', '|', '3', '|', '3.0', '|', '2.83', '', '', '\\nPiszczek', '|', '3.5', '|', '3', '|', '3.1', '|', '3.20', '', '', '', '', '\\nCastro', '|', '3.5', '|', '3.5', '|', '---', '|', '3.50', '', '', '\\nWeigl', '|', '4', '|', '4', '|', '3.4', '|', '3.80', '', '', '', '\\nPulisic', '|', '4.5', '|', '4', '|', '3.6', '|', '4.03', '', '', '', '\\nDahoud', '|', '4.5', '|', '4.5', '|', '3.8', '|', '4.27', '', '', '\\nToljan', '|', '4.5', '|', '4.5', '|', '3.9', '|', '4.30', '', '', '', '\\nAuba', '|', '5', '|', '4', '|', '5', '|', '4.67', '', '', '', '\\n\\n-------------------------------------------------------', '', '\\n\\nThought', \"I'd\", 'post', 'this,', 'as', 'the', 'post', 'match', 'thread', 'here', 'has', 'been', 'unusually', 'controversial.', 'Accusations', 'of', 'sub', 'biases', 'notwithstanding,', 'Burki,', 'Andrey,', 'Papa,', 'Bartra,', 'Kagawa', 'who', 'were', 'all', 'praised', 'here', 'and', 'in', 'the', 'match', 'thread,', 'get', 'good', 'to', 'passing', 'marks.', 'Weigl,', 'Pulisic,', 'Dahoud,', 'Toljan,', 'Auba', 'who', 'drew', 'criticism', 'here', 'all', 'get', 'sub', 'par', 'marks', 'from', 'the', 'German', 'press', 'as', 'well', 'as', 'German', 'readers', 'at', 'large.', 'Pretty', 'sure', 'it', \"isn't\", '\"American', 'hipsters\"', 'writing', 'for', 'German', 'papers.', 'Just', 'saying.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'rat', 'loc', 'press', '\\nruhr', 'nachrichtenhttpwwwruhrnachrichtendesportbvb', 'westdeutsch', 'allgemein', 'zeitunghttpswwwwazdesportfussballbvbbvbstaraubameyangverschiesstelfmeterklaeglichnote5id212097293html', '\\n\\n', '\\n\\nplayer', 'rn', 'rat', 'waz', 'rat', 'rn', 'read', 'vot', 'avg', '\\n', '\\nburki', 'two', 'twenty-five', 'twenty-one', 'two hundred and twenty', '\\nandrey', 'twenty-five', 'twenty-five', 'twenty-two', 'two hundred and forty', '\\nkagawa', 'three', 'three', 'twenty-two', 'two hundred and seventy-three', '\\nsokratis', 'two', 'thirty-five', 'twenty-seven', 'two hundred and seventy-three', '\\nbartra', 'twenty-five', 'three', 'thirty', 'two hundred and eighty-three', '\\npiszczek', 'thirty-five', 'three', 'thirty-one', 'three hundred and twenty', '\\ncastro', 'thirty-five', 'thirty-five', 'three hundred and fifty', '\\nweigl', 'four', 'four', 'thirty-four', 'three hundred and eighty', '\\npulisic', 'forty-five', 'four', 'thirty-six', 'four hundred and three', '\\ndahoud', 'forty-five', 'forty-five', 'thirty-eight', 'four hundred and twenty-seven', '\\ntoljan', 'forty-five', 'forty-five', 'thirty-nine', 'four hundred and thirty', '\\nauba', 'fiv', 'four', 'fiv', 'four hundred and sixty-seven', '\\n\\n', '\\n\\nthought', 'id', 'post', 'post', 'match', 'thread', 'unus', 'controvers', 'accus', 'sub', 'bias', 'notwithstand', 'burk', 'andrey', 'pap', 'bartr', 'kagaw', 'pra', 'match', 'thread', 'get', 'good', 'pass', 'mark', 'weigl', 'pul', 'dahoud', 'tols', 'aub', 'drew', 'crit', 'get', 'sub', 'par', 'mark', 'germ', 'press', 'wel', 'germ', 'read', 'larg', 'pretty', 'sur', 'isnt', 'am', 'hipst', 'writ', 'germ', 'pap', 'saying\\n'], ['player', 'rat', 'local', 'press', '\\nruhr', 'nachrichtenhttpwwwruhrnachrichtendesportbvb', 'westdeutsche', 'allgemeine', 'zeitunghttpswwwwazdesportfussballbvbbvbstaraubameyangverschiesstelfmeterklaeglichnote5id212097293html', '\\n\\n', '\\n\\nplayer', 'rn', 'rat', 'waz', 'rat', 'rn', 'reader', 'vote', 'avg', '\\n', '\\nburki', 'two', 'twenty-five', 'twenty-one', 'two hundred and twenty', '\\nandrey', 'twenty-five', 'twenty-five', 'twenty-two', 'two hundred and forty', '\\nkagawa', 'three', 'three', 'twenty-two', 'two hundred and seventy-three', '\\nsokratis', 'two', 'thirty-five', 'twenty-seven', 'two hundred and seventy-three', '\\nbartra', 'twenty-five', 'three', 'thirty', 'two hundred and eighty-three', '\\npiszczek', 'thirty-five', 'three', 'thirty-one', 'three hundred and twenty', '\\ncastro', 'thirty-five', 'thirty-five', 'three hundred and fifty', '\\nweigl', 'four', 'four', 'thirty-four', 'three hundred and eighty', '\\npulisic', 'forty-five', 'four', 'thirty-six', 'four hundred and three', '\\ndahoud', 'forty-five', 'forty-five', 'thirty-eight', 'four hundred and twenty-seven', '\\ntoljan', 'forty-five', 'forty-five', 'thirty-nine', 'four hundred and thirty', '\\nauba', 'five', 'four', 'five', 'four hundred and sixty-seven', '\\n\\n', '\\n\\nthought', 'id', 'post', 'post', 'match', 'thread', 'unusually', 'controversial', 'accusations', 'sub', 'bias', 'notwithstanding', 'burki', 'andrey', 'papa', 'bartra', 'kagawa', 'praise', 'match', 'thread', 'get', 'good', 'pass', 'mark', 'weigl', 'pulisic', 'dahoud', 'toljan', 'auba', 'draw', 'criticism', 'get', 'sub', 'par', 'mark', 'german', 'press', 'well', 'german', 'readers', 'large', 'pretty', 'sure', 'isnt', 'american', 'hipsters', 'write', 'german', 'paper', 'saying\\n'])\n",
      "original document: \n",
      "['I', \"didn't\", 'see', 'this', 'sign', '(or', 'the', 'shirt)', 'but', 'someone', 'told', 'me', 'about', 'it', 'from', 'my', 'previous', 'post.', 'Enjoy!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'see', 'sign', 'shirt', 'someon', 'told', 'prevy', 'post', 'enjoy'], ['didnt', 'see', 'sign', 'shirt', 'someone', 'tell', 'previous', 'post', 'enjoy'])\n",
      "original document: \n",
      "['Millwrong.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['millwrong'], ['millwrong'])\n",
      "original document: \n",
      "['I', 'have', 'sometimes', 'been', 'using', 'them', 'on', 'the', 'same', 'team,', 'but', 'not', 'all', 'the', 'time.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sometim', 'us', 'team', 'tim'], ['sometimes', 'use', 'team', 'time'])\n",
      "original document: \n",
      "[\"That's\", 'my', 'fear,', 'except', 'I', 'see', 'Cubs', 'replaced', 'with', 'Nats', 'just', 'as', 'easily.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fear', 'exceiv', 'see', 'cub', 'replac', 'nat', 'easy'], ['thats', 'fear', 'except', 'see', 'cub', 'replace', 'nats', 'easily'])\n",
      "original document: \n",
      "['At', 'least', 'he', 'had', 'the', 'decency', 'to', 'cover', 'his', 'pig', 'butt', 'cheeks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'dec', 'cov', 'pig', 'but', 'cheek'], ['least', 'decency', 'cover', 'pig', 'butt', 'cheek'])\n",
      "original document: \n",
      "['If', 'it', 'says', 'that,', \"it's\", 'outdated.', 'Contact', 'the', 'mods', 'and', 'they', 'can', 'cross', 'it', 'out.', 'Or', 'if', \"it's\", 'not', 'outdated,', 'then', \"they'll\", 'let', 'you', 'know,', 'but', 'as', 'far', 'as', 'I', 'know,', 'Memu', 'is', 'dead', 'and', 'Remix', 'OS', 'still', 'works', '(and', 'can', 'be', 'installed', 'even', 'if', \"it's\", 'not', 'still', 'updated).', 'As', 'for', 'Win', '10,', \"can't\", 'comment', 'on', 'that,', 'though', \"it's\", 'hard', 'to', 'believe', 'it', 'Remix', 'OS', \"doesn't\", 'dual', 'boot', 'on', 'Win', '10.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'outd', 'contact', 'mod', 'cross', 'outd', 'theyl', 'let', 'know', 'far', 'know', 'memu', 'dead', 'remix', 'os', 'stil', 'work', 'instal', 'ev', 'stil', 'upd', 'win', 'ten', 'cant', 'com', 'though', 'hard', 'believ', 'remix', 'os', 'doesnt', 'dual', 'boot', 'win', 'ten'], ['say', 'outdated', 'contact', 'mods', 'cross', 'outdated', 'theyll', 'let', 'know', 'far', 'know', 'memu', 'dead', 'remix', 'os', 'still', 'work', 'instal', 'even', 'still', 'update', 'win', 'ten', 'cant', 'comment', 'though', 'hard', 'believe', 'remix', 'os', 'doesnt', 'dual', 'boot', 'win', 'ten'])\n",
      "original document: \n",
      "['whilst', 'I', 'respect', 'your', 'opinion', 'I', 'must', 'disagree,', 'Ghaul', 'is', 'simply', 'a', 'Cabal', 'raised', 'to', 'fight', 'someone', \"else's\", 'fight', 'then', 'when', 'the', 'opportunity', 'arises', 'where', 'he', 'can', 'finally', 'reclaim', 'his', 'own', 'destiny', '*heh*', 'he', 'kills', 'the', 'man', 'that', 'brought', 'him', 'up', 'and', 'engulfs', 'himself', 'in', 'the', 'power', 'he', \"doesn't\", 'understand', '\\n\\nalso', 'he', 'has', 'a', 'cool', 'cape', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whilst', 'respect', 'opin', 'must', 'disagr', 'gha', 'simply', 'cab', 'rais', 'fight', 'someon', 'els', 'fight', 'opportun', 'ar', 'fin', 'reclaim', 'destiny', 'heh', 'kil', 'man', 'brought', 'engulf', 'pow', 'doesnt', 'understand', '\\n\\nalso', 'cool', 'cap'], ['whilst', 'respect', 'opinion', 'must', 'disagree', 'ghaul', 'simply', 'cabal', 'raise', 'fight', 'someone', 'elses', 'fight', 'opportunity', 'arise', 'finally', 'reclaim', 'destiny', 'heh', 'kill', 'man', 'bring', 'engulf', 'power', 'doesnt', 'understand', '\\n\\nalso', 'cool', 'cape'])\n",
      "original document: \n",
      "['Added', 'a', 'box', 'score', 'and', 'they', 'deleted', 'again.', '', 'Too', 'bad,', 'would', 'have', 'been', 'fun', 'for', 'the', 'sub', 'to', 'spark', 'a', 'meme', 'about', 'a', 'good', 'D3', 'team.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad', 'box', 'scor', 'delet', 'bad', 'would', 'fun', 'sub', 'spark', 'mem', 'good', 'd3', 'team'], ['add', 'box', 'score', 'delete', 'bad', 'would', 'fun', 'sub', 'spark', 'meme', 'good', 'd3', 'team'])\n",
      "original document: \n",
      "['Flair', 'up', 'bro']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flair', 'bro'], ['flair', 'bro'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['If', 'I', 'got', '$100', 'for', 'every', 'time', 'I', 'masturbated', \"I'd\", 'have', '$1000', 'just', 'today.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'one hundred', 'every', 'tim', 'masturb', 'id', 'one thousand', 'today'], ['get', 'one hundred', 'every', 'time', 'masturbate', 'id', 'one thousand', 'today'])\n",
      "original document: \n",
      "['The', 'gameplay', 'is', 'good.', 'Every', 'other', 'part', 'of', 'it', 'is', 'miserable', 'and', 'buggy.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gameplay', 'good', 'every', 'part', 'mis', 'buggy'], ['gameplay', 'good', 'every', 'part', 'miserable', 'buggy'])\n",
      "original document: \n",
      "['Unless', 'you', 'happen', 'to', 'be', 'trained,', 'good', 'at', 'blocking', 'kicks', 'and', 'fighting', 'back...', 'If', 'you', 'can', 'you', 'run']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unless', 'hap', 'train', 'good', 'block', 'kick', 'fight', 'back', 'run'], ['unless', 'happen', 'train', 'good', 'block', 'kick', 'fight', 'back', 'run'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"That's\", 'one', 'reason', 'why', 'I', \"don't\", 'want', 'Bcash', 'or', '2x', 'coin.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'on', 'reason', 'dont', 'want', 'bcash', '2x', 'coin'], ['thats', 'one', 'reason', 'dont', 'want', 'bcash', '2x', 'coin'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['First', 'off', 'wtf', 'i', 'never', 'mix', 'up', 'too', 'and', 'to\\n\\nSecond', 'off', 'yeah', 'it', 'seems', 'logistically', 'difficult', 'to', 'be', 'there,', 'vodafone', \"doesn't\", 'transfer', 'over', 'to', 'andorra', 'so', 'i', \"wouldn't\", 'have', 'data.', 'I', 'really', 'want', 'to', 'bike', 'from', 'andorra', 'la', 'vella', 'to', 'the', 'french', 'border,', 'and', 'tally', 'andorra', 'off', 'my', 'list', 'of', 'countries', \"I've\", 'visited,', 'but', 'its', 'starting', 'to', 'look', 'pretty', 'tough']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'wtf', 'nev', 'mix', 'to\\n\\nsecond', 'yeah', 'seem', 'log', 'difficult', 'vodafon', 'doesnt', 'transf', 'andorr', 'wouldnt', 'dat', 'real', 'want', 'bik', 'andorr', 'la', 'vell', 'french', 'bord', 'tal', 'andorr', 'list', 'country', 'iv', 'visit', 'start', 'look', 'pretty', 'tough'], ['first', 'wtf', 'never', 'mix', 'to\\n\\nsecond', 'yeah', 'seem', 'logistically', 'difficult', 'vodafone', 'doesnt', 'transfer', 'andorra', 'wouldnt', 'data', 'really', 'want', 'bike', 'andorra', 'la', 'vella', 'french', 'border', 'tally', 'andorra', 'list', 'countries', 'ive', 'visit', 'start', 'look', 'pretty', 'tough'])\n",
      "original document: \n",
      "['That', 'does', 'not', 'answer', 'the', 'question,', 'only', 'deflects.', 'Is', 'the', \"league's\", 'playoffs', 'considered', 'less', 'competitive?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['answ', 'quest', 'deflect', 'leagu', 'playoff', 'consid', 'less', 'competit'], ['answer', 'question', 'deflect', 'league', 'playoffs', 'consider', 'less', 'competitive'])\n",
      "original document: \n",
      "['Thanks', 'for', 'quick', 'trade']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'quick', 'trad'], ['thank', 'quick', 'trade'])\n",
      "original document: \n",
      "['Jesus', 'Christ', 'what', 'a', 'bunch', 'of', 'insufferable', 'twats']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jes', 'christ', 'bunch', 'insuff', 'twat'], ['jesus', 'christ', 'bunch', 'insufferable', 'twats'])\n",
      "original document: \n",
      "['And', \"didn't\", 'use', 'the', 'train', '(which', 'looks', 'like', 'the', 'worst', 'part', 'polygon', 'wise)', 'and', 'added', 'LOD.', 'and', 'probably', 'more.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'us', 'train', 'look', 'lik', 'worst', 'part', 'polygon', 'wis', 'ad', 'lod', 'prob'], ['didnt', 'use', 'train', 'look', 'like', 'worst', 'part', 'polygon', 'wise', 'add', 'lod', 'probably'])\n",
      "original document: \n",
      "['Much', 'better', 'team', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'bet', 'team'], ['much', 'better', 'team'])\n",
      "original document: \n",
      "['The', 'text', 'after', 'the', 'bit', \"you've\", 'quoted', 'says:\\n(Heads', 'up:', 'These', 'codes', 'can', 'only', 'be', 'redeemed', 'by', 'non-Business', 'Todoist', 'users.', 'Premium', 'users', 'will', 'just', 'receive', 'an', 'additional', 'three', 'months!)', '\\nSo', 'the', 'last', 'bit', 'suggests', \"they'll\", 'still', 'work?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['text', 'bit', 'youv', 'quot', 'says\\nhead', 'cod', 'redeem', 'nonbusy', 'todo', 'us', 'prem', 'us', 'receiv', 'addit', 'three', 'month', '\\nso', 'last', 'bit', 'suggest', 'theyl', 'stil', 'work'], ['text', 'bite', 'youve', 'quote', 'says\\nheads', 'cod', 'redeem', 'nonbusiness', 'todoist', 'users', 'premium', 'users', 'receive', 'additional', 'three', 'months', '\\nso', 'last', 'bite', 'suggest', 'theyll', 'still', 'work'])\n",
      "original document: \n",
      "['Discusting', 'language!', 'I', \"don't\", 'want', 'my', 'fuzzy', 'bunny', 'milk', 'money', 'using', 'words', 'like', '*bosom*!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['discust', 'langu', 'dont', 'want', 'fuzzy', 'bunny', 'milk', 'money', 'us', 'word', 'lik', 'bosom'], ['discusting', 'language', 'dont', 'want', 'fuzzy', 'bunny', 'milk', 'money', 'use', 'word', 'like', 'bosom'])\n",
      "original document: \n",
      "['Ok', 'so', 'the', 'paid', 'is', 'PVE', 'fort', 'defense', 'with', 'a', 'campaign', 'and', 'the', 'free', 'one', 'is', 'PvP', 'shooter', 'with', 'no', 'building?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'paid', 'pve', 'fort', 'defens', 'campaign', 'fre', 'on', 'pvp', 'shoot', 'build'], ['ok', 'pay', 'pve', 'fort', 'defense', 'campaign', 'free', 'one', 'pvp', 'shooter', 'build'])\n",
      "original document: \n",
      "['One', 'random', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random'], ['one', 'random'])\n",
      "original document: \n",
      "[\"I'd\", 'a', 'little', 'more', 'than', 'a', 'peak.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'littl', 'peak'], ['id', 'little', 'peak'])\n",
      "original document: \n",
      "['Son', 'grasas.', 'Ven', 'guita', 'y', 'se', 'desesperan.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['son', 'grasa', 'ven', 'guit', 'se', 'desesp'], ['son', 'grasas', 'ven', 'guita', 'se', 'desesperan'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'don’t', 'remember', 'any', 'liberals', 'making', 'fun', 'of', 'her', 'for', 'her', 'accent,', 'but', 'I', 'do', 'remember', 'her', 'being', 'made', 'fun', 'of', 'for', 'plagiarizing', 'Michelle', 'Obama.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'rememb', 'lib', 'mak', 'fun', 'acc', 'rememb', 'mad', 'fun', 'plagi', 'michel', 'obam'], ['dont', 'remember', 'liberals', 'make', 'fun', 'accent', 'remember', 'make', 'fun', 'plagiarize', 'michelle', 'obama'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['bad', 'bot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bad', 'bot'], ['bad', 'bot'])\n",
      "original document: \n",
      "['Yup', '\\n\\nThe', 'king', 'is', 'dead', 'long', 'live', 'the', 'king', '/queen']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup', '\\n\\nthe', 'king', 'dead', 'long', 'liv', 'king', 'queen'], ['yup', '\\n\\nthe', 'king', 'dead', 'long', 'live', 'king', 'queen'])\n",
      "original document: \n",
      "['[Link', 'To', 'Original', 'Submission](http://reddit.com/73ifoe)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['link', 'origin', 'submissionhttpredditcom73ifoe'], ['link', 'original', 'submissionhttpredditcom73ifoe'])\n",
      "original document: \n",
      "['da', 'noomerator', '+', '', '\\nda', 'denominattur', '=', '', '\\ndanullment']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['da', 'noom', '\\nda', 'denominat', '\\ndanullment'], ['da', 'noomerator', '\\nda', 'denominattur', '\\ndanullment'])\n",
      "original document: \n",
      "['&gt;there', 'is', 'no', 'universal', '\"motherhood\"', 'that', 'spans', 'across', 'species\\n\\nUhh...what?', 'Animals', 'have', 'mothers', 'and', 'animals', 'are', 'mothers', 'to', 'their', 'babies.', 'Cows', 'being', 'mothers', \"isn't\", 'something', 'hippies', 'made', 'up', 'to', 'try', 'to', 'anthropomorphise', 'them...', \"that's\", 'the', 'term', 'everyone', 'uses', 'for', 'a', 'female', 'that', 'bears', 'young.', 'I', 'get', 'that', \"you're\", 'saying', 'human', 'motherhood', 'is', 'especially', 'cool', 'and', 'important', 'because', \"we're\", 'smart', 'and', 'all', 'that.', 'Fair', 'enough.', 'But', 'motherhood', '*literally,', 'factually*', 'spans', 'across', 'species,', 'and', 'dairy', 'absolutely', 'is', 'related', 'to', 'motherhood.', 'The', 'industry', \"wouldn't\", 'exist', 'without', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthere', 'univers', 'moth', 'span', 'across', 'species\\n\\nuhhwhat', 'anim', 'moth', 'anim', 'moth', 'baby', 'cow', 'moth', 'isnt', 'someth', 'hippy', 'mad', 'try', 'anthropomorph', 'that', 'term', 'everyon', 'us', 'fem', 'bear', 'young', 'get', 'yo', 'say', 'hum', 'moth', 'espec', 'cool', 'import', 'smart', 'fair', 'enough', 'moth', 'lit', 'fact', 'span', 'across', 'specy', 'dairy', 'absolv', 'rel', 'moth', 'industry', 'wouldnt', 'ex', 'without'], ['gtthere', 'universal', 'motherhood', 'span', 'across', 'species\\n\\nuhhwhat', 'animals', 'mother', 'animals', 'mother', 'baby', 'cow', 'mother', 'isnt', 'something', 'hippies', 'make', 'try', 'anthropomorphise', 'thats', 'term', 'everyone', 'use', 'female', 'bear', 'young', 'get', 'youre', 'say', 'human', 'motherhood', 'especially', 'cool', 'important', 'smart', 'fair', 'enough', 'motherhood', 'literally', 'factually', 'span', 'across', 'species', 'dairy', 'absolutely', 'relate', 'motherhood', 'industry', 'wouldnt', 'exist', 'without'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['It’s', 'most', 'noticeable', 'when', 'fast', 'scrolling', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['not', 'fast', 'scrolling'], ['noticeable', 'fast', 'scroll'])\n",
      "original document: \n",
      "[\"That's\", 'awesome!', 'When', 'you', 'were', 'starting', 'out,', 'did', 'it', 'ever', 'feel', 'uncomfortable', 'to', 'be', 'in', 'charge', 'of', 'the', 'more', '\"seasoned\"', 'and', 'experienced', 'employees?', 'What', 'helped', 'you', 'to', 'be', 'a', 'better', 'leader?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'awesom', 'start', 'ev', 'feel', 'uncomfort', 'charg', 'season', 'expery', 'employ', 'help', 'bet', 'lead'], ['thats', 'awesome', 'start', 'ever', 'feel', 'uncomfortable', 'charge', 'season', 'experience', 'employees', 'help', 'better', 'leader'])\n",
      "original document: \n",
      "['Lorian', 'and', 'Lothric', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lor', 'lothr'], ['lorian', 'lothric'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Any', 'solos', 'that', 'are', 'good', 'redo', 'for', 'the', 'stats?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['solo', 'good', 'redo', 'stat'], ['solo', 'good', 'redo', 'stats'])\n",
      "original document: \n",
      "['Arden', 'Key', 'blah', 'blah', 'shut', 'uuuuuuuup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ard', 'key', 'blah', 'blah', 'shut', 'uuuuuuuup'], ['arden', 'key', 'blah', 'blah', 'shut', 'uuuuuuuup'])\n",
      "original document: \n",
      "['Just', 'to', 'be', 'clear,', 'are', 'you', 'a', 'realtor', 'or', 'similar?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['clear', 'realt', 'simil'], ['clear', 'realtor', 'similar'])\n",
      "original document: \n",
      "['Great', 'to', 'know!', 'Thanks.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'know', 'thank'], ['great', 'know', 'thank'])\n",
      "original document: \n",
      "['###IM', 'SO', 'EMOTIONAL', 'NOW.', 'THE', 'ONLY', 'WAY', 'THIS', 'CAN', 'GET', 'BETTER', 'IS', 'IF', 'WE', 'KILL', 'THE', 'DODGERS', 'TONIGHT', 'AND', 'DESTROY', 'THE', 'FACE', 'OF', 'EVIL', 'TOMORROW.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'emot', 'way', 'get', 'bet', 'kil', 'dodg', 'tonight', 'destroy', 'fac', 'evil', 'tomorrow'], ['im', 'emotional', 'way', 'get', 'better', 'kill', 'dodgers', 'tonight', 'destroy', 'face', 'evil', 'tomorrow'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['143417733|', '&gt;', 'Canada', 'Anonymous', '(ID:', 'QG6wKQuQ)\\n\\n&gt;&gt;143415870\\n\\nIDK', 'about', 'that.', 'I', 'was', 'skeptical', 'about', 'them', 'but', 'then', 'a', 'bunch', 'of', 'cucks', 'got', 'all', 'huffy', 'over', 'the', 'merger', 'and', 'left', 'the', 'Conservative', 'parties', 'and', 'that', 'made', 'it', 'all', 'a', 'lot', 'more', 'appealing', 'in', 'my', 'eyes.', 'Butt', 'hurt', 'cucks', 'means', \"there's\", 'a', 'good', 'chance', 'of', 'potential', 'there.', 'Of', 'course', \"nothing's\", 'certain', 'in', 'politics,', \"there's\", 'always', 'a', 'chance', 'of', 'backstabbing', 'kikery', 'but', \"I'm\", 'hopeful.', 'I', 'really', 'liked', 'Brian', 'and', 'Callaway', 'and', 'the', 'other', '2', 'would', 'be', '\"ok\".', 'A', \"Kike'd\", 'up', 'business', 'neo-liberal', 'is', 'still', '1000x', 'better', 'than', 'the', 'out', 'right', 'SJW', 'Commies', 'we', 'have', 'right', 'now.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, seven hundred and thirty-thr', 'gt', 'canad', 'anonym', 'id', 'qg6wkquq\\n\\ngtgt143415870\\n\\nidk', 'skept', 'bunch', 'cuck', 'got', 'huffy', 'merg', 'left', 'conserv', 'party', 'mad', 'lot', 'ap', 'ey', 'but', 'hurt', 'cuck', 'mean', 'ther', 'good', 'chant', 'pot', 'cours', 'noth', 'certain', 'polit', 'ther', 'alway', 'chant', 'backstab', 'kikery', 'im', 'hop', 'real', 'lik', 'bri', 'callaway', 'two', 'would', 'ok', 'kik', 'busy', 'neolib', 'stil', '1000x', 'bet', 'right', 'sjw', 'commy', 'right', 'now\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, seven hundred and thirty-three', 'gt', 'canada', 'anonymous', 'id', 'qg6wkquq\\n\\ngtgt143415870\\n\\nidk', 'skeptical', 'bunch', 'cucks', 'get', 'huffy', 'merger', 'leave', 'conservative', 'party', 'make', 'lot', 'appeal', 'eye', 'butt', 'hurt', 'cucks', 'mean', 'theres', 'good', 'chance', 'potential', 'course', 'nothings', 'certain', 'politics', 'theres', 'always', 'chance', 'backstabbing', 'kikery', 'im', 'hopeful', 'really', 'like', 'brian', 'callaway', 'two', 'would', 'ok', 'kiked', 'business', 'neoliberal', 'still', '1000x', 'better', 'right', 'sjw', 'commies', 'right', 'now\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['What', 'will', 'grow', 'well', 'depends', 'in', 'part', 'on', 'where', 'you', 'live', '-', 'short', 'season', 'or', 'long,', 'coolish', 'summers', 'or', 'blistering', 'heat,', 'etc.', 'Probably', 'want', 'to', 'factor', 'that', 'in.\\n\\nLate', 'afternoon', 'sun', 'tends', 'to', 'be', 'strong', 'so', 'shade', 'lovers', 'like', 'ferns', 'might', 'not', 'be', 'the', 'best', 'bet.', 'The', 'upside', 'of', 'afternoon', 'sun', 'is', 'that', 'it', 'will', 'support', 'a', 'wide', 'variety', 'of', 'colorful', 'plants:', 'annuals', 'like', 'fancy', 'trailing', 'geraniums', 'and', 'sun', 'resistant', 'coleus', '(Trailing', 'Plum', 'is', 'pretty', 'and', 'dependable', 'http://davesgarden.com/guides/pf/go/57816/)', 'in', 'hanging', 'baskets.', 'In', 'pots,', 'you', 'could', 'grow', 'dwarf', 'zinnias,', 'dwarf', 'to', 'mid-height', 'snapdragons,', 'petunias', 'in', 'a', 'zillion', 'colors', 'and', 'patterns,', 'and', 'sun', 'resistant', 'impatience', 'like', 'SunPatiens', '(http://www.costafarms.com/plants/impatiens-sunpatiens)', 'for', 'color.\\n\\nSomething', 'I', 'tried', 'for', 'the', 'first', 'time', 'this', 'year', 'that', 'was', 'super', 'cute', 'and', 'unusual', 'was', 'dwarf', 'eucomis', 'or', 'pineapple', 'lilies.', 'They', 'grow', 'from', 'spring-planted', 'bulbs', 'in', 'half', 'day', 'sub', 'and', 'look', 'great', 'from', 'June', 'into', 'fall.', 'Very', 'unusual', 'and', 'whimsical.', 'I', 'grew', 'this', 'variety', 'and', 'will', 'again:', '', 'http://www.leafari.com/eucomis-aloha-nani-hybrid.html\\n\\nYou', 'have', 'lots', 'of', 'options.', 'Have', 'fun!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['grow', 'wel', 'depend', 'part', 'liv', 'short', 'season', 'long', 'cool', 'sum', 'blist', 'heat', 'etc', 'prob', 'want', 'fact', 'in\\n\\nlate', 'afternoon', 'sun', 'tend', 'strong', 'shad', 'lov', 'lik', 'fern', 'might', 'best', 'bet', 'upsid', 'afternoon', 'sun', 'support', 'wid', 'vary', 'col', 'plant', 'an', 'lik', 'fant', 'trail', 'geran', 'sun', 'resist', 'cole', 'trail', 'plum', 'pretty', 'depend', 'httpdavesgardencomguidespfgo57816', 'hang', 'basket', 'pot', 'could', 'grow', 'dwarf', 'zinnia', 'dwarf', 'midheight', 'snapdragon', 'petunia', 'zil', 'col', 'pattern', 'sun', 'resist', 'impaty', 'lik', 'sunpaty', 'httpwwwcostafarmscomplantsimpatienssunpatiens', 'color\\n\\nsomething', 'tri', 'first', 'tim', 'year', 'sup', 'cut', 'unus', 'dwarf', 'eucom', 'pineappl', 'lily', 'grow', 'springplanted', 'bulb', 'half', 'day', 'sub', 'look', 'gre', 'jun', 'fal', 'unus', 'whims', 'grew', 'vary', 'httpwwwleafaricomeucomisalohananihybridhtml\\n\\nyou', 'lot', 'opt', 'fun'], ['grow', 'well', 'depend', 'part', 'live', 'short', 'season', 'long', 'coolish', 'summer', 'blister', 'heat', 'etc', 'probably', 'want', 'factor', 'in\\n\\nlate', 'afternoon', 'sun', 'tend', 'strong', 'shade', 'lovers', 'like', 'ferns', 'might', 'best', 'bet', 'upside', 'afternoon', 'sun', 'support', 'wide', 'variety', 'colorful', 'plant', 'annuals', 'like', 'fancy', 'trail', 'geraniums', 'sun', 'resistant', 'coleus', 'trail', 'plum', 'pretty', 'dependable', 'httpdavesgardencomguidespfgo57816', 'hang', 'baskets', 'pot', 'could', 'grow', 'dwarf', 'zinnias', 'dwarf', 'midheight', 'snapdragons', 'petunias', 'zillion', 'color', 'pattern', 'sun', 'resistant', 'impatience', 'like', 'sunpatiens', 'httpwwwcostafarmscomplantsimpatienssunpatiens', 'color\\n\\nsomething', 'try', 'first', 'time', 'year', 'super', 'cute', 'unusual', 'dwarf', 'eucomis', 'pineapple', 'lilies', 'grow', 'springplanted', 'bulbs', 'half', 'day', 'sub', 'look', 'great', 'june', 'fall', 'unusual', 'whimsical', 'grow', 'variety', 'httpwwwleafaricomeucomisalohananihybridhtml\\n\\nyou', 'lot', 'options', 'fun'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Plus', 'he', 'is', 'the', 'God.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plu', 'god'], ['plus', 'god'])\n",
      "original document: \n",
      "['Angst?', 'Dissimulation?', 'Virility?', \"That's\", 'a', 'well-trimmed', 'luxurious', 'beard,', 'not', 'gonna', 'lie.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['angst', 'dissim', 'viril', 'that', 'welltrim', 'luxury', 'beard', 'gonn', 'lie'], ['angst', 'dissimulation', 'virility', 'thats', 'welltrimmed', 'luxurious', 'beard', 'gonna', 'lie'])\n",
      "original document: \n",
      "['I', 'also', \"didn't\", 'realize', 'it', '3', 'cards', 'or', 'less,', 'not', '1,', 'so', \"I'm\", 'definitely', 'showing', 'signs', 'of', 'illiteracy', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'didnt', 'real', 'three', 'card', 'less', 'on', 'im', 'definit', 'show', 'sign', 'illit'], ['also', 'didnt', 'realize', 'three', 'card', 'less', 'one', 'im', 'definitely', 'show', 'sign', 'illiteracy'])\n",
      "original document: \n",
      "['I', 'meant', 'to', 'trade', 'from', 'strength', 'to', 'address', 'weakness...', 'though', 'I', 'do', 'like', 'how', 'our', 'D', 'is', 'progressing', 'we', 'could', 'really', 'use', 'one', 'more', 'piece.', 'Depending', 'on', 'this', 'season', 'we', 'could', 'be', 'all', 'in', 'as', 'early', 'as', 'next', 'year', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meant', 'trad', 'strength', 'address', 'weak', 'though', 'lik', 'progress', 'could', 'real', 'us', 'on', 'piec', 'depend', 'season', 'could', 'ear', 'next', 'year'], ['mean', 'trade', 'strength', 'address', 'weakness', 'though', 'like', 'progress', 'could', 'really', 'use', 'one', 'piece', 'depend', 'season', 'could', 'early', 'next', 'year'])\n",
      "original document: \n",
      "['Give', 'directions,', 'for', 'sure.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'direct', 'sur'], ['give', 'directions', 'sure'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'feel', 'so', 'bad', 'for', 'Planet', 'Dog', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'bad', 'planet', 'dog'], ['feel', 'bad', 'planet', 'dog'])\n",
      "original document: \n",
      "['Give', 'directions,', 'for', 'sure.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'direct', 'sur'], ['give', 'directions', 'sure'])\n",
      "original document: \n",
      "['I', '*rate*', 'everything,', 'but', \"I've\", 'never', 'rated', 'a', 'proper', 'series', 'less', 'than', 'a', '5', 'since', 'I', \"don't\", 'seek', 'out', 'shows', 'that', 'I', 'think', \"I'll\", 'find', 'shit', '(Except', 'for', 'Shitcom,', 'but', 'that', 'was', 'a', '1', 'minute', 'thing', 'for', 'the', 'joke).\\n\\nMy', 'score', 'deviation', 'is', 'actually', '-.5,', 'so', 'apparently', \"I'm\", 'a', 'slightly', 'harsh', 'rater', 'compared', 'to', 'average.', 'For', 'currently', 'watching', \"it's\", '-1', 'because', \"I've\", 'only', 'rated', 'Fate', 'Apocrypha', 'and', 'NGNL,', 'and', 'I', \"don't\", 'like', 'the', 'sound', 'of', 'tactical', 'nukes', 'every', 'time', 'something', 'falls', 'down.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rat', 'everyth', 'iv', 'nev', 'rat', 'prop', 'sery', 'less', 'fiv', 'sint', 'dont', 'seek', 'show', 'think', 'il', 'find', 'shit', 'exceiv', 'shitcom', 'on', 'minut', 'thing', 'joke\\n\\nmy', 'scor', 'devy', 'act', 'fiv', 'app', 'im', 'slight', 'harsh', 'rat', 'comp', 'av', 'cur', 'watch', 'on', 'iv', 'rat', 'fat', 'apocryph', 'ngnl', 'dont', 'lik', 'sound', 'tact', 'nuk', 'every', 'tim', 'someth', 'fal'], ['rate', 'everything', 'ive', 'never', 'rat', 'proper', 'series', 'less', 'five', 'since', 'dont', 'seek', 'show', 'think', 'ill', 'find', 'shit', 'except', 'shitcom', 'one', 'minute', 'thing', 'joke\\n\\nmy', 'score', 'deviation', 'actually', 'five', 'apparently', 'im', 'slightly', 'harsh', 'rater', 'compare', 'average', 'currently', 'watch', 'one', 'ive', 'rat', 'fate', 'apocrypha', 'ngnl', 'dont', 'like', 'sound', 'tactical', 'nuke', 'every', 'time', 'something', 'fall'])\n",
      "original document: \n",
      "['**[Original', 'Submission', 'by', '/u/C0nguy](https://www.reddit.com/r/kindle/comments/736zvj/should_i_upgrade_or_wait_for_a_new_generation/)**', 'into', '/r/kindle\\n\\n---\\n\\n#', 'Subreddit', 'Overview\\n*', 'A', 'community', 'for:', '**8', 'years**\\n*', '#', 'of', 'subscribers:', '**33,917**\\n*', '#', 'of', 'mods:', '**6**\\n*', 'Subscribers', 'per', 'mod:', '**5,652**\\n\\n#', 'Popular', 'Posts', 'Summary\\n*', 'Top', 'domains:', 'self.kindle', '**(100%)**,', 'goodereader.com', '**(0%)**\\n*', '%', 'NSFW:', '**0%**\\n*', 'Average', 'Score:', '**10**\\n\\n#', 'Discussion', 'Summary\\n*', 'Average', 'Comment', 'Length:', '**~32**', 'words', 'per', 'comment\\n*', 'Flesch-Kincaid', 'Reading', 'Level:', '**4**\\n*', 'Comments', 'per', 'post:', '**~10**\\n\\n#', 'A', 'sampling', 'of', 'top', 'posts:\\n*', 'Top', 'all', 'time:', '[Kindle', '2', 'joystick', 'breaks', 'in', 'half.', '', 'Amazon', 'ignored', 'my', 'expired', 'warranty', 'and', '2', 'day', 'shipped', 'me', 'a', 'new', 'Kindle', 'free', 'of', 'charge.', '', 'Oh,', 'and', 'they', 'shipped', 'me', 'a', 'Kindle', '3.', '', 'Will', 'do', 'business', 'with', 'again,', 'A++', '(292', 'points', 'by', '/u/jzzsxm)](https://www.reddit.com/r/kindle/comments/o4611/kindle_2_joystick_breaks_in_half_amazon_ignored/)\\n*', 'Top', 'this', 'month:', '[The', 'Kindle', 'Oasis', 'is', 'no', 'longer', 'available', 'for', 'purchase', '(51', 'points', 'by', '/u/ScubaSteve1219)](https://www.reddit.com/r/kindle/comments/70hgj4/the_kindle_oasis_is_no_longer_available_for/)\\n*', 'Top', 'this', 'week:', '[Is', 'there', 'going', 'to', 'be', 'a', 'new', 'Paperwhite', 'at', \"Kindle's\", '10', 'year', 'anniversary', 'this', 'year?', '(31', 'points', 'by', '/u/johnmountain)](https://www.reddit.com/r/kindle/comments/72ct4r/is_there_going_to_be_a_new_paperwhite_at_kindles/)\\n\\n##', '**[Subscribe', 'at', '/r/kindle](/r/kindle)**', '', '', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'submit', 'uc0nguyhttpswwwredditcomrkindlecomments736zvjshould_i_upgrade_or_wait_for_a_new_generation', 'rkindle\\n\\n\\n\\n', 'subreddit', 'overview\\n', 'commun', 'eight', 'years\\n', 'subscrib', '33917\\n', 'mod', '6\\n', 'subscrib', 'per', 'mod', '5652\\n\\n', 'popul', 'post', 'summary\\n', 'top', 'domain', 'selfkindl', 'one hundred', 'goodereadercom', '0\\n', 'nsfw', '0\\n', 'av', 'scor', '10\\n\\n', 'discuss', 'summary\\n', 'av', 'com', 'leng', 'thirty-two', 'word', 'per', 'comment\\n', 'fleschkincaid', 'read', 'level', '4\\n', 'com', 'per', 'post', '10\\n\\n', 'sampl', 'top', 'posts\\n', 'top', 'tim', 'kindl', 'two', 'joystick', 'break', 'half', 'amazon', 'ign', 'expir', 'warranty', 'two', 'day', 'ship', 'new', 'kindl', 'fre', 'charg', 'oh', 'ship', 'kindl', 'three', 'busy', 'two hundred and ninety-two', 'point', 'ujzzsxmhttpswwwredditcomrkindlecommentso4611kindle_2_joystick_breaks_in_half_amazon_ignored\\n', 'top', 'mon', 'kindl', 'oas', 'long', 'avail', 'purchas', 'fifty-one', 'point', 'uscubasteve1219httpswwwredditcomrkindlecomments70hgj4the_kindle_oasis_is_no_longer_available_for\\n', 'top', 'week', 'going', 'new', 'paperwhit', 'kindl', 'ten', 'year', 'annivers', 'year', 'thirty-one', 'point', 'ujohnmountainhttpswwwredditcomrkindlecomments72ct4ris_there_going_to_be_a_new_paperwhite_at_kindles\\n\\n', 'subscrib', 'rkindlerkindl', '\\n'], ['original', 'submission', 'uc0nguyhttpswwwredditcomrkindlecomments736zvjshould_i_upgrade_or_wait_for_a_new_generation', 'rkindle\\n\\n\\n\\n', 'subreddit', 'overview\\n', 'community', 'eight', 'years\\n', 'subscribers', '33917\\n', 'mods', '6\\n', 'subscribers', 'per', 'mod', '5652\\n\\n', 'popular', 'post', 'summary\\n', 'top', 'domains', 'selfkindle', 'one hundred', 'goodereadercom', '0\\n', 'nsfw', '0\\n', 'average', 'score', '10\\n\\n', 'discussion', 'summary\\n', 'average', 'comment', 'length', 'thirty-two', 'word', 'per', 'comment\\n', 'fleschkincaid', 'read', 'level', '4\\n', 'comment', 'per', 'post', '10\\n\\n', 'sample', 'top', 'posts\\n', 'top', 'time', 'kindle', 'two', 'joystick', 'break', 'half', 'amazon', 'ignore', 'expire', 'warranty', 'two', 'day', 'ship', 'new', 'kindle', 'free', 'charge', 'oh', 'ship', 'kindle', 'three', 'business', 'two hundred and ninety-two', 'point', 'ujzzsxmhttpswwwredditcomrkindlecommentso4611kindle_2_joystick_breaks_in_half_amazon_ignored\\n', 'top', 'month', 'kindle', 'oasis', 'longer', 'available', 'purchase', 'fifty-one', 'point', 'uscubasteve1219httpswwwredditcomrkindlecomments70hgj4the_kindle_oasis_is_no_longer_available_for\\n', 'top', 'week', 'go', 'new', 'paperwhite', 'kindle', 'ten', 'year', 'anniversary', 'year', 'thirty-one', 'point', 'ujohnmountainhttpswwwredditcomrkindlecomments72ct4ris_there_going_to_be_a_new_paperwhite_at_kindles\\n\\n', 'subscribe', 'rkindlerkindle', '\\n'])\n",
      "original document: \n",
      "[\"You're\", 'in', 'the', 'know', 'all', \"right!\\n\\nThat's\", 'a', 'useful', 'general', 'tip,', 'good', 'to', 'know.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'know', 'right\\n\\nthats', 'us', 'gen', 'tip', 'good', 'know'], ['youre', 'know', 'right\\n\\nthats', 'useful', 'general', 'tip', 'good', 'know'])\n",
      "original document: \n",
      "['I’m', 'not', 'very', 'familiar', 'with', 'Audition,', 'but', 'you', 'should', 'have', 'no', 'trouble', 'recording', 'at', '-12dB', 'without', 'any', 'effects', 'in', 'Audition.\\n\\nAre', 'you', 'hearing', 'clipping', 'with', 'the', 'recorded', 'signal?\\n\\nIt', 'sounds', 'like', 'something', 'is', 'wrong', 'with', 'Audition', 'if', 'normalizing', 'to', '-3', 'isn’t', 'actually', 'bringing', 'the', 'peak', 'level', 'to', '-3.', 'Like', 'the', 'meters', 'are', 'maybe', 'calibrated', 'to', 'a', 'different', 'scale', 'than', 'DBFS.', 'Again,', 'I’m', 'not', 'familiar,', 'but', 'I', 'can’t', 'imagine', 'what', 'scale', 'it', 'would', 'be', 'calibrated', 'to', 'in', 'this', 'case.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'famili', 'audit', 'troubl', 'record', '12db', 'without', 'effect', 'audition\\n\\nare', 'hear', 'clip', 'record', 'signal\\n\\nit', 'sound', 'lik', 'someth', 'wrong', 'audit', 'norm', 'three', 'isnt', 'act', 'bring', 'peak', 'level', 'three', 'lik', 'met', 'mayb', 'calibr', 'diff', 'scal', 'dbfs', 'im', 'famili', 'cant', 'imagin', 'scal', 'would', 'calibr', 'cas'], ['im', 'familiar', 'audition', 'trouble', 'record', '12db', 'without', 'effect', 'audition\\n\\nare', 'hear', 'clip', 'record', 'signal\\n\\nit', 'sound', 'like', 'something', 'wrong', 'audition', 'normalize', 'three', 'isnt', 'actually', 'bring', 'peak', 'level', 'three', 'like', 'meter', 'maybe', 'calibrate', 'different', 'scale', 'dbfs', 'im', 'familiar', 'cant', 'imagine', 'scale', 'would', 'calibrate', 'case'])\n",
      "original document: \n",
      "['Ah', 'that', 'sucks.', 'I', 'hope', \"they'll\", 'fix', 'that,', 'then.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'suck', 'hop', 'theyl', 'fix'], ['ah', 'suck', 'hope', 'theyll', 'fix'])\n",
      "original document: \n",
      "['80%?', 'Where', 'are', 'you', '*getting*', 'this', 'from?', 'It', 'reduces', 'your', '*two', 'thousand*', 'dodge', 'bonus', 'by', 'four', 'hundred', 'at', 'level', '10.', '1600', 'is', 'still', 'formidable.\\n\\nEDIT:', 'Or,', 'in', 'terms', 'of', 'the', 'percent', 'formula', 'I', 'just', 'saw', 'in', 'the', 'FAQ', 'out', 'there,', 'it', 'reduces', 'your', '+200%', 'dodge', 'bonus', 'by', '50%,', 'and', '+150%', 'is', 'still', 'incredible.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eighty', 'get', 'reduc', 'two', 'thousand', 'dodg', 'bon', 'four', 'hundr', 'level', 'ten', 'one thousand, six hundred', 'stil', 'formidable\\n\\nedit', 'term', 'perc', 'formul', 'saw', 'faq', 'reduc', 'two hundred', 'dodg', 'bon', 'fifty', 'one hundred and fifty', 'stil', 'incred'], ['eighty', 'get', 'reduce', 'two', 'thousand', 'dodge', 'bonus', 'four', 'hundred', 'level', 'ten', 'one thousand, six hundred', 'still', 'formidable\\n\\nedit', 'term', 'percent', 'formula', 'saw', 'faq', 'reduce', 'two hundred', 'dodge', 'bonus', 'fifty', 'one hundred and fifty', 'still', 'incredible'])\n",
      "original document: \n",
      "['Looking', 'for', 'both.', 'Srry']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'srry'], ['look', 'srry'])\n",
      "original document: \n",
      "['Pennsylvania', 'law:\\n\\n-', 'Felony:', 'Using', 'legal', 'documents', '(such', 'as', 'a', 'real', \"driver's\", 'license)', 'to', 'impersonate', 'another', 'individual.', 'This', 'is', 'for', 'example', 'if', 'you', 'use', 'an', 'older', \"brother's\", 'ID', 'or', 'someone', 'who', 'looks', 'like', 'you.\\n\\n-', 'Misdemeanor:', 'Possessing,', 'purchasing,', 'or', 'using', 'fake', 'legal', 'documents', '(such', 'as', 'a', 'fake', 'ID).', 'This', 'is', 'most', 'of', 'us', 'here.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pennsylvan', 'law\\n\\n', 'felony', 'us', 'leg', 'docu', 'real', 'driv', 'licens', 'imperson', 'anoth', 'individ', 'exampl', 'us', 'old', 'broth', 'id', 'someon', 'look', 'lik', 'you\\n\\n', 'misdem', 'possess', 'purchas', 'us', 'fak', 'leg', 'docu', 'fak', 'id', 'us'], ['pennsylvania', 'law\\n\\n', 'felony', 'use', 'legal', 'document', 'real', 'drivers', 'license', 'impersonate', 'another', 'individual', 'example', 'use', 'older', 'brothers', 'id', 'someone', 'look', 'like', 'you\\n\\n', 'misdemeanor', 'possess', 'purchase', 'use', 'fake', 'legal', 'document', 'fake', 'id', 'us'])\n",
      "original document: \n",
      "['&gt;', 'eating', 'with', 'your', 'hands', 'is', 'completely', 'unacceptable', 'here.\\n\\nHow', 'do', 'you', 'eat', 'burger,', 'fries,', 'and', 'pizza?\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'eat', 'hand', 'complet', 'unacceiv', 'here\\n\\nhow', 'eat', 'burg', 'fri', 'pizza\\n\\n'], ['gt', 'eat', 'hand', 'completely', 'unacceptable', 'here\\n\\nhow', 'eat', 'burger', 'fry', 'pizza\\n\\n'])\n",
      "original document: \n",
      "['[Hold', 'my', 'beer](https://steamdb.info/calculator/76561197966308041/?cc=us)\\n\\nA', 'friend', 'of', 'mine', 'on', 'another', 'site', 'celebrated', 'his', '5000th', 'game', 'with', 'Cuphead']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hold', 'beerhttpssteamdbinfocalculator76561197966308041ccus\\n\\na', 'friend', 'min', 'anoth', 'sit', 'celebr', '5000th', 'gam', 'cuphead'], ['hold', 'beerhttpssteamdbinfocalculator76561197966308041ccus\\n\\na', 'friend', 'mine', 'another', 'site', 'celebrate', '5000th', 'game', 'cuphead'])\n",
      "original document: \n",
      "['There', 'will', 'be', 'no', 'undefeated', 'Pac-12', 'teams.', 'There', 'never', 'is.', 'There’s', 'a', 'reason', 'Pac-12', 'after', 'dark', 'exists.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['undef', 'pac12', 'team', 'nev', 'ther', 'reason', 'pac12', 'dark', 'ex'], ['undefeated', 'pac12', 'team', 'never', 'theres', 'reason', 'pac12', 'dark', 'exist'])\n",
      "original document: \n",
      "['Trebol', 'and', 'Fujitora']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trebol', 'fujitor'], ['trebol', 'fujitora'])\n",
      "original document: \n",
      "['Ya', 'imo', 'my', 'friends', 'either', 'loved', 'or', 'had', 'no', 'interest', 'in', 'monogatari.', 'If', 'you', \"didn't\", 'like', 'the', 'shaft', 'aesthetics,', 'long', 'narration,', 'and', 'general', 'stye', 'of', 'the', 'show,', \"you'll\", 'prob', 'not', 'care', 'for', 'it', 'ever.', '\\n\\nOnly', 'thing', 'I', 'could', 'suggest', 'is', 'give', 'the', 'kizumonogatari', 'movies', 'a', 'try.', \"You're\", 'usually', 'suppose', 'to', 'watch', 'it', 'after', 'bakemonogatari', 'but', \"it's\", 'better', 'than', 'not', 'watching', 'at', 'all.', 'Kizu', 'is', 'more', '\"newcomer\"', 'friendly', 'than', 'the', 'rest', 'of', 'the', 'series', 'due', 'to', 'less', 'narration', 'and', 'more', 'action', 'then', 'usual.', '', \"I've\", 'gotten', 'a', 'lot', 'of', 'people', 'hooked', 'to', 'the', 'show,who', 'dropped', 'bake', 'at', 'first,', 'with', 'the', 'movies.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya', 'imo', 'friend', 'eith', 'lov', 'interest', 'monogatar', 'didnt', 'lik', 'shaft', 'aesthet', 'long', 'nar', 'gen', 'sty', 'show', 'youl', 'prob', 'car', 'ev', '\\n\\nonly', 'thing', 'could', 'suggest', 'giv', 'kizumonogatar', 'movy', 'try', 'yo', 'us', 'suppos', 'watch', 'bakemonogatar', 'bet', 'watch', 'kizu', 'newcom', 'friend', 'rest', 'sery', 'due', 'less', 'nar', 'act', 'us', 'iv', 'got', 'lot', 'peopl', 'hook', 'showwho', 'drop', 'bak', 'first', 'movy'], ['ya', 'imo', 'friends', 'either', 'love', 'interest', 'monogatari', 'didnt', 'like', 'shaft', 'aesthetics', 'long', 'narration', 'general', 'stye', 'show', 'youll', 'prob', 'care', 'ever', '\\n\\nonly', 'thing', 'could', 'suggest', 'give', 'kizumonogatari', 'movies', 'try', 'youre', 'usually', 'suppose', 'watch', 'bakemonogatari', 'better', 'watch', 'kizu', 'newcomer', 'friendly', 'rest', 'series', 'due', 'less', 'narration', 'action', 'usual', 'ive', 'get', 'lot', 'people', 'hook', 'showwho', 'drop', 'bake', 'first', 'movies'])\n",
      "original document: \n",
      "['Smokin', 'doints', 'in', 'Amish?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['smokin', 'doint', 'am'], ['smokin', 'doints', 'amish'])\n",
      "original document: \n",
      "['New', 'to', 'this', 'concept:', 'is', 'every', 'song', 'the', 'same', 'tempo,', 'or', 'do', 'artists', 'take', 'advantage', 'of', 'different', 'time', 'signatures?', 'Either', 'way', 'it', 'seems', 'odd', 'that', \"you'd\", 'listen', 'to', 'the', 'same', 'two', 'seconds', 'of', 'music', 'over', 'and', 'over.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'conceiv', 'every', 'song', 'tempo', 'art', 'tak', 'adv', 'diff', 'tim', 'sign', 'eith', 'way', 'seem', 'od', 'youd', 'list', 'two', 'second', 'mus'], ['new', 'concept', 'every', 'song', 'tempo', 'artists', 'take', 'advantage', 'different', 'time', 'signatures', 'either', 'way', 'seem', 'odd', 'youd', 'listen', 'two', 'second', 'music'])\n",
      "original document: \n",
      "['Never', 'a', 'miscommunication.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'miscommun'], ['never', 'miscommunication'])\n",
      "original document: \n",
      "['Dope', 'af.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dop', 'af'], ['dope', 'af'])\n",
      "original document: \n",
      "['one', 'random']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random'], ['one', 'random'])\n",
      "original document: \n",
      "['Ah', 'neat', '!', 'Mine', 'have', 'been', 'like', 'this', 'my', 'entire', 'life', 'though', \"I'm\", 'not', 'sure', 'if', 'mine', 'is', 'related', 'to', 'meds']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'neat', 'min', 'lik', 'entir', 'lif', 'though', 'im', 'sur', 'min', 'rel', 'med'], ['ah', 'neat', 'mine', 'like', 'entire', 'life', 'though', 'im', 'sure', 'mine', 'relate', 'meds'])\n",
      "original document: \n",
      "['Funny', 'way', 'to', 'spell', 'betamax']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['funny', 'way', 'spel', 'betamax'], ['funny', 'way', 'spell', 'betamax'])\n",
      "original document: \n",
      "['so', 'you', 'dont', 'like', 'the', 'gov', 'cuz', 'she', 'likes', 'cats?', '', 'thats', 'some', 'pretty', 'serious', 'single', 'issue', 'voting.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'lik', 'gov', 'cuz', 'lik', 'cat', 'that', 'pretty', 'sery', 'singl', 'issu', 'vot'], ['dont', 'like', 'gov', 'cuz', 'like', 'cat', 'thats', 'pretty', 'serious', 'single', 'issue', 'vote'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['&gt;Youve', 'got', 'to', 'be', 'fucking', 'kidding', 'me.', 'The', 'is', 'no', 'comparison', 'between', 'the', 'BBC', 'and', 'RT', 'on', 'this', 'issue.', 'http://www.bbc.com/news/blogs-the-papers-41209189', '.\\n\\nI', \"don't\", 'think', \"that's\", 'the', 'link', 'you', 'meant', 'to', 'include.', 'It', \"doesn't\", 'even', 'mention', 'the', 'word', 'Syria.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtyouv', 'got', 'fuck', 'kid', 'comparison', 'bbc', 'rt', 'issu', 'httpwwwbbccomnewsblogsthepapers41209189', '\\n\\ni', 'dont', 'think', 'that', 'link', 'meant', 'includ', 'doesnt', 'ev', 'ment', 'word', 'syr'], ['gtyouve', 'get', 'fuck', 'kid', 'comparison', 'bbc', 'rt', 'issue', 'httpwwwbbccomnewsblogsthepapers41209189', '\\n\\ni', 'dont', 'think', 'thats', 'link', 'mean', 'include', 'doesnt', 'even', 'mention', 'word', 'syria'])\n",
      "original document: \n",
      "['I', 'try', 'to', 'give', 'the', 'founding', 'fathers', 'the', 'same', 'courtesy.', 'They', 'were', 'imperfect', 'men', 'living', 'in', 'a', 'different', 'era.', 'However,', 'their', 'accomplishments', 'quite', 'literally', 'changed', 'the', 'world', 'they', 'lived', 'in', 'in', 'the', 'world', 'that', 'we', 'live', 'in', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'giv', 'found', 'fath', 'courtesy', 'imperfect', 'men', 'liv', 'diff', 'er', 'howev', 'accompl', 'quit', 'lit', 'chang', 'world', 'liv', 'world', 'liv', 'wel'], ['try', 'give', 'found', 'father', 'courtesy', 'imperfect', 'men', 'live', 'different', 'era', 'however', 'accomplishments', 'quite', 'literally', 'change', 'world', 'live', 'world', 'live', 'well'])\n",
      "original document: \n",
      "['1/1', '!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['elev'], ['eleven'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Wow,', \"that's\", 'too', 'deep', 'an', 'explanation.', \"I've\", 'understand', 'it', 'now.', 'Thanks', 'a', 'lot', 'for', 'taking', 'your', 'time', 'to', 'help', 'me!']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized and lemmatized document: \n",
      "(['wow', 'that', 'deep', 'expl', 'iv', 'understand', 'thank', 'lot', 'tak', 'tim', 'help'], ['wow', 'thats', 'deep', 'explanation', 'ive', 'understand', 'thank', 'lot', 'take', 'time', 'help'])\n",
      "original document: \n",
      "['[If', 'you', 'want', 'a', 'tophat,', 'then', 'this', 'might', 'interest', 'you,', 'coming', 'Halloween', 'Day](http://steamcommunity.com/sharedfiles/filedetails/?id=949840128&amp;searchtext=)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'toph', 'might', 'interest', 'com', 'halloween', 'dayhttpsteamcommunitycomsharedfilesfiledetailsid949840128ampsearchtext'], ['want', 'tophat', 'might', 'interest', 'come', 'halloween', 'dayhttpsteamcommunitycomsharedfilesfiledetailsid949840128ampsearchtext'])\n",
      "original document: \n",
      "['Warframe', 'is', 'a', 'shitty', 'game']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['warfram', 'shitty', 'gam'], ['warframe', 'shitty', 'game'])\n",
      "original document: \n",
      "['Yeah,', 'OP', 'is', 'not', 'the', 'only', 'high', 'level', 'Widow', \"I've\", 'seen', 'do', 'this.', '', 'Does', 'the', 'reload', 'cancelling', 'help', 'at', 'all?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'op', 'high', 'level', 'widow', 'iv', 'seen', 'reload', 'cancel', 'help'], ['yeah', 'op', 'high', 'level', 'widow', 'ive', 'see', 'reload', 'cancel', 'help'])\n",
      "original document: \n",
      "[\"Don't\", 'worry,', 'with', 'your', 'attitude', 'you', \"wouldn't\", 'last', 'a', 'week', 'in', 'the', 'medical', 'field.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'worry', 'attitud', 'wouldnt', 'last', 'week', 'med', 'field'], ['dont', 'worry', 'attitude', 'wouldnt', 'last', 'week', 'medical', 'field'])\n",
      "original document: \n",
      "['Quote', 'for', 'them', 'Jeremiah', '10:1-4', '(1)Hear', 'the', 'word', 'that', 'the', 'Lord', 'speaks', 'to', 'you,', 'O', 'house', 'of', 'Israel.', '(2)', 'Thus', 'says', 'the', 'Lord:', '“Learn', 'not', 'the', 'way', 'of', 'the', 'nations,', 'nor', 'be', 'dismayed', 'at', 'the', 'signs', 'of', 'the', 'heavens', 'because', 'the', 'nations', 'are', 'dismayed', 'at', 'them,', '(3)', 'for', 'the', 'customs', 'of', 'the', 'peoples', 'are', 'vanity.', 'A', 'tree', 'from', 'the', 'forest', 'is', 'cut', 'down', 'and', 'worked', 'with', 'an', 'axe', 'by', 'the', 'hands', 'of', 'a', 'craftsman.', '(4)', 'They', 'decorate', 'it', 'with', 'silver', 'and', 'gold;', 'they', 'fasten', 'it', 'with', 'hammer', 'and', 'nails', 'so', 'that', 'it', 'cannot', 'move.”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quot', 'jeremiah', 'one thousand and fourteen', '1hear', 'word', 'lord', 'speak', 'hous', 'israel', 'two', 'thu', 'say', 'lord', 'learn', 'way', 'nat', 'dismay', 'sign', 'heav', 'nat', 'dismay', 'three', 'custom', 'peopl', 'van', 'tre', 'forest', 'cut', 'work', 'ax', 'hand', 'craftsm', 'four', 'dec', 'silv', 'gold', 'fast', 'ham', 'nail', 'cannot', 'mov'], ['quote', 'jeremiah', 'one thousand and fourteen', '1hear', 'word', 'lord', 'speak', 'house', 'israel', 'two', 'thus', 'say', 'lord', 'learn', 'way', 'nations', 'dismay', 'sign', 'heavens', 'nations', 'dismay', 'three', 'customs', 'people', 'vanity', 'tree', 'forest', 'cut', 'work', 'axe', 'hand', 'craftsman', 'four', 'decorate', 'silver', 'gold', 'fasten', 'hammer', 'nail', 'cannot', 'move'])\n",
      "original document: \n",
      "['good', 'shit', 'sparty']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'shit', 'sparty'], ['good', 'shit', 'sparty'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['&gt;', 'i', 'hate', 'oversleeping', 'becouse', 'you', 'sleep', 'more', ',', 'but', 'you', 'are', 'tired', 'all', 'day', '.\\n\\nExactly.', \"It's\", 'such', 'a', 'paradox!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'hat', 'oversleep', 'bec', 'sleep', 'tir', 'day', '\\n\\nexactly', 'paradox'], ['gt', 'hate', 'oversleep', 'becouse', 'sleep', 'tire', 'day', '\\n\\nexactly', 'paradox'])\n",
      "original document: \n",
      "['you', 'should', 'probably', 'reread', 'the', 'fight', 'then']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'reread', 'fight'], ['probably', 'reread', 'fight'])\n",
      "original document: \n",
      "['Tied', 'the', 'record', 'for', 'most', 'pitchers', 'used', 'in', 'a', 'single', 'season.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tied', 'record', 'pitch', 'us', 'singl', 'season'], ['tie', 'record', 'pitchers', 'use', 'single', 'season'])\n",
      "original document: \n",
      "['which', 'one?', \"there's\", 'a', 'boomstar', 'sem', 'making', 'the', 'bubbling', 'bass', 'drone', 'thing', 'and', 'the', 'higher', 'one', 'later', 'is', 'a', 'simple', 'ob6', 'patch']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'ther', 'boomst', 'sem', 'mak', 'bubbl', 'bass', 'dron', 'thing', 'high', 'on', 'lat', 'simpl', 'ob6', 'patch'], ['one', 'theres', 'boomstar', 'sem', 'make', 'bubble', 'bass', 'drone', 'thing', 'higher', 'one', 'later', 'simple', 'ob6', 'patch'])\n",
      "original document: \n",
      "['Got', '[this', 'one', 'off', 'Amazon](https://www.amazon.com/dp/B000XTH1GY/ref=cm_sw_r_cp_api_.-c0zb0B0H2GV)', 'Had', 'it', 'for', 'almost', '3', 'years', 'now', 'and', 'works', 'great.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'on', 'amazonhttpswwwamazoncomdpb000xth1gyrefcm_sw_r_cp_api_c0zb0b0h2gv', 'almost', 'three', 'year', 'work', 'gre'], ['get', 'one', 'amazonhttpswwwamazoncomdpb000xth1gyrefcm_sw_r_cp_api_c0zb0b0h2gv', 'almost', 'three', 'years', 'work', 'great'])\n",
      "original document: \n",
      "[\"I've\", 'always', 'wondered-', \"doesn't\", 'that', 'fragile', 'ace', 'by', 'happy', 'down', 'banana', 'look', 'kind', 'of', 'sketchy?', 'Haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'alway', 'wond', 'doesnt', 'fragil', 'ac', 'happy', 'banan', 'look', 'kind', 'sketchy', 'hah'], ['ive', 'always', 'wonder', 'doesnt', 'fragile', 'ace', 'happy', 'banana', 'look', 'kind', 'sketchy', 'haha'])\n",
      "original document: \n",
      "['Yet', 'my', 'other', 'posts', 'are', 'censored', 'when', 'asking', 'simple', 'questions.', 'What', 'is', 'this', 'sub', 'afraid', 'of?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yet', 'post', 'cens', 'ask', 'simpl', 'quest', 'sub', 'afraid'], ['yet', 'post', 'censor', 'ask', 'simple', 'question', 'sub', 'afraid'])\n",
      "original document: \n",
      "['Isaiah', 'Thomas', 'licking', 'his', 'lips']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isaiah', 'thoma', 'lick', 'lip'], ['isaiah', 'thomas', 'lick', 'lips'])\n",
      "original document: \n",
      "['Ida', 'Maria', '-', 'Devil\\n\\nThirty', 'Seconds', 'to', 'Mars', '-', 'Hurricane']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'mar', 'devil\\n\\nthirty', 'second', 'mar', 'hur'], ['ida', 'maria', 'devil\\n\\nthirty', 'second', 'mar', 'hurricane'])\n",
      "original document: \n",
      "[\"It's\", '*Nickelback*.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nickelback'], ['nickelback'])\n",
      "original document: \n",
      "['Yep,', 'he', 'has', 'a', 'nice', 'lvl', '1', 'cheese,', \"it's\", 'ridiculous']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'nic', 'lvl', 'on', 'chees', 'ridic'], ['yep', 'nice', 'lvl', 'one', 'cheese', 'ridiculous'])\n",
      "original document: \n",
      "['cash', 'or', 'do', 'you', 'add', 'it', 'on', 'with', 'the', 'receipt?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cash', 'ad', 'receipt'], ['cash', 'add', 'receipt'])\n",
      "original document: \n",
      "['Yeah.', 'Nah.', 'That', '*is*', 'the', 'general', 'consensus', 'in', 'Russia;', 'post', 'collapse', 'the', 'perhaps', 'overzealous', 'economic', 'advise', 'given', 'by', 'U.S.', 'advisers', 'was', 'poor,', 'and/or', 'poorly', 'understood,', 'and/or', 'poorly', 'implemented,', 'but', 'mostly', 'thought', 'to', 'be', 'poor', 'advise', 'based', 'on', 'fundamentals', 'that', 'worked', 'just', 'fine', 'in', 'situation', 'A,', 'but', 'not', 'situation', 'F(ucked)', 'which', 'is', 'where', 'Russia', 'was.', '\\n\\nActually', 'most', 'of', 'the', 'population', 'thinks', 'Gorbachev', 'sold', 'out', 'and/or', 'was', 'bribed', 'by', 'the', 'west.', 'I', 'could', 'be', 'wrong', 'but', 'I', 'think', \"he's\", 'less', 'popular', 'than', 'Stalin.', 'Thereabouts', 'anyway.', '\\n\\nNinja', 'edit:', 'oops,', '*after*', 'the', 'collapse', 'I', 'am', 'talking', 'about.', 'The', 'collapse', 'happens', 'all', 'by', 'itself', '(or', 'was', 'it', 'Gorbachev?)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'nah', 'gen', 'consens', 'russ', 'post', 'collaps', 'perhap', 'overz', 'econom', 'adv', 'giv', 'us', 'adv', 'poor', 'and', 'poor', 'understood', 'and', 'poor', 'impl', 'most', 'thought', 'poor', 'adv', 'bas', 'funda', 'work', 'fin', 'situ', 'situ', 'fuck', 'russ', '\\n\\nactually', 'pop', 'think', 'gorbachev', 'sold', 'and', 'brib', 'west', 'could', 'wrong', 'think', 'hes', 'less', 'popul', 'stalin', 'thereabout', 'anyway', '\\n\\nninja', 'edit', 'oop', 'collaps', 'talk', 'collaps', 'hap', 'gorbachev'], ['yeah', 'nah', 'general', 'consensus', 'russia', 'post', 'collapse', 'perhaps', 'overzealous', 'economic', 'advise', 'give', 'us', 'advisers', 'poor', 'andor', 'poorly', 'understand', 'andor', 'poorly', 'implement', 'mostly', 'think', 'poor', 'advise', 'base', 'fundamentals', 'work', 'fine', 'situation', 'situation', 'fuck', 'russia', '\\n\\nactually', 'population', 'think', 'gorbachev', 'sell', 'andor', 'bribe', 'west', 'could', 'wrong', 'think', 'hes', 'less', 'popular', 'stalin', 'thereabouts', 'anyway', '\\n\\nninja', 'edit', 'oops', 'collapse', 'talk', 'collapse', 'happen', 'gorbachev'])\n",
      "original document: \n",
      "['Amazing.', \"It's\", 'just', 'baffles', 'me', 'why/how', 'they', 'treat', 'us', 'so', 'poorly.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amaz', 'baffl', 'whyhow', 'tre', 'us', 'poor'], ['amaze', 'baffle', 'whyhow', 'treat', 'us', 'poorly'])\n",
      "original document: \n",
      "['I', \"don't\", 'agree.', \"You're\", 'a', 'piece', 'of', 'shit', 'for', 'even', 'agreeing']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'agr', 'yo', 'piec', 'shit', 'ev', 'agr'], ['dont', 'agree', 'youre', 'piece', 'shit', 'even', 'agree'])\n",
      "original document: \n",
      "['That', 'sounds', 'very', 'good.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'good'], ['sound', 'good'])\n",
      "original document: \n",
      "['I', 'suppose', 'he', 'won', 'Juno', 'over', 'with', 'his', 'feminine', 'side?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suppos', 'juno', 'feminin', 'sid'], ['suppose', 'juno', 'feminine', 'side'])\n",
      "original document: \n",
      "['Zip?', 'I’m', 'local', 'to', '37211']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zip', 'im', 'loc', 'thirty-seven thousand, two hundred and eleven'], ['zip', 'im', 'local', 'thirty-seven thousand, two hundred and eleven'])\n",
      "original document: \n",
      "['I', 'HATE', 'TOM', 'TOO', 'BUT', 'YOUR', 'REASON', 'GOES', 'MORE', 'DEEP', 'THEN', 'MINES......', 'AND', 'THAT', 'IS', 'SAD']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hat', 'tom', 'reason', 'goe', 'deep', 'min', 'sad'], ['hate', 'tom', 'reason', 'go', 'deep', 'mine', 'sad'])\n",
      "original document: \n",
      "['Or', 'the', 'times', 'you', 'do', 'get', 'green', \"it's\", 'either', 'mustache', 'man', 'or', 'a', 'Merric', 'breaking', 'your', 'rate']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tim', 'get', 'green', 'eith', 'mustach', 'man', 'mer', 'break', 'rat'], ['time', 'get', 'green', 'either', 'mustache', 'man', 'merric', 'break', 'rate'])\n",
      "original document: \n",
      "['just', 'the', 'kinda', 'daddy', 'freedom', 'sub', 'is', 'looking', 'for']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'daddy', 'freedom', 'sub', 'look'], ['kinda', 'daddy', 'freedom', 'sub', 'look'])\n",
      "original document: \n",
      "['Other', 'distros', 'just', 'had', 'weird', 'effects', 'where', 'if', 'i', 'pressed', 'any', 'button', 'then', 'it', 'would', 'automatically', 'ask', 'me', 'if', 'i', 'wanted', 'to', 'shut', 'down', 'the', 'computer,', 'or', 'programs', \"wouldn't\", 'work.', 'Idk', 'remember', 'what', 'distros', 'specifically,', 'I', 'went', 'through', 'so', 'many', 'and', \"I'm\", 'happy', 'with', 'ubuntu', 'so', 'I', 'want', 'to', 'make', 'it', 'work.\\n\\nWhen', 'I', 'typed', 'that', 'in', 'tge', 'command', 'prompt', 'it', 'asked', 'for', 'my', 'password', 'but', 'when', 'I', 'type', 'nothing', 'shows', 'up.', 'Also', 'I', \"didn't\", 'even', 'setup', 'a', 'password', 'anyway']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['distro', 'weird', 'effect', 'press', 'button', 'would', 'autom', 'ask', 'want', 'shut', 'comput', 'program', 'wouldnt', 'work', 'idk', 'rememb', 'distro', 'spec', 'went', 'many', 'im', 'happy', 'ubuntu', 'want', 'mak', 'work\\n\\nwhen', 'typ', 'tge', 'command', 'prompt', 'ask', 'password', 'typ', 'noth', 'show', 'also', 'didnt', 'ev', 'setup', 'password', 'anyway'], ['distros', 'weird', 'effect', 'press', 'button', 'would', 'automatically', 'ask', 'want', 'shut', 'computer', 'program', 'wouldnt', 'work', 'idk', 'remember', 'distros', 'specifically', 'go', 'many', 'im', 'happy', 'ubuntu', 'want', 'make', 'work\\n\\nwhen', 'type', 'tge', 'command', 'prompt', 'ask', 'password', 'type', 'nothing', 'show', 'also', 'didnt', 'even', 'setup', 'password', 'anyway'])\n",
      "original document: \n",
      "['Fuck', \"you're\", 'sexy']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'yo', 'sexy'], ['fuck', 'youre', 'sexy'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Yea', 'steroids', 'did', 'a', 'lot', 'of', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'steroid', 'lot'], ['yea', 'steroids', 'lot'])\n",
      "original document: \n",
      "['[Original', 'post](https://www.reddit.com/r/NetherlandsPics/comments/73ig9u/youre_not_dutch_if_you_havent_lived_in_de_efteling/)', 'by', '/u/winterbynes', 'in', '/r/NetherlandsPics\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#whitelist', '\"netherlandspics\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'posthttpswwwredditcomrnetherlandspicscomments73ig9uyoure_not_dutch_if_you_havent_lived_in_de_efteling', 'uwinterbyn', 'rnetherlandspics\\n\\nampnbsp\\n\\n\\n\\nth', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nwhitelist', 'netherlandspics\\n'], ['original', 'posthttpswwwredditcomrnetherlandspicscomments73ig9uyoure_not_dutch_if_you_havent_lived_in_de_efteling', 'uwinterbynes', 'rnetherlandspics\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nwhitelist', 'netherlandspics\\n'])\n",
      "original document: \n",
      "['Use', 'sour', 'cream,', 'works', 'great', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'sour', 'cream', 'work', 'gre'], ['use', 'sour', 'cream', 'work', 'great'])\n",
      "original document: \n",
      "[\"It's\", 'petty', 'to', 'kick', 'children', 'off', 'a', 'sports', 'team', 'because', 'they', \"don't\", 'agree', 'with', 'you', 'politically.', 'That', \"doesn't\", 'make', 'me', 'a', 'hypocrite.', 'I', \"don't\", 'like', 'the', 'coach', 'because', 'he', 'seems', 'like', 'an', 'asshole.', 'It', 'has', 'nothing', 'to', 'do', 'with', 'his', 'response', 'to', 'the', 'kneeling,', \"it's\", 'how', 'an', 'adult', 'handled', 'an', 'interaction', 'with', \"children.\\n\\nIt's\", 'like', 'seeing', 'that', 'parent', \"who's\", 'WAY', 'too', 'into', 'their', \"kid's\", 'little', 'league.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['petty', 'kick', 'childr', 'sport', 'team', 'dont', 'agr', 'polit', 'doesnt', 'mak', 'hypocrit', 'dont', 'lik', 'coach', 'seem', 'lik', 'asshol', 'noth', 'respons', 'kneel', 'adult', 'handl', 'interact', 'children\\n\\nits', 'lik', 'see', 'par', 'who', 'way', 'kid', 'littl', 'leagu'], ['petty', 'kick', 'children', 'sport', 'team', 'dont', 'agree', 'politically', 'doesnt', 'make', 'hypocrite', 'dont', 'like', 'coach', 'seem', 'like', 'asshole', 'nothing', 'response', 'kneel', 'adult', 'handle', 'interaction', 'children\\n\\nits', 'like', 'see', 'parent', 'whos', 'way', 'kid', 'little', 'league'])\n",
      "original document: \n",
      "['/u/apetvlad', '-', 'Ghana', '(There', 'yours)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uapetvlad', 'ghan'], ['uapetvlad', 'ghana'])\n",
      "original document: \n",
      "['Other', 'than', 'using', 'pipes', 'for', 'fuel', 'and', 'waste', \"I've\", 'only', 'ever', 'used', 'the', 'flux', 'points,', 'so', \"I'm\", 'not', 'sure', 'if', 'you', 'can', 'use', 'cables.', 'That', 'being', 'said,', 'with', 'the', 'flux', 'points', 'you', 'have', 'to', 'have', 'a', 'redstone', 'flux', 'access', 'point', '(I', 'believe', 'is', 'what', \"it's\", 'called)', 'as', 'one', 'of', 'the', 'blocks,', 'with', 'the', 'flux', 'plug', 'attached', 'to', 'it.', 'Might', 'need', 'a', 'similar', 'one', 'for', 'cables.\\n\\n\\n\\nAssuming', \"you've\", 'built', 'the', 'reactor', 'correctly,', 'and', 'have', 'fuel', 'and', 'turned', 'it', 'on...as', 'I', 'may', 'or', 'may', 'not', 'have', 'done', 'once...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'pip', 'fuel', 'wast', 'iv', 'ev', 'us', 'flux', 'point', 'im', 'sur', 'us', 'cabl', 'said', 'flux', 'point', 'redston', 'flux', 'access', 'point', 'believ', 'cal', 'on', 'block', 'flux', 'plug', 'attach', 'might', 'nee', 'simil', 'on', 'cables\\n\\n\\n\\nassuming', 'youv', 'built', 'react', 'correct', 'fuel', 'turn', 'ona', 'may', 'may', 'don'], ['use', 'pip', 'fuel', 'waste', 'ive', 'ever', 'use', 'flux', 'point', 'im', 'sure', 'use', 'cable', 'say', 'flux', 'point', 'redstone', 'flux', 'access', 'point', 'believe', 'call', 'one', 'block', 'flux', 'plug', 'attach', 'might', 'need', 'similar', 'one', 'cables\\n\\n\\n\\nassuming', 'youve', 'build', 'reactor', 'correctly', 'fuel', 'turn', 'onas', 'may', 'may', 'do'])\n",
      "original document: \n",
      "['143418846|', '&gt;', 'France', 'Anonymous', '(ID:', 'vuuD9Obi)\\n\\n&gt;&gt;143412250', '(OP)\\n2012', ':', 'Marine', 'Le', 'Pen\\n2017', ':', 'Marine', 'Le', 'Pen\\n\\t\\t\\t']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, eight hundred and forty-six', 'gt', 'frant', 'anonym', 'id', 'vuud9obi\\n\\ngtgt143412250', 'op\\n2012', 'marin', 'le', 'pen\\n2017', 'marin', 'le', 'pen\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, eight hundred and forty-six', 'gt', 'france', 'anonymous', 'id', 'vuud9obi\\n\\ngtgt143412250', 'op\\n2012', 'marine', 'le', 'pen\\n2017', 'marine', 'le', 'pen\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['The', 'only', 'confirmation', 'they', 'can', 'get', 'is', 'from', 'themselves.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['confirm', 'get'], ['confirmation', 'get'])\n",
      "original document: \n",
      "['A', 'magnet', 'walks', 'into', 'a', 'bar', 'and', 'gets', 'stuck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['magnet', 'walk', 'bar', 'get', 'stuck'], ['magnet', 'walk', 'bar', 'get', 'stick'])\n",
      "original document: \n",
      "['What', 'a', 'spellbindingly', 'catty', 'thing', 'to', 'say.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spellbind', 'catty', 'thing', 'say'], ['spellbindingly', 'catty', 'thing', 'say'])\n",
      "original document: \n",
      "['The', 'highest', 'level', 'was', '31?\\nYou', 'got', '0', 'damage', 'and', '0', 'team', 'balls!?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['highest', 'level', '31\\nyou', 'got', 'zero', 'dam', 'zero', 'team', 'bal'], ['highest', 'level', '31\\nyou', 'get', 'zero', 'damage', 'zero', 'team', 'ball'])\n",
      "original document: \n",
      "['Well', 'Corporal', 'it', \"doesn't\", 'feel', 'any', 'better', 'being', 'a', '39', 'year', 'old', 'Lance', 'Corporal', '', '\\n\\nSome', 'guys', 'at', 'work', 'did', 'make', 'me', 'a', 'Senior', 'Lance', 'Corporal', 'of', 'the', 'Marine', 'Corps', 'chevron', 'so', 'I', 'got', 'that', 'going', 'for', 'me,', 'which', 'is', 'nice.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'corp', 'doesnt', 'feel', 'bet', 'thirty-nine', 'year', 'old', 'lant', 'corp', '\\n\\nsome', 'guy', 'work', 'mak', 'seny', 'lant', 'corp', 'marin', 'corp', 'chevron', 'got', 'going', 'nic'], ['well', 'corporal', 'doesnt', 'feel', 'better', 'thirty-nine', 'year', 'old', 'lance', 'corporal', '\\n\\nsome', 'guy', 'work', 'make', 'senior', 'lance', 'corporal', 'marine', 'corps', 'chevron', 'get', 'go', 'nice'])\n",
      "original document: \n",
      "['This', 'submission', 'has', 'been', 'randomly', 'featured', 'in', '/r/serendipity,', 'a', 'bot-driven', 'subreddit', 'discovery', 'engine.', 'More', 'here:', 'https://www.reddit.com/r/Serendipity/comments/73igad/should_i_upgrade_or_wait_for_a_new_generation/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'random', 'feat', 'rserendip', 'botdr', 'subreddit', 'discovery', 'engin', 'httpswwwredditcomrserendipitycomments73igadshould_i_upgrade_or_wait_for_a_new_generation'], ['submission', 'randomly', 'feature', 'rserendipity', 'botdriven', 'subreddit', 'discovery', 'engine', 'httpswwwredditcomrserendipitycomments73igadshould_i_upgrade_or_wait_for_a_new_generation'])\n",
      "original document: \n",
      "['Good', 'job!!!!', 'Feels', 'good', 'dont', 'it?', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'job', 'feel', 'good', 'dont'], ['good', 'job', 'feel', 'good', 'dont'])\n",
      "original document: \n",
      "['Sorry,', 'I', 'didnt', 'see', 'your', 'comment.', 'But', 'i', 'want', 'to', 'trade', 'for', 'Jager', ':)\\n\\nPerharps,', 'Im', 'going', 'to', 'sleep', 'and', 'will', 'be', 'on', 'in', 'about', '14-15h', 'or', 'maybe', 'in', '6-7h,', 'if', 'I', 'will', 'wake', 'up', 'earlier']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'didnt', 'see', 'com', 'want', 'trad', 'jag', '\\n\\nperharps', 'im', 'going', 'sleep', '1415h', 'mayb', '67h', 'wak', 'ear'], ['sorry', 'didnt', 'see', 'comment', 'want', 'trade', 'jager', '\\n\\nperharps', 'im', 'go', 'sleep', '1415h', 'maybe', '67h', 'wake', 'earlier'])\n",
      "original document: \n",
      "['Prior', 'to', 'IP', 'there', 'were', 'trade', 'secrets', 'that', 'protected', 'innovators', 'from', 'competition.', 'Heck,', 'many', 'candy', 'companies', 'still', 'have', 'trade', 'secrets', 'today', 'rather', 'than', 'patents.', 'The', 'idea', \"isn't\", 'that', 'IP', 'protects', 'competition,', 'as', 'competition', 'would', 'be', 'fine', 'without', 'it.', 'The', 'issue', 'is', 'that', 'IP', 'provides', 'other', 'benefits', 'besides', 'rent', 'seeking', 'to', 'innovators,', 'advantages', 'that', \"aren't\", 'talked', 'about.\\n\\nImagine', 'modern', 'medicine', 'with', 'trade', 'secrets.', \"There's\", 'no', 'way', 'for', 'it', 'to', 'work.', 'How', 'would', 'you', 'provide', 'studies', 'if', 'the', 'methodology', 'and', 'composition', \"couldn't\", 'be', 'known', 'by', 'third', 'parties?', 'NDAs', 'are', 'a', 'modest', 'solution', 'but', 'everyone', 'has', 'a', 'price.', 'Patents', 'means', 'the', 'mode', 'of', 'action', 'of', 'a', 'medicine', 'is', 'out', 'in', 'the', 'open', 'so', 'it', 'can', '*immediately*', 'begin', 'benefiting', 'future', 'science.\\n\\nTrade', 'secrets', 'are', 'also,', 'well,', 'secrets.', 'It', 'incentivizes', 'industrial', 'espionage', 'which', 'is', 'usually', 'damaging', 'and', 'illegal', 'and', 'harmful.\\n\\nHowever', 'IP', 'as', 'it', 'currently', 'exists', 'has', 'flaws.', 'Right', 'now', 'the', 'incentive', 'is', 'not', 'to', 'get', 'a', 'patent', 'and', 'get', 'back', 'to', 'inventing', 'because', 'it', 'is', 'more', 'profitable', 'to', 'get', 'a', 'patent', 'and', 'use', 'it', 'as', 'a', 'bludgeon', 'to', 'sue', 'for', 'damages', 'and', 'prevent', 'others', 'from', 'innovating.', 'Inventing', 'and', 'progress', 'must', 'wait', 'on', 'the', 'patent', 'to', 'expire', 'which', 'does', 'more', 'harm', 'to', 'innovation', 'than', 'the', 'patent', 'protects.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pri', 'ip', 'trad', 'secret', 'protect', 'innov', 'competit', 'heck', 'many', 'candy', 'company', 'stil', 'trad', 'secret', 'today', 'rath', 'pat', 'ide', 'isnt', 'ip', 'protect', 'competit', 'competit', 'would', 'fin', 'without', 'issu', 'ip', 'provid', 'benefit', 'besid', 'rent', 'seek', 'innov', 'adv', 'ar', 'talk', 'about\\n\\nimagine', 'modern', 'medicin', 'trad', 'secret', 'ther', 'way', 'work', 'would', 'provid', 'study', 'methodolog', 'composit', 'couldnt', 'known', 'third', 'party', 'nda', 'modest', 'solv', 'everyon', 'pric', 'pat', 'mean', 'mod', 'act', 'medicin', 'op', 'immedy', 'begin', 'benefit', 'fut', 'science\\n\\ntrad', 'secret', 'also', 'wel', 'secret', 'int', 'indust', 'esp', 'us', 'dam', 'illeg', 'harmful\\n\\nhowever', 'ip', 'cur', 'ex', 'flaw', 'right', 'int', 'get', 'pat', 'get', 'back', 'inv', 'profit', 'get', 'pat', 'us', 'bludgeon', 'sue', 'dam', 'prev', 'oth', 'innov', 'inv', 'progress', 'must', 'wait', 'pat', 'expir', 'harm', 'innov', 'pat', 'protect'], ['prior', 'ip', 'trade', 'secrets', 'protect', 'innovators', 'competition', 'heck', 'many', 'candy', 'company', 'still', 'trade', 'secrets', 'today', 'rather', 'patent', 'idea', 'isnt', 'ip', 'protect', 'competition', 'competition', 'would', 'fine', 'without', 'issue', 'ip', 'provide', 'benefit', 'besides', 'rent', 'seek', 'innovators', 'advantage', 'arent', 'talk', 'about\\n\\nimagine', 'modern', 'medicine', 'trade', 'secrets', 'theres', 'way', 'work', 'would', 'provide', 'study', 'methodology', 'composition', 'couldnt', 'know', 'third', 'party', 'ndas', 'modest', 'solution', 'everyone', 'price', 'patent', 'mean', 'mode', 'action', 'medicine', 'open', 'immediately', 'begin', 'benefit', 'future', 'science\\n\\ntrade', 'secrets', 'also', 'well', 'secrets', 'incentivizes', 'industrial', 'espionage', 'usually', 'damage', 'illegal', 'harmful\\n\\nhowever', 'ip', 'currently', 'exist', 'flaw', 'right', 'incentive', 'get', 'patent', 'get', 'back', 'invent', 'profitable', 'get', 'patent', 'use', 'bludgeon', 'sue', 'damage', 'prevent', 'others', 'innovate', 'invent', 'progress', 'must', 'wait', 'patent', 'expire', 'harm', 'innovation', 'patent', 'protect'])\n",
      "original document: \n",
      "['Agreed.', '', 'Our', 'games', 'have', 'been', 'super', 'close', 'with', 'wiscy', 'the', 'last', 'couple', 'of', 'years', 'and', \"I'm\", 'just', 'tired', 'of', 'being', 'teased', 'by', 'them.', '', 'Iowa', 'also', \"doesn't\", 'deserve', 'to', 'feel', 'like', \"they're\", 'good.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'gam', 'sup', 'clos', 'wiscy', 'last', 'coupl', 'year', 'im', 'tir', 'teas', 'iow', 'also', 'doesnt', 'deserv', 'feel', 'lik', 'theyr', 'good'], ['agree', 'game', 'super', 'close', 'wiscy', 'last', 'couple', 'years', 'im', 'tire', 'tease', 'iowa', 'also', 'doesnt', 'deserve', 'feel', 'like', 'theyre', 'good'])\n",
      "original document: \n",
      "['Thank', 'you!', 'It', \"doesn't\", 'have', 'to', 'be', 'traditional', 'as', 'long', 'as', \"it's\", 'good', 'biscuits', 'and', \"gravy.\\n\\nI've\", 'had', 'Bacon', 'and', 'Butter', 'three', 'times', 'and', 'I', \"don't\", 'understand', 'the', 'love.', \"I've\", 'had', 'the', 'biscuits', 'and', 'gravy', 'on', 'two', 'separate', 'occasions', 'and', 'it', 'was', 'bland', 'gravy', 'with', 'stale', 'biscuits.\\n\\nI', 'will', 'add', 'your', 'suggestions', 'to', 'my', 'list.', \"I'm\", 'going', 'to', 'have', 'to', 'spread', 'it', 'out', 'or', 'I', 'will', 'be', 'keeling', 'over', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'doesnt', 'tradit', 'long', 'good', 'biscuit', 'gravy\\n\\nive', 'bacon', 'but', 'three', 'tim', 'dont', 'understand', 'lov', 'iv', 'biscuit', 'gravy', 'two', 'sep', 'occas', 'bland', 'gravy', 'stal', 'biscuits\\n\\ni', 'ad', 'suggest', 'list', 'im', 'going', 'spread', 'keel'], ['thank', 'doesnt', 'traditional', 'long', 'good', 'biscuits', 'gravy\\n\\nive', 'bacon', 'butter', 'three', 'time', 'dont', 'understand', 'love', 'ive', 'biscuits', 'gravy', 'two', 'separate', 'occasion', 'bland', 'gravy', 'stale', 'biscuits\\n\\ni', 'add', 'suggestions', 'list', 'im', 'go', 'spread', 'keel'])\n",
      "original document: \n",
      "['Ice', \"isn't\", 'dead,', 'only', 'his', 'content.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ic', 'isnt', 'dead', 'cont'], ['ice', 'isnt', 'dead', 'content'])\n",
      "original document: \n",
      "['i', 'hope', 'they', 'add', 'more', 'in', 'the', 'future!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'ad', 'fut'], ['hope', 'add', 'future'])\n",
      "original document: \n",
      "['Nice', ':', ')', 'On', 'Vanier?', \"There's\", 'an', 'STs', 'of', 'Ontario', 'guy', 'there', 'and', \"i'm\", 'moving', 'to', 'the', 'same', 'building', 'lol', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'vany', 'ther', 'sts', 'ontario', 'guy', 'im', 'mov', 'build', 'lol'], ['nice', 'vanier', 'theres', 'sts', 'ontario', 'guy', 'im', 'move', 'build', 'lol'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'sure', 'if', 'comparing', 'Battlefield', 'and', 'Call', 'of', 'Duty', 'is', 'a', 'proper', 'thing.', 'Battlefield', 'is', 'not', 'like', 'Call', 'of', 'Duty', 'at', 'all.', 'They', 'might', 'both', 'be', 'FPS,', 'but', \"that's\", 'like', 'trying', 'to', 'compare', 'DOOM', 'to', 'Fallout.', \"They're\", '2', 'completely', 'different', 'games.', '', '\\n', '\\nThis', 'whole', '\"Is', 'one', 'better', 'than', 'the', 'other?\"', \"doesn't\", 'make', 'sense', 'to', 'me,', 'one', 'is', 'massive', 'battles', 'with', 'vehicles,', 'classes,', 'realistic', 'sounds,', 'teamwork,', 'massive', 'maps,', 'and', 'objectives.', 'The', 'other', 'is', 'tiny', 'maps,', 'fast', 'gameplay,', 'no', 'recoil,', 'and', 'few', 'players.', '', '\\n', '\\nThese', '2', 'games', 'are', 'nothing', 'alike.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'comp', 'battlefield', 'cal', 'duty', 'prop', 'thing', 'battlefield', 'lik', 'cal', 'duty', 'might', 'fps', 'that', 'lik', 'try', 'comp', 'doom', 'fallout', 'theyr', 'two', 'complet', 'diff', 'gam', '\\n', '\\nthis', 'whol', 'on', 'bet', 'doesnt', 'mak', 'sens', 'on', 'mass', 'battl', 'vehic', 'class', 'real', 'sound', 'teamwork', 'mass', 'map', 'object', 'tiny', 'map', 'fast', 'gameplay', 'recoil', 'play', '\\n', '\\nthese', 'two', 'gam', 'noth', 'alik'], ['im', 'sure', 'compare', 'battlefield', 'call', 'duty', 'proper', 'thing', 'battlefield', 'like', 'call', 'duty', 'might', 'fps', 'thats', 'like', 'try', 'compare', 'doom', 'fallout', 'theyre', 'two', 'completely', 'different', 'game', '\\n', '\\nthis', 'whole', 'one', 'better', 'doesnt', 'make', 'sense', 'one', 'massive', 'battle', 'vehicles', 'class', 'realistic', 'sound', 'teamwork', 'massive', 'map', 'objectives', 'tiny', 'map', 'fast', 'gameplay', 'recoil', 'players', '\\n', '\\nthese', 'two', 'game', 'nothing', 'alike'])\n",
      "original document: \n",
      "['#Safety', 'Warning:\\n\\n**Sites', 'below', 'can', 'potentially', 'have', 'multiple', 'ads,', 'and/or', 'popups,', 'and/or', 'misleading', 'download', 'links.**\\n\\n*', 'Always', 'remember', 'to', 'never', 'download', 'anything', 'from', 'the', 'websites', 'posted', 'here.\\n\\n*', 'Use', 'an', '**AdBlocker**', '-', 'I', 'recommend', 'using', 'uBlock', 'Origin\\n\\n*', 'Report', 'any', 'other', 'suspicious', 'links', 'to', 'the', 'moderators', 'and', 'be', 'sure', 'to', 'include', 'a', 'reason', 'why.\\n\\n*', 'Use', 'these', 'sites', 'with', 'your', 'own', 'discretion.\\n\\n*****\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/NHLStreams)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saf', 'warning\\n\\nsites', 'pot', 'multipl', 'ad', 'and', 'popup', 'and', 'mislead', 'download', 'links\\n\\n', 'alway', 'rememb', 'nev', 'download', 'anyth', 'websit', 'post', 'here\\n\\n', 'us', 'adblock', 'recommend', 'us', 'ublock', 'origin\\n\\n', 'report', 'suspicy', 'link', 'mod', 'sur', 'includ', 'reason', 'why\\n\\n', 'us', 'sit', 'discretion\\n\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetornhlstream', 'quest', 'concern'], ['safety', 'warning\\n\\nsites', 'potentially', 'multiple', 'ads', 'andor', 'popups', 'andor', 'mislead', 'download', 'links\\n\\n', 'always', 'remember', 'never', 'download', 'anything', 'websites', 'post', 'here\\n\\n', 'use', 'adblocker', 'recommend', 'use', 'ublock', 'origin\\n\\n', 'report', 'suspicious', 'link', 'moderators', 'sure', 'include', 'reason', 'why\\n\\n', 'use', 'sit', 'discretion\\n\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetornhlstreams', 'question', 'concern'])\n",
      "original document: \n",
      "['Mark', 'it', 'with', 'a', 'sharpie']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mark', 'sharpy'], ['mark', 'sharpie'])\n",
      "original document: \n",
      "['SS', 'Tier', '=', 'Sha', 'Lin', 'Smasher', 'Tier']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ss', 'tier', 'sha', 'lin', 'smash', 'tier'], ['ss', 'tier', 'sha', 'lin', 'smasher', 'tier'])\n",
      "original document: \n",
      "['This', 'is', 'true', 'yes,', 'also', 'that', 'book', 'fucking', 'rules', '(RAWs', 'other', 'stuff', 'is', 'good', 'to,', 'Robert', \"Shea's\", 'other', 'output', \"isn't\", 'bad', 'but', 'waayyyy', 'different)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'ye', 'also', 'book', 'fuck', 'rul', 'raw', 'stuff', 'good', 'robert', 'shea', 'output', 'isnt', 'bad', 'waayyyy', 'diff'], ['true', 'yes', 'also', 'book', 'fuck', 'rule', 'raws', 'stuff', 'good', 'robert', 'sheas', 'output', 'isnt', 'bad', 'waayyyy', 'different'])\n",
      "original document: \n",
      "['He', 'had', 'his', 'own', 'TV', 'show', 'on', 'RT', '-', 'he', 'literally', 'accepted', 'a', 'paycheck', 'from', 'the', 'Russian', \"government's\", 'propaganda', 'arm.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tv', 'show', 'rt', 'lit', 'acceiv', 'paycheck', 'russ', 'govern', 'propagand', 'arm\\n'], ['tv', 'show', 'rt', 'literally', 'accept', 'paycheck', 'russian', 'governments', 'propaganda', 'arm\\n'])\n",
      "original document: \n",
      "['I', 'always', 'feel', 'like', 'I', 'move', 'super', 'lightning', 'speed', 'about', '10', 'minutes', 'after', 'my', 'hit.', 'Then', 'I', 'go', 'slow', 'mo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'feel', 'lik', 'mov', 'sup', 'lightn', 'spee', 'ten', 'minut', 'hit', 'go', 'slow', 'mo'], ['always', 'feel', 'like', 'move', 'super', 'lightning', 'speed', 'ten', 'minutes', 'hit', 'go', 'slow', 'mo'])\n",
      "original document: \n",
      "['Exactly.', 'We', 'have', 'players', 'like', 'Porzingis,', 'who', 'are', 'in', 'their', '20s', 'and', 'people', 'still', 'feel', 'like', 'he', 'needs', 'to', 'physically', 'develop', 'more.', 'Gordon', 'Hayward', 'only', 'developed', 'the', 'body', 'he', 'needed', 'to', 'absorb', 'contact', 'last', 'year.', 'No', 'way', \"we'll\", 'see', '16', 'year', 'olds', 'in', 'the', 'NBA.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'play', 'lik', 'porz', '20s', 'peopl', 'stil', 'feel', 'lik', 'nee', 'phys', 'develop', 'gordon', 'hayward', 'develop', 'body', 'nee', 'absorb', 'contact', 'last', 'year', 'way', 'wel', 'see', 'sixteen', 'year', 'old', 'nba'], ['exactly', 'players', 'like', 'porzingis', '20s', 'people', 'still', 'feel', 'like', 'need', 'physically', 'develop', 'gordon', 'hayward', 'develop', 'body', 'need', 'absorb', 'contact', 'last', 'year', 'way', 'well', 'see', 'sixteen', 'year', 'olds', 'nba'])\n",
      "original document: \n",
      "['Hello,', 'your', 'title', 'indicates', 'that', 'this', 'post', 'may', 'contain', 'content', 'related', 'to', 'either', 'server', 'lag,', 'desync,', 'or', 'some', 'form', 'of', 'bug.', 'If', 'so', '-', 'we', 'would', 'like', 'to', 'encourage', 'you', 'to', 'report', 'these', 'issues', 'directly', 'to', 'Bluehole', 'using', 'whichever', 'of', 'the', 'following', 'links', 'is', 'the', 'most', 'appropriate:', '\\n\\n[Server', 'Lag', 'Reports](http://forums.playbattlegrounds.com/topic/5435-server-lag-report-thread/)', '', '\\n[Bug', 'Reports](http://forums.playbattlegrounds.com/forum/10-bug-reports/)\\n\\n\\nBecause', 'this', 'subreddit', 'is', 'fan', 'run', 'there', 'is', 'unfortunately', 'not', 'much', 'we', 'can', 'do', 'here', 'regarding', 'either', 'server', 'issues', 'or', 'in-game', 'bugs.', 'The', 'best', 'way', 'to', 'bring', 'them', 'to', 'the', \"dev's\", 'attention', 'is', 'by', 'directly', 'reporting', 'them', 'using', 'those', 'links.', 'Thanks!\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/PUBATTLEGROUNDS)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hello', 'titl', 'ind', 'post', 'may', 'contain', 'cont', 'rel', 'eith', 'serv', 'lag', 'desynt', 'form', 'bug', 'would', 'lik', 'enco', 'report', 'issu', 'direct', 'bluehol', 'us', 'whichev', 'follow', 'link', 'appropry', '\\n\\nserver', 'lag', 'reportshttpforumsplaybattlegroundscomtopic5435serverlagreportthread', '\\nbug', 'reportshttpforumsplaybattlegroundscomforum10bugreports\\n\\n\\nbecause', 'subreddit', 'fan', 'run', 'unfortun', 'much', 'regard', 'eith', 'serv', 'issu', 'ingam', 'bug', 'best', 'way', 'bring', 'dev', 'at', 'direct', 'report', 'us', 'link', 'thanks\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorpubattleground', 'quest', 'concern'], ['hello', 'title', 'indicate', 'post', 'may', 'contain', 'content', 'relate', 'either', 'server', 'lag', 'desync', 'form', 'bug', 'would', 'like', 'encourage', 'report', 'issue', 'directly', 'bluehole', 'use', 'whichever', 'follow', 'link', 'appropriate', '\\n\\nserver', 'lag', 'reportshttpforumsplaybattlegroundscomtopic5435serverlagreportthread', '\\nbug', 'reportshttpforumsplaybattlegroundscomforum10bugreports\\n\\n\\nbecause', 'subreddit', 'fan', 'run', 'unfortunately', 'much', 'regard', 'either', 'server', 'issue', 'ingame', 'bug', 'best', 'way', 'bring', 'devs', 'attention', 'directly', 'report', 'use', 'link', 'thanks\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorpubattlegrounds', 'question', 'concern'])\n",
      "original document: \n",
      "['Great.', '', 'He', 'helps', 'people', 'who', \"don't\", 'know', 'how', 'to', 'use', 'google', 'or', \"don't\", 'know', 'to', 'just', 'read', \"Wizz's\", 'MI', 'blog', 'and', 'this', 'sub.', '', '\\n\\nAnd', 'this', 'sub', 'would', 'become', 'a', 'much', 'less', 'useful', 'place', 'if', 'polluted', 'with', 'marketing', 'spam.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'help', 'peopl', 'dont', 'know', 'us', 'googl', 'dont', 'know', 'read', 'wizz', 'mi', 'blog', 'sub', '\\n\\nand', 'sub', 'would', 'becom', 'much', 'less', 'us', 'plac', 'pollut', 'market', 'spam'], ['great', 'help', 'people', 'dont', 'know', 'use', 'google', 'dont', 'know', 'read', 'wizzs', 'mi', 'blog', 'sub', '\\n\\nand', 'sub', 'would', 'become', 'much', 'less', 'useful', 'place', 'pollute', 'market', 'spam'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['My', 'last', 'LTR', 'was', 'with', 'an', 'intelligent', 'woman', 'who', 'is', 'a', 'prosecutor,', 'so', 'I', 'can', 'attest', 'that', 'everything', 'you', 'have', 'written', 'here', 'is', 'spot', 'on-', 'both', 'her', 'behaviour', 'and', 'mine.', 'I', 'found', 'TRP', 'after', 'the', 'relationship', 'failed', 'in', 'this', 'manner.', '\\n\\nFollowing', 'that,', \"doesn't\", 'that', 'mean', 'that', 'some', 'of', 'the', \"womens'\", 'quotes', 'from', 'the', 'original', 'article', 'are', 'actually', 'fair?', 'A', 'miniscule', 'sliver', 'of', 'men', 'have', 'red', 'pill', 'traits', '(learned', 'or', 'inherent).', 'So', 'when', 'these', 'women', 'self', 'identify', 'as', 'intelligent', 'any', 'man', 'who', \"can't\", 'stand', 'up', 'to', 'their', 'shit', 'is', 'going', 'to', 'be', 'perceived', 'as', 'intellectually', 'weaker,', 'whether', 'they', 'are', 'an', 'astrophysicist', 'or', 'not.', '\\n\\nThis', 'is', 'just', 'another', 'shit', 'test,', 'and', 'nothing', 'a', 'quick', 'evisceration', 'on', 'a', 'chess', 'board', \"wouldn't\", 'solve', 'to', 'quiet', 'their', 'lamentations.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['last', 'ltr', 'intellig', 'wom', 'prosecut', 'attest', 'everyth', 'writ', 'spot', 'behavio', 'min', 'found', 'trp', 'rel', 'fail', 'man', '\\n\\nfollowing', 'doesnt', 'mean', 'wom', 'quot', 'origin', 'artic', 'act', 'fair', 'minisc', 'sliv', 'men', 'red', 'pil', 'trait', 'learn', 'inh', 'wom', 'self', 'ident', 'intellig', 'man', 'cant', 'stand', 'shit', 'going', 'perceiv', 'intellect', 'weak', 'wheth', 'astrophys', '\\n\\nthis', 'anoth', 'shit', 'test', 'noth', 'quick', 'evisc', 'chess', 'board', 'wouldnt', 'solv', 'quiet', 'lam'], ['last', 'ltr', 'intelligent', 'woman', 'prosecutor', 'attest', 'everything', 'write', 'spot', 'behaviour', 'mine', 'find', 'trp', 'relationship', 'fail', 'manner', '\\n\\nfollowing', 'doesnt', 'mean', 'womens', 'quote', 'original', 'article', 'actually', 'fair', 'miniscule', 'sliver', 'men', 'red', 'pill', 'traits', 'learn', 'inherent', 'women', 'self', 'identify', 'intelligent', 'man', 'cant', 'stand', 'shit', 'go', 'perceive', 'intellectually', 'weaker', 'whether', 'astrophysicist', '\\n\\nthis', 'another', 'shit', 'test', 'nothing', 'quick', 'evisceration', 'chess', 'board', 'wouldnt', 'solve', 'quiet', 'lamentations'])\n",
      "original document: \n",
      "['[link', 'to', 'a', 'similar', 'post', '10', 'days', 'ago](https://www.reddit.com/r/SquaredCircle/comments/71dun1/wwe_survey_sent_out_roh_tv14_inring_show_local/)\\n\\nI', 'would', 'really', 'love', 'to', 'be', 'able', 'to', 'get', 'NJPW/RoH/WWE/etc', 'all', 'in', 'one', 'place', 'for', 'one', 'price', 'rather', 'than', 'multiple', 'sites.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['link', 'simil', 'post', 'ten', 'day', 'agohttpswwwredditcomrsquaredcirclecomments71dun1wwe_survey_sent_out_roh_tv14_inring_show_local\\n\\ni', 'would', 'real', 'lov', 'abl', 'get', 'njpwrohwweetc', 'on', 'plac', 'on', 'pric', 'rath', 'multipl', 'sit'], ['link', 'similar', 'post', 'ten', 'days', 'agohttpswwwredditcomrsquaredcirclecomments71dun1wwe_survey_sent_out_roh_tv14_inring_show_local\\n\\ni', 'would', 'really', 'love', 'able', 'get', 'njpwrohwweetc', 'one', 'place', 'one', 'price', 'rather', 'multiple', 'sit'])\n",
      "original document: \n",
      "['I', 'really', 'hope', '', 'that', 'boys', 'parents', 'see', 'this', 'video...so', 'his', 'dad', 'can', 'send', 'it', 'to', 'every', 'scout', 'in', 'the', 'major', 'leagues', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'hop', 'boy', 'par', 'see', 'videoso', 'dad', 'send', 'every', 'scout', 'maj', 'leagu'], ['really', 'hope', 'boys', 'parent', 'see', 'videoso', 'dad', 'send', 'every', 'scout', 'major', 'league'])\n",
      "original document: \n",
      "['I', 'say', 'we', 'sacrifice', 'White', 'to', 'the', 'Lord', 'of', 'Light']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'sacr', 'whit', 'lord', 'light'], ['say', 'sacrifice', 'white', 'lord', 'light'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['No,', \"it's\", 'probably', 'portillo.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'portillo'], ['probably', 'portillo'])\n",
      "original document: \n",
      "['In', 'a', 'heartbeat', 'but', 'i', 'doubt', 'anyone', 'bites', 'for', 'that', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heartb', 'doubt', 'anyon', 'bit', '\\n'], ['heartbeat', 'doubt', 'anyone', 'bite', '\\n'])\n",
      "original document: \n",
      "['I', 'think', 'this', 'is', 'the', 'correct', 'course', 'of', 'action.', 'Marincin', 'has', 'effectively', 'played', 'his', 'way', 'off', 'our', 'roster', 'and', 'needs', 'to', 'be', 'removed.', 'I', 'don’t', 'care', 'how', 'much', 'of', 'an', 'analytical', 'darling', 'he', 'is.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'correct', 'cours', 'act', 'marincin', 'effect', 'play', 'way', 'rost', 'nee', 'remov', 'dont', 'car', 'much', 'analys', 'darl'], ['think', 'correct', 'course', 'action', 'marincin', 'effectively', 'play', 'way', 'roster', 'need', 'remove', 'dont', 'care', 'much', 'analytical', 'darling'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['What', 'would', 'you', 'say', 'is', 'the', 'perfect', 'amount', 'of', 'almonds?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'say', 'perfect', 'amount', 'almond'], ['would', 'say', 'perfect', 'amount', 'almonds'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Welcome', 'to', 'IOTA.', '+200', 'IOTA']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welcom', 'iot', 'two hundred', 'iot'], ['welcome', 'iota', 'two hundred', 'iota'])\n",
      "original document: \n",
      "['K', '&amp;', 'R', 'is', 'an', 'awesome', 'book.', 'But', 'you', 'should', 'know', 'some', 'C', 'first.', '\\n\\nMy', 'favorite', 'beginner', 'C', 'book', 'is', '[C', 'By', 'Discovery', 'by', 'Foster', '&amp;', 'Foster.](http://catalog.lib.utexas.edu/search/0?searchtype=o&amp;searcharg=63168068)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['k', 'amp', 'r', 'awesom', 'book', 'know', 'c', 'first', '\\n\\nmy', 'favorit', 'begin', 'c', 'book', 'c', 'discovery', 'fost', 'amp', 'fosterhttpcataloglibutexasedusearch0searchtypeoampsearcharg63168068'], ['k', 'amp', 'r', 'awesome', 'book', 'know', 'c', 'first', '\\n\\nmy', 'favorite', 'beginner', 'c', 'book', 'c', 'discovery', 'foster', 'amp', 'fosterhttpcataloglibutexasedusearch0searchtypeoampsearcharg63168068'])\n",
      "original document: \n",
      "['That', 'you', 'think', 'that', 'proves', \"it's\", 'not.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'prov'], ['think', 'prove'])\n",
      "original document: \n",
      "['The', 'caps', 'are', 'probably', 'duds.', '50mg', 'and', 'you', 'feel', 'nothing', \"isn't\", 'normal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cap', 'prob', 'dud', '50mg', 'feel', 'noth', 'isnt', 'norm'], ['cap', 'probably', 'duds', '50mg', 'feel', 'nothing', 'isnt', 'normal'])\n",
      "original document: \n",
      "['Does', 'anyone', 'have', 'any', 'tips', 'on', 'using', 'the', 'work', 'sharp?', 'I', 'have', 'the', 'ken', 'onion', 'edition.', 'It', 'seems', 'like', 'no', 'matter', 'what', 'I', 'do', 'I', 'cannot', 'get', 'the', 'blade', 'sharp', 'enough', 'to', 'shave', 'with.', \"\\n\\nI'm\", 'starting', 'to', 'look', 'at', 'getting', 'the', 'wicked', 'edge', 'system', 'but', 'I', 'figured', 'I', 'would', 'look', 'for', 'some', 'pointers', 'before', 'abandoning', 'it', 'completely.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'tip', 'us', 'work', 'sharp', 'ken', 'on', 'edit', 'seem', 'lik', 'mat', 'cannot', 'get', 'blad', 'sharp', 'enough', 'shav', '\\n\\nim', 'start', 'look', 'get', 'wick', 'edg', 'system', 'fig', 'would', 'look', 'point', 'abandon', 'complet'], ['anyone', 'tip', 'use', 'work', 'sharp', 'ken', 'onion', 'edition', 'seem', 'like', 'matter', 'cannot', 'get', 'blade', 'sharp', 'enough', 'shave', '\\n\\nim', 'start', 'look', 'get', 'wicked', 'edge', 'system', 'figure', 'would', 'look', 'pointers', 'abandon', 'completely'])\n",
      "original document: \n",
      "[\"It's\", 'a', 'sad,', 'sad', 'world...', 'Why', \"didn't\", 'I', 'buy', 'a', 'crate', 'when', 'I', 'turned', '18', 'and', 'they', 'were', 'less', 'than', '$200?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'sad', 'world', 'didnt', 'buy', 'crat', 'turn', 'eighteen', 'less', 'two hundred'], ['sad', 'sad', 'world', 'didnt', 'buy', 'crate', 'turn', 'eighteen', 'less', 'two hundred'])\n",
      "original document: \n",
      "['Yes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['At', 'least', 'there', 'won’t', 'be', 'a', 'MAC', 'school', 'going', 'undefeated', 'against', 'a', 'garbage', 'schedule']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'wont', 'mac', 'school', 'going', 'undef', 'garb', 'schedule'], ['least', 'wont', 'mac', 'school', 'go', 'undefeated', 'garbage', 'schedule'])\n",
      "original document: \n",
      "['The', 'Internet.', 'You', 'use', 'it', 'every', 'day', 'but', 'how', 'often', 'do', 'you', 'consider', 'how', 'powerful', 'it', 'really', 'is?', 'All', 'things', 'considered', 'it', \"hasn't\", 'been', 'around', 'for', 'very', 'long,', 'yet', 'our', 'society', 'world', 'collapse', 'pretty', 'damn', 'quickly', 'if', 'we', 'ever', 'lost', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['internet', 'us', 'every', 'day', 'oft', 'consid', 'pow', 'real', 'thing', 'consid', 'hasnt', 'around', 'long', 'yet', 'socy', 'world', 'collaps', 'pretty', 'damn', 'quick', 'ev', 'lost'], ['internet', 'use', 'every', 'day', 'often', 'consider', 'powerful', 'really', 'things', 'consider', 'hasnt', 'around', 'long', 'yet', 'society', 'world', 'collapse', 'pretty', 'damn', 'quickly', 'ever', 'lose'])\n",
      "original document: \n",
      "['But', 'the', 'more', 'we', 'spam', 'them,', 'the', 'faster', 'he', 'will', 'get', 'confirmed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spam', 'fast', 'get', 'confirm'], ['spam', 'faster', 'get', 'confirm'])\n",
      "original document: \n",
      "['Neither.', 'Fine', 'as-is.', 'Fort', 'eats', 'enough', 'rounds', 'to', 'be', 'worth', 'the', 'increase,', 'but', 'it', \"isn't\", 'made', 'to', 'make', 'you', 'immortal.', 'The', '7.62x25mm', 'would', 'probably', 'be', 'able', 'to', 'knock', 'through', 'the', 'slightly-less-than', 'IIIa', 'protection', 'that', 'fort', 'provides,', 'with', 'a', 'few', 'rounds', 'driven', 'into', 'a', 'tight', 'grouping', 'IRL.\\n\\nIn', 'terms', 'of', 'game', 'mechanics,', \"it's\", 'possible', 'that', 'you', \"might've\", 'just', 'had', 'god-tier', 'RNG', 'and', 'gotten', '3', 'penetrating', 'shots', 'in', 'a', 'row', '(roughly', '.1%', 'chance', 'of', 'happening)', 'which', \"would've\", 'killed', 'him', 'through', 'the', 'armor,', 'plus', 'one', 'round', 'that', 'caught', 'on', 'the', 'armor.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['neith', 'fin', 'as', 'fort', 'eat', 'enough', 'round', 'wor', 'increas', 'isnt', 'mad', 'mak', 'immort', '762x25mm', 'would', 'prob', 'abl', 'knock', 'slightlylessth', 'ii', 'protect', 'fort', 'provid', 'round', 'driv', 'tight', 'group', 'irl\\n\\nin', 'term', 'gam', 'mech', 'poss', 'mightv', 'godty', 'rng', 'got', 'three', 'penet', 'shot', 'row', 'rough', 'on', 'chant', 'hap', 'wouldv', 'kil', 'arm', 'plu', 'on', 'round', 'caught', 'arm'], ['neither', 'fine', 'asis', 'fort', 'eat', 'enough', 'round', 'worth', 'increase', 'isnt', 'make', 'make', 'immortal', '762x25mm', 'would', 'probably', 'able', 'knock', 'slightlylessthan', 'iiia', 'protection', 'fort', 'provide', 'round', 'drive', 'tight', 'group', 'irl\\n\\nin', 'term', 'game', 'mechanics', 'possible', 'mightve', 'godtier', 'rng', 'get', 'three', 'penetrate', 'shots', 'row', 'roughly', 'one', 'chance', 'happen', 'wouldve', 'kill', 'armor', 'plus', 'one', 'round', 'catch', 'armor'])\n",
      "original document: \n",
      "['So...', 'fuck', 'women,', 'basically.\\r\\rOk.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'wom', 'basically\\r\\rok'], ['fuck', 'women', 'basically\\r\\rok'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'people', 'would', 'burn', 'the', 'flag', 'for', 'fun', 'here', 'in', 'the', 'UK', 'and', 'people', 'would', 'find', 'it', 'entertaining', 'instead', 'of', 'offensive.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'peopl', 'would', 'burn', 'flag', 'fun', 'uk', 'peopl', 'would', 'find', 'entertain', 'instead', 'offend'], ['feel', 'like', 'people', 'would', 'burn', 'flag', 'fun', 'uk', 'people', 'would', 'find', 'entertain', 'instead', 'offensive'])\n",
      "original document: \n",
      "['He', \"can't\", 'buy', 'fakes', 'he', 'gets', 'like', '3m', 'views', 'a', 'vid']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'buy', 'fak', 'get', 'lik', '3m', 'view', 'vid'], ['cant', 'buy', 'fake', 'get', 'like', '3m', 'view', 'vid'])\n",
      "original document: \n",
      "['I', 'think', 'a', 'lot', 'of', 'people', \"don't\", 'realize', 'this', 'about', 'indigenous', 'folks', 'is', 'that', 'a', 'lot', 'of', 'the', 'councils', 'are', 'insanely', 'corrupt.', '', 'Generally', 'any', 'money', 'made', 'on', 'reserve', 'usually', 'stays', 'among', 'the', 'council', 'and', 'their', 'families.', '', 'I', 'feel', 'sorry', 'for', 'those', 'people.', '', 'I', 'used', 'to', 'live', 'in', 'North', 'Western', 'Ontario', 'and', 'man', 'they', 'got', 'the', 'short', 'end', 'of', 'the', 'stick', 'from', 'both', 'the', 'government', 'and', 'their', 'own', 'people', 'who', 'were', 'supposed', 'to', 'be', 'their', '\"leaders\".\\n\\nMany', \"weren't\", 'allowed', 'to', 'drink', 'on', 'the', 'res', 'so', 'usually', 'once', 'a', 'month', 'when', 'the', 'cheques', 'came', 'in', \"they'd\", 'drive', 'to', 'the', 'nearest', 'town', 'that', 'had', 'an', 'LCBO,', 'Beer', 'Store,', 'and', 'Walmart/major', 'grocery', 'store', 'and', 'just', 'clear', 'the', 'places', 'out.', '', 'Two', 'things', \"wouldn't\", 'be', 'uncommon', 'to', 'see', 'at', 'the', 'local', 'hotels/motels', 'in', 'town', 'at', 'the', 'end', 'of', 'the', 'month', 'A.', 'Pick', 'up', 'trucks', 'full', 'of', 'food/supplies/etc', 'and', 'drunk/passed', 'out', 'natives', 'everywhere.', '', '\\n\\nNaturally', 'they', \"couldn't\", 'get', 'TOO', 'drunk', '(many', 'of', 'the', 'young', 'ones', 'would)', 'otherwise', 'they', 'ran', 'the', 'risk', 'of', 'someone', 'nosey', 'person', 'who', 'lived', 'in', 'town', '(in', 'small', 'towns', 'everyone', 'knew', 'everyone', 'even', 'if', 'they', 'lived', 'on', 'the', 'res)', 'to', 'call', 'the', 'council', 'leaders', 'and', 'rat', 'on', 'some', 'poor', 'drunk', 'native', 'kid.', '', '\\n\\nAdd', 'to', 'the', 'fact', 'that', 'they', 'would', 'jack', 'up', 'the', 'prices', 'on', 'all', 'the', 'food/supplies', 'on', 'the', 'res', 'because', 'a', 'lot', 'of', 'times', 'the', 'council', 'leaders', 'were', 'in', 'league', 'with', 'either', 'the', 'airline', 'that', 'would', 'fly', 'the', 'stuff', 'in', 'or', 'the', 'companies', 'that', 'would', 'truck', 'the', 'stuff', 'in', 'and', 'get', 'a', 'cut', 'of', 'the', 'profit', 'from', 'that.', '', '\\n\\nOverall', 'they', 'were', 'just', 'insanely', 'corrupt', 'and', 'treated', 'their', 'own', 'people', 'like', 'shit', 'and', 'then', 'just', 'told', 'them', 'it', 'was', 'the', 'white', 'man.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['think', 'lot', 'peopl', 'dont', 'real', 'indig', 'folk', 'lot', 'council', 'ins', 'corrupt', 'gen', 'money', 'mad', 'reserv', 'us', 'stay', 'among', 'council', 'famy', 'feel', 'sorry', 'peopl', 'us', 'liv', 'nor', 'western', 'ontario', 'man', 'got', 'short', 'end', 'stick', 'govern', 'peopl', 'suppos', 'leaders\\n\\nmany', 'wer', 'allow', 'drink', 'res', 'us', 'mon', 'chequ', 'cam', 'theyd', 'driv', 'nearest', 'town', 'lcbo', 'beer', 'stor', 'walmartmas', 'grocery', 'stor', 'clear', 'plac', 'two', 'thing', 'wouldnt', 'uncommon', 'see', 'loc', 'hotelsmotel', 'town', 'end', 'mon', 'pick', 'truck', 'ful', 'foodsuppliesetc', 'drunkpass', 'nat', 'everywh', '\\n\\nnaturally', 'couldnt', 'get', 'drunk', 'many', 'young', 'on', 'would', 'otherw', 'ran', 'risk', 'someon', 'nosey', 'person', 'liv', 'town', 'smal', 'town', 'everyon', 'knew', 'everyon', 'ev', 'liv', 'res', 'cal', 'council', 'lead', 'rat', 'poor', 'drunk', 'nat', 'kid', '\\n\\nadd', 'fact', 'would', 'jack', 'pric', 'foodsupply', 'res', 'lot', 'tim', 'council', 'lead', 'leagu', 'eith', 'airlin', 'would', 'fly', 'stuff', 'company', 'would', 'truck', 'stuff', 'get', 'cut', 'profit', '\\n\\noverall', 'ins', 'corrupt', 'tre', 'peopl', 'lik', 'shit', 'told', 'whit', 'man'], ['think', 'lot', 'people', 'dont', 'realize', 'indigenous', 'folks', 'lot', 'councils', 'insanely', 'corrupt', 'generally', 'money', 'make', 'reserve', 'usually', 'stay', 'among', 'council', 'families', 'feel', 'sorry', 'people', 'use', 'live', 'north', 'western', 'ontario', 'man', 'get', 'short', 'end', 'stick', 'government', 'people', 'suppose', 'leaders\\n\\nmany', 'werent', 'allow', 'drink', 'res', 'usually', 'month', 'cheque', 'come', 'theyd', 'drive', 'nearest', 'town', 'lcbo', 'beer', 'store', 'walmartmajor', 'grocery', 'store', 'clear', 'place', 'two', 'things', 'wouldnt', 'uncommon', 'see', 'local', 'hotelsmotels', 'town', 'end', 'month', 'pick', 'truck', 'full', 'foodsuppliesetc', 'drunkpassed', 'natives', 'everywhere', '\\n\\nnaturally', 'couldnt', 'get', 'drink', 'many', 'young', 'ones', 'would', 'otherwise', 'run', 'risk', 'someone', 'nosey', 'person', 'live', 'town', 'small', 'towns', 'everyone', 'know', 'everyone', 'even', 'live', 'res', 'call', 'council', 'leaders', 'rat', 'poor', 'drink', 'native', 'kid', '\\n\\nadd', 'fact', 'would', 'jack', 'price', 'foodsupplies', 'res', 'lot', 'time', 'council', 'leaders', 'league', 'either', 'airline', 'would', 'fly', 'stuff', 'company', 'would', 'truck', 'stuff', 'get', 'cut', 'profit', '\\n\\noverall', 'insanely', 'corrupt', 'treat', 'people', 'like', 'shit', 'tell', 'white', 'man'])\n",
      "original document: \n",
      "['Greed', 'mode', 'was', 'already', 'possible,', 'Greedier', 'is', 'what', 'was', 'unbalanced.', 'The', 'movement', 'was', 'fine', 'too', 'imo,', 'I', 'got', 'used', 'to', 'it', 'after', 'a', 'few', 'runs.\\n\\nThe', 'bug', 'fixes', 'that', 'made', 'the', 'game', 'litterally', 'unbeatable', 'are', 'what', 'are', 'really', 'big', 'here.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gree', 'mod', 'already', 'poss', 'greedy', 'unb', 'mov', 'fin', 'imo', 'got', 'us', 'runs\\n\\nthe', 'bug', 'fix', 'mad', 'gam', 'lit', 'unb', 'real', 'big'], ['greed', 'mode', 'already', 'possible', 'greedier', 'unbalance', 'movement', 'fine', 'imo', 'get', 'use', 'runs\\n\\nthe', 'bug', 'fix', 'make', 'game', 'litterally', 'unbeatable', 'really', 'big'])\n",
      "original document: \n",
      "['This', 'is', 'literally', 'how', 'he', 'campaigned.', 'What', 'is', 'wrong', 'with', 'you?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'campaign', 'wrong'], ['literally', 'campaign', 'wrong'])\n",
      "original document: \n",
      "['Ooh', \"that's\", 'nice', ':)', 'thank', 'you', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ooh', 'that', 'nic', 'thank'], ['ooh', 'thats', 'nice', 'thank'])\n",
      "original document: \n",
      "['do', 'you', 'buy', 'the', 'pink', 'ones', 'just', 'in', 'the', 'store?', \"I'm\", 'looking', 'to', 'over', 'haul', 'my', 'brushes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['buy', 'pink', 'on', 'stor', 'im', 'look', 'haul', 'brush'], ['buy', 'pink', 'ones', 'store', 'im', 'look', 'haul', 'brush'])\n",
      "original document: \n",
      "['When', 'someone', 'asks', 'for', 'help', 'they', 'probably', 'already', 'tried', 'it', 'by', 'themselves.', 'No', 'need', 'to', 'go', 'elitist', 'even', 'as', 'a', 'joke.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someon', 'ask', 'help', 'prob', 'already', 'tri', 'nee', 'go', 'elit', 'ev', 'jok'], ['someone', 'ask', 'help', 'probably', 'already', 'try', 'need', 'go', 'elitist', 'even', 'joke'])\n",
      "original document: \n",
      "['DAE', 'think', 'we', 'should', 'just', 'brand', 'users', 'of', 'subs', 'we', \"don't\", 'like', 'with', 'a', 'universal', 'flair', 'so', 'we', 'know', 'their', 'subjective', 'opinions', 'are', 'wrong', 'by', 'default?', '😂']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dae', 'think', 'brand', 'us', 'sub', 'dont', 'lik', 'univers', 'flair', 'know', 'subject', 'opin', 'wrong', 'default'], ['dae', 'think', 'brand', 'users', 'sub', 'dont', 'like', 'universal', 'flair', 'know', 'subjective', 'opinions', 'wrong', 'default'])\n",
      "original document: \n",
      "['gotta', 'maximize', 'dat', 'karma', 'homie']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gott', 'maxim', 'dat', 'karm', 'hom'], ['gotta', 'maximize', 'dat', 'karma', 'homie'])\n",
      "original document: \n",
      "['Any', 'two', 'episodes.', 'Each', 'episode', 'has', 'its', 'own', \"'score'.\", 'I', 'think', 'the', 'term', 'is', \"'music\", \"bed',\", 'which', 'is', 'different', 'from', \"'cue',\", 'but', 'I', 'may', 'be', 'wrong', 'on', 'the', 'terms.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'episod', 'episod', 'scor', 'think', 'term', 'mus', 'bed', 'diff', 'cue', 'may', 'wrong', 'term'], ['two', 'episodes', 'episode', 'score', 'think', 'term', 'music', 'bed', 'different', 'cue', 'may', 'wrong', 'term'])\n",
      "original document: \n",
      "['', 'roughly', 'translates', 'to', '\"I', 'love', 'to', 'hockey\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rough', 'transl', 'lov', 'hockey'], ['roughly', 'translate', 'love', 'hockey'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'have', 'a', 'very', 'strong', 'feeling', 'against', 'fusing', 'limited', 'units', 'ever.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['strong', 'feel', 'fus', 'limit', 'unit', 'ever\\n\\n'], ['strong', 'feel', 'fuse', 'limit', 'units', 'ever\\n\\n'])\n",
      "original document: \n",
      "['In', 'that', 'case', 'make', 'sure', 'you', \"haven't\", 'just', 'popped', 'a', 'hemorrhoid.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cas', 'mak', 'sur', 'hav', 'pop', 'hemorrhoid'], ['case', 'make', 'sure', 'havent', 'pop', 'hemorrhoid'])\n",
      "original document: \n",
      "['Definitely', 'not', 'mono-fire.', \"It's\", 'just', 'weaker', 'and', 'you', 'get', 'a', 'lot', 'of', 'mileage', 'from', 'a', 'second', 'faction', '(either', 'shadow', 'or', 'justice).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'monofir', 'weak', 'get', 'lot', 'mil', 'second', 'fact', 'eith', 'shadow', 'just'], ['definitely', 'monofire', 'weaker', 'get', 'lot', 'mileage', 'second', 'faction', 'either', 'shadow', 'justice'])\n",
      "original document: \n",
      "['When', 'virtue', 'signaling', 'and', 'dating', 'cross', 'streams.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['virtu', 'sign', 'dat', 'cross', 'streams'], ['virtue', 'signal', 'date', 'cross', 'stream'])\n",
      "original document: \n",
      "['Hope', 'someone', 'figures', 'out', 'which', 'Democrats', 'have', 'ties', 'with', 'this', 'union', 'leader!', 'This', 'sounds', 'like', 'a', 'setup', 'to', 'make', 'President', 'Trump', 'look', 'bad.', 'They', \"couldn't\", 'find', 'much', 'to', 'criticize', 'him', 'for', 'with', 'the', 'other', 'hurricanes,', 'the', 'Russia', 'narrative', 'is', 'falling', 'apart,', 'etc', 'and', 'they', 'needed', 'something', 'to', 'trash', 'him', 'for', 'that', 'people', 'would', 'care', 'about', '(obviously', 'nobody', 'gives', 'a', 'shit', 'about', 'the', 'NFL).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'someon', 'fig', 'democr', 'tie', 'un', 'lead', 'sound', 'lik', 'setup', 'mak', 'presid', 'trump', 'look', 'bad', 'couldnt', 'find', 'much', 'crit', 'hur', 'russ', 'nar', 'fal', 'apart', 'etc', 'nee', 'someth', 'trash', 'peopl', 'would', 'car', 'obvy', 'nobody', 'giv', 'shit', 'nfl'], ['hope', 'someone', 'figure', 'democrats', 'tie', 'union', 'leader', 'sound', 'like', 'setup', 'make', 'president', 'trump', 'look', 'bad', 'couldnt', 'find', 'much', 'criticize', 'hurricanes', 'russia', 'narrative', 'fall', 'apart', 'etc', 'need', 'something', 'trash', 'people', 'would', 'care', 'obviously', 'nobody', 'give', 'shit', 'nfl'])\n",
      "original document: \n",
      "['The', 'environmental', 'cost', 'of', 'earplugs', 'adds', 'up', 'over', 'time,', 'both', 'in', 'terms', 'of', 'the', 'actual', 'item', 'and', 'the', 'packaging.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['environ', 'cost', 'earplug', 'ad', 'tim', 'term', 'act', 'item', 'pack'], ['environmental', 'cost', 'earplugs', 'add', 'time', 'term', 'actual', 'item', 'package'])\n",
      "original document: \n",
      "['Oakland', 'Raiders', 'fans', 'love', 'to', 'watch', 'their', 'team', 'lose', 'games.', \"They're\", 'Oakland', 'Raiders', 'fans,', 'after', 'all.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oakland', 'raid', 'fan', 'lov', 'watch', 'team', 'los', 'gam', 'theyr', 'oakland', 'raid', 'fan'], ['oakland', 'raiders', 'fan', 'love', 'watch', 'team', 'lose', 'game', 'theyre', 'oakland', 'raiders', 'fan'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Love', 'the', 'podcast']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'podcast'], ['love', 'podcast'])\n",
      "original document: \n",
      "['He', 'just', 'abuses', 'tackles', 'with', 'power', 'and', 'then', 'when', 'they', 'start', 'bracing', 'themselves', 'for', 'it', 'he', 'flies', 'around', 'them', 'with', 'speed', 'and', 'bend.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['abus', 'tackl', 'pow', 'start', 'brac', 'fli', 'around', 'spee', 'bend'], ['abuse', 'tackle', 'power', 'start', 'brace', 'fly', 'around', 'speed', 'bend'])\n",
      "original document: \n",
      "['Goku', 'was', 'way', 'beyond', 'Frieza.', '', 'Goku', 'held', 'back', 'because', 'he', 'wanted', 'a', 'good', 'fight.', '', 'Frieza', 'was', 'at', '120', 'million', 'and', 'Goku', 'was', 'at', '150', 'million.', '', 'Goku', 'since', 'then', 'got', 'stronger', 'so', 'it', 'makes', 'sense', \"he'd\", 'be', 'above', 'Cooler.', '', 'Although', 'I', 'would', 'have', 'liked', 'a', 'better', 'fight', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goku', 'way', 'beyond', 'friez', 'goku', 'held', 'back', 'want', 'good', 'fight', 'friez', 'one hundred and twenty', 'mil', 'goku', 'one hundred and fifty', 'mil', 'goku', 'sint', 'got', 'stronger', 'mak', 'sens', 'hed', 'cool', 'although', 'would', 'lik', 'bet', 'fight'], ['goku', 'way', 'beyond', 'frieza', 'goku', 'hold', 'back', 'want', 'good', 'fight', 'frieza', 'one hundred and twenty', 'million', 'goku', 'one hundred and fifty', 'million', 'goku', 'since', 'get', 'stronger', 'make', 'sense', 'hed', 'cooler', 'although', 'would', 'like', 'better', 'fight'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'on', 'the', 'Zarvox', 'level', 'of', 'Elite', 'RMT.', 'I', 'only', 'make', 'about', '$7', 'per', 'day.', 'Kappa']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'zarvox', 'level', 'elit', 'rmt', 'mak', 'sev', 'per', 'day', 'kapp'], ['im', 'zarvox', 'level', 'elite', 'rmt', 'make', 'seven', 'per', 'day', 'kappa'])\n",
      "original document: \n",
      "['143418344|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'dO1vilZS)\\n\\n&gt;&gt;143417804\\nWhy', 'have', 'you', 'gotten', 'more', 'liberal?', \"I've\", 'gotten', 'more', 'conservative,', 'but', 'still', 'voted', 'hillary/obama/obama\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, three hundred and forty-four', 'gt', 'unit', 'stat', 'anonym', 'id', 'do1vilzs\\n\\ngtgt143417804\\nwhy', 'got', 'lib', 'iv', 'got', 'conserv', 'stil', 'vot', 'hillaryobamaobama\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, three hundred and forty-four', 'gt', 'unite', 'state', 'anonymous', 'id', 'do1vilzs\\n\\ngtgt143417804\\nwhy', 'get', 'liberal', 'ive', 'get', 'conservative', 'still', 'vote', 'hillaryobamaobama\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Haha,', 'you', \"didn't\", 'buy', 'mine.', 'I', 'listed', 'mine', 'for', '850k', 'and', 'it', 'sold', 'a', 'little', 'while', 'ago', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'didnt', 'buy', 'min', 'list', 'min', '850k', 'sold', 'littl', 'ago'], ['haha', 'didnt', 'buy', 'mine', 'list', 'mine', '850k', 'sell', 'little', 'ago'])\n",
      "original document: \n",
      "['Evil', 'within', '2,', 'Gran', 'Turismo,', 'forza', '7,', 'ac', 'origins,', 'cod', 'ww2,', 'need', 'for', 'speed,', 'battlefront,', 'Wolfenstein,', 'ni', 'no', 'kuni,', 'Mario,', 'Southpark...', 'Probably', 'more']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['evil', 'within', 'two', 'gran', 'turismo', 'forz', 'sev', 'ac', 'origin', 'cod', 'ww2', 'nee', 'spee', 'battlefront', 'wolfenstein', 'ni', 'kun', 'mario', 'southpark', 'prob'], ['evil', 'within', 'two', 'gran', 'turismo', 'forza', 'seven', 'ac', 'origins', 'cod', 'ww2', 'need', 'speed', 'battlefront', 'wolfenstein', 'ni', 'kuni', 'mario', 'southpark', 'probably'])\n",
      "original document: \n",
      "['I', 'wanna', 'see', 'dat\\n\\nMcCaw', '-', 'Swaggy', 'P', '-', 'Iguodala', '-', 'Casspi', '-', 'Draymond', 'lineup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wann', 'see', 'dat\\n\\nmccaw', 'swaggy', 'p', 'iguodal', 'cassp', 'draymond', 'lineup'], ['wanna', 'see', 'dat\\n\\nmccaw', 'swaggy', 'p', 'iguodala', 'casspi', 'draymond', 'lineup'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Consider', 'the', 'situation', 'President', 'Clinton', 'inherited.', '', 'America', 'at', 'peace,', 'prosperous,', 'and', 'an', 'unchallenged', 'colossus', 'on', 'the', 'world', 'stage.', '', 'What', 'happened?', '', 'These', 'three', 'guys.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['consid', 'situ', 'presid', 'clinton', 'inherit', 'americ', 'peac', 'prosp', 'unchalleng', 'coloss', 'world', 'stag', 'hap', 'three', 'guy'], ['consider', 'situation', 'president', 'clinton', 'inherit', 'america', 'peace', 'prosperous', 'unchallenged', 'colossus', 'world', 'stage', 'happen', 'three', 'guy'])\n",
      "original document: \n",
      "['You', 'do', 'have', 'a', 'point,', 'he', 'is', 'a', '\"public\"', 'person', 'and', 'in', 'his', 'position/experience,', 'probably', 'he', 'is', 'used', 'to', 'all', 'the', 'flame/hate', 'and', 'controls', 'himself', 'in', 'front', 'of', 'the', 'camera.', 'Maybe', 'he', 'curse', 'us', 'on', 'his', 'way', 'home.\\n\\nBut', 'do', 'we', 'need', 'this?', 'Should', 'we', 'keep', 'doing', '\"our', 'part\"', 'and', 'flame', 'him,', 'so', 'he', 'does', 'his', 'part', 'and', 'keep', 'this', 'mind', 'game', 'going?', 'What', 'I', 'need', 'to', 'understand', 'is', 'what', 'benefits', 'we', 'get', 'from', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['point', 'publ', 'person', 'positionexpery', 'prob', 'us', 'flameh', 'control', 'front', 'camer', 'mayb', 'curs', 'us', 'way', 'home\\n\\nbut', 'nee', 'keep', 'part', 'flam', 'part', 'keep', 'mind', 'gam', 'going', 'nee', 'understand', 'benefit', 'get'], ['point', 'public', 'person', 'positionexperience', 'probably', 'use', 'flamehate', 'control', 'front', 'camera', 'maybe', 'curse', 'us', 'way', 'home\\n\\nbut', 'need', 'keep', 'part', 'flame', 'part', 'keep', 'mind', 'game', 'go', 'need', 'understand', 'benefit', 'get'])\n",
      "original document: \n",
      "['**PLEASE', 'READ', 'THIS', 'MESSAGE', 'IN', 'ITS', 'ENTIRETY', 'BEFORE', 'TAKING', 'ACTION.**\\n\\nHi', 'there,', 'your', 'post', 'has', 'been', 'removed', 'for', 'one', 'of', 'the', 'following', 'reasons:', '\\n\\n*', '[Rule', '3:](https://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_3-)', 'Askreddit', 'is', 'for', 'open-ended', 'discussion', 'questions.', 'If', \"you've\", 'posted', 'a', 'question', 'that', 'could', 'be', 'answered', 'with', 'just', 'yes', 'or', 'no,', 'it', 'needs', 'to', 'explicitly', 'ask', 'for', 'more', 'discussion', 'like', 'asking', '\"What\\'s', 'the', 'story\"', 'or', '\"Why', 'or', 'why', 'not?\"', '', 'Also,', 'questions', 'with', 'a', 'single', 'correct', 'answer,', 'that', 'can', 'be', 'researched', 'elsewhere', 'or', 'provide', 'a', 'limited', 'scope', 'for', 'discussion', '(yes/no,', 'DAE,', 'polls', 'etc.)', 'are', 'not', 'appropriate.', '', '\\n\\n*', '[Rule', '1:](http://www.reddit.com/r/AskReddit/wiki/index#wiki_-rule_1-)', 'You', 'must', 'post', 'a', 'clear', 'and', 'direct', 'question', 'in', 'the', 'title.', '\\n\\nIf', 'you', 'have', 'any', 'queries', 'or', 'concerns,', 'please', 'feel', 'free', 'to', '[contact', 'the', 'mods](http://www.reddit.com/message/compose?to=%2Fr%2FAskReddit&amp;subject=Yes/No+Related+Post+Review+Request&amp;message=My+post+was+removed,+automoderator+said+Rule+1+or+3,+please', 'review%3a%0d%0a%0d%0ahttp%3A%2F%2Fwww.reddit.com/r/AskReddit/comments/73igbd/has_you_or_anyone_you_know_ever_try_human_meat/).', 'Thank', 'you.\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/AskReddit)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'read', 'mess', 'entir', 'tak', 'action\\n\\nhi', 'post', 'remov', 'on', 'follow', 'reason', '\\n\\n', 'rul', '3httpswwwredditcomraskredditwikiindexwiki_rule_3', 'askreddit', 'openend', 'discuss', 'quest', 'youv', 'post', 'quest', 'could', 'answ', 'ye', 'nee', 'explicit', 'ask', 'discuss', 'lik', 'ask', 'what', 'story', 'also', 'quest', 'singl', 'correct', 'answ', 'research', 'elsewh', 'provid', 'limit', 'scop', 'discuss', 'yesno', 'dae', 'pol', 'etc', 'appropry', '\\n\\n', 'rul', '1httpwwwredditcomraskredditwikiindexwiki_rule_1', 'must', 'post', 'clear', 'direct', 'quest', 'titl', '\\n\\nif', 'query', 'concern', 'pleas', 'feel', 'fre', 'contact', 'modshttpwwwredditcommessagecomposeto2fr2faskredditampsubjectyesnorelatedpostreviewrequestampmessagemypostwasremovedautomoderatorsaidrule1or3please', 'review3a0d0a0d0ahttp3a2f2fwwwredditcomraskredditcomments73igbdhas_you_or_anyone_you_know_ever_try_human_meat', 'thank', 'you\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoraskreddit', 'quest', 'concern'], ['please', 'read', 'message', 'entirety', 'take', 'action\\n\\nhi', 'post', 'remove', 'one', 'follow', 'reason', '\\n\\n', 'rule', '3httpswwwredditcomraskredditwikiindexwiki_rule_3', 'askreddit', 'openended', 'discussion', 'question', 'youve', 'post', 'question', 'could', 'answer', 'yes', 'need', 'explicitly', 'ask', 'discussion', 'like', 'ask', 'whats', 'story', 'also', 'question', 'single', 'correct', 'answer', 'research', 'elsewhere', 'provide', 'limit', 'scope', 'discussion', 'yesno', 'dae', 'poll', 'etc', 'appropriate', '\\n\\n', 'rule', '1httpwwwredditcomraskredditwikiindexwiki_rule_1', 'must', 'post', 'clear', 'direct', 'question', 'title', '\\n\\nif', 'query', 'concern', 'please', 'feel', 'free', 'contact', 'modshttpwwwredditcommessagecomposeto2fr2faskredditampsubjectyesnorelatedpostreviewrequestampmessagemypostwasremovedautomoderatorsaidrule1or3please', 'review3a0d0a0d0ahttp3a2f2fwwwredditcomraskredditcomments73igbdhas_you_or_anyone_you_know_ever_try_human_meat', 'thank', 'you\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoraskreddit', 'question', 'concern'])\n",
      "original document: \n",
      "['This', 'gif', 'could', 'go', 'in', 'so', 'many', 'places.', 'Sure', 'her', 'falling', 'is', 'expected,', 'but', 'him', 'kicking', 'the', 'fence', 'is', '/r/unexpected', 'and', 'also', '/r/IdiotsFightingThings', '.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gif', 'could', 'go', 'many', 'plac', 'sur', 'fal', 'expect', 'kick', 'fent', 'runexpect', 'also', 'ridiotsfightingth'], ['gif', 'could', 'go', 'many', 'place', 'sure', 'fall', 'expect', 'kick', 'fence', 'runexpected', 'also', 'ridiotsfightingthings'])\n",
      "original document: \n",
      "['The', 'full', 'quote', 'just', 'confused', 'me', 'even', 'more.', 'That', 'did', 'nothing', 'to', 'clear', 'up', 'what', 'he', 'meant.', 'It', 'looks', 'like', 'several', 'half', 'thoughts', 'thrown', 'together.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ful', 'quot', 'confus', 'ev', 'noth', 'clear', 'meant', 'look', 'lik', 'sev', 'half', 'thought', 'thrown', 'togeth'], ['full', 'quote', 'confuse', 'even', 'nothing', 'clear', 'mean', 'look', 'like', 'several', 'half', 'thoughts', 'throw', 'together'])\n",
      "original document: \n",
      "['Cheers', 'G!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['che', 'g'], ['cheer', 'g'])\n",
      "original document: \n",
      "['I', 'have', 'literally', 'seen', 'this', 'repost', '5', 'times', 'this', 'week']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'seen', 'repost', 'fiv', 'tim', 'week'], ['literally', 'see', 'repost', 'five', 'time', 'week'])\n",
      "original document: \n",
      "['Bizarre', 'in', 'the', 'variation', 'between', 'cinemas', 'as', 'well.', 'Vue', 'on', 'Fulham', 'Broadway', 'is', '£6.99', 'for', 'any', 'film', 'any', 'day', 'of', 'the', 'week.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bizar', 'vary', 'cinema', 'wel', 'vue', 'fulham', 'broadway', 'six hundred and ninety-nine', 'film', 'day', 'week'], ['bizarre', 'variation', 'cinemas', 'well', 'vue', 'fulham', 'broadway', 'six hundred and ninety-nine', 'film', 'day', 'week'])\n",
      "original document: \n",
      "[\"I'm\", 'so', 'sorry', 'for', 'your', 'loss,', \"it's\", 'always', 'hard', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'loss', 'alway', 'hard'], ['im', 'sorry', 'loss', 'always', 'hard'])\n",
      "original document: \n",
      "['Wait', '?', 'The', 'government', 'does', 'not', 'need', '50%', 'of', 'the', 'seats', 'in', 'Parliament', '?', 'Are', 'you', 'sure', '?', 'We', 'can', 'have', 'that', 'over', 'here', 'in', 'Germany,', 'but', \"that's\", 'a', 'big', 'exception', 'to', 'happen', 'and', 'you', 'need', '50%', 'to', 'pass', 'any', 'simple', 'laws.\\n\\nAlso,', 'thanks', 'for', 'answering.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'govern', 'nee', 'fifty', 'seat', 'parlia', 'sur', 'germany', 'that', 'big', 'exceiv', 'hap', 'nee', 'fifty', 'pass', 'simpl', 'laws\\n\\nalso', 'thank', 'answ'], ['wait', 'government', 'need', 'fifty', 'seat', 'parliament', 'sure', 'germany', 'thats', 'big', 'exception', 'happen', 'need', 'fifty', 'pass', 'simple', 'laws\\n\\nalso', 'thank', 'answer'])\n",
      "original document: \n",
      "['It’s', 'good', 'to', 'do', 'both,', 'I', 'really', 'like', 'writing', 'away', 'from', 'picture,', 'especially', 'at', 'the', 'beginning.', 'But', 'I', 'still', 'feel', 'like', 'you’ll', 'learn', 'more', 'as', 'a', 'composer', 'by', 'trying', 'to', 'write', 'through', 'a', 'scene', 'than', 'if', 'you', 'wrote', 'the', 'music', 'first', 'and', 'tried', 'to', 'find', 'a', 'home', 'for', 'it.', '\\n\\nEdit:', 'autocorrect!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'real', 'lik', 'writ', 'away', 'pict', 'espec', 'begin', 'stil', 'feel', 'lik', 'youl', 'learn', 'compos', 'try', 'writ', 'scen', 'wrot', 'mus', 'first', 'tri', 'find', 'hom', '\\n\\nedit', 'autocorrect'], ['good', 'really', 'like', 'write', 'away', 'picture', 'especially', 'begin', 'still', 'feel', 'like', 'youll', 'learn', 'composer', 'try', 'write', 'scene', 'write', 'music', 'first', 'try', 'find', 'home', '\\n\\nedit', 'autocorrect'])\n",
      "original document: \n",
      "['Brilliant', 'rebuttal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bril', 'rebut'], ['brilliant', 'rebuttal'])\n",
      "original document: \n",
      "['Supporting', 'Real', 'Madrid', 'has', 'that', 'depressing', 'effect', 'on', 'people.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['support', 'real', 'madrid', 'depress', 'effect', 'peopl'], ['support', 'real', 'madrid', 'depress', 'effect', 'people'])\n",
      "original document: \n",
      "['Farsi', 'means', 'persian.', 'Just', 'like', 'Persian', 'can', 'refer', 'to', 'the', 'language', 'and', 'the', 'people,', 'Farsi', 'is', 'the', 'same', 'way']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fars', 'mean', 'pers', 'lik', 'pers', 'ref', 'langu', 'peopl', 'fars', 'way'], ['farsi', 'mean', 'persian', 'like', 'persian', 'refer', 'language', 'people', 'farsi', 'way'])\n",
      "original document: \n",
      "['If', \"you're\", 'be', 'the', 'one', 'to', 'do', 'this,', 'at', 'least', 'have', 'something', 'to', 'say', 'about', 'the', 'shot', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'on', 'least', 'someth', 'say', 'shot'], ['youre', 'one', 'least', 'something', 'say', 'shoot'])\n",
      "original document: \n",
      "['refund', 'me', 'for', 'all', 'the', 'garbage', 'DLC', 'content', 'i', 'bought', 'at', 'full', 'price.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['refund', 'garb', 'dlc', 'cont', 'bought', 'ful', 'pric'], ['refund', 'garbage', 'dlc', 'content', 'buy', 'full', 'price'])\n",
      "original document: \n",
      "['Colo', 'kanjuro', 'can', 'do', 'it', 'for', 'free', 'spirit', 'and', 'striker', 'teams.', 'RR', 'Fukuro', 'can', 'be', 'used', 'convert', 'qck', 'orbs', 'on', 'fighter', 'teams', '(works', 'well', 'with', 'gear', '4).', 'Haloween', 'Cora', 'can', 'convert', 'qck', 'into', 'matching', 'for', 'free', 'spirit', 'and', 'cerebral.', 'Also', 'the', 'new', 'RR', 'Heracles', 'can', 'convert', 'right', 'column', 'type', 'slots', 'into', 'matching', 'for', 'shooters', 'and', 'striker.\\n\\nEdit:', 'I', 'forgot', 'about', 'the', 'best', 'one', 'for', 'slashers,', 'RR', 'Onigumo.', 'Makes', 'STR/DEX/QCK', 'count', 'as', 'matching', 'for', 'driven', 'and', 'slashers,', 'easy', 'to', 'socket', 'and', 'relatively', 'low', 'base', 'CD.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['colo', 'kanjuro', 'fre', 'spirit', 'striker', 'team', 'rr', 'fukuro', 'us', 'convert', 'qck', 'orb', 'fight', 'team', 'work', 'wel', 'gear', 'four', 'haloween', 'cor', 'convert', 'qck', 'match', 'fre', 'spirit', 'cerebr', 'also', 'new', 'rr', 'herac', 'convert', 'right', 'column', 'typ', 'slot', 'match', 'shoot', 'striker\\n\\nedit', 'forgot', 'best', 'on', 'slash', 'rr', 'onigumo', 'mak', 'strdexqck', 'count', 'match', 'driv', 'slash', 'easy', 'socket', 'rel', 'low', 'bas', 'cd'], ['colo', 'kanjuro', 'free', 'spirit', 'striker', 'team', 'rr', 'fukuro', 'use', 'convert', 'qck', 'orb', 'fighter', 'team', 'work', 'well', 'gear', 'four', 'haloween', 'cora', 'convert', 'qck', 'match', 'free', 'spirit', 'cerebral', 'also', 'new', 'rr', 'heracles', 'convert', 'right', 'column', 'type', 'slot', 'match', 'shooters', 'striker\\n\\nedit', 'forget', 'best', 'one', 'slashers', 'rr', 'onigumo', 'make', 'strdexqck', 'count', 'match', 'drive', 'slashers', 'easy', 'socket', 'relatively', 'low', 'base', 'cd'])\n",
      "original document: \n",
      "['Updoot', 'for', 'Callahan.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['updoot', 'callah'], ['updoot', 'callahan'])\n",
      "original document: \n",
      "['I', 'spent', 'every', 'other', 'day', 'in', 'a', 'guitar', 'store', '\"just', 'looking\"', 'last', 'year', 'and', 'eventually', 'walked', 'out', 'with', 'my', 'dream', 'guitar', 'and', 'my', '#1,', 'a', 'Fender', 'Jaguar.', \"It's\", 'everything', \"I've\", 'ever', 'wanted', 'in', 'a', 'guitar', 'and', 'more.\\n\\n\\nBut', 'now', 'that', 'you', 'mention', 'it', 'I', '*could*', 'use', 'a', 'new', 'Humbucker', 'guitar...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spent', 'every', 'day', 'guit', 'stor', 'look', 'last', 'year', 'ev', 'walk', 'dream', 'guit', 'on', 'fend', 'jagu', 'everyth', 'iv', 'ev', 'want', 'guit', 'more\\n\\n\\nbut', 'ment', 'could', 'us', 'new', 'humbuck', 'guit'], ['spend', 'every', 'day', 'guitar', 'store', 'look', 'last', 'year', 'eventually', 'walk', 'dream', 'guitar', 'one', 'fender', 'jaguar', 'everything', 'ive', 'ever', 'want', 'guitar', 'more\\n\\n\\nbut', 'mention', 'could', 'use', 'new', 'humbucker', 'guitar'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'perfectly', 'logical', 'strategy', 'to', 'get', 'the', 'most', 'game', 'for', 'your', 'buck', 'but', 'lowering', 'their', 'day1', 'price', 'so', 'that', 'you', 'buy', 'the', 'game', 'earlier', \"isn't\", 'doing', 'the', 'studio', 'any', 'favors.', '', \"They're\", 'better', 'off', 'selling', 'at', 'full', 'price', 'initially', 'and', 'slowly', 'working', 'their', 'way', 'down', 'to', 'your', 'pricepoint.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'perfect', 'log', 'strategy', 'get', 'gam', 'buck', 'low', 'day1', 'pric', 'buy', 'gam', 'ear', 'isnt', 'studio', 'fav', 'theyr', 'bet', 'sel', 'ful', 'pric', 'init', 'slow', 'work', 'way', 'pricepoint'], ['thats', 'perfectly', 'logical', 'strategy', 'get', 'game', 'buck', 'lower', 'day1', 'price', 'buy', 'game', 'earlier', 'isnt', 'studio', 'favor', 'theyre', 'better', 'sell', 'full', 'price', 'initially', 'slowly', 'work', 'way', 'pricepoint'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'simple', 'man,', 'I', 'see', 'Mississippi', 'State,', 'I', 'root', 'for', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'simpl', 'man', 'see', 'mississipp', 'stat', 'root'], ['im', 'simple', 'man', 'see', 'mississippi', 'state', 'root'])\n",
      "original document: \n",
      "['FWIW', \"You're\", 'actually', \"what's\", 'known', 'as', 'a', '\"reverse', 'splitter,\"', 'not', 'a', '\"splitter\"', 'because', 'you', 'have', 'a', 'high', 'GPA/low', 'LSAT.', 'Cycles', 'are', 'hard', 'to', 'predict', 'for', 'splitters,', 'potentially', 'even', 'more', 'so', 'for', 'reverse', 'splitters.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fwiw', 'yo', 'act', 'what', 'known', 'revers', 'splitter', 'splitter', 'high', 'gpalow', 'lsat', 'cyc', 'hard', 'predict', 'splitters', 'pot', 'ev', 'revers', 'splitters'], ['fwiw', 'youre', 'actually', 'whats', 'know', 'reverse', 'splitter', 'splitter', 'high', 'gpalow', 'lsat', 'cycle', 'hard', 'predict', 'splitters', 'potentially', 'even', 'reverse', 'splitters'])\n",
      "original document: \n",
      "[\"It's\", 'a', 'couple', 'of', 'different', 'things:\\n\\nI', 'could', 'be', 'mistaken', 'here,', 'but', 'in', 'the', 'traditional', 'fighting', 'game', 'sense:\\n\\n1.', 'Tech', '=', '\"Throw', 'Tech\\'ing\"', 'or', 'breaking', 'and/or', 'escaping', 'a', 'throw.', '', \"You'll\", 'usually', 'hear', 'the', 'word', '\"throw\"', 'closely', 'associated', 'with', 'the', 'word', 'tech', 'to', 'seperate', 'it', 'from', 'the', 'second', 'definition:\\n\\n2.', 'Tech', '=', 'short', 'for', '\"Technology\"', 'or', 'a', 'method,', 'strategy,', 'or', 'technique', 'for', 'dealing', 'with', 'a', 'particular', 'situation.', '', \"It's\", 'slang', 'and', \"doesn't\", 'really', 'mean', 'anything', 'specifically', 'but', \"you'll\", 'hear', 'it', 'used', 'in', 'a', 'phrase', 'such', 'as:\\n\\n\"Did', 'you', 'see', 'that', 'new', 'reality', 'stone-Spiderman', 'tech?!', '', 'You', 'know,', 'the', \"'tech'\", 'where', 'he', 'can', 'keep', 'looping', 'the', 'freeze', 'into', 'web', 'ball', 'over', 'and', 'over', 'again', 'into', 'massive', 'combo', 'damage.\"\\n\\nFinally,', \"I'm\", 'not', 'a', 'Smash/Melee', 'player', 'but', 'I', 'think', '\"tech\"', 'means', 'something', 'slightly', 'different', 'there', 'where', 'it', 'has', 'more', 'to', 'do', 'with', 'ones', 'ability', 'to', 'do', 'certain', '(difficult)', 'techniques', 'on', 'the', 'pad.', '', 'Usually', 'it', 'follows', 'closely', 'with', 'the', 'second', 'definition', 'above,', 'and', 'contextually', 'it', 'usually', 'make', 'sense,', 'but', 'at', 'times', 'it', 'could', 'mean', 'something', 'slightly', 'different.\\n\\nTl;dr', '--', \"it's\", 'generally', 'the', 'second', 'definition', 'unless', 'the', 'word', '\"throw\"', 'is', 'used', 'closely', 'to', 'the', 'word', '\"Tech\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['coupl', 'diff', 'things\\n\\ni', 'could', 'mistak', 'tradit', 'fight', 'gam', 'sense\\n\\n1', 'tech', 'throw', 'tech', 'break', 'and', 'escap', 'throw', 'youl', 'us', 'hear', 'word', 'throw', 'clos', 'assocy', 'word', 'tech', 'sep', 'second', 'definition\\n\\n2', 'tech', 'short', 'technolog', 'method', 'strategy', 'techn', 'deal', 'particul', 'situ', 'slang', 'doesnt', 'real', 'mean', 'anyth', 'spec', 'youl', 'hear', 'us', 'phrase', 'as\\n\\ndid', 'see', 'new', 'real', 'stonespiderm', 'tech', 'know', 'tech', 'keep', 'loop', 'freez', 'web', 'bal', 'mass', 'combo', 'damage\\n\\nfinally', 'im', 'smashmel', 'play', 'think', 'tech', 'mean', 'someth', 'slight', 'diff', 'on', 'abl', 'certain', 'difficult', 'techn', 'pad', 'us', 'follow', 'clos', 'second', 'definit', 'context', 'us', 'mak', 'sens', 'tim', 'could', 'mean', 'someth', 'slight', 'different\\n\\ntldr', 'gen', 'second', 'definit', 'unless', 'word', 'throw', 'us', 'clos', 'word', 'tech'], ['couple', 'different', 'things\\n\\ni', 'could', 'mistake', 'traditional', 'fight', 'game', 'sense\\n\\n1', 'tech', 'throw', 'teching', 'break', 'andor', 'escape', 'throw', 'youll', 'usually', 'hear', 'word', 'throw', 'closely', 'associate', 'word', 'tech', 'seperate', 'second', 'definition\\n\\n2', 'tech', 'short', 'technology', 'method', 'strategy', 'technique', 'deal', 'particular', 'situation', 'slang', 'doesnt', 'really', 'mean', 'anything', 'specifically', 'youll', 'hear', 'use', 'phrase', 'as\\n\\ndid', 'see', 'new', 'reality', 'stonespiderman', 'tech', 'know', 'tech', 'keep', 'loop', 'freeze', 'web', 'ball', 'massive', 'combo', 'damage\\n\\nfinally', 'im', 'smashmelee', 'player', 'think', 'tech', 'mean', 'something', 'slightly', 'different', 'ones', 'ability', 'certain', 'difficult', 'techniques', 'pad', 'usually', 'follow', 'closely', 'second', 'definition', 'contextually', 'usually', 'make', 'sense', 'time', 'could', 'mean', 'something', 'slightly', 'different\\n\\ntldr', 'generally', 'second', 'definition', 'unless', 'word', 'throw', 'use', 'closely', 'word', 'tech'])\n",
      "original document: \n",
      "['Obama', 'was', 'stuck', 'between', 'a', 'rock', 'and', 'a', 'hard', 'place.', 'Prisoners', 'were', 'there', 'no', 'clue', 'if', 'Iran', 'would', 'have', 'even', 'considered', 'something', 'else,', 'and', \"there's\", 'no', 'clue', 'if', 'Iran', 'actually', 'helped', 'profit', 'the', 'terrorist', 'with', 'money,', 'plus', 'that', 'dug', 'us', 'in', 'more', 'debt', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['obam', 'stuck', 'rock', 'hard', 'plac', 'prison', 'clu', 'ir', 'would', 'ev', 'consid', 'someth', 'els', 'ther', 'clu', 'ir', 'act', 'help', 'profit', 'ter', 'money', 'plu', 'dug', 'us', 'debt'], ['obama', 'stick', 'rock', 'hard', 'place', 'prisoners', 'clue', 'iran', 'would', 'even', 'consider', 'something', 'else', 'theres', 'clue', 'iran', 'actually', 'help', 'profit', 'terrorist', 'money', 'plus', 'dig', 'us', 'debt'])\n",
      "original document: \n",
      "['yeah', 'even', 'in', 'the', 'title', 'he', 'calls', 'it', 'the', 'holidays', 'lol,', 'should', 'be', '\"Happy', 'Holidays\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'ev', 'titl', 'cal', 'holiday', 'lol', 'happy', 'holiday'], ['yeah', 'even', 'title', 'call', 'holiday', 'lol', 'happy', 'holiday'])\n",
      "original document: \n",
      "['Rand', 'Paul/Ben', 'Carson', '2024\\n\\n\"The', 'Doctors', 'will', 'see', 'you', 'now!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rand', 'paulb', 'carson', '2024\\n\\nthe', 'doct', 'see'], ['rand', 'paulben', 'carson', '2024\\n\\nthe', 'doctor', 'see'])\n",
      "original document: \n",
      "['r/keming', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rkem'], ['rkeming'])\n",
      "original document: \n",
      "['I', 'feel', 'physically', 'bad', 'for', 'that', 'woman.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'phys', 'bad', 'wom'], ['feel', 'physically', 'bad', 'woman'])\n",
      "original document: \n",
      "['She', 'was', 'used', 'to', 'get', 'a', 'horrible,', 'unfounded,', 'pizzagate', 'related', 'shitpost', 'to', 'the', 'front', 'page', '(with', 'all', 'the', 'shills', 'that', 'come', 'with', 'it,)', 'to', 'discredit', 'the', 'theory.', '\\n\\nIt', 'was', 'staged', 'by', 'the', \"anti's.\", '\\n\\nEdit:', 'TMoR', 'alert.', 'Fuck', 'off', 'TMoR.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'get', 'horr', 'unfound', 'pizz', 'rel', 'shitpost', 'front', 'pag', 'shil', 'com', 'discredit', 'the', '\\n\\nit', 'stag', 'ant', '\\n\\nedit', 'tmor', 'alert', 'fuck', 'tmor'], ['use', 'get', 'horrible', 'unfounded', 'pizzagate', 'relate', 'shitpost', 'front', 'page', 'shill', 'come', 'discredit', 'theory', '\\n\\nit', 'stag', 'antis', '\\n\\nedit', 'tmor', 'alert', 'fuck', 'tmor'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[';)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Too', 'many', 'empty', 'seats', 'for', 'a', 'night', 'game', 'against', 'a', 'big', 'opponent.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'empty', 'seat', 'night', 'gam', 'big', 'oppon'], ['many', 'empty', 'seat', 'night', 'game', 'big', 'opponent'])\n",
      "original document: \n",
      "['Can', 'anyone', 'recommend', 'suitable', 'alternatives', 'to', 'Google', '(Gmail,', 'Google', 'search', 'engine,', 'Chrome),', 'Microsoft,', 'Apple', 'etc', 'for', 'someone', 'that', \"isn't\", 'a', 'computer', 'whiz?\\n\\n', \"I've\", 'used', 'Ubuntu', 'in', 'the', 'past', 'and', \"I'm\", 'considering', 'going', 'back', 'to', 'that.', 'One', 'problem', 'I', 'had', 'with', 'it', 'was', 'sometimes', 'not', 'being', 'able', 'to', 'get', 'some', 'programs', 'to', 'run', 'on', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'recommend', 'suit', 'altern', 'googl', 'gmail', 'googl', 'search', 'engin', 'chrome', 'microsoft', 'appl', 'etc', 'someon', 'isnt', 'comput', 'whiz\\n\\n', 'iv', 'us', 'ubuntu', 'past', 'im', 'consid', 'going', 'back', 'on', 'problem', 'sometim', 'abl', 'get', 'program', 'run'], ['anyone', 'recommend', 'suitable', 'alternatives', 'google', 'gmail', 'google', 'search', 'engine', 'chrome', 'microsoft', 'apple', 'etc', 'someone', 'isnt', 'computer', 'whiz\\n\\n', 'ive', 'use', 'ubuntu', 'past', 'im', 'consider', 'go', 'back', 'one', 'problem', 'sometimes', 'able', 'get', 'program', 'run'])\n",
      "original document: \n",
      "['Plus,', 'Chancellor', 'of', 'the', 'free', 'world', 'sounds', 'better', 'anyway.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plu', 'chancel', 'fre', 'world', 'sound', 'bet', 'anyway'], ['plus', 'chancellor', 'free', 'world', 'sound', 'better', 'anyway'])\n",
      "original document: \n",
      "[\"I'll\", 'make', 'you', 'feel', 'better', 'and', 'tell', 'you', 'NE', 'was', 'my', 'first', 'guess,', 'with', 'IA', 'the', 'second.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'mak', 'feel', 'bet', 'tel', 'ne', 'first', 'guess', 'ia', 'second'], ['ill', 'make', 'feel', 'better', 'tell', 'ne', 'first', 'guess', 'ia', 'second'])\n",
      "original document: \n",
      "['I', \"don't\", 'want', 'to', 'risk', 'turning', 'into', 'a', 'pillar', 'of', 'salt', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'want', 'risk', 'turn', 'pill', 'salt'], ['dont', 'want', 'risk', 'turn', 'pillar', 'salt'])\n",
      "original document: \n",
      "['They', 'can', 'get', 'on', 'the', 'public', 'speaking', 'tour', 'and', 'get', 'paid', 'by', 'some', 'non-profit', 'so', 'they', 'can', 'speak', 'about', 'how', 'they', 'persevered', 'through', 'self', 'generated', 'adversity.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'publ', 'speak', 'tour', 'get', 'paid', 'nonprofit', 'speak', 'persev', 'self', 'gen', 'advers'], ['get', 'public', 'speak', 'tour', 'get', 'pay', 'nonprofit', 'speak', 'persevere', 'self', 'generate', 'adversity'])\n",
      "original document: \n",
      "['banana', 'for', 'tripping\\n\\nmost', 'fruits', 'really\\n\\nrolling?', 'While', 'peaking', 'im', 'not', 'gonna', 'eat', 'most', 'things,', 'some', 'oranges', 'or', 'some', 'sort', 'of', 'juicy', 'citrus', 'are', 'nice,', 'cherries', 'are', 'bomb\\n\\non', 'the', 'come', 'down', 'i', 'like', 'to', 'munch', 'on', 'chips', 'after', 'i', 'take', 'a', 'little', 'xan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['banan', 'tripping\\n\\nmost', 'fruit', 'really\\n\\nrolling', 'peak', 'im', 'gonn', 'eat', 'thing', 'orang', 'sort', 'juicy', 'citr', 'nic', 'cherry', 'bomb\\n\\non', 'com', 'lik', 'munch', 'chip', 'tak', 'littl', 'xan'], ['banana', 'tripping\\n\\nmost', 'fruit', 'really\\n\\nrolling', 'peak', 'im', 'gonna', 'eat', 'things', 'oranges', 'sort', 'juicy', 'citrus', 'nice', 'cherries', 'bomb\\n\\non', 'come', 'like', 'munch', 'chip', 'take', 'little', 'xan'])\n",
      "original document: \n",
      "['Oh', 'my', 'gosh,', 'a', 'megablep!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'gosh', 'megablep'], ['oh', 'gosh', 'megablep'])\n",
      "original document: \n",
      "['Anyone', 'remember', 'the', 'game', 'with', 'LSU', 'and', 'Oregon', 'State', 'in', 'the', 'rain', 'in', '2004?', 'Where', 'Oregon', 'State', 'missed', '3', 'extra', 'points', 'that', 'day', 'and', 'ended', 'up', 'losing', 'by', 'one', 'in', 'OT?\\n\\n\\nMakes', 'me', 'LOL.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'rememb', 'gam', 'lsu', 'oregon', 'stat', 'rain', 'two thousand and four', 'oregon', 'stat', 'miss', 'three', 'extr', 'point', 'day', 'end', 'los', 'on', 'ot\\n\\n\\nmakes', 'lol'], ['anyone', 'remember', 'game', 'lsu', 'oregon', 'state', 'rain', 'two thousand and four', 'oregon', 'state', 'miss', 'three', 'extra', 'point', 'day', 'end', 'lose', 'one', 'ot\\n\\n\\nmakes', 'lol'])\n",
      "original document: \n",
      "['And', 'what', 'if', 'you', 'just', 'summon', 'the', 'NPC?', '', 'I', 'beat', 'them', 'solo', 'my', 'first', 'playthrough,', 'but', 'on', 'all', 'subsequent', 'playthroughs', 'I', 'summon', 'Gael', 'because', \"I'm\", 'lazy', 'and', 'it', 'makes', 'the', 'boss', '10x', 'easier.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['summon', 'npc', 'beat', 'solo', 'first', 'playthrough', 'subsequ', 'playthrough', 'summon', 'gael', 'im', 'lazy', 'mak', 'boss', '10x', 'easy'], ['summon', 'npc', 'beat', 'solo', 'first', 'playthrough', 'subsequent', 'playthroughs', 'summon', 'gael', 'im', 'lazy', 'make', 'boss', '10x', 'easier'])\n",
      "original document: \n",
      "[\"There's\", 'other', 'ways', 'of', 'dealing', 'with', 'the', 'pain.', '', 'The', 'surgeries', 'and', 'the', 'medications', 'opiate', 'based', 'that', 'follow', 'will', 'put', 'you', 'in', 'serious', 'risk', 'of', 'restarting', 'the', 'cycle', 'again.', '', 'This', 'is', 'what', 'happened', 'to', 'me', 'after', 'years', 'clean', 'so', 'be', 'careful.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'way', 'deal', 'pain', 'surgery', 'med', 'op', 'bas', 'follow', 'put', 'sery', 'risk', 'restart', 'cyc', 'hap', 'year', 'cle', 'car'], ['theres', 'ways', 'deal', 'pain', 'surgeries', 'medications', 'opiate', 'base', 'follow', 'put', 'serious', 'risk', 'restart', 'cycle', 'happen', 'years', 'clean', 'careful'])\n",
      "original document: \n",
      "['good', 'point']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'point'], ['good', 'point'])\n",
      "original document: \n",
      "['November', '3', '–', '5', 'at', 'the', 'Portland', 'Expo', 'Center']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['novemb', 'three', 'fiv', 'portland', 'expo', 'cent'], ['november', 'three', 'five', 'portland', 'expo', 'center'])\n",
      "original document: \n",
      "['You', 'did', 'say', 'end-game.', 'Old', 'man', \"can't\", 'read.\\n\\nRight.', 'So.', 'Bad', 'mage', 'with', 'subjugated', 'mages.', \"He's\", 'gotta', 'have', 'some', 'control', 'over', 'them', 'beyond', 'his', 'threats,', 'yes?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'endgam', 'old', 'man', 'cant', 'read\\n\\nright', 'bad', 'mag', 'subjug', 'mag', 'hes', 'gott', 'control', 'beyond', 'threats', 'ye'], ['say', 'endgame', 'old', 'man', 'cant', 'read\\n\\nright', 'bad', 'mage', 'subjugate', 'mages', 'hes', 'gotta', 'control', 'beyond', 'threats', 'yes'])\n",
      "original document: \n",
      "['New', 'York', 'is', 'brought', 'up', 'because', 'it', 'is', 'the', 'densest', 'city', 'in', 'North', 'America,', 'and', 'as', 'an', 'example', 'that', 'people', 'are', 'not', '“giving', 'up', 'privacy”', 'for', 'density.', 'Reread', 'the', 'whole', 'comment', 'thread.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'york', 'brought', 'densest', 'city', 'nor', 'americ', 'exampl', 'peopl', 'giv', 'priv', 'dens', 'reread', 'whol', 'com', 'thread'], ['new', 'york', 'bring', 'densest', 'city', 'north', 'america', 'example', 'people', 'give', 'privacy', 'density', 'reread', 'whole', 'comment', 'thread'])\n",
      "original document: \n",
      "['You', 'are', 'absolutely', 'right', 'as', 'at', 'least', 'where', 'I', 'live,', 'etizolam', 'is', 'not', 'a', 'controlled', 'substance', 'and', 'is', 'not', 'tested', 'for.', 'So', 'they', 'would', 'not', 'have', 'a', 'etizolam', 'solution', 'to', 'run', 'through', 'the', 'gc', 'ms', 'for', 'reference', 'versus', 'another', 'sample']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['absolv', 'right', 'least', 'liv', 'etizolam', 'control', 'subst', 'test', 'would', 'etizolam', 'solv', 'run', 'gc', 'ms', 'ref', 'vers', 'anoth', 'sampl'], ['absolutely', 'right', 'least', 'live', 'etizolam', 'control', 'substance', 'test', 'would', 'etizolam', 'solution', 'run', 'gc', 'ms', 'reference', 'versus', 'another', 'sample'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['So', 'you', 'are', 'the', 'ignorant', 'jackass', 'I', 'thought', 'you', 'were?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ign', 'jackass', 'thought'], ['ignorant', 'jackass', 'think'])\n",
      "original document: \n",
      "['They', 'actually', 'fixed', 'that', 'in', 'the', '[sequel](https://www.youtube.com/watch?v=kL1QUmeEZQc)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'fix', 'sequelhttpswwwyoutubecomwatchvkl1qumeezqc'], ['actually', 'fix', 'sequelhttpswwwyoutubecomwatchvkl1qumeezqc'])\n",
      "original document: \n",
      "['On']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['show', 'us', 'samples']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['show', 'us', 'sampl'], ['show', 'us', 'sample'])\n",
      "original document: \n",
      "['disagree,', 'Soraka', 'much', 'more', 'anti-fun.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['disagr', 'sorak', 'much', 'antifun'], ['disagree', 'soraka', 'much', 'antifun'])\n",
      "original document: \n",
      "['143413934|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413660\\nYou', 'are', 'a', 'traitor', 'to', 'the', 'Democratic', 'Party', 'and', 'a', 'sad', 'human', 'being', 'for', 'wanting', 'to', 'watch', 'the', 'world', 'burn.', 'SJW', 'is', 'not', 'bullshit.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, nine hundred and thirty-four', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413660\\nyou', 'trait', 'democr', 'party', 'sad', 'hum', 'want', 'watch', 'world', 'burn', 'sjw', 'bullshit\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, nine hundred and thirty-four', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413660\\nyou', 'traitor', 'democratic', 'party', 'sad', 'human', 'want', 'watch', 'world', 'burn', 'sjw', 'bullshit\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Nova', 'launcher', 'has', 'so', 'many', 'gestures.', \"It's\", 'so', 'good.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nov', 'launch', 'many', 'gest', 'good'], ['nova', 'launcher', 'many', 'gesture', 'good'])\n",
      "original document: \n",
      "['I', 'detect', 'sarcasm.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['detect', 'sarcasm'], ['detect', 'sarcasm'])\n",
      "original document: \n",
      "['Thank', 'you', '.', 'Idk', 'why', 'more', 'people', \"don't\", 'know', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'idk', 'peopl', 'dont', 'know'], ['thank', 'idk', 'people', 'dont', 'know'])\n",
      "original document: \n",
      "['I', 'would', 'have', 'figured', 'that', 'since', \"it's\", 'a', 'govt', 'office', 'they', \"wouldn't\", 'care', 'whether', 'anyone', 'showed', 'up', 'or', 'not...', 'In', 'fact', 'I', 'would', 'have', 'expected', 'them', 'to', 'be', 'happy', 'nobody', 'was', 'there.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'fig', 'sint', 'govt', 'off', 'wouldnt', 'car', 'wheth', 'anyon', 'show', 'fact', 'would', 'expect', 'happy', 'nobody'], ['would', 'figure', 'since', 'govt', 'office', 'wouldnt', 'care', 'whether', 'anyone', 'show', 'fact', 'would', 'expect', 'happy', 'nobody'])\n",
      "original document: \n",
      "[\"That's\", 'fucking', 'ridiculous.\\n\\nSomeone', 'said', 'something', 'like', 'that', 'during', 'the', 'Sens-Rags', 'series', 'and', 'I', \"couldn't\", 'stop', 'laughing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fuck', 'ridiculous\\n\\nsomeone', 'said', 'someth', 'lik', 'sensr', 'sery', 'couldnt', 'stop', 'laugh'], ['thats', 'fuck', 'ridiculous\\n\\nsomeone', 'say', 'something', 'like', 'sensrags', 'series', 'couldnt', 'stop', 'laugh'])\n",
      "original document: \n",
      "['You', 'just', 'wait.', 'Whether', 'she', 'is', 'or', 'is', 'not', 'pregnant', 'will', 'reveal', 'itself', 'with', 'time.', 'Your', 'friends', 'are', 'right,', \"it's\", 'incredibly', 'likely', \"she's\", 'not', 'pregnant,', 'so', 'try', 'not', 'to', 'sweat', 'it.', \"It's\", 'not', 'a', 'bell', 'you', 'can', 'unring', 'anyway', 'so', 'no', 'use', 'freaking', 'out', 'over', 'it.', \"What's\", 'done', 'is', 'done.\\n\\nIf', 'in', '8', 'months', 'or', 'so', 'she', 'has', 'a', 'kid,', 'you', 'get', 'a', 'paternity', 'test', 'and', 'go', 'from', 'there.', 'Obviously', 'if', \"it's\", 'your', 'kid', 'you', \"won't\", 'have', 'a', 'choice', 'but', 'to', 'pay', 'child', 'support,', 'etc.', '\\n\\nI', 'recommend', 'you', 'go', 'no', 'contact', 'until', 'then.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'wheth', 'pregn', 'rev', 'tim', 'friend', 'right', 'incred', 'lik', 'she', 'pregn', 'try', 'swe', 'bel', 'unr', 'anyway', 'us', 'freak', 'what', 'don', 'done\\n\\nif', 'eight', 'month', 'kid', 'get', 'patern', 'test', 'go', 'obvy', 'kid', 'wont', 'cho', 'pay', 'child', 'support', 'etc', '\\n\\ni', 'recommend', 'go', 'contact'], ['wait', 'whether', 'pregnant', 'reveal', 'time', 'friends', 'right', 'incredibly', 'likely', 'shes', 'pregnant', 'try', 'sweat', 'bell', 'unring', 'anyway', 'use', 'freak', 'whats', 'do', 'done\\n\\nif', 'eight', 'months', 'kid', 'get', 'paternity', 'test', 'go', 'obviously', 'kid', 'wont', 'choice', 'pay', 'child', 'support', 'etc', '\\n\\ni', 'recommend', 'go', 'contact'])\n",
      "original document: \n",
      "['I', 'said', 'surrender', 'cobra,', 'not', 'game', 'over', 'brother']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'surrend', 'cobr', 'gam', 'broth'], ['say', 'surrender', 'cobra', 'game', 'brother'])\n",
      "original document: \n",
      "['where', 'can', 'I', 'find', 'this', 'information', 'then?', 'nothing', 'in', 'the', 'sub', 'has', 'the', 'answer']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['find', 'inform', 'noth', 'sub', 'answ'], ['find', 'information', 'nothing', 'sub', 'answer'])\n",
      "original document: \n",
      "['Isoprop', 'all', 'the', 'way']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isoprop', 'way'], ['isoprop', 'way'])\n",
      "original document: \n",
      "['うんこ']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['I', \"didn't\", 'say', 'he', 'had', 'the', 'keys', 'to', 'all', 'information,', 'though', 'I', 'do', 'think', 'the', 'Russians', 'are', 'feeding', 'Wikileaks', 'a', 'good', 'bit,', 'but', 'I', 'do', 'love', 'the', 'mechanical', 'way', 'he', 'goes', 'through', 'people', 'in', 'a', 'debate,', 'the', \"guy's\", 'a', 'machine.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'say', 'key', 'inform', 'though', 'think', 'russ', 'fee', 'wikileak', 'good', 'bit', 'lov', 'mech', 'way', 'goe', 'peopl', 'deb', 'guy', 'machin'], ['didnt', 'say', 'key', 'information', 'though', 'think', 'russians', 'feed', 'wikileaks', 'good', 'bite', 'love', 'mechanical', 'way', 'go', 'people', 'debate', 'guy', 'machine'])\n",
      "original document: \n",
      "['No', 'very', 'different', 'than', 'the', 'ceramic', 'donuts', 'that', 'yocan', 'makes.', 'This', 'is', 'not', 'a', 'donut', 'and', 'somewhat', 'raised', ',', 'I', 'have', 'not', 'used', 'it', 'yet', ',', 'I', 'been', 'using', 'the', 'dual', 'Quartz', 'so', 'far', 'but', \"I'll\", 'try', 'it', 'and', 'write', 'back', '.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['diff', 'ceram', 'donut', 'yoc', 'mak', 'donut', 'somewh', 'rais', 'us', 'yet', 'us', 'dual', 'quartz', 'far', 'il', 'try', 'writ', 'back'], ['different', 'ceramic', 'donuts', 'yocan', 'make', 'donut', 'somewhat', 'raise', 'use', 'yet', 'use', 'dual', 'quartz', 'far', 'ill', 'try', 'write', 'back'])\n",
      "original document: \n",
      "[\"I've\", 'had', 'nothing', 'but', 'problems', 'with', 'UCCI', 'since', 'the', 'switch.', 'They', 'canceled', 'my', 'policy', 'for', 'nonpayment', 'a', 'week', 'after', 'cashing', 'the', 'premium', 'check', 'that', 'we', 'sent', 'them.', '\\n\\nThey', 'keep', 'mailing', 'me', '\"past', 'due\"', 'bills', 'even', 'though', 'I', 'have', 'electronic', 'payments', 'set', 'up', 'and', \"they're\", 'being', 'paid.', 'But,', 'the', 'worst', 'is', 'that', 'every', 'time', 'I', 'call', 'them', 'their', 'customer', 'service', 'reps', 'are', 'clueless.', 'They', 'take', 'days', 'to', 'respond', 'to', 'anything', 'that', \"can't\", 'be', 'immediately', 'solved', 'over', 'the', 'phone.', 'Same', 'thing', 'if', 'you', 'ask', 'for', 'a', 'supervisor', '-', '24-48', 'hour', 'call', 'back.', \"\\n\\nI'm\", 'ready', 'to', 'give', 'up', 'and', 'pay', 'for', 'private', 'dental.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'noth', 'problem', 'ucc', 'sint', 'switch', 'cancel', 'policy', 'nonpay', 'week', 'cash', 'prem', 'check', 'sent', '\\n\\nthey', 'keep', 'mail', 'past', 'due', 'bil', 'ev', 'though', 'electron', 'pay', 'set', 'theyr', 'paid', 'worst', 'every', 'tim', 'cal', 'custom', 'serv', 'rep', 'clueless', 'tak', 'day', 'respond', 'anyth', 'cant', 'immedy', 'solv', 'phon', 'thing', 'ask', 'superv', 'two thousand, four hundred and forty-eight', 'hour', 'cal', 'back', '\\n\\nim', 'ready', 'giv', 'pay', 'priv', 'dent'], ['ive', 'nothing', 'problems', 'ucci', 'since', 'switch', 'cancel', 'policy', 'nonpayment', 'week', 'cash', 'premium', 'check', 'send', '\\n\\nthey', 'keep', 'mail', 'past', 'due', 'bill', 'even', 'though', 'electronic', 'payments', 'set', 'theyre', 'pay', 'worst', 'every', 'time', 'call', 'customer', 'service', 'reps', 'clueless', 'take', 'days', 'respond', 'anything', 'cant', 'immediately', 'solve', 'phone', 'thing', 'ask', 'supervisor', 'two thousand, four hundred and forty-eight', 'hour', 'call', 'back', '\\n\\nim', 'ready', 'give', 'pay', 'private', 'dental'])\n",
      "original document: \n",
      "['OK.', 'this', 'proves', 'it', 'whoever', 'did', 'this', 'is', 'a', 'moron', 'and', \"can't\", 'read', 'intel', 'sites', 'showing', 'the', 'real', 'relations', 'between', 'these', 'entities.', 'Nice', 'propaganda', 'tho.', 'Keep', 'trying', 'while', 'going', 'down']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'prov', 'whoev', 'moron', 'cant', 'read', 'intel', 'sit', 'show', 'real', 'rel', 'ent', 'nic', 'propagand', 'tho', 'keep', 'try', 'going'], ['ok', 'prove', 'whoever', 'moron', 'cant', 'read', 'intel', 'sit', 'show', 'real', 'relations', 'entities', 'nice', 'propaganda', 'tho', 'keep', 'try', 'go'])\n",
      "original document: \n",
      "['Those', 'are', 'pretty', 'good.', 'I', 'like', 'his', 'style.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'good', 'lik', 'styl'], ['pretty', 'good', 'like', 'style'])\n",
      "original document: \n",
      "['From', 'Portland', 'timberline', 'and', 'skibowl', 'are', 'the', 'closest', 'with', 'meadows', 'being', 'the', 'furthest,', 'but', 'not', 'by', 'too', 'much', 'i', 'feel.', '\\n\\nAs', 'for', 'traffic,', 'I', 'usually', 'go', 'up', 'on', 'the', 'weekends,', 'because', 'of', 'my', 'schedule,', 'but', 'theres', 'not', 'a', 'lot', 'of', 'traffic', 'when', \"they're\", 'opening(except', 'for', 'some', 'holidays)', 'but', 'leaving', 'is', 'always', 'the', 'worst.', 'Mostly', 'because', 'of', 'the', 'traffic', 'converging', 'from', 'both', 'timberline', 'and', 'skibowl,', 'all', 'trying', 'to', 'get', 'back', 'to', 'Portland.', 'Some', 'days', 'can', 'be', 'a', 'breeze', 'and', 'others', 'horrendous', 'but', 'with', 'government', 'camp', 'being', 'decently', 'close', 'stopping', 'for', 'a', 'quick', 'bite', 'to', 'let', 'some', 'time', 'pass', 'is', 'always', 'a', 'solid', 'choice.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['portland', 'timberlin', 'skibowl', 'closest', 'meadow', 'furthest', 'much', 'feel', '\\n\\nas', 'traff', 'us', 'go', 'weekend', 'schedule', 'ther', 'lot', 'traff', 'theyr', 'openingexceiv', 'holiday', 'leav', 'alway', 'worst', 'most', 'traff', 'converg', 'timberlin', 'skibowl', 'try', 'get', 'back', 'portland', 'day', 'breez', 'oth', 'horrend', 'govern', 'camp', 'dec', 'clos', 'stop', 'quick', 'bit', 'let', 'tim', 'pass', 'alway', 'solid', 'cho'], ['portland', 'timberline', 'skibowl', 'closest', 'meadows', 'furthest', 'much', 'feel', '\\n\\nas', 'traffic', 'usually', 'go', 'weekend', 'schedule', 'theres', 'lot', 'traffic', 'theyre', 'openingexcept', 'holiday', 'leave', 'always', 'worst', 'mostly', 'traffic', 'converge', 'timberline', 'skibowl', 'try', 'get', 'back', 'portland', 'days', 'breeze', 'others', 'horrendous', 'government', 'camp', 'decently', 'close', 'stop', 'quick', 'bite', 'let', 'time', 'pass', 'always', 'solid', 'choice'])\n",
      "original document: \n",
      "['Like', 'the', 'rangers', 'game,', 'this', 'game', 'was', 'a', 'little', 'heated', 'too.', 'Bats', 'also', 'fucked', 'up', 'and', 'admitted', 'to', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'rang', 'gam', 'gam', 'littl', 'heat', 'bat', 'also', 'fuck', 'admit'], ['like', 'rangers', 'game', 'game', 'little', 'heat', 'bat', 'also', 'fuck', 'admit'])\n",
      "original document: \n",
      "['Oh', 'wow,', \"didn't\", 'know.', 'Uninstalling,', 'its', 'a', 'shame', 'though,', 'its', 'pretty', 'cool', ':/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'wow', 'didnt', 'know', 'uninstal', 'sham', 'though', 'pretty', 'cool'], ['oh', 'wow', 'didnt', 'know', 'uninstalling', 'shame', 'though', 'pretty', 'cool'])\n",
      "original document: \n",
      "['Me', 'and', 'my', 'buds', 'were', 'cracking', 'up', 'for', 'a', 'whole', 'match', 'of', 'Cod', 'BO2', 'when', 'we', 'were', 'getting', 'destroyed', 'by', 'a', 'guy', 'named', \"'Tactical\", \"Grandpa'.\", \"I'll\", 'never', 'forget', 'the', 'name.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bud', 'crack', 'whol', 'match', 'cod', 'bo2', 'get', 'destroy', 'guy', 'nam', 'tact', 'grandp', 'il', 'nev', 'forget', 'nam'], ['bud', 'crack', 'whole', 'match', 'cod', 'bo2', 'get', 'destroy', 'guy', 'name', 'tactical', 'grandpa', 'ill', 'never', 'forget', 'name'])\n",
      "original document: \n",
      "['0-3', '3OT', 'win...?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['three', '3ot', 'win'], ['three', '3ot', 'win'])\n",
      "original document: \n",
      "['Hi', 'there!', 'Your', 'post', 'was', 'removed', 'because', 'it', 'uses', 'the', 'text', 'box.', 'Per', '[rule', '1](/r/AskReddit/wiki/index#wiki_-rule_1-),', 'use', 'of', 'the', 'text', 'box', 'is', 'prohibited.', 'You', 'can', 'resubmit', 'your', 'post', '[here](/r/askreddit/submit?selftext=true&amp;title=HELP!', 'Need', 'help', 'finding', 'a', 'drop', 'ship', 't-shirt', 'manufacturer.', 'REDDIT', 'BE', 'MY', 'SAVIOR!)', 'without', 'the', 'textbox.\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/AskReddit)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'post', 'remov', 'us', 'text', 'box', 'per', 'rul', '1raskredditwikiindexwiki_rule_1', 'us', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitlehelp', 'nee', 'help', 'find', 'drop', 'ship', 'tshirt', 'manufact', 'reddit', 'savy', 'without', 'textbox\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoraskreddit', 'quest', 'concern'], ['hi', 'post', 'remove', 'use', 'text', 'box', 'per', 'rule', '1raskredditwikiindexwiki_rule_1', 'use', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitlehelp', 'need', 'help', 'find', 'drop', 'ship', 'tshirt', 'manufacturer', 'reddit', 'savior', 'without', 'textbox\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoraskreddit', 'question', 'concern'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'mean', \"Shangela's\", 'original', 'elimination', 'was', 'distinctly', 'racist,', 'and', 'separate', 'from', 'the', 'ground', 'with', 'her', 'whole', 'power', 'lesbian', 'aesthetic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'shangela', 'origin', 'elimin', 'distinct', 'rac', 'sep', 'ground', 'whol', 'pow', 'lesb', 'aesthet'], ['mean', 'shangelas', 'original', 'elimination', 'distinctly', 'racist', 'separate', 'grind', 'whole', 'power', 'lesbian', 'aesthetic'])\n",
      "original document: \n",
      "['two', 'randoms', 'please']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'pleas'], ['two', 'randoms', 'please'])\n",
      "original document: \n",
      "['Now', \"you're\", 'asking', 'the', 'right', 'questions.', 'I', 'dunno', 'I', 'reckon', 'something', 'like', 'disaffection,', 'alienation,', 'neglect', 'and', 'multitude', 'of', 'other', 'factors.', 'What', 'do', 'you', 'reckon?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'ask', 'right', 'quest', 'dunno', 'reckon', 'someth', 'lik', 'disaffect', 'aly', 'neglect', 'multitud', 'fact', 'reckon'], ['youre', 'ask', 'right', 'question', 'dunno', 'reckon', 'something', 'like', 'disaffection', 'alienation', 'neglect', 'multitude', 'factor', 'reckon'])\n",
      "original document: \n",
      "['How', 'exactly', 'are', 'these', 'CL', 'scams', 'supposed', 'to', 'work', 'where', 'someone', 'is', 'selling', 'a', 'car', 'that', \"doesn't\", 'exist?', '', 'Are', 'they', 'thinking', 'that', 'an', 'unsuspecting', 'buyer', 'is', 'going', 'to', 'send', 'them', 'money', 'before', 'actually', 'seeing', 'the', 'car/having', 'the', 'title', 'in', 'hand?', '', 'Who', 'the', 'hell', 'would', 'pay', 'for', 'a', 'used', 'Accord', 'without', 'going', 'to', 'look', 'at', 'at', 'first?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'cl', 'scam', 'suppos', 'work', 'someon', 'sel', 'car', 'doesnt', 'ex', 'think', 'unsuspect', 'buy', 'going', 'send', 'money', 'act', 'see', 'carhav', 'titl', 'hand', 'hel', 'would', 'pay', 'us', 'accord', 'without', 'going', 'look', 'first'], ['exactly', 'cl', 'scam', 'suppose', 'work', 'someone', 'sell', 'car', 'doesnt', 'exist', 'think', 'unsuspecting', 'buyer', 'go', 'send', 'money', 'actually', 'see', 'carhaving', 'title', 'hand', 'hell', 'would', 'pay', 'use', 'accord', 'without', 'go', 'look', 'first'])\n",
      "original document: \n",
      "['wew.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wew'], ['wew'])\n",
      "original document: \n",
      "[\"He's\", 'just', 'awful.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'aw'], ['hes', 'awful'])\n",
      "original document: \n",
      "['how', 'did', 'you', 'take', 'such', 'gorgeous', 'photo', 'with', 'the', 'kit', 'lens?', 'all', 'i', 'get', 'when', 'using', 'the', 'kit', 'lens', 'is', 'wash', 'out', 'and', 'non', 'sharp', 'images.', 'please', 'teach', 'me', 'master', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'gorg', 'photo', 'kit', 'len', 'get', 'us', 'kit', 'len', 'wash', 'non', 'sharp', 'im', 'pleas', 'teach', 'mast'], ['take', 'gorgeous', 'photo', 'kit', 'lens', 'get', 'use', 'kit', 'lens', 'wash', 'non', 'sharp', 'image', 'please', 'teach', 'master'])\n",
      "original document: \n",
      "['Nooooooo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nooooooo'], ['nooooooo'])\n",
      "original document: \n",
      "['This', 'sounds', 'really', 'cool!', 'Hopefully', 'this', 'idea', 'can', 'be', 'used', 'at', 'some', 'point.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'real', 'cool', 'hop', 'ide', 'us', 'point'], ['sound', 'really', 'cool', 'hopefully', 'idea', 'use', 'point'])\n",
      "original document: \n",
      "['one', 'would', 'assume', 'they', 'would', 'conduct', 'an', 'investigation', 'to', 'determine', 'if', 'criminal', 'activity', 'occurred.', '', 'Were', 'they', 'to', 'determine', 'it', 'had,', 'a', 'criminal', 'case', 'might', 'ensue', '(the', 'process', 'would', 'involve', 'handing', 'it', 'over', 'to', 'the', 'prosecutors', 'office', 'to', 'make', 'a', 'determination).', '', 'If', 'they', 'determined', 'it', 'was', 'civil,', 'that', 'would', 'be', 'the', 'end', 'of', 'it.', '', 'Law', 'enforcement', \"doesn't\", 'pursue', 'civil', 'matters', '(outside', 'of', 'some', 'minor', 'exceptions', 'like', 'contempt', 'or', 'certain', 'child', 'support', 'issues.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'would', 'assum', 'would', 'conduc', 'investig', 'determin', 'crimin', 'act', 'occur', 'determin', 'crimin', 'cas', 'might', 'ensu', 'process', 'would', 'involv', 'hand', 'prosecut', 'off', 'mak', 'determin', 'determin', 'civil', 'would', 'end', 'law', 'enforc', 'doesnt', 'pursu', 'civil', 'mat', 'outsid', 'min', 'exceiv', 'lik', 'contempt', 'certain', 'child', 'support', 'issu'], ['one', 'would', 'assume', 'would', 'conduct', 'investigation', 'determine', 'criminal', 'activity', 'occur', 'determine', 'criminal', 'case', 'might', 'ensue', 'process', 'would', 'involve', 'hand', 'prosecutors', 'office', 'make', 'determination', 'determine', 'civil', 'would', 'end', 'law', 'enforcement', 'doesnt', 'pursue', 'civil', 'matter', 'outside', 'minor', 'exceptions', 'like', 'contempt', 'certain', 'child', 'support', 'issue'])\n",
      "original document: \n",
      "['Yeah,', 'another', 'post', 'or', 'two', 'and', \"you'll\", 'be', 'challenging', 'me', 'to', 'meet', 'you', 'behind', 'the', 'monkey', 'bars', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'anoth', 'post', 'two', 'youl', 'challeng', 'meet', 'behind', 'monkey', 'bar'], ['yeah', 'another', 'post', 'two', 'youll', 'challenge', 'meet', 'behind', 'monkey', 'bar'])\n",
      "original document: \n",
      "['I', 'unfortunately', 'agree']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unfortun', 'agr'], ['unfortunately', 'agree'])\n",
      "original document: \n",
      "['Breanne', 'at', 'Stone', 'Salon', 'in', 'Hoover', 'is', 'awesome!', \"She's\", 'great', 'at', 'being', 'chatty', 'if', 'you', 'want,', 'or', 'cutting', 'in', 'silence', 'if', \"you'd\", 'rather', 'not', 'talk.', '\\n\\nEdited', 'to', 'add:', 'she', 'has', 'a', 'Facebook', 'page', 'if', 'you', 'wanna', 'check', 'out', 'her', 'work!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brean', 'ston', 'salon', 'hoov', 'awesom', 'she', 'gre', 'chatty', 'want', 'cut', 'sil', 'youd', 'rath', 'talk', '\\n\\nedited', 'ad', 'facebook', 'pag', 'wann', 'check', 'work'], ['breanne', 'stone', 'salon', 'hoover', 'awesome', 'shes', 'great', 'chatty', 'want', 'cut', 'silence', 'youd', 'rather', 'talk', '\\n\\nedited', 'add', 'facebook', 'page', 'wanna', 'check', 'work'])\n",
      "original document: \n",
      "[\"Brad's\", 'dope', 'pedagogy', 'is', 'so', 'much', 'better', 'than', 'Lonzo', \"Ball's\", '6-0', 'scrimmage', 'record.', 'Fuck', 'the', 'Lakers.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brad', 'dop', 'pedagog', 'much', 'bet', 'lonzo', 'bal', 'sixty', 'scrimmage', 'record', 'fuck', 'lak'], ['brad', 'dope', 'pedagogy', 'much', 'better', 'lonzo', 'ball', 'sixty', 'scrimmage', 'record', 'fuck', 'lakers'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['oh', 'man']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'man'], ['oh', 'man'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Basically', 'most', 'RHONJ', 'need', 'to', 'step', 'up', 'their', 'tag', 'line', 'games.\\n\\nI', 'do', 'like', \"Teresa's\", 'this', 'up', 'coming', 'season,', 'but', 'only', 'because', \"it's\", 'a', 'little', 'more', 'creative/', 'different', 'compared', 'to', 'the', 'others.\\n\\n\"The', 'only', 'life', 'I', 'envy', 'is', 'my', 'own\"', '...', \"I'm\", 'sorry', 'idk', 'how', 'to', 'fix', 'it...', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bas', 'rhond', 'nee', 'step', 'tag', 'lin', 'games\\n\\ni', 'lik', 'teresa', 'com', 'season', 'littl', 'cre', 'diff', 'comp', 'others\\n\\nthe', 'lif', 'envy', 'im', 'sorry', 'idk', 'fix'], ['basically', 'rhonj', 'need', 'step', 'tag', 'line', 'games\\n\\ni', 'like', 'teresas', 'come', 'season', 'little', 'creative', 'different', 'compare', 'others\\n\\nthe', 'life', 'envy', 'im', 'sorry', 'idk', 'fix'])\n",
      "original document: \n",
      "['watch', 'out', 'for', 'it!', ':)', 'blue', 'or', 'red?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['watch', 'blu', 'red'], ['watch', 'blue', 'red'])\n",
      "original document: \n",
      "['\"If', 'I', 'told', 'you', 'the', 'ache', 'I', 'had', \"you'd\", 'ask', 'why', 'I', 'had', 'a', 'cat', 'with', 'me.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['told', 'ach', 'youd', 'ask', 'cat'], ['tell', 'ache', 'youd', 'ask', 'cat'])\n",
      "original document: \n",
      "['r/pareidolia', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rpareidol'], ['rpareidolia'])\n",
      "original document: \n",
      "['Being', 'Human', 'is', 'a', 'werewolf,', 'a', 'vampire', 'and', 'a', 'ghost', 'as', 'roomates(it', 'sorta', 'has', 'a', 'similar', 'situation', 'to', 'shameless', 'where', 'the', 'UK', 'version', 'is', 'better', 'but', 'the', 'US', 'version', 'exists.', 'And', 'yes', 'Phoebe', 'Tonkins', 'is', 'on', 'The', 'Secret', 'Circle.\\n\\nI', 'really', 'do', 'think', 'you', 'should', 'try', 'Lucifer(its', 'really', 'funny)', 'but', 'I', \"can't\", 'force', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hum', 'werewolf', 'vampir', 'ghost', 'roomatesit', 'sort', 'simil', 'situ', 'shameless', 'uk', 'vert', 'bet', 'us', 'vert', 'ex', 'ye', 'phoeb', 'tonkin', 'secret', 'circle\\n\\ni', 'real', 'think', 'try', 'luciferit', 'real', 'funny', 'cant', 'forc'], ['human', 'werewolf', 'vampire', 'ghost', 'roomatesit', 'sorta', 'similar', 'situation', 'shameless', 'uk', 'version', 'better', 'us', 'version', 'exist', 'yes', 'phoebe', 'tonkins', 'secret', 'circle\\n\\ni', 'really', 'think', 'try', 'luciferits', 'really', 'funny', 'cant', 'force'])\n",
      "original document: \n",
      "['Lets', 'say', 'all', 'these', 'are', 'like', '$200', 'a', 'month', 'I', 'think.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'say', 'lik', 'two hundred', 'mon', 'think'], ['let', 'say', 'like', 'two hundred', 'month', 'think'])\n",
      "original document: \n",
      "['A', 'quick', 'look', 'at', 'their', 'career', 'stats', 'will', 'show', 'that', 'Brees', 'is', 'better', 'in', 'almost', 'every', 'category,', 'by', 'alot.', 'idk', 'where', 'you', 'got', 'that', 'they', 'were', \"similar.\\n\\nI'm\", 'not', 'downvoting', 'you', 'btw.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quick', 'look', 'car', 'stat', 'show', 'bre', 'bet', 'almost', 'every', 'categ', 'alot', 'idk', 'got', 'similar\\n\\nim', 'downvot', 'btw'], ['quick', 'look', 'career', 'stats', 'show', 'brees', 'better', 'almost', 'every', 'category', 'alot', 'idk', 'get', 'similar\\n\\nim', 'downvoting', 'btw'])\n",
      "original document: \n",
      "['Basically,', 'if', 'you', 'have', 'an', 'expression', 'pedal', 'and', 'a', 'pitchfork,', 'you', 'can', 'create', 'this', 'cool', 'detuned', 'chorus', 'sound', 'by', 'tapping', 'the', 'expression', 'pedal', 'verrrrrry', 'slightly', 'from', 'when', \"it's\", 'at', 'dry', 'signal', '-', 'pretty', 'neat', 'when', 'you', \"don't\", 'have', 'a', 'chorus,', 'even', 'if', 'it', 'is', 'a', 'bit', 'fiddly!\\nNow,', 'two', 'apologies:', 'sorry', 'the', 'footage', 'angles', 'toward', 'the', 'pitchfork', 'rather', 'than', 'the', 'expression', 'pedal', 'when', \"I'm\", 'tapping', 'it,', 'my', 'brother', 'is', 'a', 'goobus,', 'and', 'also,', 'apologies', 'for', 'the', 'bad', 'guitar', 'playing,', 'its', 'my', 'brothers', 'guitar', 'and', \"I'm\", 'normally', 'a', 'keys', 'guy', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bas', 'express', 'ped', 'pitchfork', 'cre', 'cool', 'detun', 'chor', 'sound', 'tap', 'express', 'ped', 'verrrrrry', 'slight', 'dry', 'sign', 'pretty', 'neat', 'dont', 'chor', 'ev', 'bit', 'fiddly\\nnow', 'two', 'apolog', 'sorry', 'foot', 'angl', 'toward', 'pitchfork', 'rath', 'express', 'ped', 'im', 'tap', 'broth', 'goob', 'also', 'apolog', 'bad', 'guit', 'play', 'broth', 'guit', 'im', 'norm', 'key', 'guy', 'lol'], ['basically', 'expression', 'pedal', 'pitchfork', 'create', 'cool', 'detuned', 'chorus', 'sound', 'tap', 'expression', 'pedal', 'verrrrrry', 'slightly', 'dry', 'signal', 'pretty', 'neat', 'dont', 'chorus', 'even', 'bite', 'fiddly\\nnow', 'two', 'apologies', 'sorry', 'footage', 'angle', 'toward', 'pitchfork', 'rather', 'expression', 'pedal', 'im', 'tap', 'brother', 'goobus', 'also', 'apologies', 'bad', 'guitar', 'play', 'brothers', 'guitar', 'im', 'normally', 'key', 'guy', 'lol'])\n",
      "original document: \n",
      "['2', 'hours', 'eh?', \"I'll\", 'try', 'that', 'instead', 'next', 'time', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'hour', 'eh', 'il', 'try', 'instead', 'next', 'tim'], ['two', 'hours', 'eh', 'ill', 'try', 'instead', 'next', 'time'])\n",
      "original document: \n",
      "['Heatwaffle', '280', 'warlock']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heatwaffl', 'two hundred and eighty', 'warlock'], ['heatwaffle', 'two hundred and eighty', 'warlock'])\n",
      "original document: \n",
      "['lmao.', 'Only', 'reddit', 'is', 'making', 'it', 'about', 'race.', 'Not', 'like', 'there', \"aren't\", 'white', 'people', 'in', 'Puerto', 'rico.', 'Its', 'a', 'red,', 'conservative', 'voting', 'state.', 'Get', 'outta', 'here', 'with', 'this', 'weak', 'shit', 'bruh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lmao', 'reddit', 'mak', 'rac', 'lik', 'ar', 'whit', 'peopl', 'puerto', 'rico', 'red', 'conserv', 'vot', 'stat', 'get', 'outt', 'weak', 'shit', 'bruh'], ['lmao', 'reddit', 'make', 'race', 'like', 'arent', 'white', 'people', 'puerto', 'rico', 'red', 'conservative', 'vote', 'state', 'get', 'outta', 'weak', 'shit', 'bruh'])\n",
      "original document: \n",
      "['Who', 'among', 'the', 'fringe', 'QBs', 'do', 'you', 'folks', 'like', 'for', 'next', 'week?', 'Just', 'need', 'a', 'streamer', 'for', 'Brees', 'during', 'his', 'bye', 'next', 'week.', 'I', 'would', 'go', 'ahead', 'and', 'roster', 'the', 'guy', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['among', 'fring', 'qbs', 'folk', 'lik', 'next', 'week', 'nee', 'streamer', 'bre', 'bye', 'next', 'week', 'would', 'go', 'ahead', 'rost', 'guy'], ['among', 'fringe', 'qbs', 'folks', 'like', 'next', 'week', 'need', 'streamer', 'brees', 'bye', 'next', 'week', 'would', 'go', 'ahead', 'roster', 'guy'])\n",
      "original document: \n",
      "['#SELLER']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sel'], ['seller'])\n",
      "original document: \n",
      "['I', 'would', 'love', 'to', 'see', 'more!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lov', 'see'], ['would', 'love', 'see'])\n",
      "original document: \n",
      "['Nah,', \"you're\", 'cool', 'man.', 'Play', 'what', 'works', ':)', \"it's\", 'not', 'been', 'too', 'oppressive', 'since', 'the', 'nerfs', 'anyway.\\n\\nDo', 'you', 'play', 'any', 'other', 'games?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah', 'yo', 'cool', 'man', 'play', 'work', 'oppress', 'sint', 'nerf', 'anyway\\n\\ndo', 'play', 'gam'], ['nah', 'youre', 'cool', 'man', 'play', 'work', 'oppressive', 'since', 'nerfs', 'anyway\\n\\ndo', 'play', 'game'])\n",
      "original document: \n",
      "['THIS', 'IS', 'SO', 'WORTH', 'IT.', 'I', 'felt', 'really', 'guilty', 'hiring', 'someone', 'to', 'clean', 'my', 'house', 'like', 'what,', \"I'm\", 'too', 'lazy', 'to', 'do', 'it', 'myself?', 'WELL', 'I', 'AM.', 'It', 'is', 'always', 'the', 'last', 'thing', 'on', 'the', 'list', 'meaning', 'it', 'never', 'gets', 'done.', 'For', 'both', 'of', 'us!', 'Would', 'I', 'rather', 'we', 'spent', 'one', 'of', 'our', 'precious', 'days', 'off', 'together', 'cleaning?', 'Or', 'banging?\\nThe', 'woman', 'and', 'associates', 'who', 'clean', 'my', 'place', 'make', 'it', 'look', 'so', 'good,', 'that', 'when', 'they', 'finished', 'the', 'first', 'time', 'I', 'cried.', 'Not', 'a', 'full', 'on', 'weep', 'but', 'it', 'was', 'so', 'simple', 'to', 'pay', 'a', 'professional', 'to', 'do', 'something', 'I', 'just', 'hate.', 'This', 'is', 'not', 'worth', 'the', 'heartache', 'to', 'me.', 'Dear', 'OP,', 'consider', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wor', 'felt', 'real', 'guil', 'hir', 'someon', 'cle', 'hous', 'lik', 'im', 'lazy', 'wel', 'alway', 'last', 'thing', 'list', 'mean', 'nev', 'get', 'don', 'us', 'would', 'rath', 'spent', 'on', 'precy', 'day', 'togeth', 'cle', 'banging\\nthe', 'wom', 'assocy', 'cle', 'plac', 'mak', 'look', 'good', 'fin', 'first', 'tim', 'cri', 'ful', 'weep', 'simpl', 'pay', 'profess', 'someth', 'hat', 'wor', 'heartach', 'dear', 'op', 'consid'], ['worth', 'felt', 'really', 'guilty', 'hire', 'someone', 'clean', 'house', 'like', 'im', 'lazy', 'well', 'always', 'last', 'thing', 'list', 'mean', 'never', 'get', 'do', 'us', 'would', 'rather', 'spend', 'one', 'precious', 'days', 'together', 'clean', 'banging\\nthe', 'woman', 'associate', 'clean', 'place', 'make', 'look', 'good', 'finish', 'first', 'time', 'cry', 'full', 'weep', 'simple', 'pay', 'professional', 'something', 'hate', 'worth', 'heartache', 'dear', 'op', 'consider'])\n",
      "original document: \n",
      "['At', 'least', '1', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'on'], ['least', 'one'])\n",
      "original document: \n",
      "['&gt;Joe', 'Arpaio\\n\\nLol', 'I', 'mean', 'if', 'you', 'want', 'to', 'throw', 'around', 'falsified', 'information', 'as', 'though', 'it', 'were', 'factual,', 'I', \"wouldn't\", 'start', 'out', 'by', 'bringing', 'up', 'Arpaio', 'if', 'you', 'want', 'ANYBODY', 'to', 'take', 'you', 'seriously']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtjoe', 'arpaio\\n\\nlol', 'mean', 'want', 'throw', 'around', 'fals', 'inform', 'though', 'fact', 'wouldnt', 'start', 'bring', 'arpaio', 'want', 'anybody', 'tak', 'sery'], ['gtjoe', 'arpaio\\n\\nlol', 'mean', 'want', 'throw', 'around', 'falsify', 'information', 'though', 'factual', 'wouldnt', 'start', 'bring', 'arpaio', 'want', 'anybody', 'take', 'seriously'])\n",
      "original document: \n",
      "['jameis', 'pls']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jam', 'pls'], ['jameis', 'pls'])\n",
      "original document: \n",
      "['did', 'u', 'just', 'call', 'me', 'a', 'holocaust', 'denier']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['u', 'cal', 'holocaust', 'deny'], ['u', 'call', 'holocaust', 'denier'])\n",
      "original document: \n",
      "['ye', 'fam']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'fam'], ['ye', 'fam'])\n",
      "original document: \n",
      "['No', 'that', 'would', 'be', 'mean']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'mean'], ['would', 'mean'])\n",
      "original document: \n",
      "['What', 'i', \"can't\", 'believe', 'no', 'one', 'said', 'Te', 'Kā', 'from', 'Muana.', 'Easily', 'the', 'most', 'understandable', 'and', 'relatable', 'villain', 'out', 'there.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'believ', 'on', 'said', 'te', 'ka', 'muan', 'easy', 'understand', 'rel', 'villain'], ['cant', 'believe', 'one', 'say', 'te', 'ka', 'muana', 'easily', 'understandable', 'relatable', 'villain'])\n",
      "original document: \n",
      "['Worth', 'clicking', 'though']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wor', 'click', 'though'], ['worth', 'click', 'though'])\n",
      "original document: \n",
      "['There', 'are', 'a', 'few', 'instances', 'in', 'the', 'show', 'where', 'the', 'world', 'does', 'find', 'out', 'about', 'their', 'magic', 'and', 'it', 'causes', 'chaos,', 'the', 'world', \"can't\", 'handle', 'the', 'truth', 'about', 'the', 'magical', 'world', 'which', 'is', 'why', \"it's\", 'kept', 'a', 'secret....', 'for', 'the', 'greater', 'good']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['inst', 'show', 'world', 'find', 'mag', 'caus', 'chao', 'world', 'cant', 'handl', 'tru', 'mag', 'world', 'kept', 'secret', 'gre', 'good'], ['instance', 'show', 'world', 'find', 'magic', 'cause', 'chaos', 'world', 'cant', 'handle', 'truth', 'magical', 'world', 'keep', 'secret', 'greater', 'good'])\n",
      "original document: \n",
      "['Did', 'he', 'have', 'to', 'gain', 'weight', 'to', 'look', 'like', 'that?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gain', 'weight', 'look', 'lik'], ['gain', 'weight', 'look', 'like'])\n",
      "original document: \n",
      "['Yeah', 'I', 'got', 'some', '4', 'cent', 'skins', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'got', 'four', 'cent', 'skin', 'lol'], ['yeah', 'get', 'four', 'cent', 'skin', 'lol'])\n",
      "original document: \n",
      "['CATCH', 'IT,', 'BANANA!', 'CATCH', 'IT!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['catch', 'banan', 'catch'], ['catch', 'banana', 'catch'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['143413728|', '&gt;', 'None', 'Anonymous', '(ID:', 'ycvOUfOn)\\n\\n&gt;&gt;143412250', '(OP)\\nTrump.', 'All', 'other', 'votes', 'were', 'unimportant', 'as', 'if', 'waking', 'from', 'a', 'nightmare.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, seven hundred and twenty-eight', 'gt', 'non', 'anonym', 'id', 'ycvoufon\\n\\ngtgt143412250', 'op\\ntrump', 'vot', 'unimport', 'wak', 'nightmare\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, seven hundred and twenty-eight', 'gt', 'none', 'anonymous', 'id', 'ycvoufon\\n\\ngtgt143412250', 'op\\ntrump', 'vote', 'unimportant', 'wake', 'nightmare\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['I', \"don't\", 'want', 'to', 'hijack,', 'and', \"don't\", 'want', 'to', 'post', 'a', 'new', 'thread,', 'but', 'on', 'the', 'note', 'of', \"'making\", 'bloat', \"easier',\", \"I'm\", 'working', 'on', 'Aprils', 'Fool', 'and', 'it', 'turns', 'out', 'The', 'Wiz', 'is', 'actually', 'super', 'useful.', 'Keeps', 'you', 'at', 'a', 'good', 'angle', 'to', 'avoid', 'most', 'of', 'his', 'crap.', 'Spectral', 'tears', 'so', 'you', 'can', 'stay', 'behind', 'the', 'rocks.', 'Not', 'a', 'run', 'winner,', 'but', 'for', 'an', 'item', 'I', 'laughed', 'off', 'at', 'first,', \"I'm\", 'finding', 'more', 'and', 'more', 'reasons', 'to', 'pick', 'it', 'up.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'want', 'hijack', 'dont', 'want', 'post', 'new', 'thread', 'not', 'mak', 'blo', 'easy', 'im', 'work', 'april', 'fool', 'turn', 'wiz', 'act', 'sup', 'us', 'keep', 'good', 'angl', 'avoid', 'crap', 'spect', 'tear', 'stay', 'behind', 'rock', 'run', 'win', 'item', 'laugh', 'first', 'im', 'find', 'reason', 'pick'], ['dont', 'want', 'hijack', 'dont', 'want', 'post', 'new', 'thread', 'note', 'make', 'bloat', 'easier', 'im', 'work', 'aprils', 'fool', 'turn', 'wiz', 'actually', 'super', 'useful', 'keep', 'good', 'angle', 'avoid', 'crap', 'spectral', 'tear', 'stay', 'behind', 'rock', 'run', 'winner', 'item', 'laugh', 'first', 'im', 'find', 'reason', 'pick'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Of', 'course', 'I', 'would,', 'why', 'would', 'you', 'think', 'I', \"wouldn't.\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cours', 'would', 'would', 'think', 'wouldnt'], ['course', 'would', 'would', 'think', 'wouldnt'])\n",
      "original document: \n",
      "['I', 'wear', 'hijab', 'but', 'I', 'know', 'the', 'struggle.', 'It', 'was', 'very', 'difficult', 'for', 'me', 'to', 'take', 'the', 'step.', '\\n\\nIt', \"wasn't\", 'that', 'I', 'thought', 'hijab', 'is', 'oppressive', 'and', 'backwards.', '\\nIt', \"wasn't\", 'that', 'I', 'denied', 'that', \"it's\", 'an', 'obligation', 'in', 'Islam.', '\\nIt', \"wasn't\", 'that', 'I', 'wanted', 'to', 'dress', 'in', 'a', 'way', 'that', 'would', 'arouse', 'men.', '\\nIt', \"wasn't\", 'that', 'I', \"didn't\", 'pray', 'and', 'had', 'practically', 'no', 'personal', 'relationship', 'with', 'Allah.', '\\nIt', \"wasn't\", 'that', 'I', 'had', 'big', 'sins', 'I', \"wasn't\", 'ready', 'to', 'let', 'go', 'of', 'yet', 'and', 'felt', 'it', 'would', 'be', 'hypocritical', 'to', 'wear', 'hijab.', '\\nIt', \"wasn't\", 'that', 'I', 'had', 'no', 'knowledge', 'of', 'Islam.', '\\nIt', 'was', 'simply', 'that', 'I', 'was', 'afraid...\\n\\nAfraid', 'of', \"people's\", 'reactions,', 'afraid', 'of', 'being', 'treated', 'differently', '(I', 'know', 'how', 'they', 'think', 'about', 'Islam', 'and', 'Muslims),', 'afraid', 'of', 'having', 'less', 'chances', 'etc.', 'I', '*hated*', 'myself', 'for', 'apparently', 'fearing', 'people', 'more', 'than', 'Allah,', 'but', 'it', 'was', 'just', 'extremely', 'difficult', 'for', 'me.', 'I', \"can't\", 'really', 'explain', 'how', 'hard', 'it', 'was.', 'It', 'was', 'a', 'form', 'of', 'social', 'anxiety/phobia', 'and', 'it', 'took', 'me', 'years', 'to', 'finally', 'be', 'able', 'to', 'take', 'the', 'step.', 'The', 'tears', 'I', 'dropped', 'in', 'those', 'years', 'could', 'probably', 'fill', 'a', 'pool.', 'The', 'struggle', 'was', 'real,', \"that's\", 'all', \"I'm\", 'saying.', 'But', 'people', \"don't\", 'see', 'that.', 'They', 'just', 'see', 'your', 'public', 'sin', 'and', 'judge', 'you', 'based', 'on', 'that.\\n\\nThis', 'was', '*my*', 'personal', 'struggle', 'with', 'hijab,', 'but', 'other', 'girls', 'can', 'have', 'different', 'issues', 'with', 'hijab.', 'Or', 'they', 'do', 'wear', 'hijab', 'but', 'have', 'different', 'struggles.', 'Everybody', 'sins,', 'but', 'they', 'sin', 'differently.', 'I', 'know', 'a', 'lot', 'of', 'people', 'struggle', 'with', 'praying', 'on', 'time,', 'porn', 'addiction,', 'masturbation,', 'boyfriends/girlfriends,', 'gambling,', 'drinking,', 'whatever.', 'Some', 'feel', 'guilty', 'about', 'it', 'and', 'others', \"don't.\", 'The', 'only', 'consistent', 'thing', 'is', 'that', 'everybody', 'has', 'their', 'own', 'struggles', 'and', 'we', \"don't\", 'know', \"who's\", 'more', 'beloved', 'by', 'Allah.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wear', 'hijab', 'know', 'struggle', 'difficult', 'tak', 'step', '\\n\\nit', 'wasnt', 'thought', 'hijab', 'oppress', 'backward', '\\nit', 'wasnt', 'deny', 'oblig', 'islam', '\\nit', 'wasnt', 'want', 'dress', 'way', 'would', 'ar', 'men', '\\nit', 'wasnt', 'didnt', 'pray', 'pract', 'person', 'rel', 'allah', '\\nit', 'wasnt', 'big', 'sin', 'wasnt', 'ready', 'let', 'go', 'yet', 'felt', 'would', 'hypocrit', 'wear', 'hijab', '\\nit', 'wasnt', 'knowledg', 'islam', '\\nit', 'simply', 'afraid\\n\\nafraid', 'peopl', 'react', 'afraid', 'tre', 'diff', 'know', 'think', 'islam', 'muslim', 'afraid', 'less', 'chant', 'etc', 'hat', 'app', 'fear', 'peopl', 'allah', 'extrem', 'difficult', 'cant', 'real', 'explain', 'hard', 'form', 'soc', 'anxietyphob', 'took', 'year', 'fin', 'abl', 'tak', 'step', 'tear', 'drop', 'year', 'could', 'prob', 'fil', 'pool', 'struggle', 'real', 'that', 'im', 'say', 'peopl', 'dont', 'see', 'see', 'publ', 'sin', 'judg', 'bas', 'that\\n\\nthis', 'person', 'struggle', 'hijab', 'girl', 'diff', 'issu', 'hijab', 'wear', 'hijab', 'diff', 'struggles', 'everybody', 'sin', 'sin', 'diff', 'know', 'lot', 'peopl', 'struggle', 'pray', 'tim', 'porn', 'addict', 'masturb', 'boyfriendsgirlfriend', 'gambl', 'drink', 'whatev', 'feel', 'guil', 'oth', 'dont', 'consist', 'thing', 'everybody', 'struggles', 'dont', 'know', 'who', 'belov', 'allah'], ['wear', 'hijab', 'know', 'struggle', 'difficult', 'take', 'step', '\\n\\nit', 'wasnt', 'think', 'hijab', 'oppressive', 'backwards', '\\nit', 'wasnt', 'deny', 'obligation', 'islam', '\\nit', 'wasnt', 'want', 'dress', 'way', 'would', 'arouse', 'men', '\\nit', 'wasnt', 'didnt', 'pray', 'practically', 'personal', 'relationship', 'allah', '\\nit', 'wasnt', 'big', 'sin', 'wasnt', 'ready', 'let', 'go', 'yet', 'felt', 'would', 'hypocritical', 'wear', 'hijab', '\\nit', 'wasnt', 'knowledge', 'islam', '\\nit', 'simply', 'afraid\\n\\nafraid', 'people', 'reactions', 'afraid', 'treat', 'differently', 'know', 'think', 'islam', 'muslims', 'afraid', 'less', 'chance', 'etc', 'hat', 'apparently', 'fear', 'people', 'allah', 'extremely', 'difficult', 'cant', 'really', 'explain', 'hard', 'form', 'social', 'anxietyphobia', 'take', 'years', 'finally', 'able', 'take', 'step', 'tear', 'drop', 'years', 'could', 'probably', 'fill', 'pool', 'struggle', 'real', 'thats', 'im', 'say', 'people', 'dont', 'see', 'see', 'public', 'sin', 'judge', 'base', 'that\\n\\nthis', 'personal', 'struggle', 'hijab', 'girls', 'different', 'issue', 'hijab', 'wear', 'hijab', 'different', 'struggle', 'everybody', 'sin', 'sin', 'differently', 'know', 'lot', 'people', 'struggle', 'pray', 'time', 'porn', 'addiction', 'masturbation', 'boyfriendsgirlfriends', 'gamble', 'drink', 'whatever', 'feel', 'guilty', 'others', 'dont', 'consistent', 'thing', 'everybody', 'struggle', 'dont', 'know', 'whos', 'beloved', 'allah'])\n",
      "original document: \n",
      "['I', 'mean', 'why', 'is', 'your', 'name', 'in', 'the', 'history', 'books?', 'Gutenberg', 'who?', 'The', 'printing', 'press', 'guy?', 'Nobody', 'would', 'give', 'a', 'shit', 'nowadays', 'if', 'your', 'last', 'name', 'was', 'Gutenberg.', \"It's\", 'not', 'even', 'famous', 'really.', \"Isn't\", \"everybody's\", 'name', 'in', 'the', 'history', 'books', 'if', 'you', 'look', 'long', 'enough?', \"It's\", 'a', 'meaningless', 'statement', 'to', 'say', 'your', 'name', 'is', 'famous', 'without', 'more', 'information.\\n\\nI', 'also', 'highly', 'doubt', 'that', 'if', 'your', 'family', 'is', 'actually', 'famous', 'that', 'it', \"doesn't\", 'help', 'you', 'in', 'life.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'nam', 'hist', 'book', 'gutenberg', 'print', 'press', 'guy', 'nobody', 'would', 'giv', 'shit', 'nowaday', 'last', 'nam', 'gutenberg', 'ev', 'fam', 'real', 'isnt', 'everybody', 'nam', 'hist', 'book', 'look', 'long', 'enough', 'meaningless', 'stat', 'say', 'nam', 'fam', 'without', 'information\\n\\ni', 'also', 'high', 'doubt', 'famy', 'act', 'fam', 'doesnt', 'help', 'lif'], ['mean', 'name', 'history', 'book', 'gutenberg', 'print', 'press', 'guy', 'nobody', 'would', 'give', 'shit', 'nowadays', 'last', 'name', 'gutenberg', 'even', 'famous', 'really', 'isnt', 'everybodys', 'name', 'history', 'book', 'look', 'long', 'enough', 'meaningless', 'statement', 'say', 'name', 'famous', 'without', 'information\\n\\ni', 'also', 'highly', 'doubt', 'family', 'actually', 'famous', 'doesnt', 'help', 'life'])\n",
      "original document: \n",
      "['\"Problem', 'solving', 'skills\"', 'are', 'improved', 'when', 'you', 'waistband', 'your', 'dick', 'in', 'school', 'so', 'the', 'girls', \"can't\", 'see', 'your', 'wee', 'wee']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'solv', 'skil', 'improv', 'waistband', 'dick', 'school', 'girl', 'cant', 'see', 'wee', 'wee'], ['problem', 'solve', 'skills', 'improve', 'waistband', 'dick', 'school', 'girls', 'cant', 'see', 'wee', 'wee'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['https://imgur.com/a/DbbfT']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsimgurcomadbbft'], ['httpsimgurcomadbbft'])\n",
      "original document: \n",
      "['I', 'would', 'go', 'for', 'Sorcery', 'because', 'of', 'The', 'Ultimate', 'Hat', 'most', 'of', 'the', 'time,', 'it', 'sounds', 'really', 'cool', 'to', 'be', 'in', 'Huge', 'Doggo', 'form', 'more', 'often,', '33s', 'downtime', 'on', 'Ult', 'sounds', 'insane.\\n\\nIf', 'the', 'enemy', 'is', 'a', 'CC', 'Heavy', 'Comp', 'I', 'would', 'opt', 'for', 'Precision,', 'with', 'the', \"'Super\", 'Dangerous', \"Game'\", 'rune', 'and', 'the', 'Tenacity', 'one,', 'sounds', 'good', 'for', 'surviving', 'teamfights', 'and', 'waves', 'of', 'CC.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'go', 'sorcery', 'ultim', 'hat', 'tim', 'sound', 'real', 'cool', 'hug', 'doggo', 'form', 'oft', '33s', 'downtim', 'ult', 'sound', 'insane\\n\\nif', 'enemy', 'cc', 'heavy', 'comp', 'would', 'opt', 'precid', 'sup', 'dang', 'gam', 'run', 'tenac', 'on', 'sound', 'good', 'surv', 'teamfight', 'wav', 'cc'], ['would', 'go', 'sorcery', 'ultimate', 'hat', 'time', 'sound', 'really', 'cool', 'huge', 'doggo', 'form', 'often', '33s', 'downtime', 'ult', 'sound', 'insane\\n\\nif', 'enemy', 'cc', 'heavy', 'comp', 'would', 'opt', 'precision', 'super', 'dangerous', 'game', 'rune', 'tenacity', 'one', 'sound', 'good', 'survive', 'teamfights', 'wave', 'cc'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['He', 'does', 'have', 'pig', 'legs.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pig', 'leg'], ['pig', 'legs'])\n",
      "original document: \n",
      "['Welcome', 'to', 'r\\\\/RandomActsOfBlowJob!', '\\n\\n[Search', 'for', 'others', 'in', 'LosAngeles!](https://reddit.com/r/RandomActsOfBlowJob/search?q=title%3ALosAngeles+%28+subreddit%3ARandomActsOfBlowjob+OR+subreddit%3ARandomActsOfMuffDive+%29&amp;sort=new&amp;t=all)', '***New!!!***', 'Stay', 'up', 'to', 'date', 'with', 'an', '[RSS', 'feed!](https://reddit.com/r/RandomActsOfBlowJob/search.rss?q=title%3ALosAngeles+%28+subreddit%3ARandomActsOfBlowjob+OR+subreddit%3ARandomActsOfMuffDive+%29&amp;sort=new&amp;t=all)\\n\\n[sidebar](https://www.reddit.com/r/RandomActsOfBlowJob/about/sidebar)', '-', '[rules](https://www.reddit.com/r/RandomActsOfBlowJob/about/rules)', '-', '[message', 'mods](https://www.reddit.com/message/compose?to=%2Fr%2FRandomActsOfBlowJob)\\n\\n---\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/RandomActsOfBlowJob)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welcom', 'rrandomactsofblowjob', '\\n\\nsearch', 'oth', 'losangeleshttpsredditcomrrandomactsofblowjobsearchqtitle3alosangeles28subreddit3arandomactsofblowjoborsubreddit3arandomactsofmuffdive29ampsortnewamptall', 'new', 'stay', 'dat', 'rss', 'feedhttpsredditcomrrandomactsofblowjobsearchrssqtitle3alosangeles28subreddit3arandomactsofblowjoborsubreddit3arandomactsofmuffdive29ampsortnewamptall\\n\\nsidebarhttpswwwredditcomrrandomactsofblowjobaboutsidebar', 'ruleshttpswwwredditcomrrandomactsofblowjobaboutr', 'mess', 'modshttpswwwredditcommessagecomposeto2fr2frandomactsofblowjob\\n\\n\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorrandomactsofblowjob', 'quest', 'concern'], ['welcome', 'rrandomactsofblowjob', '\\n\\nsearch', 'others', 'losangeleshttpsredditcomrrandomactsofblowjobsearchqtitle3alosangeles28subreddit3arandomactsofblowjoborsubreddit3arandomactsofmuffdive29ampsortnewamptall', 'new', 'stay', 'date', 'rss', 'feedhttpsredditcomrrandomactsofblowjobsearchrssqtitle3alosangeles28subreddit3arandomactsofblowjoborsubreddit3arandomactsofmuffdive29ampsortnewamptall\\n\\nsidebarhttpswwwredditcomrrandomactsofblowjobaboutsidebar', 'ruleshttpswwwredditcomrrandomactsofblowjobaboutrules', 'message', 'modshttpswwwredditcommessagecomposeto2fr2frandomactsofblowjob\\n\\n\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorrandomactsofblowjob', 'question', 'concern'])\n",
      "original document: \n",
      "['Seconded!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['second'], ['second'])\n",
      "original document: \n",
      "['If', 'you', 'insert', 'a', 'print', 'statement', 'that', 'shows', 'low,', 'high,', 'and', 'mid', 'each', 'time', 'you', 'recalculate', 'them,', 'I', 'think', \"you'll\", 'see', 'some', 'strange', 'values.\\n\\nConsider', 'this', 'code:\\n\\n', '', '', '', '', '', '', '', '', '', '', '', 'low', '=', 'values[mid]', '+', '1;\\n\\nIf', 'low=0', 'and', 'high=4', '(because', 'there', 'are', '5', 'items', 'in', 'the', 'array),', 'this', 'will', 'set', 'low', 'equal', 'to', '1', '+', 'whatever', '**value**', 'is', 'in', 'the', 'middle', 'of', 'the', 'array.', '', 'It', 'could', 'be', '655,', 'if', 'the', 'input', 'was', '{0,', '11,', '655,', '765,', '888},.', '', 'You', \"don't\", 'want', 'low=656,', 'you', 'want', 'low=3.\\n\\nSee', 'the', 'difference?\\n\\nThat', 'mistake', 'is', 'in', 'both', 'your', 'recalculation', 'of', 'low', 'and', 'of', 'high.', '\\n\\nAnd', 'then:', 'once', 'you', 'reset', 'either', 'low', 'or', 'high,', 'you', 'can', 'recalc', 'mid', 'based', 'on', 'these', 'new', 'values.', '', 'mid=(low+high)/2', 'no', 'matter', 'what,', 'right?\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['insert', 'print', 'stat', 'show', 'low', 'high', 'mid', 'tim', 'recalc', 'think', 'youl', 'see', 'strange', 'values\\n\\nconsider', 'code\\n\\n', 'low', 'valuesmid', '1\\n\\nif', 'low0', 'high4', 'fiv', 'item', 'array', 'set', 'low', 'eq', 'on', 'whatev', 'valu', 'middl', 'array', 'could', 'six hundred and fifty-five', 'input', 'zero', 'elev', 'six hundred and fifty-five', 'seven hundred and sixty-five', 'eight hundred and eighty-eight', 'dont', 'want', 'low656', 'want', 'low3\\n\\nsee', 'difference\\n\\nthat', 'mistak', 'recalc', 'low', 'high', '\\n\\nand', 'reset', 'eith', 'low', 'high', 'recalc', 'mid', 'bas', 'new', 'valu', 'midlowhigh2', 'mat', 'right\\n'], ['insert', 'print', 'statement', 'show', 'low', 'high', 'mid', 'time', 'recalculate', 'think', 'youll', 'see', 'strange', 'values\\n\\nconsider', 'code\\n\\n', 'low', 'valuesmid', '1\\n\\nif', 'low0', 'high4', 'five', 'items', 'array', 'set', 'low', 'equal', 'one', 'whatever', 'value', 'middle', 'array', 'could', 'six hundred and fifty-five', 'input', 'zero', 'eleven', 'six hundred and fifty-five', 'seven hundred and sixty-five', 'eight hundred and eighty-eight', 'dont', 'want', 'low656', 'want', 'low3\\n\\nsee', 'difference\\n\\nthat', 'mistake', 'recalculation', 'low', 'high', '\\n\\nand', 'reset', 'either', 'low', 'high', 'recalc', 'mid', 'base', 'new', 'value', 'midlowhigh2', 'matter', 'right\\n'])\n",
      "original document: \n",
      "['I', 'think', \"it's\", 'been', 'called', 'a', 'trace', 'rifle.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'cal', 'trac', 'rifl'], ['think', 'call', 'trace', 'rifle'])\n",
      "original document: \n",
      "['This', 'is', 'some', 'really', 'dumb', \"statistics.\\n\\nLet's\", 'not', 'ignore', \"Ossoff's\", 'race', 'and', 'all', 'the', 'other', 'races', 'we', 'lost', 'earlier', 'in', 'the', 'year.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'dumb', 'statistics\\n\\nlet', 'ign', 'ossoff', 'rac', 'rac', 'lost', 'ear', 'year'], ['really', 'dumb', 'statistics\\n\\nlets', 'ignore', 'ossoffs', 'race', 'race', 'lose', 'earlier', 'year'])\n",
      "original document: \n",
      "['gg.', '', 'gg.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gg', 'gg'], ['gg', 'gg'])\n",
      "original document: \n",
      "['&gt;', \"'Leonardo\", 'da', 'Vinci', 'may', 'have', 'drawn', 'nude', 'Mona', \"Lisa'\\n\\nThere\", 'really', 'are', 'no', 'exceptions', 'to', 'Rule', '34.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['gt', 'leonardo', 'da', 'vinc', 'may', 'drawn', 'nud', 'mon', 'lisa\\n\\nthere', 'real', 'exceiv', 'rul', 'thirty-four'], ['gt', 'leonardo', 'da', 'vinci', 'may', 'draw', 'nude', 'mona', 'lisa\\n\\nthere', 'really', 'exceptions', 'rule', 'thirty-four'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Crimson', 'Hexphase?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['crimson', 'hexphas'], ['crimson', 'hexphase'])\n",
      "original document: \n",
      "['I', 'would', 'love', 'something', 'like', 'this!', \"It's\", 'a', 'shame', 'that', 'more', 'of', 'the', 'competitive', 'scene', \"isn't\", 'documented', 'in', 'video', 'form.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lov', 'someth', 'lik', 'sham', 'competit', 'scen', 'isnt', 'docu', 'video', 'form'], ['would', 'love', 'something', 'like', 'shame', 'competitive', 'scene', 'isnt', 'document', 'video', 'form'])\n",
      "original document: \n",
      "['Yeah...', \"it'll\", 'amplify', 'any', 'non', 'true', 'damage,', 'damage.', '', '\\n\\nTrinity', 'will', 'be', 'amplified.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'itl', 'ampl', 'non', 'tru', 'dam', 'dam', '\\n\\ntrinity', 'ampl'], ['yeah', 'itll', 'amplify', 'non', 'true', 'damage', 'damage', '\\n\\ntrinity', 'amplify'])\n",
      "original document: \n",
      "['Secret', 'volcano', 'lairrrrrrrrrrrrr']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['secret', 'volcano', 'lairrrrrrrrrrrr'], ['secret', 'volcano', 'lairrrrrrrrrrrrr'])\n",
      "original document: \n",
      "[\"What's\", 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what'], ['whats'])\n",
      "original document: \n",
      "['[MovieposterFans](https://www.reddit.com/r/MovieposterFans/comments/73iga0/the_graduate_1967_1000_1500/)', '|', '[Link', 'To', 'Original', 'Submission](http://reddit.com/73ifoe)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['movieposterfanshttpswwwredditcomrmovieposterfanscomments73iga0the_graduate_1967_1000_1500', 'link', 'origin', 'submissionhttpredditcom73ifoe'], ['movieposterfanshttpswwwredditcomrmovieposterfanscomments73iga0the_graduate_1967_1000_1500', 'link', 'original', 'submissionhttpredditcom73ifoe'])\n",
      "original document: \n",
      "['One', 'random', 'please\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'please\\n'], ['one', 'random', 'please\\n'])\n",
      "original document: \n",
      "['Sup', \"y'all\", '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sup', 'yal'], ['sup', 'yall'])\n",
      "original document: \n",
      "['*a', 'hush', 'falls', 'over', 'the', 'crowd*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hush', 'fal', 'crowd'], ['hush', 'fall', 'crowd'])\n",
      "original document: \n",
      "['&gt;Time', 'will', 'tell', 'with', 'this', 'one.\\n\\nI', 'feel', 'like', 'they', 'do', 'it', 'when', 'it', 'thematically', 'makes', 'sense.', 'Zendikar', 'was', 'about', 'discovery,', 'Journey', 'was', 'about', 'going', 'to', 'the', 'realm', 'of', 'the', 'gods,', 'and', 'Ixalan', 'is', 'about', 'exploration,', 'so', 'it', 'makes', 'sense.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gttime', 'tel', 'one\\n\\ni', 'feel', 'lik', 'them', 'mak', 'sens', 'zendik', 'discovery', 'journey', 'going', 'realm', 'god', 'ix', 'expl', 'mak', 'sens'], ['gttime', 'tell', 'one\\n\\ni', 'feel', 'like', 'thematically', 'make', 'sense', 'zendikar', 'discovery', 'journey', 'go', 'realm', 'gods', 'ixalan', 'exploration', 'make', 'sense'])\n",
      "original document: \n",
      "['Hence', '**prediction**']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hent', 'predict'], ['hence', 'prediction'])\n",
      "original document: \n",
      "['what', 'is', 'the', 'point', 'of', 'this']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['point'], ['point'])\n",
      "original document: \n",
      "['\"So', \"it's\", 'safe', 'to', 'assume', 'the', 'short', 'timeline', 'of', 'events', 'that', 'occured', 'in', 'the', 'interval', 'before', 'the', 'rewind', 'would', 'cease', 'to', 'exist?\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saf', 'assum', 'short', 'timelin', 'ev', 'occ', 'interv', 'rewind', 'would', 'ceas', 'ex'], ['safe', 'assume', 'short', 'timeline', 'events', 'occur', 'interval', 'rewind', 'would', 'cease', 'exist'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Right', 'now', 'you', 'should', 'catch', 'up', 'with', 'the', '7', 'deadly', 'dragons.', 'Get', 'Key', 'of', '\"insert', 'dragon', 'name', 'here\"', 'from', 'each', 'dragon', 'and', 'finish', 'all', 'story', 'quest', 'in', '/battleundere.', 'Then', 'do', 'the', 'first', 'quest', 'from', 'a', 'glowing', 'LED', 'dragon', 'and', 'pick', 'up', 'the', 'drop.', 'They', 'would', 'later', 'release', 'items', 'for', 'those', 'who', 'possess', 'the', '7', 'keys', 'and', 'the', 'quest', 'drop.', '\\n\\nIf', \"you're\", 'looking', 'to', 'farm', 'xp', 'and', 'gold,', '/battleground', 'is', 'the', 'best', 'place.', '\\n\\nFor', 'weapons,', 'get', 'the', 'doomblade', 'of', 'destruction.', \"It's\", 'worth', 'getting', 'it', 'considering', 'the', 'dmg', 'and', 'xp,', 'rep,', 'and', 'gold', 'boost', 'it', 'gives', 'you.', 'Probably', 'the', 'only', 'weapon', 'in', 'game', 'with', 'these', 'boosts', 'combined', 'in', 'one', 'weapon.\\n\\nIf', 'you', \"haven't\", 'been', 'on', 'for', '4-5', 'years,', 'then', 'I', 'assume', 'you', 'missed', 'finishing', '13', 'lord', 'of', 'chaos.', \"It's\", 'an', 'interesting', 'storyline', 'so', 'I', 'would', 'recommend', 'completing', 'it.\\n\\nOther', 'than', 'that,', 'aqw', 'is', 'packed', 'with', 'adventures', 'and', 'new', 'items,', 'so', 'keep', 'in', 'track', 'with', 'the', 'news,', 'and', 'patch', 'notes.', 'Also', 'use', 'your', 'map', 'to', 'keep', 'track', 'of', 'the', 'storyline.', 'It', 'would', 'help', 'big', 'time.', 'Hope', 'this', 'helps.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'catch', 'sev', 'dead', 'dragon', 'get', 'key', 'insert', 'dragon', 'nam', 'dragon', 'fin', 'story', 'quest', 'battleund', 'first', 'quest', 'glow', 'led', 'dragon', 'pick', 'drop', 'would', 'lat', 'releas', 'item', 'possess', 'sev', 'key', 'quest', 'drop', '\\n\\nif', 'yo', 'look', 'farm', 'xp', 'gold', 'battleground', 'best', 'plac', '\\n\\nfor', 'weapon', 'get', 'doomblad', 'destruct', 'wor', 'get', 'consid', 'dmg', 'xp', 'rep', 'gold', 'boost', 'giv', 'prob', 'weapon', 'gam', 'boost', 'combin', 'on', 'weapon\\n\\nif', 'hav', 'forty-five', 'year', 'assum', 'miss', 'fin', 'thirteen', 'lord', 'chao', 'interest', 'storylin', 'would', 'recommend', 'complet', 'it\\n\\nother', 'aqw', 'pack', 'adv', 'new', 'item', 'keep', 'track', 'new', 'patch', 'not', 'also', 'us', 'map', 'keep', 'track', 'storylin', 'would', 'help', 'big', 'tim', 'hop', 'helps\\n'], ['right', 'catch', 'seven', 'deadly', 'dragons', 'get', 'key', 'insert', 'dragon', 'name', 'dragon', 'finish', 'story', 'quest', 'battleundere', 'first', 'quest', 'glow', 'lead', 'dragon', 'pick', 'drop', 'would', 'later', 'release', 'items', 'possess', 'seven', 'key', 'quest', 'drop', '\\n\\nif', 'youre', 'look', 'farm', 'xp', 'gold', 'battleground', 'best', 'place', '\\n\\nfor', 'weapons', 'get', 'doomblade', 'destruction', 'worth', 'get', 'consider', 'dmg', 'xp', 'rep', 'gold', 'boost', 'give', 'probably', 'weapon', 'game', 'boost', 'combine', 'one', 'weapon\\n\\nif', 'havent', 'forty-five', 'years', 'assume', 'miss', 'finish', 'thirteen', 'lord', 'chaos', 'interest', 'storyline', 'would', 'recommend', 'complete', 'it\\n\\nother', 'aqw', 'pack', 'adventure', 'new', 'items', 'keep', 'track', 'news', 'patch', 'note', 'also', 'use', 'map', 'keep', 'track', 'storyline', 'would', 'help', 'big', 'time', 'hope', 'helps\\n'])\n",
      "original document: \n",
      "['wow', 'amazing', 'here', ':www.gooodooo.me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'amaz', 'wwwgooodooome'], ['wow', 'amaze', 'wwwgooodooome'])\n",
      "original document: \n",
      "['2', 'random', 'spots', 'please!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'spot', 'pleas'], ['two', 'random', 'spot', 'please'])\n",
      "original document: \n",
      "['My', 'partner', 'and', 'I', 'have', 'the', 'exact', 'same', 'situation!', 'Except', 'our', 'rent', 'is', '$1100.', \"We'd\", 'love', 'to', 'move', 'into', 'a', 'one', 'bedroom', 'but', \"it's\", 'too', 'expensive.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['partn', 'exact', 'situ', 'exceiv', 'rent', 'one thousand, one hundred', 'wed', 'lov', 'mov', 'on', 'bedroom', 'expend'], ['partner', 'exact', 'situation', 'except', 'rent', 'one thousand, one hundred', 'wed', 'love', 'move', 'one', 'bedroom', 'expensive'])\n",
      "original document: \n",
      "[\"He's\", 'a', 'dataminer,', 'he', 'likely', 'just', 'datamined', 'them', 'out', 'of', 'the', '.dat', 'file']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'datamin', 'lik', 'datamin', 'dat', 'fil'], ['hes', 'dataminer', 'likely', 'datamined', 'dat', 'file'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'may', 'be', 'rooting', 'for', 'Clemson', 'tonight', 'but', 'you', 'gobblers', 'have', 'an', 'awesome', 'intro.\\n\\n[Hit', 'it!](https://www.youtube.com/watch?v=YxRxd8aNd6I)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'root', 'clemson', 'tonight', 'gobbl', 'awesom', 'intro\\n\\nhit', 'ithttpswwwyoutubecomwatchvyxrxd8and6i'], ['may', 'root', 'clemson', 'tonight', 'gobblers', 'awesome', 'intro\\n\\nhit', 'ithttpswwwyoutubecomwatchvyxrxd8and6i'])\n",
      "original document: \n",
      "['Higher', 'highs', 'and', 'lower', 'lows.', 'Although', 'to', 'be', 'honest', 'he', \"hasn't\", 'really', 'put', 'a', 'foot', 'wrong', 'in', 'like', '2', 'years.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['high', 'high', 'low', 'low', 'although', 'honest', 'hasnt', 'real', 'put', 'foot', 'wrong', 'lik', 'two', 'year'], ['higher', 'highs', 'lower', 'low', 'although', 'honest', 'hasnt', 'really', 'put', 'foot', 'wrong', 'like', 'two', 'years'])\n",
      "original document: \n",
      "[\"That's\", 'cool,', 'but', 'most', 'of', 'your', 'fellow', 'veterans', 'find', 'the', 'protest', 'disrespectful.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'cool', 'fellow', 'vet', 'find', 'protest', 'disrespect'], ['thats', 'cool', 'fellow', 'veterans', 'find', 'protest', 'disrespectful'])\n",
      "original document: \n",
      "['one', 'random', 'please\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'please\\n'], ['one', 'random', 'please\\n'])\n",
      "original document: \n",
      "['Oh', 'shit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'shit'], ['oh', 'shit'])\n",
      "original document: \n",
      "['Seriously.', 'They', 'were', 'so', 'set', 'on', 'penalizing', 'FSU', 'they', 'even', 'said', 'a', 'pass', 'interference', 'was', 'against', 'the', 'offense', 'that', 'resulted', 'in', 'a', '1st', 'down...', 'When', 'FSU', 'was', 'on', 'offense....', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sery', 'set', 'pen', 'fsu', 'ev', 'said', 'pass', 'interf', 'offens', 'result', '1st', 'fsu', 'offens'], ['seriously', 'set', 'penalize', 'fsu', 'even', 'say', 'pass', 'interference', 'offense', 'result', '1st', 'fsu', 'offense'])\n",
      "original document: \n",
      "['It', 'hurts', 'me', 'that', 'the', 'only', 'two', \"Cordelia's\", \"I've\", 'pulled', 'are', '-spd.', \"She's\", 'still', 'really', 'good,', 'but', 'I', 'just', \"can't\", 'bring', 'myself', 'to', 'invest', 'in', 'a', 'firesweep', 'or', 'quad', 'build', 'when', 'she', 'has', 'such', 'a', 'terrible', 'IV.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hurt', 'two', 'cordelia', 'iv', 'pul', 'spd', 'she', 'stil', 'real', 'good', 'cant', 'bring', 'invest', 'firesweep', 'quad', 'build', 'terr', 'iv'], ['hurt', 'two', 'cordelias', 'ive', 'pull', 'spd', 'shes', 'still', 'really', 'good', 'cant', 'bring', 'invest', 'firesweep', 'quad', 'build', 'terrible', 'iv'])\n",
      "original document: \n",
      "['I', 'can', 'confirm', 'that', 'one.', '', 'Just', 'down', 'Lake', 'St.', 'from', 'my', 'office.', '', 'Good', 'Italian', 'Beef', 'sandwich', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['confirm', 'on', 'lak', 'st', 'off', 'good', 'it', 'beef', 'sandwich', 'wel'], ['confirm', 'one', 'lake', 'st', 'office', 'good', 'italian', 'beef', 'sandwich', 'well'])\n",
      "original document: \n",
      "['if', 'you', 'go', 'back', 'to', 'when', 'Rockstar', 'released', 'the', 'screenshots,', 'one', 'of', 'them', 'was', 'a', 'gang.', 'And', 'the', 'entire', 'gang', 'was', 'ginger.', 'He', 'could', 'have', 'something', 'to', 'do', 'with', 'that.', 'Maybe', 'an', 'Irish', 'gang?', 'Here', 'is', 'what', 'I', 'am', 'referring', 'to', '\\n\\nhttps://media.rockstargames.com/rockstargames-newsite/uploads/d0d67b9e853c2cebe6eb01a89cb524274fc949bc.jpg\\n\\n\\nEDIT:', 'I', 'also', 'just', 'realized', 'that', 'Arthur', 'is', 'the', 'guy', 'who', 'is', 'getting', 'beat', 'up', 'by', 'the', 'big', 'ginger...interesting']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'back', 'rockst', 'releas', 'screenshots', 'on', 'gang', 'entir', 'gang', 'ging', 'could', 'someth', 'mayb', 'ir', 'gang', 'refer', '\\n\\nhttpsmediarockstargamescomrockstargamesnewsiteuploadsd0d67b9e853c2cebe6eb01a89cb524274fc949bcjpg\\n\\n\\nedit', 'also', 'real', 'arth', 'guy', 'get', 'beat', 'big', 'gingerinterest'], ['go', 'back', 'rockstar', 'release', 'screenshots', 'one', 'gang', 'entire', 'gang', 'ginger', 'could', 'something', 'maybe', 'irish', 'gang', 'refer', '\\n\\nhttpsmediarockstargamescomrockstargamesnewsiteuploadsd0d67b9e853c2cebe6eb01a89cb524274fc949bcjpg\\n\\n\\nedit', 'also', 'realize', 'arthur', 'guy', 'get', 'beat', 'big', 'gingerinteresting'])\n",
      "original document: \n",
      "['They', 'are', 'indeed!', 'Tried', 'them', 'on', 'a', 'whim', 'a', 'few', 'months', 'ago,', 'been', 'hooked', 'ever', 'since.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indee', 'tri', 'whim', 'month', 'ago', 'hook', 'ev', 'sint'], ['indeed', 'try', 'whim', 'months', 'ago', 'hook', 'ever', 'since'])\n",
      "original document: \n",
      "['r/unexpectedfuturama']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['runexpectedfuturam'], ['runexpectedfuturama'])\n",
      "original document: \n",
      "['I', 'already', 'spent', 'all', 'my', 'stones', 'on', 'the', 'banner,', 'and', 'finished', 'the', 'medal', 'grinds', 'expecting', 'to', 'get', 'lr', 'gohan', '😢']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'spent', 'ston', 'ban', 'fin', 'med', 'grind', 'expect', 'get', 'lr', 'goh'], ['already', 'spend', 'stone', 'banner', 'finish', 'medal', 'grind', 'expect', 'get', 'lr', 'gohan'])\n",
      "original document: \n",
      "['Sounds', 'like', 'a', 'great', 'idea.', 'The', 'MCU', \"hasn't\", 'been', 'compelling', 'since', 'Avengers', '1.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'lik', 'gre', 'ide', 'mcu', 'hasnt', 'compel', 'sint', 'aveng', 'on'], ['sound', 'like', 'great', 'idea', 'mcu', 'hasnt', 'compel', 'since', 'avengers', 'one'])\n",
      "original document: \n",
      "['Also', 'looks', 'really', 'close', 'to', 'Heroes', 'and', 'Generals,', 'which', 'is', 'free2play', 'on', 'steam.', 'https://www.youtube.com/watch?v=NqTynawPjug']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'look', 'real', 'clos', 'hero', 'gen', 'free2play', 'steam', 'httpswwwyoutubecomwatchvnqtynawpjug'], ['also', 'look', 'really', 'close', 'heroes', 'general', 'free2play', 'steam', 'httpswwwyoutubecomwatchvnqtynawpjug'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['If', 'and', 'only', 'if', 'they', 'already', 'have', 'CTE', 'which', 'is', 'degenerative.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'cte', 'deg'], ['already', 'cte', 'degenerative'])\n",
      "original document: \n",
      "['You', 'may', 'know', 'me', 'daughter', 'Pearl.', \"She's\", 'growing', 'up', 'fast.', 'It', 'seems', 'like', 'it', 'was', 'just', 'yesterday', 'I', 'was', 'teaching', 'her', 'how', 'to', 'breach.', 'Me', 'mammalian', 'angel.', 'OH-', 'Anyway,', 'uh,', 'so', \"she's\", 'going', 'to', 'be', 'working', 'here', 'during', 'her', 'summer', 'vacation.', \"She's\", 'got', 'a', 'lot', 'of', 'fresh', 'ideas', 'to', 'bring', 'in', 'some', 'hungry', 'customers!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'know', 'daught', 'pearl', 'she', 'grow', 'fast', 'seem', 'lik', 'yesterday', 'teach', 'breach', 'mam', 'angel', 'oh', 'anyway', 'uh', 'she', 'going', 'work', 'sum', 'vac', 'she', 'got', 'lot', 'fresh', 'idea', 'bring', 'hungry', 'custom'], ['may', 'know', 'daughter', 'pearl', 'shes', 'grow', 'fast', 'seem', 'like', 'yesterday', 'teach', 'breach', 'mammalian', 'angel', 'oh', 'anyway', 'uh', 'shes', 'go', 'work', 'summer', 'vacation', 'shes', 'get', 'lot', 'fresh', 'ideas', 'bring', 'hungry', 'customers'])\n",
      "original document: \n",
      "['You', 'can', 'multiply', 'by', '(1', '+', 'csc(x))/(1', '+', 'csc(x))', 'and', 'use', 'the', 'fact', 'that', 'tan^(2)(x)', '+', '1', '=', 'csc^(2)(x).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['multiply', 'on', 'cscx1', 'cscx', 'us', 'fact', 'tan2x', 'on', 'csc2x'], ['multiply', 'one', 'cscx1', 'cscx', 'use', 'fact', 'tan2x', 'one', 'csc2x'])\n",
      "original document: \n",
      "['R', 'I', 'P']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['r', 'p'], ['r', 'p'])\n",
      "original document: \n",
      "['one', 'sounds', 'like', 'the', 'japanese', 'version', 'of', 'the', 'other,', 'but', 'he', 'is', 'not.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'sound', 'lik', 'japanes', 'vert'], ['one', 'sound', 'like', 'japanese', 'version'])\n",
      "original document: \n",
      "[\"Don't\", 'be', 'supportive', 'at', 'all', 'of', 'her', 'and', 'their', 'relationship.', 'She', 'needs', 'to', 'see', 'for', 'herself', 'how', 'toxic', 'it', 'is', 'and', 'she', \"can't\", 'lean', 'on', 'you', 'for', 'support', 'anymore', 'because', 'she', \"won't\", 'listen', 'to', 'your', 'advice.', 'Tell', 'her', 'you', 'will', 'have', 'nothing', 'to', 'do', 'with', 'him', 'and', 'you', \"don't\", 'want', 'it', 'to', 'be', 'like', 'this,', 'but', 'he', 'is', 'affecting', 'you', 'as', 'well', 'and', \"you're\", 'under', 'no', 'obligation', 'to', 'be', 'friendly', 'with', 'him.', \"Don't\", 'interact', 'with', 'him', 'at', 'all.\\n\\nThe', 'worst', 'thing', 'you', 'could', 'do', 'is', 'be', 'supportive', 'of', 'her', 'relationship', 'with', 'him', 'just', 'to', 'make', 'her', 'happy', 'or', 'keep', 'the', 'pace.', 'She', 'will', 'keep', 'getting', 'hurt', 'by', 'him', 'but', \"she'll\", 'have', 'you', 'as', 'a', 'safety', 'net.', 'She', 'needs', 'to', 'realize', 'how', 'bad', 'it', 'is', 'without', 'a', 'safety', 'net', 'and', 'then', 'maybe', \"she'll\", 'finally', 'leave', 'him.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'support', 'rel', 'nee', 'see', 'tox', 'cant', 'lean', 'support', 'anym', 'wont', 'list', 'adv', 'tel', 'noth', 'dont', 'want', 'lik', 'affect', 'wel', 'yo', 'oblig', 'friend', 'dont', 'interact', 'all\\n\\nthe', 'worst', 'thing', 'could', 'support', 'rel', 'mak', 'happy', 'keep', 'pac', 'keep', 'get', 'hurt', 'shel', 'saf', 'net', 'nee', 'real', 'bad', 'without', 'saf', 'net', 'mayb', 'shel', 'fin', 'leav'], ['dont', 'supportive', 'relationship', 'need', 'see', 'toxic', 'cant', 'lean', 'support', 'anymore', 'wont', 'listen', 'advice', 'tell', 'nothing', 'dont', 'want', 'like', 'affect', 'well', 'youre', 'obligation', 'friendly', 'dont', 'interact', 'all\\n\\nthe', 'worst', 'thing', 'could', 'supportive', 'relationship', 'make', 'happy', 'keep', 'pace', 'keep', 'get', 'hurt', 'shell', 'safety', 'net', 'need', 'realize', 'bad', 'without', 'safety', 'net', 'maybe', 'shell', 'finally', 'leave'])\n",
      "original document: \n",
      "['What', 'is', 'your', 'scenario', 'then?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['scenario'], ['scenario'])\n",
      "original document: \n",
      "['143413837|', '&gt;', 'France', 'Anonymous', '(ID:', 'G0Z4ZOPh)\\n\\n&gt;&gt;143412250', '(OP)\\nWhy', \"didn't\", 'you', 'vote', 'for', 'her', 'in', '2008?\\nEven', 'though', 'she', 'won', 'the', 'popular', 'vote', 'back', 'then.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, eight hundred and thirty-seven', 'gt', 'frant', 'anonym', 'id', 'g0z4zoph\\n\\ngtgt143412250', 'op\\nwhy', 'didnt', 'vot', '2008\\neven', 'though', 'popul', 'vot', 'back', 'then\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, eight hundred and thirty-seven', 'gt', 'france', 'anonymous', 'id', 'g0z4zoph\\n\\ngtgt143412250', 'op\\nwhy', 'didnt', 'vote', '2008\\neven', 'though', 'popular', 'vote', 'back', 'then\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Å', 'være', 'is', 'to', 'be', 'and', 'et', 'vær/været', 'is', 'a', 'weather/the', 'weather.', 'without', 'the', 'e', 'is', 'a', 'command', 'so', 'it’s', '“be', 'so', 'kind”', 'or', '“be', 'so', 'good”.', 'They', 'are', 'both', 'idioms', 'so', 'they', 'can’t', 'really', 'be', 'translated', 'directly', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vre', 'et', 'vrvret', 'weatherth', 'weath', 'without', 'e', 'command', 'kind', 'good', 'idiom', 'cant', 'real', 'transl', 'direct'], ['vre', 'et', 'vrvret', 'weatherthe', 'weather', 'without', 'e', 'command', 'kind', 'good', 'idioms', 'cant', 'really', 'translate', 'directly'])\n",
      "original document: \n",
      "['FYI,', 'the', 'Caddy', 'Limo', 'is', 'available', 'in', 'the', 'first', 'seeker', 'league', 'in', 'a', 'showcase', 'event,', 'I', 'got', 'mine', 'yesterday.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fyi', 'caddy', 'limo', 'avail', 'first', 'seek', 'leagu', 'showcas', 'ev', 'got', 'min', 'yesterday'], ['fyi', 'caddy', 'limo', 'available', 'first', 'seeker', 'league', 'showcase', 'event', 'get', 'mine', 'yesterday'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Chips', 'and', 'Salsa(concept):\\nTabantha', 'Wheat', '+', 'Rock', 'Salt', '+', 'Any', 'vegetable,', 'flower,', 'or', 'herb.', 'Salty', 'wheat', 'chips', 'with', 'a', 'refreshing', 'vegetable', 'salsa.(Effect', 'changes', 'with', 'ingredients)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chip', 'salsaconcept\\ntabantha', 'whe', 'rock', 'salt', 'veget', 'flow', 'herb', 'sal', 'whe', 'chip', 'refresh', 'veget', 'salsaeffect', 'chang', 'ingredy'], ['chip', 'salsaconcept\\ntabantha', 'wheat', 'rock', 'salt', 'vegetable', 'flower', 'herb', 'salty', 'wheat', 'chip', 'refresh', 'vegetable', 'salsaeffect', 'change', 'ingredients'])\n",
      "original document: \n",
      "['\"STAB', 'THE', 'SHARK!', 'SLICE', \"'EM\", \"'TWEEN\", 'THE', 'EYES!!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stab', 'shark', 'slic', 'em', 'tween', 'ey'], ['stab', 'shark', 'slice', 'em', 'tween', 'eye'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Oh,', 'go', 'beak', 'some', 'DS9', \"DVD's.\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'go', 'beak', 'ds9', 'dvds'], ['oh', 'go', 'beak', 'ds9', 'dvds'])\n",
      "original document: \n",
      "['Economic', 'Prosperity', 'is', 'how', 'the', 'black', 'community', 'will', 'achieve', \"'equal\", \"treatment'.\", 'Stop', 'joining', 'Gangs', 'and', 'Start', 'sending', 'in', 'Applications.', 'Until', 'the', 'community', 'stands', 'up', 'it', \"won't\", 'ever', 'end.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['econom', 'prosp', 'black', 'commun', 'achiev', 'eq', 'tre', 'stop', 'join', 'gang', 'start', 'send', 'apply', 'commun', 'stand', 'wont', 'ev', 'end'], ['economic', 'prosperity', 'black', 'community', 'achieve', 'equal', 'treatment', 'stop', 'join', 'gang', 'start', 'send', 'applications', 'community', 'stand', 'wont', 'ever', 'end'])\n",
      "original document: \n",
      "['My', 'VT', 'coworker', 'says', 'GOBBLE', 'everytime', 'he', 'passed', 'my', 'desk', 'this', 'week.', 'Please', 'Clemson.', 'Help', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vt', 'cowork', 'say', 'gobbl', 'everytim', 'pass', 'desk', 'week', 'pleas', 'clemson', 'help'], ['vt', 'coworker', 'say', 'gobble', 'everytime', 'pass', 'desk', 'week', 'please', 'clemson', 'help'])\n",
      "original document: \n",
      "['That', 'time', 'when', 'the', 'Voyager', 'cast', 'was', 'on', '*Jeopardy*', 'in', 'character.', '\\n\\nhttps://www.youtube.com/watch?v=7oncAGFBHzY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tim', 'voy', 'cast', 'jeopardy', 'charact', '\\n\\nhttpswwwyoutubecomwatchv7oncagfbhzy'], ['time', 'voyager', 'cast', 'jeopardy', 'character', '\\n\\nhttpswwwyoutubecomwatchv7oncagfbhzy'])\n",
      "original document: \n",
      "['Go', 'Clemson!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'clemson'], ['go', 'clemson'])\n",
      "original document: \n",
      "['Yep.', 'Espn', 'streams', 'have', 'sucked', 'lately', 'on', 'many', 'levels', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'espn', 'streams', 'suck', 'lat', 'many', 'level'], ['yep', 'espn', 'stream', 'suck', 'lately', 'many', 'level'])\n",
      "original document: \n",
      "['That', 'would', 'be', 'great', 'if', \"they're\", 'still', 'available!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'gre', 'theyr', 'stil', 'avail'], ['would', 'great', 'theyre', 'still', 'available'])\n",
      "original document: \n",
      "['That’s', 'a', 'fair', 'way', 'to', 'go', 'I', 'would', 'however', 'not', 'get', 'the', 'twin', 'lascanon', 'turret,', 'the', 'predator', 'auto', 'cannon', 'is', '2', 'd3', 'S7', 'at', '-1,', 'but', 'it', 'does', '3', 'damage', 'flat.\\nTo', 'me', 'that’s', 'superior', 'then', 'the', 'twin', 'lascanon.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fair', 'way', 'go', 'would', 'howev', 'get', 'twin', 'lascanon', 'turret', 'pred', 'auto', 'cannon', 'two', 'd3', 's7', 'on', 'three', 'dam', 'flat\\nto', 'that', 'supery', 'twin', 'lascanon'], ['thats', 'fair', 'way', 'go', 'would', 'however', 'get', 'twin', 'lascanon', 'turret', 'predator', 'auto', 'cannon', 'two', 'd3', 's7', 'one', 'three', 'damage', 'flat\\nto', 'thats', 'superior', 'twin', 'lascanon'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Nty', 'dude.', 'Just', 'keys.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nty', 'dud', 'key'], ['nty', 'dude', 'key'])\n",
      "original document: \n",
      "['SuperDisk', 'LS-120', 'baby!', 'No', 'one', 'needs', 'transfer', 'speeds', 'above', 'parallel!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['superdisk', 'ls120', 'baby', 'on', 'nee', 'transf', 'spee', 'parallel'], ['superdisk', 'ls120', 'baby', 'one', 'need', 'transfer', 'speed', 'parallel'])\n",
      "original document: \n",
      "['Except', 'for', 'fixing', 'the', 'secondary', 'suites', 'situation,', 'at', 'least.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exceiv', 'fix', 'second', 'suit', 'situ', 'least'], ['except', 'fix', 'secondary', 'suit', 'situation', 'least'])\n",
      "original document: \n",
      "['iamflapjak', 'only', 'a', 'bachelors', 'sorry']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iamflapjak', 'bachel', 'sorry'], ['iamflapjak', 'bachelor', 'sorry'])\n",
      "original document: \n",
      "['hmm,', 'I', 'was', '12', 'when', 'I', 'first', 'got', 'started', 'in', '/r/Ravenclaw', ':D\\n\\nNow', \"I'm\", 'a', 'tad', 'older', 'tho\\n\\nbtw', 'oomps', 'is', 'definitely', 'catfishing', 'us']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmm', 'twelv', 'first', 'got', 'start', 'rravenclaw', 'd\\n\\nnow', 'im', 'tad', 'old', 'tho\\n\\nbtw', 'oomp', 'definit', 'catf', 'us'], ['hmm', 'twelve', 'first', 'get', 'start', 'rravenclaw', 'd\\n\\nnow', 'im', 'tad', 'older', 'tho\\n\\nbtw', 'oomps', 'definitely', 'catfishing', 'us'])\n",
      "original document: \n",
      "['It', 'might', 'be', 'the', 'blanket.', 'My', 'old', 'cat', 'would', 'only', 'ever', 'sit', 'on', 'my', 'lap', 'if', 'I', 'was', 'wearing', 'a', 'skirt', 'or', 'had', 'a', 'blanket', 'over', 'my', 'lap.', 'Made', 'a', 'warm', 'hammock.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['might', 'blanket', 'old', 'cat', 'would', 'ev', 'sit', 'lap', 'wear', 'skirt', 'blanket', 'lap', 'mad', 'warm', 'hammock'], ['might', 'blanket', 'old', 'cat', 'would', 'ever', 'sit', 'lap', 'wear', 'skirt', 'blanket', 'lap', 'make', 'warm', 'hammock'])\n",
      "original document: \n",
      "['OMg', 'I', \"didn't\", 'even', 'notice', 'that', 'it', 'was', 'you', 'lol', \"don't\", 'worry']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['omg', 'didnt', 'ev', 'not', 'lol', 'dont', 'worry'], ['omg', 'didnt', 'even', 'notice', 'lol', 'dont', 'worry'])\n",
      "original document: \n",
      "['\"Whale', 'shaped\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whal', 'shap'], ['whale', 'shape'])\n",
      "original document: \n",
      "['For', 'the', 'uninitiated,', 'the', 'other', '4', 'elements', 'are:\\n\\n-', 'Emceeing\\n\\n-', 'B-boying\\n\\n-', \"DJ'ing\\n\\n-\", 'Graffiti']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unin', 'four', 'el', 'are\\n\\n', 'emceeing\\n\\n', 'bboying\\n\\n', 'djing\\n\\n', 'graffit'], ['uninitiated', 'four', 'elements', 'are\\n\\n', 'emceeing\\n\\n', 'bboying\\n\\n', 'djing\\n\\n', 'graffiti'])\n",
      "original document: \n",
      "['Perhaps', 'a', 'slight', 'exaggeration.', 'I', 'put', 'about', '0.2-0.3', 'in', 'my', 'evo', 'and', 'have', 'the', 'temps', 'around', '2', 'and', 'take', '30sec+', 'draws', 'which', 'nearly', 'clear', 'the', 'elb', 'in', 'one', 'hit-', '2', 'at', 'most.', 'So', 'about', 'half', 'a', 'g', 'gets', 'me', 'more', 'fucked', 'then', 'a', 'J', 'that', 'big.', '\\n\\nI', 'was', 'always', 'a', 'billy', 'smoker', 'so', \"J's/blunts/pipes\", 'never', 'did', 'much', 'for', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['perhap', 'slight', 'exag', 'put', 'two hundred and three', 'evo', 'temp', 'around', 'two', 'tak', '30sec', 'draw', 'near', 'clear', 'elb', 'on', 'hit', 'two', 'half', 'g', 'get', 'fuck', 'j', 'big', '\\n\\ni', 'alway', 'bil', 'smok', 'jsbluntspipes', 'nev', 'much'], ['perhaps', 'slight', 'exaggeration', 'put', 'two hundred and three', 'evo', 'temps', 'around', 'two', 'take', '30sec', 'draw', 'nearly', 'clear', 'elb', 'one', 'hit', 'two', 'half', 'g', 'get', 'fuck', 'j', 'big', '\\n\\ni', 'always', 'billy', 'smoker', 'jsbluntspipes', 'never', 'much'])\n",
      "original document: \n",
      "['You', 'can', 'play', 'ranked', 'with', 'your', 'bud.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'rank', 'bud'], ['play', 'rank', 'bud'])\n",
      "original document: \n",
      "['But', 'is', 'it', 'more', 'addictive', 'than', 'TVTropes?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['addict', 'tvtropes'], ['addictive', 'tvtropes'])\n",
      "original document: \n",
      "['So', 'so', 'painful', 'to', 'watch.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pain', 'watch'], ['painful', 'watch'])\n",
      "original document: \n",
      "['Oh', 'I', 'see.\\nThanks', 'again', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'see\\nthanks'], ['oh', 'see\\nthanks'])\n",
      "original document: \n",
      "['Weird', 'how', 'the', 'title', 'is', 'not', 'in', 'the', 'article.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weird', 'titl', 'artic'], ['weird', 'title', 'article'])\n",
      "original document: \n",
      "['One', 'or', 'both', 'of', 'my', 'cats', '100%', 'would', 'have', 'pooped', 'in', 'that', 'dirt', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'cat', 'one hundred', 'would', 'poop', 'dirt'], ['one', 'cat', 'one hundred', 'would', 'pooped', 'dirt'])\n",
      "original document: \n",
      "[\"He's\", 'got', 'a', 'sweet', 'rack,', 'then.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'got', 'sweet', 'rack'], ['hes', 'get', 'sweet', 'rack'])\n",
      "original document: \n",
      "['God', \"I'm\", 'having', 'flashbacks', 'of', 'middle', 'school', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'im', 'flashback', 'middl', 'school'], ['god', 'im', 'flashbacks', 'middle', 'school'])\n",
      "original document: \n",
      "['&gt;', 'You', 'are', 'a', 'moronic', 'piece', 'of', 'garbage.', \"\\n\\nyou're\", 'right.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'moron', 'piec', 'garb', '\\n\\nyoure', 'right'], ['gt', 'moronic', 'piece', 'garbage', '\\n\\nyoure', 'right'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'll\", 'take', 'good', 'and', 'boring', 'over', 'pathetic', 'and', 'more', 'boring', 'any', 'day\\n\\nBut', 'also', 'our', 'good', 'years', 'were', 'when', 'Les', 'earned', 'the', '\"Mad', 'Hatter\"', 'nickname.', '', 'We', \"didn't\", 'really', 'get', 'boring', 'until', \"'14.\", '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'tak', 'good', 'bor', 'pathet', 'bor', 'day\\n\\nbut', 'also', 'good', 'year', 'les', 'earn', 'mad', 'hat', 'nicknam', 'didnt', 'real', 'get', 'bor', 'fourteen'], ['ill', 'take', 'good', 'bore', 'pathetic', 'bore', 'day\\n\\nbut', 'also', 'good', 'years', 'les', 'earn', 'mad', 'hatter', 'nickname', 'didnt', 'really', 'get', 'bore', 'fourteen'])\n",
      "original document: \n",
      "['I', 'always', 'need', 'strings.', '', 'I', 'walk', 'out', 'with', 'strings.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'nee', 'strings', 'walk', 'strings'], ['always', 'need', 'string', 'walk', 'string'])\n",
      "original document: \n",
      "['Well,', 'thats', 'pretty', 'much', 'what', 'i', 'did', 'and', 'i', 'ended', 'up', 'dropping', 'a', 'whole', 'semester', 'and', 'take', '2', 'courses', 'the', 'following', 'semester', 'thinking', 'things', 'would', 'get', 'better', 'but', 'crazy', 'stuff', 'are', 'back', 'already', 'and', 'i', 'cant', 'afford', 'any', 'more', 'time.', 'Thing', 'is', 'family', 'is', 'extremely', 'important', 'to', 'me', 'so', 'stuck', 'between', 'a', 'rock', 'and', 'a', 'hard', 'place.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'that', 'pretty', 'much', 'end', 'drop', 'whol', 'semest', 'tak', 'two', 'cours', 'follow', 'semest', 'think', 'thing', 'would', 'get', 'bet', 'crazy', 'stuff', 'back', 'already', 'cant', 'afford', 'tim', 'thing', 'famy', 'extrem', 'import', 'stuck', 'rock', 'hard', 'plac'], ['well', 'thats', 'pretty', 'much', 'end', 'drop', 'whole', 'semester', 'take', 'two', 'course', 'follow', 'semester', 'think', 'things', 'would', 'get', 'better', 'crazy', 'stuff', 'back', 'already', 'cant', 'afford', 'time', 'thing', 'family', 'extremely', 'important', 'stick', 'rock', 'hard', 'place'])\n",
      "original document: \n",
      "['K/D', 'matters', 'in', 'every', 'situation,', 'including', 'public', 'events.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kd', 'mat', 'every', 'situ', 'includ', 'publ', 'ev'], ['kd', 'matter', 'every', 'situation', 'include', 'public', 'events'])\n",
      "original document: \n",
      "['My', 'counter', 'argument', 'to', 'that', 'is', 'that', \"we've\", 'faced', 'two', 'really', 'bad', 'offenses,', 'specifically', 'two', 'really', 'bad', 'running', 'teams,', 'the', 'cardinals', 'and', 'the', 'browns', 'who', 'are', 'bottom', '10', 'in', 'rushing.', 'Heck,', 'even', 'the', 'rams', 'are', '20th', 'in', 'rushing', 'yards', 'per', 'game.', 'If', 'we', 'face', 'a', 'high', 'powered', 'offense', 'like', 'the', 'patriots', 'or', 'falcons,', 'we', 'will', 'get', 'exposed.', 'Big', 'time.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['count', 'argu', 'wev', 'fac', 'two', 'real', 'bad', 'offens', 'spec', 'two', 'real', 'bad', 'run', 'team', 'cardin', 'brown', 'bottom', 'ten', 'rush', 'heck', 'ev', 'ram', '20th', 'rush', 'yard', 'per', 'gam', 'fac', 'high', 'pow', 'offens', 'lik', 'patriot', 'falcon', 'get', 'expos', 'big', 'tim'], ['counter', 'argument', 'weve', 'face', 'two', 'really', 'bad', 'offenses', 'specifically', 'two', 'really', 'bad', 'run', 'team', 'cardinals', 'brown', 'bottom', 'ten', 'rush', 'heck', 'even', 'ram', '20th', 'rush', 'yards', 'per', 'game', 'face', 'high', 'power', 'offense', 'like', 'patriots', 'falcon', 'get', 'expose', 'big', 'time'])\n",
      "original document: \n",
      "['is', 'that', 'good', 'or', 'bad?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'bad'], ['good', 'bad'])\n",
      "original document: \n",
      "['The', 'upside', 'is', 'that', 'if', 'she', 'ever', 'decides', 'to', 'quit,', 'she', 'will', 'still', 'have', 'her', 'children', 'in', 'her', 'life.', 'You', \"won't\", 'abandon', 'her,', 'which', 'is', 'more', 'than', 'many', 'can', 'say.', \"She's\", 'a', 'lucky', 'woman,', 'but', \"doesn't\", 'even', 'know', 'it.', 'I', 'hope', \"she'll\", 'come', 'to', 'her', 'senses', 'after', 'not', 'too', 'much', 'time', 'has', 'passed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['upsid', 'ev', 'decid', 'quit', 'stil', 'childr', 'lif', 'wont', 'abandon', 'many', 'say', 'she', 'lucky', 'wom', 'doesnt', 'ev', 'know', 'hop', 'shel', 'com', 'sens', 'much', 'tim', 'pass'], ['upside', 'ever', 'decide', 'quit', 'still', 'children', 'life', 'wont', 'abandon', 'many', 'say', 'shes', 'lucky', 'woman', 'doesnt', 'even', 'know', 'hope', 'shell', 'come', 'sense', 'much', 'time', 'pass'])\n",
      "original document: \n",
      "['turrible']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turr'], ['turrible'])\n",
      "original document: \n",
      "[':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Is', 'it', 'gonorhea?', 'That', 'shits', 'curable', 'with', 'one', 'dose.', 'Source:', 'A', 'busy', 'had', 'it,', 'I', 'got', 'tested,', 'results', 'came', 'back', 'negative', 'but', 'they', 'dosed', 'me', 'just', 'to', 'be', 'sure.', 'Did', 'your', 'test', 'come', 'back', 'positive?', 'Sure', 'you', \"haven't\", 'had', 'sex', 'since?', 'Sounds', 'like', 'you', 'need', 'another', 'round.\\n\\nEdit:', 'This', 'is', 'purely', 'based', 'on', 'experience.', 'Waiting', 'for', 'a', 'Dr', 'to', 'come', 'by...', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gonorhe', 'shit', 'cur', 'on', 'dos', 'sourc', 'busy', 'got', 'test', 'result', 'cam', 'back', 'neg', 'dos', 'sur', 'test', 'com', 'back', 'posit', 'sur', 'hav', 'sex', 'sint', 'sound', 'lik', 'nee', 'anoth', 'round\\n\\nedit', 'pur', 'bas', 'expery', 'wait', 'dr', 'com'], ['gonorhea', 'shit', 'curable', 'one', 'dose', 'source', 'busy', 'get', 'test', 'result', 'come', 'back', 'negative', 'dose', 'sure', 'test', 'come', 'back', 'positive', 'sure', 'havent', 'sex', 'since', 'sound', 'like', 'need', 'another', 'round\\n\\nedit', 'purely', 'base', 'experience', 'wait', 'dr', 'come'])\n",
      "original document: \n",
      "['And', 'Niki', 'Ashton', 'is', 'literally', 'the', 'definition', 'of', 'identity', 'Politics.', '', 'Shes', 'worse', 'than', 'Hillary', 'Clinton', 'in', 'terms', 'of', 'identity', 'Politics.', '', 'She', 'is', 'the', 'most', 'extreme', 'SJW', 'I', 'have', 'ever', 'seen', 'to', 'run', 'for', 'office,', 'anywhere.\\n\\nJust', 'a', 'recent', 'example:', '\"Believe', 'survivors\"', 'whats', 'that', 'mean?', '', 'That', 'means', 'assume', 'guilt', 'on', 'the', 'accused,', 'thats', 'nonsense.', '', \"That's\", 'completely', 'irrational', 'and', 'disgusting.', '', 'Assume', 'guilt', 'on', 'the', 'accused', 'even', 'if', \"they're\", 'innocent,', 'right?', '', 'Who', 'cares', 'if', 'we', 'ruin', 'their', 'lives.', '???']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nik', 'ashton', 'lit', 'definit', 'id', 'polit', 'she', 'wors', 'hil', 'clinton', 'term', 'id', 'polit', 'extrem', 'sjw', 'ev', 'seen', 'run', 'off', 'anywhere\\n\\njust', 'rec', 'exampl', 'believ', 'surv', 'what', 'mean', 'mean', 'assum', 'guilt', 'accus', 'that', 'nonsens', 'that', 'complet', 'ir', 'disgust', 'assum', 'guilt', 'accus', 'ev', 'theyr', 'innoc', 'right', 'car', 'ruin', 'liv'], ['niki', 'ashton', 'literally', 'definition', 'identity', 'politics', 'shes', 'worse', 'hillary', 'clinton', 'term', 'identity', 'politics', 'extreme', 'sjw', 'ever', 'see', 'run', 'office', 'anywhere\\n\\njust', 'recent', 'example', 'believe', 'survivors', 'whats', 'mean', 'mean', 'assume', 'guilt', 'accuse', 'thats', 'nonsense', 'thats', 'completely', 'irrational', 'disgust', 'assume', 'guilt', 'accuse', 'even', 'theyre', 'innocent', 'right', 'care', 'ruin', 'live'])\n",
      "original document: \n",
      "['I', 'love', 'my', 'country', 'everyday.', 'I', \"don't\", 'need', 'to', 'be', 'told', 'when', 'and', 'where', 'I', 'should', '\"prove', 'it\"', '\\n\\nEdit:\\nThis', 'dude', 'changed', 'his', 'comment', 'completely.\\n\\nIt', 'originally', 'was', '\"why', 'are', 'you', 'so', 'afraid', 'to', 'love', 'your', 'country\"', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'country', 'everyday', 'dont', 'nee', 'told', 'prov', '\\n\\nedit\\nthis', 'dud', 'chang', 'com', 'completely\\n\\nit', 'origin', 'afraid', 'lov', 'country'], ['love', 'country', 'everyday', 'dont', 'need', 'tell', 'prove', '\\n\\nedit\\nthis', 'dude', 'change', 'comment', 'completely\\n\\nit', 'originally', 'afraid', 'love', 'country'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Any', 'source', 'of', 'light', 'causes', 'them', 'to', 'illuminate', 'a', 'rainbow-like', 'aura', 'around', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sourc', 'light', 'caus', 'illumin', 'rainbowlik', 'aur', 'around'], ['source', 'light', 'cause', 'illuminate', 'rainbowlike', 'aura', 'around'])\n",
      "original document: \n",
      "['good', 'on', 'you', 'for', 'buying', 'the', 'star', 'wars', 'prequels', 'of', 'type', 'r', 'civics']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'buy', 'star', 'war', 'prequel', 'typ', 'r', 'civ'], ['good', 'buy', 'star', 'war', 'prequels', 'type', 'r', 'civics'])\n",
      "original document: \n",
      "['Hajimete', 'no', 'Gal', 'has', 'been', 'the', 'most', 'enjoyable', 'trash', 'pile', 'Ive', 'seen', 'in', 'awhile']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hajimet', 'gal', 'enjoy', 'trash', 'pil', 'iv', 'seen', 'awhil'], ['hajimete', 'gal', 'enjoyable', 'trash', 'pile', 'ive', 'see', 'awhile'])\n",
      "original document: \n",
      "['Q1', 'take:', 'You', 'cant', 'have', 'facial', 'hair', 'unless', 'you', 'look', 'like', 'a', 'man', 'without', 'the', 'facial', 'hair.', 'If', 'you', 'use', 'it', 'to', 'appear', 'manlier', 'you', 'are', 'just', 'a', 'liar.', 'Shave', 'your', 'goddamn', 'face', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['q1', 'tak', 'cant', 'fac', 'hair', 'unless', 'look', 'lik', 'man', 'without', 'fac', 'hair', 'us', 'appear', 'man', 'liar', 'shav', 'goddamn', 'fac'], ['q1', 'take', 'cant', 'facial', 'hair', 'unless', 'look', 'like', 'man', 'without', 'facial', 'hair', 'use', 'appear', 'manlier', 'liar', 'shave', 'goddamn', 'face'])\n",
      "original document: \n",
      "['Sorry.', '', '', 'Thought', 'that', 'was', 'in', 'the', 'title.', '', '', 'Albuquerque/rio', 'rancho.', '', 'Posted', 'by', 'phone.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'thought', 'titl', 'albuquerquerio', 'rancho', 'post', 'phon'], ['sorry', 'think', 'title', 'albuquerquerio', 'rancho', 'post', 'phone'])\n",
      "original document: \n",
      "['Hey', 'everyone!', 'Since', 'there', 'are', 'a', 'lot', 'of', 'people', 'talking', 'about', 'my', 'window', 'size,', 'instead', 'of', 'replying', 'individually', 'to', 'everyone', \"I'll\", 'just', 'say', 'it', 'here', '-', \"it's\", 'just', 'personal', 'preference.', 'I', \"don't\", 'really', 'enjoy', 'playing', 'with', 'a', 'big', 'screen,', 'and', 'I', 'feel', 'that', 'I', \"don't\", 'perform', 'as', 'well', 'when', 'doing', 'so.', 'I', 'kinda', 'like', 'being', 'able', 'to', 'see', 'my', 'surroundings', 'without', 'really', 'having', 'to', 'move', 'my', 'eyes', 'too', 'much', '(or', 'at', 'all),', 'and', 'am', 'very', 'used', 'to', 'the', 'SQM', 'size', 'of', 'this', 'particular', 'size.', 'I', 'also', 'enjoy', 'having', 'a', 'bigger', 'chat,', 'but', \"that's\", 'just', 'a', 'nice', 'bonus,', 'I', 'mainly', 'do', 'it', 'for', 'the', \"screen's\", 'size.', 'Too', 'used', 'to', 'it', 'by', 'now,', 'I', 'guess!', '\\n\\nI', 'apologize', 'if', 'it', 'made', 'the', 'video', 'uncomfortable', 'to', 'watch', 'for', 'some', 'of', 'you.', 'Fullscreen', 'should', 'make', 'it', 'much', 'better,', 'but', 'if', \"you're\", 'on', 'a', 'phone,', 'it', 'really', 'would', 'suck,', 'so,', 'sorry', 'bout', 'that!', '\\n\\nI', 'usually', 'do', 'my', 'videos', '[like', 'this](https://www.youtube.com/watch?v=Dcf5x3wpV3Q),', 'the', 'only', 'reason', \"I've\", 'been', 'uploading', 'my', 'hunting', 'videos', 'with', 'the', 'entire', 'screen', 'is', 'so', 'you', 'guys', 'can', 'see', 'the', 'Hunt', 'Analyzer,', 'Preys,', 'stuff', 'like', 'that.', 'But', 'yeah,', 'note', 'taken,', 'on', 'future', 'videos', \"I'll\", 'work', 'on', 'a', 'way', 'to', 'make', 'it', 'less', 'painful', 'to', 'watch!', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'everyon', 'sint', 'lot', 'peopl', 'talk', 'window', 'siz', 'instead', 'reply', 'individ', 'everyon', 'il', 'say', 'person', 'pref', 'dont', 'real', 'enjoy', 'play', 'big', 'screen', 'feel', 'dont', 'perform', 'wel', 'kind', 'lik', 'abl', 'see', 'surround', 'without', 'real', 'mov', 'ey', 'much', 'us', 'sqm', 'siz', 'particul', 'siz', 'also', 'enjoy', 'big', 'chat', 'that', 'nic', 'bon', 'main', 'screens', 'siz', 'us', 'guess', '\\n\\ni', 'apolog', 'mad', 'video', 'uncomfort', 'watch', 'fullscreen', 'mak', 'much', 'bet', 'yo', 'phon', 'real', 'would', 'suck', 'sorry', 'bout', '\\n\\ni', 'us', 'video', 'lik', 'thishttpswwwyoutubecomwatchvdcf5x3wpv3q', 'reason', 'iv', 'upload', 'hunt', 'video', 'entir', 'screen', 'guy', 'see', 'hunt', 'analys', 'prey', 'stuff', 'lik', 'yeah', 'not', 'tak', 'fut', 'video', 'il', 'work', 'way', 'mak', 'less', 'pain', 'watch'], ['hey', 'everyone', 'since', 'lot', 'people', 'talk', 'window', 'size', 'instead', 'reply', 'individually', 'everyone', 'ill', 'say', 'personal', 'preference', 'dont', 'really', 'enjoy', 'play', 'big', 'screen', 'feel', 'dont', 'perform', 'well', 'kinda', 'like', 'able', 'see', 'surround', 'without', 'really', 'move', 'eye', 'much', 'use', 'sqm', 'size', 'particular', 'size', 'also', 'enjoy', 'bigger', 'chat', 'thats', 'nice', 'bonus', 'mainly', 'screen', 'size', 'use', 'guess', '\\n\\ni', 'apologize', 'make', 'video', 'uncomfortable', 'watch', 'fullscreen', 'make', 'much', 'better', 'youre', 'phone', 'really', 'would', 'suck', 'sorry', 'bout', '\\n\\ni', 'usually', 'videos', 'like', 'thishttpswwwyoutubecomwatchvdcf5x3wpv3q', 'reason', 'ive', 'upload', 'hunt', 'videos', 'entire', 'screen', 'guy', 'see', 'hunt', 'analyzer', 'prey', 'stuff', 'like', 'yeah', 'note', 'take', 'future', 'videos', 'ill', 'work', 'way', 'make', 'less', 'painful', 'watch'])\n",
      "original document: \n",
      "['I', 'edited', 'my', 'comment', 'with', 'the', 'salaries.', 'Butch', 'makes', '4.1.', 'Idk', 'man.', 'That', 'would', 'be', 'a', 'fucking', 'home', 'run', 'hire', 'for', 'Tennessee.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['edit', 'com', 'sal', 'butch', 'mak', 'forty-one', 'idk', 'man', 'would', 'fuck', 'hom', 'run', 'hir', 'ten'], ['edit', 'comment', 'salaries', 'butch', 'make', 'forty-one', 'idk', 'man', 'would', 'fuck', 'home', 'run', 'hire', 'tennessee'])\n",
      "original document: \n",
      "['Np', 'thx']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['np', 'thx'], ['np', 'thx'])\n",
      "original document: \n",
      "[\"There's\", 'a', 'bit', 'of', 'a', 'difference', 'between', '\"didn\\'t', 'happen\"', 'and', '\"happened,', 'but', 'at', 'a', 'different', 'event\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'bit', 'diff', 'didnt', 'hap', 'hap', 'diff', 'ev'], ['theres', 'bite', 'difference', 'didnt', 'happen', 'happen', 'different', 'event'])\n",
      "original document: \n",
      "['One', 'of', 'my', 'favorite', 'alt', 'movie', 'posters.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'favorit', 'alt', 'movy', 'post'], ['one', 'favorite', 'alt', 'movie', 'posters'])\n",
      "original document: \n",
      "['You', 'can', 'try', 'carpooling,', 'perhaps.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'carpool', 'perhap'], ['try', 'carpooling', 'perhaps'])\n",
      "original document: \n",
      "['I', 'just', 'blocked', 'him', 'today.', 'I', \"don't\", 'follow', 'him,', 'but', 'too', 'many', 'people', 'that', 'I', 'do', 'follow', 'retweet', 'his', 'shit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['block', 'today', 'dont', 'follow', 'many', 'peopl', 'follow', 'retweet', 'shit'], ['block', 'today', 'dont', 'follow', 'many', 'people', 'follow', 'retweet', 'shit'])\n",
      "original document: \n",
      "['*One', 'time', 'at', 'band', 'camp...*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'tim', 'band', 'camp'], ['one', 'time', 'band', 'camp'])\n",
      "original document: \n",
      "['[link](https://www.youtube.com/watch?v=Jdxa9o3poeE)', 'to', 'full', 'video\\n\\n^lol', '^if', '^you', '^saw', '^the', '^first', '^post', '^my', '^b', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['linkhttpswwwyoutubecomwatchvjdxa9o3poee', 'ful', 'video\\n\\nlol', 'saw', 'first', 'post', 'b'], ['linkhttpswwwyoutubecomwatchvjdxa9o3poee', 'full', 'video\\n\\nlol', 'saw', 'first', 'post', 'b'])\n",
      "original document: \n",
      "['That', 'means', 'nothing.', 'Tamir', 'Rice', 'got', 'shot', 'because', 'he', 'had', 'a', 'pallet', 'gun', 'and', 'that', 'was', 'considered', 'justified.', 'If', 'there', 'is', 'even', 'the', 'tiniest', 'reason', 'to', 'be', 'scared', 'its', 'a', 'justified', 'shooting', 'which', 'is', 'a', 'meaningless', 'standard', 'because', 'in', 'the', 'end', 'an', 'innocent', 'person', 'just', 'died.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'noth', 'tamir', 'ric', 'got', 'shot', 'pallet', 'gun', 'consid', 'just', 'ev', 'tiniest', 'reason', 'scar', 'just', 'shoot', 'meaningless', 'standard', 'end', 'innoc', 'person', 'died'], ['mean', 'nothing', 'tamir', 'rice', 'get', 'shoot', 'pallet', 'gun', 'consider', 'justify', 'even', 'tiniest', 'reason', 'scar', 'justify', 'shoot', 'meaningless', 'standard', 'end', 'innocent', 'person', 'die'])\n",
      "original document: \n",
      "[\"I've\", 'tried', 'both', 'PayPal', 'and', 'my', 'regular', 'debit', 'card.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'tri', 'payp', 'regul', 'debit', 'card'], ['ive', 'try', 'paypal', 'regular', 'debit', 'card'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['The', 'title', 'is', 'just', 'so', 'out', 'of', 'context.', 'He', 'is', 'literally', 'sending', 'millions', 'of', 'dollars', 'of', 'relief', 'and', 'the', 'media', 'acts', 'like', 'he', \"isn't\", 'doing', 'anything.', 'What', 'do', 'you', 'want', 'him', 'to', 'do,', 'just', 'magically', 'make', 'all', 'of', 'Puerto', 'Ricos', 'problems', 'disappear?', 'Sorry', 'people', 'but', 'this', 'is', 'the', 'real', 'world', 'where', 'you', \"can't\", 'sleep', 'in', 'your', 'Minecraft', 'bed', 'to', 'speed', 'up', 'the', 'time.', 'Theres', 'only', 'so', 'much', 'he', 'can', 'do,', 'the', 'rest', 'is', 'on', 'the', 'Puerto', 'Ricans', 'to', 'actually', 'rebuild', 'themselves.', 'Just', 'look', 'at', 'all', 'of', 'his', 'tweets', 'about', 'Puerto', 'Rico', 'from', 'the', 'past', '24', 'hours.', 'Does', 'it', 'seem', 'like', 'he', \"doesn't\", 'care?', 'Count', 'the', 'number', 'of', 'his', 'Puerto', 'Rico', 'related', 'tweets', 'in', 'the', 'past', 'week.', 'I', 'mean', 'honestly', \"it's\", 'like', 'CNN', 'and', 'the', 'rest', 'of', 'them', 'are', 'living', 'in', 'a', 'parallel', 'universe.', \"He's\", 'going', 'there', 'on', 'Tuesday.', 'I', \"don't\", 'have', 'a', 'problem', 'with', 'a', 'human', 'taking', 'a', 'golf', 'break,', 'especially', 'after', 'all', 'the', 'work', 'he', 'does.', 'Come', 'on', 'people,', \"it's\", 'a', 'Saturday,', 'he', 'spent', 'the', 'entire', 'week', 'mucking', 'through', 'the', 'tax', 'code', 'for', 'US,', 'The', 'American', 'people.', 'Let', 'him', 'golf', 'for', 'an', 'hour.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['titl', 'context', 'lit', 'send', 'mil', 'doll', 'reliev', 'med', 'act', 'lik', 'isnt', 'anyth', 'want', 'mag', 'mak', 'puerto', 'rico', 'problem', 'disappear', 'sorry', 'peopl', 'real', 'world', 'cant', 'sleep', 'minecraft', 'bed', 'spee', 'tim', 'ther', 'much', 'rest', 'puerto', 'ric', 'act', 'rebuild', 'look', 'tweet', 'puerto', 'rico', 'past', 'twenty-four', 'hour', 'seem', 'lik', 'doesnt', 'car', 'count', 'numb', 'puerto', 'rico', 'rel', 'tweet', 'past', 'week', 'mean', 'honest', 'lik', 'cnn', 'rest', 'liv', 'parallel', 'univers', 'hes', 'going', 'tuesday', 'dont', 'problem', 'hum', 'tak', 'golf', 'break', 'espec', 'work', 'com', 'peopl', 'saturday', 'spent', 'entir', 'week', 'muck', 'tax', 'cod', 'us', 'am', 'peopl', 'let', 'golf', 'hour'], ['title', 'context', 'literally', 'send', 'millions', 'dollars', 'relief', 'media', 'act', 'like', 'isnt', 'anything', 'want', 'magically', 'make', 'puerto', 'ricos', 'problems', 'disappear', 'sorry', 'people', 'real', 'world', 'cant', 'sleep', 'minecraft', 'bed', 'speed', 'time', 'theres', 'much', 'rest', 'puerto', 'ricans', 'actually', 'rebuild', 'look', 'tweet', 'puerto', 'rico', 'past', 'twenty-four', 'hours', 'seem', 'like', 'doesnt', 'care', 'count', 'number', 'puerto', 'rico', 'relate', 'tweet', 'past', 'week', 'mean', 'honestly', 'like', 'cnn', 'rest', 'live', 'parallel', 'universe', 'hes', 'go', 'tuesday', 'dont', 'problem', 'human', 'take', 'golf', 'break', 'especially', 'work', 'come', 'people', 'saturday', 'spend', 'entire', 'week', 'muck', 'tax', 'code', 'us', 'american', 'people', 'let', 'golf', 'hour'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Mickey', 'Gall', 'was', 'only', '1', 'fight', 'removed', 'from', 'being', 'an', 'amateur', 'himself', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mickey', 'gal', 'on', 'fight', 'remov', 'am'], ['mickey', 'gall', 'one', 'fight', 'remove', 'amateur'])\n",
      "original document: \n",
      "['Thats', 'my', 'bad', 'i', 'used', 'to', 'drive', 'past', 'it', 'all', 'the', 'time', 'on', 'my', 'way', 'home', 'from', 'work,', 'sorry', 'everyone']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'bad', 'us', 'driv', 'past', 'tim', 'way', 'hom', 'work', 'sorry', 'everyon'], ['thats', 'bad', 'use', 'drive', 'past', 'time', 'way', 'home', 'work', 'sorry', 'everyone'])\n",
      "original document: \n",
      "['No,', 'it', 'means', 'you', 'wont', 'get', 'good', 'unscuffed', 'vods,', 'instead', \"you'll\", 'be', 'watching', \"ice's\", 'normie', 'vids', 'with', 'jumpcuts', 'every', '2', 'secs.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'wont', 'get', 'good', 'unscuff', 'vod', 'instead', 'youl', 'watch', 'ic', 'normy', 'vid', 'jumpcut', 'every', 'two', 'sec'], ['mean', 'wont', 'get', 'good', 'unscuffed', 'vods', 'instead', 'youll', 'watch', 'ice', 'normie', 'vids', 'jumpcuts', 'every', 'two', 'secs'])\n",
      "original document: \n",
      "['We', 'play', 'in', 'Neyland', 'this', 'year?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'neyland', 'year'], ['play', 'neyland', 'year'])\n",
      "original document: \n",
      "['As', 'long', 'as', 'they', \"don't\", 'leave', 'the', 'base', 'line', \"it's\", 'legal']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['long', 'dont', 'leav', 'bas', 'lin', 'leg'], ['long', 'dont', 'leave', 'base', 'line', 'legal'])\n",
      "original document: \n",
      "['Thanks,', 'this', 'is', 'why', \"I'm\", 'not', 'higher', 'in', 'arena.', 'I', 'definitely', \"would've\", 'went', 'Psamathe', 'then', 'Orion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'im', 'high', 'aren', 'definit', 'wouldv', 'went', 'psamath', 'or'], ['thank', 'im', 'higher', 'arena', 'definitely', 'wouldve', 'go', 'psamathe', 'orion'])\n",
      "original document: \n",
      "['🤢🤢🤢']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['A', 'Turing', 'test!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tur', 'test'], ['turing', 'test'])\n",
      "original document: \n",
      "['The', '6', 'iv', 'popplio', 'sounds', 'good,', 'give', 'me', 'a', 'few', 'minutes', 'to', 'breed', 'the', 'exeggcute.', 'Is', 'there', 'anything', 'else', '(like', 'Moon', 'ultrabeasts/legendaries/anything', 'of', 'a', 'higher', 'value)', \"you'd\", 'trade', 'for', 'the', '', 'Type:', 'Null', 'though?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['six', 'iv', 'popplio', 'sound', 'good', 'giv', 'minut', 'bree', 'exeggcut', 'anyth', 'els', 'lik', 'moon', 'ultrabeastslegendariesanyth', 'high', 'valu', 'youd', 'trad', 'typ', 'nul', 'though'], ['six', 'iv', 'popplio', 'sound', 'good', 'give', 'minutes', 'breed', 'exeggcute', 'anything', 'else', 'like', 'moon', 'ultrabeastslegendariesanything', 'higher', 'value', 'youd', 'trade', 'type', 'null', 'though'])\n",
      "original document: \n",
      "[\"Let's\", 'go', 'Briscoe!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'go', 'brisco'], ['let', 'go', 'briscoe'])\n",
      "original document: \n",
      "['大人のふりかけの明太子だよ！']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['I', 'was', 'living', 'in', 'North', 'Moorhead', '2-3', 'years', 'ago,', 'they', 'were', 're-doing', '15th', 'between', 'the', 'river', 'and', '11th.', 'I', \"don't\", 'think', 'they', 'did', 'anything', 'east', 'of', '11th', 'though.\\n\\nAlthough', 'I', 'have', 'noticed', 'that', 'the', 'Fargo', 'area', 'has', 'a', 'really', 'strange', 'habit', 'of', 're-doing', 'the', 'same', 'damn', 'roads', 'every', 'year.', \"They're\", '\"fixing\"', '19th', 'North', 'for', 'the', 'fourth', 'time', 'in', 'the', 'last', 'five', 'years.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['liv', 'nor', 'moorhead', 'twenty-three', 'year', 'ago', 'redo', '15th', 'riv', '11th', 'dont', 'think', 'anyth', 'east', '11th', 'though\\n\\nalthough', 'not', 'fargo', 'are', 'real', 'strange', 'habit', 'redo', 'damn', 'road', 'every', 'year', 'theyr', 'fix', '19th', 'nor', 'four', 'tim', 'last', 'fiv', 'year'], ['live', 'north', 'moorhead', 'twenty-three', 'years', 'ago', 'redo', '15th', 'river', '11th', 'dont', 'think', 'anything', 'east', '11th', 'though\\n\\nalthough', 'notice', 'fargo', 'area', 'really', 'strange', 'habit', 'redo', 'damn', 'roads', 'every', 'year', 'theyre', 'fix', '19th', 'north', 'fourth', 'time', 'last', 'five', 'years'])\n",
      "original document: \n",
      "['It', 'said', 'it', 'was', 'a', 'Christian', 'school.', 'Religious', 'school', '=', 'private.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'christian', 'school', 'religy', 'school', 'priv'], ['say', 'christian', 'school', 'religious', 'school', 'private'])\n",
      "original document: \n",
      "['Nope.', \"They're\", 'going', 'to', 'deify', 'him', 'as', 'they', 'did', 'Reagan.', 'Supreme', 'leader', 'can', 'do', 'no', 'wrong.', 'Not', 'worshiping', 'him', 'will', 'be', 'seen', 'as', 'disrespecting', 'the', 'troops.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nop', 'theyr', 'going', 'deify', 'reag', 'suprem', 'lead', 'wrong', 'wor', 'seen', 'disrespect', 'troop'], ['nope', 'theyre', 'go', 'deify', 'reagan', 'supreme', 'leader', 'wrong', 'worship', 'see', 'disrespect', 'troop'])\n",
      "original document: \n",
      "['143413069|', '&gt;', 'Canada', 'Anonymous', '(ID:', '8h4XA49X)\\n\\n&gt;&gt;143412829\\n&gt;&gt;143412925\\nyep', 'thx', 'bro\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand and sixty-nin', 'gt', 'canad', 'anonym', 'id', '8h4xa49x\\n\\ngtgt143412829\\ngtgt143412925\\nyep', 'thx', 'bro\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand and sixty-nine', 'gt', 'canada', 'anonymous', 'id', '8h4xa49x\\n\\ngtgt143412829\\ngtgt143412925\\nyep', 'thx', 'bro\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Only', 'because', 'I', \"don't\", 'think', 'McCain', 'will', 'run', 'again.', 'But', \"it'll\", 'still', 'be', 'a', 'straight', 'R', 'ticket', 'for', 'me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'mccain', 'run', 'itl', 'stil', 'straight', 'r', 'ticket'], ['dont', 'think', 'mccain', 'run', 'itll', 'still', 'straight', 'r', 'ticket'])\n",
      "original document: \n",
      "['No,', \"I'm\", 'saying', 'I', \"don't\", 'watch', 'him', 'because', 'I', 'disagree', 'with', 'his', 'reviews,', 'why', 'would', 'I', 'watch', 'someone', 'who', \"doesn't\", 'share', 'the', 'same', 'music', 'tastes', 'as', 'me?', \"That's\", 'all', \"I'm\", 'saying,', 'I', 'think', \"he's\", 'a', 'great', 'reviewer,', 'just', 'not', 'for', 'me', 'and', 'my', 'tastes.', 'I', 'did', 'word', 'my', 'original', 'statement', 'wrong,', 'I', \"shouldn't\", 'say', \"he's\", 'not', 'a', 'legitimate', 'source', 'for', 'reviews,', 'I', 'just', \"don't\", 'think', 'for', 'me,', 'personally,', 'that', \"he's\", 'a', 'good', 'source', 'for', 'reviews,', 'not', 'in', 'general.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'say', 'dont', 'watch', 'disagr', 'review', 'would', 'watch', 'someon', 'doesnt', 'shar', 'mus', 'tast', 'that', 'im', 'say', 'think', 'hes', 'gre', 'review', 'tast', 'word', 'origin', 'stat', 'wrong', 'shouldnt', 'say', 'hes', 'legitim', 'sourc', 'review', 'dont', 'think', 'person', 'hes', 'good', 'sourc', 'review', 'gen'], ['im', 'say', 'dont', 'watch', 'disagree', 'review', 'would', 'watch', 'someone', 'doesnt', 'share', 'music', 'taste', 'thats', 'im', 'say', 'think', 'hes', 'great', 'reviewer', 'taste', 'word', 'original', 'statement', 'wrong', 'shouldnt', 'say', 'hes', 'legitimate', 'source', 'review', 'dont', 'think', 'personally', 'hes', 'good', 'source', 'review', 'general'])\n",
      "original document: \n",
      "['nikaigotskills', '299', 'hunter/titan/warlock']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nikaigotskil', 'two hundred and ninety-nine', 'huntertitanwarlock'], ['nikaigotskills', 'two hundred and ninety-nine', 'huntertitanwarlock'])\n",
      "original document: \n",
      "['Uhhhh......a', '$9.99', 'deck', \"doesn't\", 'sound', 'like', 'something', \"I'd\", 'see', 'in', 'a', 'F2P', 'store.', 'That', 'sounds', 'like', 'a', 'DLC.\\n\\nA', 'F2P', 'store', 'would', 'be', 'purely', 'cosmetic', 'and', 'provide', 'no', 'in', 'game', 'advantages.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uhhhh', 'nine hundred and ninety-nin', 'deck', 'doesnt', 'sound', 'lik', 'someth', 'id', 'see', 'f2p', 'stor', 'sound', 'lik', 'dlc\\n\\na', 'f2p', 'stor', 'would', 'pur', 'cosmet', 'provid', 'gam', 'adv'], ['uhhhha', 'nine hundred and ninety-nine', 'deck', 'doesnt', 'sound', 'like', 'something', 'id', 'see', 'f2p', 'store', 'sound', 'like', 'dlc\\n\\na', 'f2p', 'store', 'would', 'purely', 'cosmetic', 'provide', 'game', 'advantage'])\n",
      "original document: \n",
      "['Wow', 'how', 'did', 'you', 'get', 'your', 'hands', 'on', 'such', 'a', '\"rare\"', 'video?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'get', 'hand', 'rar', 'video'], ['wow', 'get', 'hand', 'rare', 'video'])\n",
      "original document: \n",
      "['In', 'one', 'the', 'HGCs', 'where', 'you', 'could', 'vote', 'on', 'a', 'player', 'and', 'get', 'packs', 'based', 'on', 'how', 'far', 'they', 'went', 'in.', 'People', 'thought', 'he', 'would', 'get', 'far', 'in', 'the', 'tournament,', 'but', 'he', 'ended', 'up', 'dropping', 'out', 'really', 'early', 'resulting', 'in', 'all', 'those', 'people', 'getting', 'only', '1', 'pack.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'hgcs', 'could', 'vot', 'play', 'get', 'pack', 'bas', 'far', 'went', 'peopl', 'thought', 'would', 'get', 'far', 'tourna', 'end', 'drop', 'real', 'ear', 'result', 'peopl', 'get', 'on', 'pack'], ['one', 'hgcs', 'could', 'vote', 'player', 'get', 'pack', 'base', 'far', 'go', 'people', 'think', 'would', 'get', 'far', 'tournament', 'end', 'drop', 'really', 'early', 'result', 'people', 'get', 'one', 'pack'])\n",
      "original document: \n",
      "['Danny', 'Mills', 'can', 'suck', 'a', 'fat', 'one,', 'cunt', 'of', 'a', 'man.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['danny', 'mil', 'suck', 'fat', 'on', 'cunt', 'man'], ['danny', 'mill', 'suck', 'fat', 'one', 'cunt', 'man'])\n",
      "original document: \n",
      "['I', \"don't\", 'know', 'what', 'it', 'is,', 'but', 'your', 'comment', 'had', 'me', 'rolling']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'com', 'rol'], ['dont', 'know', 'comment', 'roll'])\n",
      "original document: \n",
      "[\"It's\", 'the', 'chords', 'to', 'the', 'song.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chord', 'song'], ['chord', 'song'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"That's\", 'not', 'exactly', 'true']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'exact', 'tru'], ['thats', 'exactly', 'true'])\n",
      "original document: \n",
      "['Looks', 'fucking', 'sick.', 'Like', 'a', 'real', 'bullet', 'hell.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'fuck', 'sick', 'lik', 'real', 'bullet', 'hel'], ['look', 'fuck', 'sick', 'like', 'real', 'bullet', 'hell'])\n",
      "original document: \n",
      "['royal', 'yacht\\n\\nsalty', 'dogs\\n\\nluxury', 'bullseye', 'flake\\n\\nno', 'particular', 'order']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['roy', 'yacht\\n\\nsalty', 'dogs\\n\\nluxury', 'bullsey', 'flake\\n\\nno', 'particul', 'ord'], ['royal', 'yacht\\n\\nsalty', 'dogs\\n\\nluxury', 'bullseye', 'flake\\n\\nno', 'particular', 'order'])\n",
      "original document: \n",
      "['Miralo', 'a', 'este', 'tan', 'progre', 'que', 'era.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['miralo', 'est', 'tan', 'progr', 'que', 'er'], ['miralo', 'este', 'tan', 'progre', 'que', 'era'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['You', 'have', 'successfully', 'tipped', 'muddd3d', '200', 'iota($0.000124).\\n\\n[Deposit](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Deposit&amp;message=Deposit', 'iota!)', '|', '[Withdraw](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Withdraw&amp;message=I', 'want', 'to', 'withdraw', 'my', 'iota!\\nxxx', 'iota', '\\naddress', 'here)', '|', '[Balance](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Balance&amp;message=I', 'want', 'to', 'check', 'my', 'balance!)', '|', '[Help](https://www.reddit.com/r/iotaTipBot/wiki/index)', '|', '[Donate](https://np.reddit.com/message/compose/?to=iotaTipBot&amp;subject=Donate&amp;message=I', 'want', 'to', 'support', 'iotaTipBot!)\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['success', 'tip', 'muddd3d', 'two hundred', 'iota0000124\\n\\ndeposithttpsnpredditcommessagecomposetoiotatipbotampsubjectdepositampmessagedeposit', 'iot', 'withdrawhttpsnpredditcommessagecomposetoiotatipbotampsubjectwithdrawampmessage', 'want', 'withdraw', 'iota\\nxxx', 'iot', '\\naddress', 'balancehttpsnpredditcommessagecomposetoiotatipbotampsubjectbalanceampmessage', 'want', 'check', 'bal', 'helphttpswwwredditcomriotatipbotwikiindex', 'donatehttpsnpredditcommessagecomposetoiotatipbotampsubjectdonateampmessage', 'want', 'support', 'iotatipbot\\n'], ['successfully', 'tip', 'muddd3d', 'two hundred', 'iota0000124\\n\\ndeposithttpsnpredditcommessagecomposetoiotatipbotampsubjectdepositampmessagedeposit', 'iota', 'withdrawhttpsnpredditcommessagecomposetoiotatipbotampsubjectwithdrawampmessagei', 'want', 'withdraw', 'iota\\nxxx', 'iota', '\\naddress', 'balancehttpsnpredditcommessagecomposetoiotatipbotampsubjectbalanceampmessagei', 'want', 'check', 'balance', 'helphttpswwwredditcomriotatipbotwikiindex', 'donatehttpsnpredditcommessagecomposetoiotatipbotampsubjectdonateampmessagei', 'want', 'support', 'iotatipbot\\n'])\n",
      "original document: \n",
      "['Can', 'you', 'say', 'Fünf?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'funf'], ['say', 'funf'])\n",
      "original document: \n",
      "['Basically', 'the', 'inverse', 'of', \"'the\", 'social', \"contract'.\\n\\n\", '-', 'Property', 'rights', 'are', 'to', 'be', 'upheld.\\n\\n', '-', 'No', 'entity', 'shall', 'have', 'authority', 'over', 'you', 'on', 'your', 'own', 'property.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bas', 'invers', 'soc', 'contract\\n\\n', 'property', 'right', 'upheld\\n\\n', 'ent', 'shal', 'auth', 'property'], ['basically', 'inverse', 'social', 'contract\\n\\n', 'property', 'right', 'upheld\\n\\n', 'entity', 'shall', 'authority', 'property'])\n",
      "original document: \n",
      "['this', 'is', 'a', 'good', 'idea,,', 'you', 'can', 'probably', 'get', 'a', 'very', 'good', 'gradient', 'of', 'amount', 'of', 'effect&gt;how', 'hardcore.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'ide', 'prob', 'get', 'good', 'grady', 'amount', 'effectgthow', 'hardc'], ['good', 'idea', 'probably', 'get', 'good', 'gradient', 'amount', 'effectgthow', 'hardcore'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['It', 'was', 'more', 'like', '$10', 'million', 'dollars', 'after', 'the', 'previous', 'government', 'infringed', 'on', 'his', 'rights', 'within', 'the', 'Charter', 'of', 'Rights', 'and', 'Freedoms', '(basically', 'the', 'Canadian', 'constitution).', 'Trudeau', 'knew', 'Khadr', 'would', 'win', 'in', 'court,', 'and', 'settled', 'for', 'paying', '$10', 'million', 'instead', 'of', 'an', 'amount', 'multiple', 'times', 'more.', '\\n\\nNot', 'saying', \"I'm\", 'happy', 'with', 'Khadr', 'getting', '$10', 'million,', 'but', 'this', 'was', 'more', 'of', 'a', 'fuck', 'up', 'on', 'the', 'previous', 'government', 'since', 'they', 'blatantly', 'violated', 'his', 'rights', 'as', 'a', 'Canadian.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'ten', 'mil', 'doll', 'prevy', 'govern', 'infr', 'right', 'within', 'chart', 'right', 'freedom', 'bas', 'canad', 'constitut', 'trudeau', 'knew', 'khadr', 'would', 'win', 'court', 'settl', 'pay', 'ten', 'mil', 'instead', 'amount', 'multipl', 'tim', '\\n\\nnot', 'say', 'im', 'happy', 'khadr', 'get', 'ten', 'mil', 'fuck', 'prevy', 'govern', 'sint', 'blat', 'viol', 'right', 'canad'], ['like', 'ten', 'million', 'dollars', 'previous', 'government', 'infringe', 'right', 'within', 'charter', 'right', 'freedoms', 'basically', 'canadian', 'constitution', 'trudeau', 'know', 'khadr', 'would', 'win', 'court', 'settle', 'pay', 'ten', 'million', 'instead', 'amount', 'multiple', 'time', '\\n\\nnot', 'say', 'im', 'happy', 'khadr', 'get', 'ten', 'million', 'fuck', 'previous', 'government', 'since', 'blatantly', 'violate', 'right', 'canadian'])\n",
      "original document: \n",
      "['Touche']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['touch'], ['touche'])\n",
      "original document: \n",
      "['Oh.', 'So', 'they', 'feel', 'bad', 'about', 'punching', 'him', 'in', 'the', 'head', 'to', 'stick', 'his', 'tongue', 'out,', 'but', 'not', 'to', 'springboard', 'off', 'him', 'to', 'make', 'a', 'long', 'gap', 'leaving', 'him', 'to', 'die', 'in', 'a', 'seemingly', 'bottomless', 'pit?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'feel', 'bad', 'punch', 'head', 'stick', 'tongu', 'springboard', 'mak', 'long', 'gap', 'leav', 'die', 'seem', 'bottomless', 'pit'], ['oh', 'feel', 'bad', 'punch', 'head', 'stick', 'tongue', 'springboard', 'make', 'long', 'gap', 'leave', 'die', 'seemingly', 'bottomless', 'pit'])\n",
      "original document: \n",
      "['Subscribed!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['subscrib'], ['subscribe'])\n",
      "original document: \n",
      "['No', 'more', 'Pepe', 'and', 'fake', 'cards']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pep', 'fak', 'card'], ['pepe', 'fake', 'card'])\n",
      "original document: \n",
      "['So....she', 'refuses', 'to', 'meet', 'with', 'FEMA', 'and', 'to', 'coordinate', 'efforts', 'for', 'help', 'and', 'all', 'we', 'focus', 'on', 'this', 'BS', 'photo', 'opp?', '', 'She', 'has', 'been', 'so,', 'so', 'bad', 'at', 'helping', 'out', 'here', 'and', 'Trump', 'did', 'his', 'dumb', 'twitter', 'thing', 'and', 'the', 'issue', 'is', 'being', 'lost', 'here.\\n\\nYulin', 'Cruz', 'is', 'being', 'a', 'dumb', 'fuck', 'and', 'not', 'coordinating', 'help', 'efforts', 'like', 'she', 'should.', '', 'She', \"won't\", 'meet', 'with', 'FEMA', 'because', \"she's\", 'too', 'proud', 'and', 'her', 'agenda', 'of', 'anti-USA', 'is', 'getting', 'in', 'the', 'way', 'of', 'people', 'getting', 'help.\\n\\nDamnit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sosh', 'refus', 'meet', 'fem', 'coordin', 'effort', 'help', 'foc', 'bs', 'photo', 'op', 'bad', 'help', 'trump', 'dumb', 'twit', 'thing', 'issu', 'lost', 'here\\n\\nyulin', 'cruz', 'dumb', 'fuck', 'coordin', 'help', 'effort', 'lik', 'wont', 'meet', 'fem', 'she', 'proud', 'agend', 'antius', 'get', 'way', 'peopl', 'get', 'help\\n\\ndamnit'], ['soshe', 'refuse', 'meet', 'fema', 'coordinate', 'efforts', 'help', 'focus', 'bs', 'photo', 'opp', 'bad', 'help', 'trump', 'dumb', 'twitter', 'thing', 'issue', 'lose', 'here\\n\\nyulin', 'cruz', 'dumb', 'fuck', 'coordinate', 'help', 'efforts', 'like', 'wont', 'meet', 'fema', 'shes', 'proud', 'agenda', 'antiusa', 'get', 'way', 'people', 'get', 'help\\n\\ndamnit'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'mean', \"I'm\", 'in', 'favour', 'of', 'making', 'it', 'a', 'criminal', 'offence', 'to', 'knowingly', 'have', 'sex', 'with', 'someone', 'who', 'has', 'HIV', '(unless', 'you', 'have', 'it', 'too)', 'if', \"that's\", 'more', 'palatable', 'to', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'im', 'favo', 'mak', 'crimin', 'off', 'know', 'sex', 'someon', 'hiv', 'unless', 'that', 'pal'], ['mean', 'im', 'favour', 'make', 'criminal', 'offence', 'knowingly', 'sex', 'someone', 'hiv', 'unless', 'thats', 'palatable'])\n",
      "original document: \n",
      "['I', 'appreciate', 'your', 'comment,', 'thank', 'you.\\n\\nWhile', 'we', 'may', 'disagree', 'on', 'the', 'solution', 'to', 'problems,', 'I', 'appreciate', 'you', 'being', 'level-headed', 'enough', 'to', 'see', 'through', 'the', 'various', 'headlines', 'and', 'actually', 'view', 'the', 'source', 'material.', '\\n', 'Too', 'often', 'anymore', 'people', 'take', 'the', 'headline', 'as', 'fact', 'and', \"don't\", 'verify', 'themselves.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['apprecy', 'com', 'thank', 'you\\n\\nwhile', 'may', 'disagr', 'solv', 'problem', 'apprecy', 'levelhead', 'enough', 'see', 'vary', 'headlin', 'act', 'view', 'sourc', 'mat', '\\n', 'oft', 'anym', 'peopl', 'tak', 'headlin', 'fact', 'dont', 'ver'], ['appreciate', 'comment', 'thank', 'you\\n\\nwhile', 'may', 'disagree', 'solution', 'problems', 'appreciate', 'levelheaded', 'enough', 'see', 'various', 'headline', 'actually', 'view', 'source', 'material', '\\n', 'often', 'anymore', 'people', 'take', 'headline', 'fact', 'dont', 'verify'])\n",
      "original document: \n",
      "['A', 'handful', 'of', 'them', 'can', 'be.', 'If', 'not,', 'all', 'of', 'them.', 'Trying', 'to', 'rack', 'through', 'my', 'mind', 'of', 'each', \"one's\", 'mechanics', 'to', 'think', 'of', 'anything', 'majorly', 'important.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hand', 'try', 'rack', 'mind', 'on', 'mech', 'think', 'anyth', 'maj', 'import'], ['handful', 'try', 'rack', 'mind', 'ones', 'mechanics', 'think', 'anything', 'majorly', 'important'])\n",
      "original document: \n",
      "['I', 'pulled', 'some', 'strings', 'for', 'ya.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pul', 'strings', 'ya'], ['pull', 'string', 'ya'])\n",
      "original document: \n",
      "['I', 'wouldnt', 'Havr', 'understood', 'this', 'had', 'i', 'not', 'watched', '', 'a', 'video', 'about', 'disrespect', 'in', 'the', 'mlb']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'havr', 'understood', 'watch', 'video', 'disrespect', 'mlb'], ['wouldnt', 'havr', 'understand', 'watch', 'video', 'disrespect', 'mlb'])\n",
      "original document: \n",
      "['I', 'think', '50', 'percent', 'of', 'the', 'roster', 'could', 'be', 'as', '“great”', 'as', 'Roman', 'if', 'they', 'got', 'the', 'same', 'amount', 'of', 'time', 'and', 'work', 'put', 'into', 'them.', 'Most', 'wrestlers', 'on', 'the', 'roster', 'don’t', 'get', '25', 'minutes', 'in', 'a', 'main', 'event', 'to', 'do', 'whatever', 'they', 'want', 'too.', 'I’m', 'not', 'going', 'to', 'credit', 'others,', 'it', 'takes', 'two', 'to', 'make', 'a', 'match,', 'but', 'he', 'is', 'put', 'in', 'positions', 'that', 'could', 'be', 'so', 'much', 'more', 'watchable', 'if', 'he', 'wasn’t', 'just', 'so', 'god', 'damn', 'boring.\\n\\nAs', 'for', 'him', 'being', 'better', 'than', 'Cena,', 'some', 'people', 'have', 'short', 'memories.', 'I’ve', 'never', 'seen', 'Roman', 'have', 'matches', 'as', 'good', 'as', 'Cena', 'Vs', 'Michaels,', 'Cena', 'Vs', 'Rollins,', 'he’s', 'never', 'done', 'anything', 'near', 'most', 'of', 'the', 'Cena', 'Vs', 'Punk', 'matches,', 'his', 'three', 'way', 'with', 'Rollins', 'and', 'Lesnar', 'was', 'better', 'than', 'the', 'exact', 'same', 'match', 'but', 'with', 'Reigns.\\n\\nAnd', 'just', 'like', 'with', 'Reigns,', 'it', 'took', 'two', 'people', 'to', 'make', 'all', 'of', 'those', 'matches.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'fifty', 'perc', 'rost', 'could', 'gre', 'rom', 'got', 'amount', 'tim', 'work', 'put', 'wrestl', 'rost', 'dont', 'get', 'twenty-five', 'minut', 'main', 'ev', 'whatev', 'want', 'im', 'going', 'credit', 'oth', 'tak', 'two', 'mak', 'match', 'put', 'posit', 'could', 'much', 'watch', 'wasnt', 'god', 'damn', 'boring\\n\\nas', 'bet', 'cen', 'peopl', 'short', 'mem', 'iv', 'nev', 'seen', 'rom', 'match', 'good', 'cen', 'vs', 'michael', 'cen', 'vs', 'rollin', 'hes', 'nev', 'don', 'anyth', 'near', 'cen', 'vs', 'punk', 'match', 'three', 'way', 'rollin', 'lesn', 'bet', 'exact', 'match', 'reigns\\n\\nand', 'lik', 'reign', 'took', 'two', 'peopl', 'mak', 'match'], ['think', 'fifty', 'percent', 'roster', 'could', 'great', 'roman', 'get', 'amount', 'time', 'work', 'put', 'wrestlers', 'roster', 'dont', 'get', 'twenty-five', 'minutes', 'main', 'event', 'whatever', 'want', 'im', 'go', 'credit', 'others', 'take', 'two', 'make', 'match', 'put', 'position', 'could', 'much', 'watchable', 'wasnt', 'god', 'damn', 'boring\\n\\nas', 'better', 'cena', 'people', 'short', 'memories', 'ive', 'never', 'see', 'roman', 'match', 'good', 'cena', 'vs', 'michaels', 'cena', 'vs', 'rollins', 'hes', 'never', 'do', 'anything', 'near', 'cena', 'vs', 'punk', 'match', 'three', 'way', 'rollins', 'lesnar', 'better', 'exact', 'match', 'reigns\\n\\nand', 'like', 'reign', 'take', 'two', 'people', 'make', 'match'])\n",
      "original document: \n",
      "['Your', 'submission', 'has', 'been', 'automatically', 'removed', 'because', 'it', 'does', 'not', 'start', 'with', 'a', 'continent', 'tag', 'inside', 'brackets.', 'The', 'continent', 'tags', 'available', 'are', '[NA],', '[SA],', '[EU],', '[ASIA],', '[AUS],', '[AFRICA]', 'and', '[ME].\\n\\nPlease', 'submit', 'a', 'new', 'post', 'that', 'includes', 'your', 'current', 'country', 'or', 'state', 'after', 'the', 'continent', 'tag,', 'as', 'in', 'the', 'examples', 'below.\\n\\n*', '[EU]', 'Norway', 'DMG', 'looking', 'to', 'play', 'MM\\n\\n*', '[NA]', 'California,', 'team', 'looking', 'for', '5th', 'player\\n\\n*', '[ASIA]', 'Korea,', 'looking', 'for', 'mentor\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/RecruitCS)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'autom', 'remov', 'start', 'contin', 'tag', 'insid', 'bracket', 'contin', 'tag', 'avail', 'na', 'sa', 'eu', 'as', 'au', 'afric', 'me\\n\\npleas', 'submit', 'new', 'post', 'includ', 'cur', 'country', 'stat', 'contin', 'tag', 'exampl', 'below\\n\\n', 'eu', 'norway', 'dmg', 'look', 'play', 'mm\\n\\n', 'na', 'californ', 'team', 'look', '5th', 'player\\n\\n', 'as', 'kore', 'look', 'mentor\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorrecruitc', 'quest', 'concern'], ['submission', 'automatically', 'remove', 'start', 'continent', 'tag', 'inside', 'bracket', 'continent', 'tag', 'available', 'na', 'sa', 'eu', 'asia', 'aus', 'africa', 'me\\n\\nplease', 'submit', 'new', 'post', 'include', 'current', 'country', 'state', 'continent', 'tag', 'examples', 'below\\n\\n', 'eu', 'norway', 'dmg', 'look', 'play', 'mm\\n\\n', 'na', 'california', 'team', 'look', '5th', 'player\\n\\n', 'asia', 'korea', 'look', 'mentor\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorrecruitcs', 'question', 'concern'])\n",
      "original document: \n",
      "['A', 'lot', 'of', 'minorities', 'feel', 'unfairly', 'represented.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lot', 'min', 'feel', 'unfair', 'repres'], ['lot', 'minorities', 'feel', 'unfairly', 'represent'])\n",
      "original document: \n",
      "['Thank', 'you.', '', 'I', 'had', 'an', 'eye', 'op', '8', 'weeks', 'ago', 'and', 'the', 'resulting', 'double', 'vision', 'has', 'taken', 'a', 'while', 'to', 'go.', '', \"I've\", 'been', 'depressed', 'by', 'how', 'quickly', 'I', 'lost', 'my', '5k', 'ability', 'and', 'have', 'started', 'c25k', 'again.', '', 'If', 'you', 'can,', 'I', 'can!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'ey', 'op', 'eight', 'week', 'ago', 'result', 'doubl', 'vis', 'tak', 'go', 'iv', 'depress', 'quick', 'lost', '5k', 'abl', 'start', 'c25k'], ['thank', 'eye', 'op', 'eight', 'weeks', 'ago', 'result', 'double', 'vision', 'take', 'go', 'ive', 'depress', 'quickly', 'lose', '5k', 'ability', 'start', 'c25k'])\n",
      "original document: \n",
      "['I', 'believe', 'JediMasterBen', 'on', 'a', 'few', 'reefing', 'forums', 'uses', 'one.', 'He', 'was', 'recently', 'given', 'TOTM', 'on', 'Reef2Reef', 'if', \"I'm\", 'not', 'mistaken.', \"He's\", 'is', 'widely', 'considered', 'one', 'of', 'the', 'top', 'LED', 'dudes', 'of', 'the', 'hobby', 'and', 'helped', 'bring', 'forth', 'quite', 'a', 'few', 'of', 'the', 'now', 'standards', 'in', 'reef', 'LEDs.', 'So', 'if', 'he', 'still', 'enjoys', 'it', 'now', 'that', \"he's\", 'had', 'it', 'a', 'while', 'then', \"it's\", 'likely', 'safe', 'to', 'say', \"they're\", 'a', 'pretty', 'decent', 'unit.', '\\n\\nEdit:', 'turns', 'out', 'I', 'was', 'wrong,', 'he', 'uses', 'a', 'Coralvue', 'system,', 'my', 'bad.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['believ', 'jedimasterb', 'reef', 'forum', 'us', 'on', 'rec', 'giv', 'totm', 'reef2reef', 'im', 'mistak', 'hes', 'wid', 'consid', 'on', 'top', 'led', 'dud', 'hobby', 'help', 'bring', 'for', 'quit', 'standard', 'reef', 'led', 'stil', 'enjoy', 'hes', 'lik', 'saf', 'say', 'theyr', 'pretty', 'dec', 'unit', '\\n\\nedit', 'turn', 'wrong', 'us', 'coralvu', 'system', 'bad'], ['believe', 'jedimasterben', 'reef', 'forums', 'use', 'one', 'recently', 'give', 'totm', 'reef2reef', 'im', 'mistake', 'hes', 'widely', 'consider', 'one', 'top', 'lead', 'dudes', 'hobby', 'help', 'bring', 'forth', 'quite', 'standards', 'reef', 'leds', 'still', 'enjoy', 'hes', 'likely', 'safe', 'say', 'theyre', 'pretty', 'decent', 'unit', '\\n\\nedit', 'turn', 'wrong', 'use', 'coralvue', 'system', 'bad'])\n",
      "original document: \n",
      "['Not', 'from', 'USA,', 'but', 'who', 'is', 'the', 'HOA?', 'If', \"they're\", 'the', 'homeowners', \"can't\", 'anyone', 'they', 'want', 'live', 'in', 'it', 'and', 'do', 'the', 'fuck', 'they', 'want', 'with', 'it', '(obviously', 'with', 'it', 'being', 'legal)?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'hoa', 'theyr', 'homeown', 'cant', 'anyon', 'want', 'liv', 'fuck', 'want', 'obvy', 'leg'], ['usa', 'hoa', 'theyre', 'homeowners', 'cant', 'anyone', 'want', 'live', 'fuck', 'want', 'obviously', 'legal'])\n",
      "original document: \n",
      "['grasp']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['grasp'], ['grasp'])\n",
      "original document: \n",
      "['What', 'do', 'you', 'think?', 'Good', 'idea', 'to', 'sell', 'the', 'only', 'insurance', 'available', 'if', 'btc', 'goes', 'boom?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'good', 'ide', 'sel', 'ins', 'avail', 'btc', 'goe', 'boom'], ['think', 'good', 'idea', 'sell', 'insurance', 'available', 'btc', 'go', 'boom'])\n",
      "original document: \n",
      "['Yup,', 'craziness.', 'And', \"wasn't\", 'that', 'long', 'ago', 'really..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup', 'crazy', 'wasnt', 'long', 'ago', 'real'], ['yup', 'craziness', 'wasnt', 'long', 'ago', 'really'])\n",
      "original document: \n",
      "['My', 'sac', 'sack']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sac', 'sack'], ['sac', 'sack'])\n",
      "original document: \n",
      "['Your', 'not', 'the', 'boss', 'of', 'me!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['boss'], ['boss'])\n",
      "original document: \n",
      "['Thought', 'that', \"I'd\", 'share', 'my', 'two', 'femme', 'fatales.\\n\\nKonahrik', 'is', 'sort', 'of', 'a', 'mage/scholarly', 'character,', 'but', 'with', 'shouts.\\n\\nNightshade', 'is', 'a', 'sort', 'of', 'stealthy', 'assassin', 'mage,', 'but', 'with', 'style.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'id', 'shar', 'two', 'fem', 'fatales\\n\\nkonahrik', 'sort', 'mageschol', 'charact', 'shouts\\n\\nnightshade', 'sort', 'stealthy', 'assassin', 'mag', 'styl'], ['think', 'id', 'share', 'two', 'femme', 'fatales\\n\\nkonahrik', 'sort', 'magescholarly', 'character', 'shouts\\n\\nnightshade', 'sort', 'stealthy', 'assassin', 'mage', 'style'])\n",
      "original document: \n",
      "[\"I've\", 'seen', 'a', 'recreation', 'of', 'the', 'first', 'model', 'in', 'real', 'life;', \"it's\", 'not', 'as', 'striking', 'as', 'on', 'video', 'but', 'the', 'effect', 'is', 'still', 'pretty', 'compelling', 'and', 'not', 'as', 'easily', 'broken', 'as', 'you', 'suggest.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'seen', 'recr', 'first', 'model', 'real', 'lif', 'striking', 'video', 'effect', 'stil', 'pretty', 'compel', 'easy', 'brok', 'suggest'], ['ive', 'see', 'recreation', 'first', 'model', 'real', 'life', 'strike', 'video', 'effect', 'still', 'pretty', 'compel', 'easily', 'break', 'suggest'])\n",
      "original document: \n",
      "['Zai', 'is', 'incredible.', 'Loving', 'this', 'Centaur', 'pick', 'so', 'far']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zai', 'incred', 'lov', 'centa', 'pick', 'far'], ['zai', 'incredible', 'love', 'centaur', 'pick', 'far'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'hope', \"you're\", 'not', 'involved', 'in', 'any', 'economic', 'or', 'scientific', 'positions', 'if', 'your', 'understanding', 'of', 'stats', 'is', 'that', 'bad.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'yo', 'involv', 'econom', 'sci', 'posit', 'understand', 'stat', 'bad'], ['hope', 'youre', 'involve', 'economic', 'scientific', 'position', 'understand', 'stats', 'bad'])\n",
      "original document: \n",
      "['Hey,', 'I', 'have', 'your', 'snorla', 'ready.', 'Let', 'me', 'know', 'when', \"you're\", 'ready', 'to', 'trade']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'snorl', 'ready', 'let', 'know', 'yo', 'ready', 'trad'], ['hey', 'snorla', 'ready', 'let', 'know', 'youre', 'ready', 'trade'])\n",
      "original document: \n",
      "['Opening', 'act', 'just', 'started.', '', 'You', 'might', 'be', 'okay']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'act', 'start', 'might', 'okay'], ['open', 'act', 'start', 'might', 'okay'])\n",
      "original document: \n",
      "['I', 'wonder', 'if', 'the', 'airport', 'cleaners', 'do', 'find', 'valuable', 'treasures:', 'like', 'all', 'the', 'drugs', 'that', 'smugglers', 'dump', 'in', 'bathroom', 'bins', 'in', 'a', 'panic', 'at', 'the', 'last', 'minute?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wond', 'airport', 'cle', 'find', 'valu', 'treas', 'lik', 'drug', 'smuggl', 'dump', 'bathroom', 'bin', 'pan', 'last', 'minut'], ['wonder', 'airport', 'cleaners', 'find', 'valuable', 'treasure', 'like', 'drug', 'smugglers', 'dump', 'bathroom', 'bin', 'panic', 'last', 'minute'])\n",
      "original document: \n",
      "['My', 'mom', 'is', 'a', 'single', 'and', 'independent', 'woman,', 'I', \"don't\", 'give', 'a', 'shit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mom', 'singl', 'independ', 'wom', 'dont', 'giv', 'shit'], ['mom', 'single', 'independent', 'woman', 'dont', 'give', 'shit'])\n",
      "original document: \n",
      "['The', 'FYE', 'by', 'me', 'has', 'told', 'me', 'at', 'least', 'twice', 'when', \"I've\", 'asked', 'that', 'they', 'will', 'restock', 'on', 'Oct.', '1st,', 'but', 'that', 'seems', 'strange', 'because', \"it's\", 'a', 'Sunday.', 'An', 'employee', 'at', 'another', 'FYE', 'by', 'my', 'work', 'said', 'that', 'they', 'got', 'them', 'in', 'on', 'Thursday', 'with', 'some', 'extras', 'that', \"weren't\", 'preorder', 'and', 'those', 'all', 'sold', 'out', 'the', 'same', 'day.', 'She', 'said', 'to', 'check', 'back', 'on', 'Wednesday', 'though', 'because', 'they', 'get', 'stock', 'on', 'Wednesday', 'and', 'Thursday', 'but', 'Wednesday', 'is', 'when', 'they', 'get', 'the', 'most', \"stock.\\n\\nI'll\", 'check', 'both', 'tomorrow', 'and', 'Wednesday', 'and', 'cross', 'my', 'fingers!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fye', 'told', 'least', 'twic', 'iv', 'ask', 'restock', 'oct', '1st', 'seem', 'strange', 'sunday', 'employ', 'anoth', 'fye', 'work', 'said', 'got', 'thursday', 'extra', 'wer', 'preord', 'sold', 'day', 'said', 'check', 'back', 'wednesday', 'though', 'get', 'stock', 'wednesday', 'thursday', 'wednesday', 'get', 'stock\\n\\nill', 'check', 'tomorrow', 'wednesday', 'cross', 'fing'], ['fye', 'tell', 'least', 'twice', 'ive', 'ask', 'restock', 'oct', '1st', 'seem', 'strange', 'sunday', 'employee', 'another', 'fye', 'work', 'say', 'get', 'thursday', 'extras', 'werent', 'preorder', 'sell', 'day', 'say', 'check', 'back', 'wednesday', 'though', 'get', 'stock', 'wednesday', 'thursday', 'wednesday', 'get', 'stock\\n\\nill', 'check', 'tomorrow', 'wednesday', 'cross', 'finger'])\n",
      "original document: \n",
      "['Look,', \"it's\", 'simple.\\n\\n&gt;So', 'he', 'definitely', 'applied', 'the', '\"very', 'fine', 'people\"', 'label', 'to', 'neo-Nazis,', 'although', \"it's\", 'possible', \"he's\", 'too', 'stupid', 'to', 'realize', 'that', 'they', 'were', 'neo-Nazis,', 'if', 'he', 'thinks', 'that', 'all', 'neo-Nazis', 'are', 'willing', 'to', 'tell', 'you', \"they're\", 'neo-Nazis.', '\\n\\nWere', 'they', '***all***', 'neo', 'Nazis?', 'Can', 'you', 'say', 'that', 'with', 'certainty?', \"It's\", 'a', 'simple', 'yes', 'or', 'no.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'simple\\n\\ngtso', 'definit', 'apply', 'fin', 'peopl', 'label', 'neonaz', 'although', 'poss', 'hes', 'stupid', 'real', 'neonaz', 'think', 'neonaz', 'wil', 'tel', 'theyr', 'neonaz', '\\n\\nwere', 'neo', 'naz', 'say', 'certainty', 'simpl', 'ye'], ['look', 'simple\\n\\ngtso', 'definitely', 'apply', 'fine', 'people', 'label', 'neonazis', 'although', 'possible', 'hes', 'stupid', 'realize', 'neonazis', 'think', 'neonazis', 'will', 'tell', 'theyre', 'neonazis', '\\n\\nwere', 'neo', 'nazis', 'say', 'certainty', 'simple', 'yes'])\n",
      "original document: \n",
      "['Or', 'she', 'could', 'be', 'getting', 'disability', 'and', 'only', 'suing', 'you', 'for', 'the', 'difference', 'in', 'her', 'full', 'pay', 'and', 'disability.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'get', 'dis', 'suing', 'diff', 'ful', 'pay', 'dis'], ['could', 'get', 'disability', 'sue', 'difference', 'full', 'pay', 'disability'])\n",
      "original document: \n",
      "['I', 'saw', 'the', 'picture', 'and', 'had', 'a', 'rush', 'of', 'memories,', 'thank', 'you', 'for', 'posting', 'the', 'name', 'of', 'it,', 'I', 'had', 'completely', 'forgotten.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'pict', 'rush', 'mem', 'thank', 'post', 'nam', 'complet', 'forgot'], ['saw', 'picture', 'rush', 'memories', 'thank', 'post', 'name', 'completely', 'forget'])\n",
      "original document: \n",
      "[\"It's\", 'nice', 'and', 'quiet']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'quiet'], ['nice', 'quiet'])\n",
      "original document: \n",
      "['Religion', 'is', 'a', 'cancer', 'that', 'has', 'held', 'humanity', 'back', 'for', 'thousands', 'of', 'years.', 'Imagine', 'what', 'kind', 'of', 'incredible', 'technology', 'we', 'would', 'have', 'today', 'without', 'idiots', 'brainwashed', 'by', 'fairy', 'tales', 'sabotaging', 'scientific', 'progress.\\n\\nIf', 'god', 'exists,', 'it', 'clearly', \"isn't\", 'interested', 'in', 'intervening', 'in', 'anything', 'we', 'do.', 'So', 'effectively,', 'there', 'is', 'no', 'god', 'even', 'if', 'one', 'exists.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['relig', 'cant', 'held', 'hum', 'back', 'thousand', 'year', 'imagin', 'kind', 'incred', 'technolog', 'would', 'today', 'without', 'idiot', 'brainwash', 'fairy', 'tal', 'sabot', 'sci', 'progress\\n\\nif', 'god', 'ex', 'clear', 'isnt', 'interest', 'interv', 'anyth', 'effect', 'god', 'ev', 'on', 'ex'], ['religion', 'cancer', 'hold', 'humanity', 'back', 'thousands', 'years', 'imagine', 'kind', 'incredible', 'technology', 'would', 'today', 'without', 'idiots', 'brainwash', 'fairy', 'tales', 'sabotage', 'scientific', 'progress\\n\\nif', 'god', 'exist', 'clearly', 'isnt', 'interest', 'intervene', 'anything', 'effectively', 'god', 'even', 'one', 'exist'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Agreed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree'], ['agree'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['A', 'friend', 'is', 'a', 'huge', 'fan', 'of', 'those,', 'I', 'also', 'love', 'how', 'aggressive', 'they', 'look!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['friend', 'hug', 'fan', 'also', 'lov', 'aggress', 'look'], ['friend', 'huge', 'fan', 'also', 'love', 'aggressive', 'look'])\n",
      "original document: \n",
      "['neaux']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['neaux'], ['neaux'])\n",
      "original document: \n",
      "['Woah', 'spicy', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['woah', 'spicy'], ['woah', 'spicy'])\n",
      "original document: \n",
      "['I', 'know', 'that', 'this', \"doesn't\", 'help', 'you,', 'but', 'I', 'am', 'so', 'relieved', 'that', \"it's\", 'not', 'just', 'me...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'doesnt', 'help', 'reliev'], ['know', 'doesnt', 'help', 'relieve'])\n",
      "original document: \n",
      "['Hi', 'jessicamshannon,\\n\\n\\n\\nYour', 'submission', 'has', 'been', 'removed', 'because', 'it', 'violates:\\n\\n&gt;RULE', '#7:', 'Off-topic\\n\\n\\n\\nLink', 'goes', 'to', 'GTA', 'screenshot\\n\\n\\n\\nYou', 'can', 'find', 'all', 'of', 'our', 'rules', 'in', 'the', 'sidebar.', 'Please', 'look', 'them', 'over', 'before', 'contributing', 'again.\\n\\nIf', 'you', 'have', 'any', 'questions', 'or', 'concerns,', 'please', '[message', 'the', 'moderators](https://www\\\\.reddit\\\\.com/message/compose?to=%2Fr%2FCrimeScene&amp;subject=about', 'my', 'removed', \"submission&amp;message=I'm\", 'writing', 'to', 'you', 'about', 'the', 'following', 'submission:', 'https://www.reddit.com/r/CrimeScene/comments/6ryulq/soldiers_pov_during_operation_overlord_dday_june/', '%0D%0DMy', 'issue', 'is...).', 'Direct', 'replies', 'to', 'official', 'mod', 'comments', 'will', 'be', 'removed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'jessicamshannon\\n\\n\\n\\nyour', 'submit', 'remov', 'violates\\n\\ngtrule', 'sev', 'offtopic\\n\\n\\n\\nlink', 'goe', 'gta', 'screenshot\\n\\n\\n\\nyou', 'find', 'rul', 'sideb', 'pleas', 'look', 'contribut', 'again\\n\\nif', 'quest', 'concern', 'pleas', 'mess', 'moderatorshttpswwwredditcommessagecomposeto2fr2fcrimesceneampsubjectabout', 'remov', 'submissionampmessageim', 'writ', 'follow', 'submit', 'httpswwwredditcomrcrimescenecomments6ryulqsoldiers_pov_during_operation_overlord_dday_june', '0d0dmy', 'issu', 'direct', 'reply', 'off', 'mod', 'com', 'remov'], ['hi', 'jessicamshannon\\n\\n\\n\\nyour', 'submission', 'remove', 'violates\\n\\ngtrule', 'seven', 'offtopic\\n\\n\\n\\nlink', 'go', 'gta', 'screenshot\\n\\n\\n\\nyou', 'find', 'rule', 'sidebar', 'please', 'look', 'contribute', 'again\\n\\nif', 'question', 'concern', 'please', 'message', 'moderatorshttpswwwredditcommessagecomposeto2fr2fcrimesceneampsubjectabout', 'remove', 'submissionampmessageim', 'write', 'follow', 'submission', 'httpswwwredditcomrcrimescenecomments6ryulqsoldiers_pov_during_operation_overlord_dday_june', '0d0dmy', 'issue', 'direct', 'reply', 'official', 'mod', 'comment', 'remove'])\n",
      "original document: \n",
      "['If', 'you', 'have', 'the', 'steam', 'version,', 'you', 'have', 'to', 'buy', 'it', 'through', 'steam.', \"I'm\", 'not', 'sure', 'about', 'the', 'exact', 'process', 'but', 'you', 'can', 'use', 'your', 'steam', 'wallet.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['steam', 'vert', 'buy', 'steam', 'im', 'sur', 'exact', 'process', 'us', 'steam', 'wallet'], ['steam', 'version', 'buy', 'steam', 'im', 'sure', 'exact', 'process', 'use', 'steam', 'wallet'])\n",
      "original document: \n",
      "['I', \"haven't\", 'seen', 'that', 'in', 'the', 'stores', 'here.', 'I', 'always', 'gravitate', 'towards', 'the', 'zero', 'carb/calorie', 'ones', 'anyway', 'but', \"I've\", 'been', 'known', 'to', 'dabble', 'in', 'their', 'excellent', 'Tea', 'line.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hav', 'seen', 'stor', 'alway', 'gravit', 'toward', 'zero', 'carbc', 'on', 'anyway', 'iv', 'known', 'dabbl', 'excel', 'tea', 'lin'], ['havent', 'see', 'store', 'always', 'gravitate', 'towards', 'zero', 'carbcalorie', 'ones', 'anyway', 'ive', 'know', 'dabble', 'excellent', 'tea', 'line'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'wish', 'that', 'they', 'would', 'pay', 'Baka', 'Tsuki', 'and', 'print', 'their', 'translations', 'and', 'the', 'translators', 'for', 'their', 'time,', 'perhaps', 'to', 'translate', 'full', 'time?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wish', 'would', 'pay', 'bak', 'tsuk', 'print', 'transl', 'transl', 'tim', 'perhap', 'transl', 'ful', 'tim'], ['wish', 'would', 'pay', 'baka', 'tsuki', 'print', 'translations', 'translators', 'time', 'perhaps', 'translate', 'full', 'time'])\n",
      "original document: \n",
      "['Follow', 'Your', 'Heart', 'says', 'all', 'of', 'the', 'plastic', 'they', 'use', 'is', 'recyclable', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['follow', 'heart', 'say', 'plast', 'us', 'recyc'], ['follow', 'heart', 'say', 'plastic', 'use', 'recyclable'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['All', 'these', 'streams', 'be', 'Texas', 'A&amp;M', 'Laggy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['streams', 'texa', 'aampm', 'laggy'], ['stream', 'texas', 'aampm', 'laggy'])\n",
      "original document: \n",
      "['Nope,', 'he', 'says', 'knees.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nop', 'say', 'kne'], ['nope', 'say', 'knees'])\n",
      "original document: \n",
      "['The', 'problem', 'is', 'that', 'the', 'EDA', 'only', 'starts', 'to', 'act', 'after', 'the', 'block', 'rate', 'got', 'very', 'low', 'for', '12', 'hours,', 'then', 'it', 'gives', 'a', 'fast', 'block', 'rate', 'for', 'a', 'while,', 'then', 'the', 'normal', 'DDA', 'kicks', 'in,', 'shoos', 'the', 'miners', 'away,', 'and', 'brings', 'the', 'block', 'rate', 'to', 'snail', 'pace', 'again', '--', 'over', 'and', 'over.', '', 'Each', 'swing', 'is', 'unpredictable,', 'and', 'it', 'may', 'never', 'stabilize.', '\\n\\nThe', 'Bitcoin', 'Cash', 'devs', 'are', 'working', 'on', 'a', 'better', 'DAA', 'that', 'should', 'ensure', 'a', 'stable', 'block', 'rate', 'and', 'hashpower,', 'even', 'if', 'Bitcoin', 'Core', 'retains', 'the', 'old', 'DAA.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'ed', 'start', 'act', 'block', 'rat', 'got', 'low', 'twelv', 'hour', 'giv', 'fast', 'block', 'rat', 'norm', 'dda', 'kick', 'shoo', 'min', 'away', 'bring', 'block', 'rat', 'snail', 'pac', 'swing', 'unpredict', 'may', 'nev', 'stabl', '\\n\\nthe', 'bitcoin', 'cash', 'dev', 'work', 'bet', 'daa', 'ens', 'stabl', 'block', 'rat', 'hashpow', 'ev', 'bitcoin', 'cor', 'retain', 'old', 'daa'], ['problem', 'eda', 'start', 'act', 'block', 'rate', 'get', 'low', 'twelve', 'hours', 'give', 'fast', 'block', 'rate', 'normal', 'dda', 'kick', 'shoo', 'miners', 'away', 'bring', 'block', 'rate', 'snail', 'pace', 'swing', 'unpredictable', 'may', 'never', 'stabilize', '\\n\\nthe', 'bitcoin', 'cash', 'devs', 'work', 'better', 'daa', 'ensure', 'stable', 'block', 'rate', 'hashpower', 'even', 'bitcoin', 'core', 'retain', 'old', 'daa'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['The', 'lineup', 'you', 'wanted', 'this', 'season', 'from', 'your', 'post', 'history\\n\\nPG:', 'CP3', '', '\\nSG:', 'Gordon', '', '\\nSF:', 'Harden', '', '\\nPF:', '**Melo**', '', '\\nC:', 'Capela\\n\\nYeah,', 'sorry', 'about', 'that.', 'Thats', 'sad', 'as', 'fuck']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lineup', 'want', 'season', 'post', 'history\\n\\npg', 'cp3', '\\nsg', 'gordon', '\\nsf', 'hard', '\\npf', 'melo', '\\nc', 'capela\\n\\nyeah', 'sorry', 'that', 'sad', 'fuck'], ['lineup', 'want', 'season', 'post', 'history\\n\\npg', 'cp3', '\\nsg', 'gordon', '\\nsf', 'harden', '\\npf', 'melo', '\\nc', 'capela\\n\\nyeah', 'sorry', 'thats', 'sad', 'fuck'])\n",
      "original document: \n",
      "[\"That's\", 'just', 'not', 'true.', 'Some', 'features', 'can', 'be', 'more', 'recent', 'innovations', 'resulting', 'from', 'all', 'sorts', 'of', 'different', 'processes.', '', '\\n\\n', '', 'This', 'just', 'happens', 'to', 'be', 'a', 'grammatical', 'feature', 'with', 'an', 'unbroken', 'presence', 'in', 'the', 'language', 'since', 'Old', 'English.', 'These', 'are', 'not', 'unusual,', 'so', 'you', \"shouldn't\", 'be', 'surprised', 'to', 'see', \"'it's\", 'conserved', 'from', 'Old', \"English'\", 'crop', 'up', 'quite', 'a', 'lot.', '', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'tru', 'feat', 'rec', 'innov', 'result', 'sort', 'diff', 'process', '\\n\\n', 'hap', 'gram', 'feat', 'unbrok', 'pres', 'langu', 'sint', 'old', 'engl', 'unus', 'shouldnt', 'surpr', 'see', 'conserv', 'old', 'engl', 'crop', 'quit', 'lot', '\\n'], ['thats', 'true', 'feature', 'recent', 'innovations', 'result', 'sort', 'different', 'process', '\\n\\n', 'happen', 'grammatical', 'feature', 'unbroken', 'presence', 'language', 'since', 'old', 'english', 'unusual', 'shouldnt', 'surprise', 'see', 'conserve', 'old', 'english', 'crop', 'quite', 'lot', '\\n'])\n",
      "original document: \n",
      "['*\"NiCE\"*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic'], ['nice'])\n",
      "original document: \n",
      "['Wow', 'you’re', 'gorgeous!', 'You’re', 'Hair!', 'That', 'eyeliner', 'is', 'so', 'sharp', 'it', 'can', 'stab.', '\\nI’m', 'sorry', 'about', 'your', 'mother’s', 'passing.', 'I', 'promise', 'it', 'hurts', 'a', 'little', 'less', 'each', 'day.', 'Focus', 'on', 'the', 'good', 'memories', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'yo', 'gorg', 'yo', 'hair', 'eyelin', 'sharp', 'stab', '\\nim', 'sorry', 'moth', 'pass', 'prom', 'hurt', 'littl', 'less', 'day', 'foc', 'good', 'mem'], ['wow', 'youre', 'gorgeous', 'youre', 'hair', 'eyeliner', 'sharp', 'stab', '\\nim', 'sorry', 'mother', 'pass', 'promise', 'hurt', 'little', 'less', 'day', 'focus', 'good', 'memories'])\n",
      "original document: \n",
      "['The', 'side', 'of', 'her', 'tit', 'looks', 'like', 'gum', 'stretching', 'between', 'a', 'shoe', 'and', 'a', 'sidewalk.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sid', 'tit', 'look', 'lik', 'gum', 'stretching', 'sho', 'sidewalk'], ['side', 'tit', 'look', 'like', 'gum', 'stretch', 'shoe', 'sidewalk'])\n",
      "original document: \n",
      "['143414817|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'zR7qrd9W)\\n\\n&gt;&gt;143412442\\nThis', 'was', 'the', 'right', 'choice\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, eight hundred and seventeen', 'gt', 'unit', 'stat', 'anonym', 'id', 'zr7qrd9w\\n\\ngtgt143412442\\nthis', 'right', 'choice\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, eight hundred and seventeen', 'gt', 'unite', 'state', 'anonymous', 'id', 'zr7qrd9w\\n\\ngtgt143412442\\nthis', 'right', 'choice\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['1d20', '/u/Razqn', '**Overall', 'Success**:', '**1**\\n\\n(*1*)\\n*****\\n1d20', '/u/Razqn', '**Secrecy**:', '**13**\\n\\n(13)\\n*****\\n\\n\\n\\n^(Hey', 'there!', \"I'm\", 'a', 'bot', 'that', 'can', 'roll', 'dice', 'if', 'you', 'mention', 'me', 'in', 'your', 'comments.', 'Check', 'out', '/r/rollme', 'for', 'more', 'info.)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['1d20', 'urazqn', 'overal', 'success', '1\\n\\n1\\n\\n1d20', 'urazqn', 'secrecy', '13\\n\\n13\\n\\n\\n\\n\\nhey', 'im', 'bot', 'rol', 'dic', 'ment', 'com', 'check', 'rrollm', 'info'], ['1d20', 'urazqn', 'overall', 'success', '1\\n\\n1\\n\\n1d20', 'urazqn', 'secrecy', '13\\n\\n13\\n\\n\\n\\n\\nhey', 'im', 'bot', 'roll', 'dice', 'mention', 'comment', 'check', 'rrollme', 'info'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Okay,', 'you', 'talked', 'me', 'into', 'it!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'talk'], ['okay', 'talk'])\n",
      "original document: \n",
      "['So', \"why's\", 'this', 'guy', 'being', 'downvoted', 'so', 'much?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['why', 'guy', 'downvot', 'much'], ['whys', 'guy', 'downvoted', 'much'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Holy', 'shit', 'man,', 'I', \"don't\", 'know', 'what', 'the', 'fuck', 'is', 'it', 'with', 'this', 'song', 'but', 'it', 'makes', 'every', 'female', 'in', 'the', 'club', 'lose', 'their', 'shit.', 'I', 'went', 'yesterday', 'with', 'some', 'mates', 'to', 'the', 'club', 'and', 'it', 'was', 'all', 'mellow', 'and', 'shit.....', 'Until', 'Bodak', 'Yellow', 'came', 'on.', 'Everyone', 'went', 'bat', 'shit,', 'two', 'girls', 'got', 'into', 'a', 'fist', 'fight,', 'a', 'group', 'went', 'full', 'on', 'rave,', 'and', 'one', 'of', 'my', 'mates', 'got', 'slapped', 'randomly.\\n\\nGood', 'shit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['holy', 'shit', 'man', 'dont', 'know', 'fuck', 'song', 'mak', 'every', 'fem', 'club', 'los', 'shit', 'went', 'yesterday', 'mat', 'club', 'mellow', 'shit', 'bodak', 'yellow', 'cam', 'everyon', 'went', 'bat', 'shit', 'two', 'girl', 'got', 'fist', 'fight', 'group', 'went', 'ful', 'rav', 'on', 'mat', 'got', 'slap', 'randomly\\n\\ngood', 'shit'], ['holy', 'shit', 'man', 'dont', 'know', 'fuck', 'song', 'make', 'every', 'female', 'club', 'lose', 'shit', 'go', 'yesterday', 'mat', 'club', 'mellow', 'shit', 'bodak', 'yellow', 'come', 'everyone', 'go', 'bat', 'shit', 'two', 'girls', 'get', 'fist', 'fight', 'group', 'go', 'full', 'rave', 'one', 'mat', 'get', 'slap', 'randomly\\n\\ngood', 'shit'])\n",
      "original document: \n",
      "['Legally', 'no.', 'In', 'terms', 'of', 'the', 'mental', 'capacity', 'of', 'a', 'minor', 'to', 'consent,', \"it's\", 'definitely', 'not', 'a', 'yes', 'or', 'no', 'question', 'unless', 'you', 'think', 'turning', '18', 'is', 'like', 'having', 'a', 'switch', 'turned', 'on', 'in', 'your', 'brain', 'that', 'suddenly', 'allows', 'you', 'to', 'make', 'informed', 'decisions', 'about', 'sex', '(it', \"doesn't).\", '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['leg', 'term', 'ment', 'capac', 'min', 'cons', 'definit', 'ye', 'quest', 'unless', 'think', 'turn', 'eighteen', 'lik', 'switch', 'turn', 'brain', 'sud', 'allow', 'mak', 'inform', 'decid', 'sex', 'doesnt'], ['legally', 'term', 'mental', 'capacity', 'minor', 'consent', 'definitely', 'yes', 'question', 'unless', 'think', 'turn', 'eighteen', 'like', 'switch', 'turn', 'brain', 'suddenly', 'allow', 'make', 'inform', 'decisions', 'sex', 'doesnt'])\n",
      "original document: \n",
      "['Thanks', 'for', 'posting', 'to', '/r/dirtykikpals,', '/u/anotherthingcoming!', 'We', 'encourage', 'all', 'of', 'our', 'users', 'here', 'to', 'verify', 'themselves,', 'to', 'possibly', 'get', 'more/better', 'responses,', 'as', 'well', 'as', 'help', 'us', 'in', 'dealing', 'with', 'sellers,', 'scammers,', 'etc.', 'For', 'information', 'on', 'how', 'to', 'verify,', 'please', 'check', 'our', 'sidebar', 'or', 'message', 'us', 'at', '[modmail](https://www.reddit.com/message/compose?to=%2Fr%2Fdirtykikpals).\\nAlso,', 'be', 'sure', 'to', 'familiarize', 'yourself', 'with', 'the', 'rules', 'of', 'this', 'subreddit,', 'as', 'well', 'as', 'helpful', 'tips', '[here](https://redd.it/6ojo0r).\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/dirtykikpals)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'post', 'rdirtykikp', 'uanotherthingcom', 'enco', 'us', 'ver', 'poss', 'get', 'morebet', 'respons', 'wel', 'help', 'us', 'deal', 'sel', 'scam', 'etc', 'inform', 'ver', 'pleas', 'check', 'sideb', 'mess', 'us', 'modmailhttpswwwredditcommessagecomposeto2fr2fdirtykikpals\\nalso', 'sur', 'famili', 'rul', 'subreddit', 'wel', 'help', 'tip', 'herehttpsreddit6ojo0r\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetordirtykikp', 'quest', 'concern'], ['thank', 'post', 'rdirtykikpals', 'uanotherthingcoming', 'encourage', 'users', 'verify', 'possibly', 'get', 'morebetter', 'responses', 'well', 'help', 'us', 'deal', 'sellers', 'scammers', 'etc', 'information', 'verify', 'please', 'check', 'sidebar', 'message', 'us', 'modmailhttpswwwredditcommessagecomposeto2fr2fdirtykikpals\\nalso', 'sure', 'familiarize', 'rule', 'subreddit', 'well', 'helpful', 'tip', 'herehttpsreddit6ojo0r\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetordirtykikpals', 'question', 'concern'])\n",
      "original document: \n",
      "['Woooo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['woooo'], ['woooo'])\n",
      "original document: \n",
      "[\"It's\", 'possible', 'he', 'called', 'the', 'wrong', 'number.', 'There', 'are', 'actually', 'quite', 'a', 'few', 'that', 'are', 'similar', 'to', 'the', 'customer', 'care', 'line', 'and', 'the', 'equipment', 'activation', 'line', 'that', 'some', 'scammers', 'must', 'have', 'set', 'up.', '', \"It's\", 'also', 'possible', 'that', 'the', 'call', 'center', 'rep', 'just', \"didn't\", 'have', 'a', 'clue.\\n\\nThe', 'person', 'who', 'responded', 'first', 'does', 'have', 'the', 'proper', 'answer,', 'though.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poss', 'cal', 'wrong', 'numb', 'act', 'quit', 'simil', 'custom', 'car', 'lin', 'equip', 'act', 'lin', 'scam', 'must', 'set', 'also', 'poss', 'cal', 'cent', 'rep', 'didnt', 'clue\\n\\nth', 'person', 'respond', 'first', 'prop', 'answ', 'though'], ['possible', 'call', 'wrong', 'number', 'actually', 'quite', 'similar', 'customer', 'care', 'line', 'equipment', 'activation', 'line', 'scammers', 'must', 'set', 'also', 'possible', 'call', 'center', 'rep', 'didnt', 'clue\\n\\nthe', 'person', 'respond', 'first', 'proper', 'answer', 'though'])\n",
      "original document: \n",
      "['Woosh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['woosh'], ['woosh'])\n",
      "original document: \n",
      "['No', 'time', 'to', 'worry', 'about', 'the', 'technicalities\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tim', 'worry', 'technicalities\\n'], ['time', 'worry', 'technicalities\\n'])\n",
      "original document: \n",
      "['The', 'car', 'in', 'front', 'slowing', 'down', 'to', 'turn,', 'how', 'disruptive', 'are', 'they.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['car', 'front', 'slow', 'turn', 'disrupt'], ['car', 'front', 'slow', 'turn', 'disruptive'])\n",
      "original document: \n",
      "['My', 'eyes', 'have', 'been', 'opened', 'forcefully', 'like', 'Podesta', 'does', 'with', 'his', 'pizza!', '\\n\\n\\n\\n\\nHow', 'could', 'i', 'have', 'been', 'so', 'blind?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ey', 'op', 'forc', 'lik', 'podest', 'pizz', '\\n\\n\\n\\n\\nhow', 'could', 'blind'], ['eye', 'open', 'forcefully', 'like', 'podesta', 'pizza', '\\n\\n\\n\\n\\nhow', 'could', 'blind'])\n",
      "original document: \n",
      "['Are', 'you', 'fucking', 'retarded?', 'If', 'I', 'drink', 'a', 'bottle', 'of', 'whiskey,', 'immediately', 'go', 'for', 'a', 'drive', 'and', 'get', 'into', 'a', 'crash', 'that', 'kills', 'people', \"that's\", 'absolutely', '100%', 'my', 'fault.', 'You', 'make', 'the', 'choice', 'to', 'drink', 'and', 'then', 'drive,', 'just', 'like', 'the', 'guy', 'in', \"OP's\", 'story', 'chose', 'to', 'find', 'a', 'gun', 'and', 'act', 'like', 'an', 'idiot', 'with', 'it.', '\\n\\n\\nYour', 'logic', 'is', 'that', 'the', 'gun', 'owner', 'should', 'be', 'responsible', 'by', 'making', 'it', 'impossible', 'for', 'a', 'drunk', 'person', 'to', 'get', 'a', 'hold', 'of,', 'so', 'how', 'does', 'that', 'work', 'when', 'using', 'your', 'logic', 'with', 'drunk', 'driving?', 'If', 'I', 'use', 'my', 'friends', 'car', 'to', 'drive', 'while', 'drunk', 'without', 'his', 'permission,', 'is', 'that', 'my', \"friend's\", 'fault', 'for', \"'letting\", \"me'\", 'use', 'it?', 'No,', \"that's\", 'fucking', 'stupid.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'retard', 'drink', 'bottl', 'whiskey', 'immedy', 'go', 'driv', 'get', 'crash', 'kil', 'peopl', 'that', 'absolv', 'one hundred', 'fault', 'mak', 'cho', 'drink', 'driv', 'lik', 'guy', 'op', 'story', 'chos', 'find', 'gun', 'act', 'lik', 'idiot', '\\n\\n\\nyour', 'log', 'gun', 'own', 'respons', 'mak', 'imposs', 'drunk', 'person', 'get', 'hold', 'work', 'us', 'log', 'drunk', 'driv', 'us', 'friend', 'car', 'driv', 'drunk', 'without', 'permit', 'friend', 'fault', 'let', 'us', 'that', 'fuck', 'stupid'], ['fuck', 'retard', 'drink', 'bottle', 'whiskey', 'immediately', 'go', 'drive', 'get', 'crash', 'kill', 'people', 'thats', 'absolutely', 'one hundred', 'fault', 'make', 'choice', 'drink', 'drive', 'like', 'guy', 'ops', 'story', 'choose', 'find', 'gun', 'act', 'like', 'idiot', '\\n\\n\\nyour', 'logic', 'gun', 'owner', 'responsible', 'make', 'impossible', 'drink', 'person', 'get', 'hold', 'work', 'use', 'logic', 'drink', 'drive', 'use', 'friends', 'car', 'drive', 'drink', 'without', 'permission', 'friends', 'fault', 'let', 'use', 'thats', 'fuck', 'stupid'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Sold', 'Z97', 'Stinger', 'motherboard', 'to', '/u/Korrd']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sold', 'z97', 'sting', 'motherboard', 'ukorrd'], ['sell', 'z97', 'stinger', 'motherboard', 'ukorrd'])\n",
      "original document: \n",
      "[\"It's\", 'something', 'similar', 'to', 'high', 'school', 'prom', '(my', 'school', \"doesn't\", 'have', 'a', 'homecoming', 'celebration', 'either).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someth', 'simil', 'high', 'school', 'prom', 'school', 'doesnt', 'homecom', 'celebr', 'eith'], ['something', 'similar', 'high', 'school', 'prom', 'school', 'doesnt', 'homecoming', 'celebration', 'either'])\n",
      "original document: \n",
      "['Little', 'of', 'Column', 'A.', 'Little', 'of', 'Column', 'B.', \"I'm\", 'glad', 'workers', 'receive', 'protections', 'but', 'management', 'needs', 'to', 'step', 'it', 'up', 'or', 'be', 'replaced.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['littl', 'column', 'littl', 'column', 'b', 'im', 'glad', 'work', 'receiv', 'protect', 'man', 'nee', 'step', 'replac'], ['little', 'column', 'little', 'column', 'b', 'im', 'glad', 'workers', 'receive', 'protections', 'management', 'need', 'step', 'replace'])\n",
      "original document: \n",
      "['Greinke', 'is', 'stinky!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['greink', 'stinky'], ['greinke', 'stinky'])\n",
      "original document: \n",
      "['Hmm,', 'legally', 'allowed', 'maybe.', 'But', 'you', 'think', 'their', 'husbands', 'will', 'allow', 'it?', \"I'm\", 'not', 'so', 'sure.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmm', 'leg', 'allow', 'mayb', 'think', 'husband', 'allow', 'im', 'sur'], ['hmm', 'legally', 'allow', 'maybe', 'think', 'husband', 'allow', 'im', 'sure'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Industrialization', 'yes\\n\\nAgriculture', 'no,', 'there', 'was', 'this', 'quack', 'in', 'the', 'agriculture', 'that', 'did', 'serious', 'damage', 'and', 'was', 'a', 'favorite', 'of', 'Stalin,', '\"Trofim', 'Lysenko\"', 'was', 'his', 'name', 'and', 'he', 'did', 'enormous', 'damage', 'to', 'the', 'Soviet', 'agriculture.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indust', 'yes\\n\\nagriculture', 'quack', 'agricult', 'sery', 'dam', 'favorit', 'stalin', 'trofim', 'lysenko', 'nam', 'enorm', 'dam', 'soviet', 'agricult'], ['industrialization', 'yes\\n\\nagriculture', 'quack', 'agriculture', 'serious', 'damage', 'favorite', 'stalin', 'trofim', 'lysenko', 'name', 'enormous', 'damage', 'soviet', 'agriculture'])\n",
      "original document: \n",
      "['Haha', 'thanks!Very', 'honoured', 'to', 'have', 'so', 'many', 'questions', 'ahahahaha.\\n\\nNEVER', 'SOLOQ.The', 'people', 'there', 'are', 'stupid', 'and', 'will', 'waste', 'your', 'time.They', \"don't\", 'even', 'know', 'the', 'meta', 'right', 'now', 'which', 'is', 'full', 'of', 'mages,and', 'they', 'keep', 'insisting', 'on', 'playing', 'mm', 'which', 'is', 'just', 'stupid.\\n\\nMany', 'high', 'elo', 'players', 'play', 'in', 'their', 'teams', '(say', '5', 'man)but', 'personally', 'I', \"don't.In\", 'fact', 'I', \"don't\", 'really', 'play', 'with', 'my', 'squad', 'but', 'with', 'other', 'friends', 'from', 'other', 'squads.But', 'in', 'my', 'squad', 'there', 'is', 'a', 'whatsapp', 'group', 'and', 'you', 'can', 'just', 'ask', 'for', 'someone', 'and', 'then', 'play', 'tgt', 'haha.\\n\\nI', \"don't\", 'plan', 'I', 'just', 'play', 'when', 'I', 'feel', 'like', 'playing.\\n\\nI', 'got', 'it', 'one', 'day', 'before', 'the', 'season', 'ended.Personally', 'i', \"don't\", 'recommend', 'gaming', 'so', 'much', 'to', '600', 'because', 'the', 'only', 'good', 'thing', 'u', 'get', 'out', 'of', 'it', 'is', '\"bragging', 'rights\"/\"self-fulfilment\"but', 'to', 'be', 'honest', '..\\n-Nobody', 'really', 'cares', 'how', 'many', 'stars', 'u', 'have', '\\n-You', 'will', 'never', 'feel', 'happy', 'because', 'there', 'will', 'always', 'be', 'someone', 'higher', 'than', 'you.\\n\\nI', 'had', 'to', 'spend', 'ALOT', 'of', 'time(luckily', 'it', 'was', 'holidays).I', 'only', 'played', 'hard', 'in', 'the', 'last', 'month', 'of', 'the', 'season.I', 'climbed', 'from', '200', 'to', '600', 'in', 'one', 'month', 'lol.\\n\\n\\nAdvice', \"ermmm....Don't\", 'feel', 'nervous', 'if', 'you', 'meet', 'Top', 'squads.Because', 'they', 'are', 'human', 'and', 'they', 'make', 'mistakes', 'in', 'game', 'and', 'can', 'lose', 'sometimes.They', 'have', 'high', 'stars', 'but', 'stars', \"aren't\", 'really', 'a', 'good', 'indicator', 'of', 'skill.ive', 'seen', '200', 'star', 'legends', 'beat', 'a', '5', 'man', 'team', 'all', 'above', '1k', 'stars', 'so', 'yeah.\\n\\nTry', 'to', 'go', 'for', 'a', 'trio', 'q', 'with', 'your', 'friends.One', 'of', 'the', '3', 'people', 'must', 'play', 'critical', 'roles', 'in', 'the', 'team', 'like', 'mages', 'or', 'assasin', 'in', 'case', 'pubs', \"can't\", 'carry.(most', 'of', 'the', 'time', 'they', \"can't).If\", 'you', 'get', 'first', 'pick,do', 'choose', \"what's\", 'currently', 'in', 'the', 'meta.When', 'I', 'played', 'trio', 'there', 'was', 'a', 'time', 'I', 'won', '30', 'straight', 'rank', 'games', 'in', 'a', 'row', 'so', 'yea.\\n\\nDuo', 'is', 'a', 'little', 'more', 'risky', 'in', 'my', 'opinion', 'as', 'there', 'will', 'be', '3', 'heavy', 'people', 'that', 'u', 'have', 'to', 'carry.\\n\\n5', 'man', 'is', 'hard', 'since', 'there', 'are', 'a', 'lot', 'of', 'pros', 'who', 'are', 'playing', 'in', '5', 'man', 'teams', 'so', 'personally', 'I', 'stay', 'away', 'from', 'it.But', 'it', 'can', 'be', 'great', 'if', 'your', 'squad', 'has', 'a', 'lot', 'of', 'experience', 'together.\\n\\nAlways', 'try', 'to', 'improve', 'and', 'ask', 'yourself', 'what', 'you', 'could', 'have', \"don't\", 'better', 'whenever', 'you', 'die.Maybe', 'your', 'Friend', 'typed', '\"enemy', 'missing', 'in', 'action\"and', 'you', 'thought', 'meh', 'they', 'may', 'be', 'farming', 'but', 'then', 'u', 'get', 'ganked.\\n\\nThis', 'is', 'a', 'little', 'weird', 'but', 'personally', 'i', 'feel', 'if', 'you', 'play', 'in', 'the', 'wee', 'hours', 'of', 'the', 'morning,that', 'is', 'the', 'most', 'optimal', 'time', 'to', 'raise', 'stars', 'because', 'there', 'are', 'many', 'noobs', 'at', 'that', 'time.It', 'sounds', 'funny', 'but', 'without', 'it', 'I', \"wouldn't\", 'be', 'at', '600', 'LOL.\\n\\nAnd', 'lastl', 'but', 'not', \"least,don't\", 'be', 'addicted', 'to', 'the', 'game.I', 'see', 'many', 'of', 'my', 'friends', 'above', '400', 'stars', 'who', 'play', 'overnight', 'until', '7am', 'and', 'then', 'have', 'to', 'go', 'to', 'work.You', 'should', 'know', 'your', 'priorities', 'and', 'not', 'waste', 'your', 'life', 'for', 'a', 'number', 'that', 'means', 'shit.\\n\\nIn', 'the', 'next', 'season', 'I', 'will', 'retire', 'as', 'I', 'feel', 'that', \"I'm\", 'too', 'addicted', 'and', 'I', 'have', 'better', 'things', 'to', 'focus', 'on.Aft', 'getting', '600', 'stars', \"I'm\", 'abit', 'sick', 'of', 'ranked', 'games', 'haha.\\nI', \"won't\", 'go', 'for', 'it', 'because', 'there', 'are', 'a', 'lot', 'of', 'things', 'to', 'sacrifice', 'to', 'gain', 'it', 'so', 'yea\\n\\nHappy', 'to', 'ans', 'your', 'qns:)\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'thanksvery', 'hono', 'many', 'quest', 'ahahahaha\\n\\nnever', 'soloqth', 'peopl', 'stupid', 'wast', 'timethey', 'dont', 'ev', 'know', 'met', 'right', 'ful', 'magesand', 'keep', 'insist', 'play', 'mm', 'stupid\\n\\nmany', 'high', 'elo', 'play', 'play', 'team', 'say', 'fiv', 'manbut', 'person', 'dontin', 'fact', 'dont', 'real', 'play', 'squad', 'friend', 'squadsbut', 'squad', 'whatsap', 'group', 'ask', 'someon', 'play', 'tgt', 'haha\\n\\ni', 'dont', 'plan', 'play', 'feel', 'lik', 'playing\\n\\ni', 'got', 'on', 'day', 'season', 'endedperson', 'dont', 'recommend', 'gam', 'much', 'six hundred', 'good', 'thing', 'u', 'get', 'brag', 'rightsselffulfilmentbut', 'honest', '\\nnobody', 'real', 'car', 'many', 'star', 'u', '\\nyou', 'nev', 'feel', 'happy', 'alway', 'someon', 'high', 'you\\n\\ni', 'spend', 'alot', 'timelucky', 'holidays', 'play', 'hard', 'last', 'mon', 'season', 'climb', 'two hundred', 'six hundred', 'on', 'mon', 'lol\\n\\n\\nadvice', 'ermmmdont', 'feel', 'nerv', 'meet', 'top', 'squadsbecaus', 'hum', 'mak', 'mistak', 'gam', 'los', 'sometimesthey', 'high', 'star', 'star', 'ar', 'real', 'good', 'ind', 'skil', 'seen', 'two hundred', 'star', 'legend', 'beat', 'fiv', 'man', 'team', '1k', 'star', 'yeah\\n\\ntry', 'go', 'trio', 'q', 'friendson', 'three', 'peopl', 'must', 'play', 'crit', 'rol', 'team', 'lik', 'mag', 'assasin', 'cas', 'pub', 'cant', 'carrymost', 'tim', 'cant', 'get', 'first', 'pickdo', 'choos', 'what', 'cur', 'metawh', 'play', 'trio', 'tim', 'thirty', 'straight', 'rank', 'gam', 'row', 'yea\\n\\nduo', 'littl', 'risky', 'opin', 'three', 'heavy', 'peopl', 'u', 'carry\\n\\n5', 'man', 'hard', 'sint', 'lot', 'pro', 'play', 'fiv', 'man', 'team', 'person', 'stay', 'away', 'itbut', 'gre', 'squad', 'lot', 'expery', 'together\\n\\nalways', 'try', 'improv', 'ask', 'could', 'dont', 'bet', 'whenev', 'diemayb', 'friend', 'typ', 'enemy', 'miss', 'actionand', 'thought', 'meh', 'may', 'farm', 'u', 'get', 'ganked\\n\\nthis', 'littl', 'weird', 'person', 'feel', 'play', 'wee', 'hour', 'morningth', 'optim', 'tim', 'rais', 'star', 'many', 'noob', 'timeit', 'sound', 'funny', 'without', 'wouldnt', 'six hundred', 'lol\\n\\nand', 'lastl', 'leastdont', 'addict', 'game', 'see', 'many', 'friend', 'four hundred', 'star', 'play', 'overnight', '7am', 'go', 'workyou', 'know', 'pri', 'wast', 'lif', 'numb', 'mean', 'shit\\n\\nin', 'next', 'season', 'retir', 'feel', 'im', 'addict', 'bet', 'thing', 'foc', 'onaft', 'get', 'six hundred', 'star', 'im', 'abit', 'sick', 'rank', 'gam', 'haha\\ni', 'wont', 'go', 'lot', 'thing', 'sacr', 'gain', 'yea\\n\\nhappy', 'an', 'qns\\n\\n\\n'], ['haha', 'thanksvery', 'honour', 'many', 'question', 'ahahahaha\\n\\nnever', 'soloqthe', 'people', 'stupid', 'waste', 'timethey', 'dont', 'even', 'know', 'meta', 'right', 'full', 'magesand', 'keep', 'insist', 'play', 'mm', 'stupid\\n\\nmany', 'high', 'elo', 'players', 'play', 'team', 'say', 'five', 'manbut', 'personally', 'dontin', 'fact', 'dont', 'really', 'play', 'squad', 'friends', 'squadsbut', 'squad', 'whatsapp', 'group', 'ask', 'someone', 'play', 'tgt', 'haha\\n\\ni', 'dont', 'plan', 'play', 'feel', 'like', 'playing\\n\\ni', 'get', 'one', 'day', 'season', 'endedpersonally', 'dont', 'recommend', 'game', 'much', 'six hundred', 'good', 'thing', 'u', 'get', 'brag', 'rightsselffulfilmentbut', 'honest', '\\nnobody', 'really', 'care', 'many', 'star', 'u', '\\nyou', 'never', 'feel', 'happy', 'always', 'someone', 'higher', 'you\\n\\ni', 'spend', 'alot', 'timeluckily', 'holidaysi', 'play', 'hard', 'last', 'month', 'seasoni', 'climb', 'two hundred', 'six hundred', 'one', 'month', 'lol\\n\\n\\nadvice', 'ermmmdont', 'feel', 'nervous', 'meet', 'top', 'squadsbecause', 'human', 'make', 'mistake', 'game', 'lose', 'sometimesthey', 'high', 'star', 'star', 'arent', 'really', 'good', 'indicator', 'skillive', 'see', 'two hundred', 'star', 'legends', 'beat', 'five', 'man', 'team', '1k', 'star', 'yeah\\n\\ntry', 'go', 'trio', 'q', 'friendsone', 'three', 'people', 'must', 'play', 'critical', 'roles', 'team', 'like', 'mages', 'assasin', 'case', 'pubs', 'cant', 'carrymost', 'time', 'cantif', 'get', 'first', 'pickdo', 'choose', 'whats', 'currently', 'metawhen', 'play', 'trio', 'time', 'thirty', 'straight', 'rank', 'game', 'row', 'yea\\n\\nduo', 'little', 'risky', 'opinion', 'three', 'heavy', 'people', 'u', 'carry\\n\\n5', 'man', 'hard', 'since', 'lot', 'pros', 'play', 'five', 'man', 'team', 'personally', 'stay', 'away', 'itbut', 'great', 'squad', 'lot', 'experience', 'together\\n\\nalways', 'try', 'improve', 'ask', 'could', 'dont', 'better', 'whenever', 'diemaybe', 'friend', 'type', 'enemy', 'miss', 'actionand', 'think', 'meh', 'may', 'farm', 'u', 'get', 'ganked\\n\\nthis', 'little', 'weird', 'personally', 'feel', 'play', 'wee', 'hours', 'morningthat', 'optimal', 'time', 'raise', 'star', 'many', 'noobs', 'timeit', 'sound', 'funny', 'without', 'wouldnt', 'six hundred', 'lol\\n\\nand', 'lastl', 'leastdont', 'addict', 'gamei', 'see', 'many', 'friends', 'four hundred', 'star', 'play', 'overnight', '7am', 'go', 'workyou', 'know', 'priorities', 'waste', 'life', 'number', 'mean', 'shit\\n\\nin', 'next', 'season', 'retire', 'feel', 'im', 'addict', 'better', 'things', 'focus', 'onaft', 'get', 'six hundred', 'star', 'im', 'abit', 'sick', 'rank', 'game', 'haha\\ni', 'wont', 'go', 'lot', 'things', 'sacrifice', 'gain', 'yea\\n\\nhappy', 'ans', 'qns\\n\\n\\n'])\n",
      "original document: \n",
      "['Just', 'started', 'watching', 'on', 'ESPN3.', 'Go', 'Herd']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'watch', 'espn3', 'go', 'herd'], ['start', 'watch', 'espn3', 'go', 'herd'])\n",
      "original document: \n",
      "['I', 'dont', 'care', 'if', 'its', 'a', 'rapist', 'a', 'murder', 'whomever', 'it', 'may', 'be', ',', 'this', 'is', 'no', 'way', 'to', 'go,', '\\nNo', 'one', 'should', 'feel', 'their', 'skin', 'melting', 'after', 'being', 'beaten', 'near', 'death,', 'and', 'the', 'last', 'thing', 'i', 'need', 'is', 'comments', 'by', 'edgy', 'teens', 'saying', '\"he', 'deserves', 'it', 'he', 'is', 'a', 'rapist!!!111', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'car', 'rap', 'murd', 'whomev', 'may', 'way', 'go', '\\nno', 'on', 'feel', 'skin', 'melt', 'beat', 'near', 'dea', 'last', 'thing', 'nee', 'com', 'edgy', 'teen', 'say', 'deserv', 'rapist111'], ['dont', 'care', 'rapist', 'murder', 'whomever', 'may', 'way', 'go', '\\nno', 'one', 'feel', 'skin', 'melt', 'beat', 'near', 'death', 'last', 'thing', 'need', 'comment', 'edgy', 'teens', 'say', 'deserve', 'rapist111'])\n",
      "original document: \n",
      "['Bud', 'Light', 'is', 'the', 'worst', 'beer', 'on', 'the', 'market\\n\\nYes,', 'that', 'includes', 'any', 'and', 'all', '30-rack', 'beer.', 'Bud', 'Light', 'is', 'honestly', 'just', 'so', 'god', 'damn', 'bad']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bud', 'light', 'worst', 'beer', 'market\\n\\nyes', 'includ', '30rack', 'beer', 'bud', 'light', 'honest', 'god', 'damn', 'bad'], ['bud', 'light', 'worst', 'beer', 'market\\n\\nyes', 'include', '30rack', 'beer', 'bud', 'light', 'honestly', 'god', 'damn', 'bad'])\n",
      "original document: \n",
      "['Women', 'go', 'their', 'own', 'way']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wom', 'go', 'way'], ['women', 'go', 'way'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['It', 'plays', 'a', 'big', 'role', 'in', 'linkffn(Harry', 'Potter', 'and', 'the', 'Enemy', 'Within', 'by', 'Theowyn', 'of', 'HPG)', 'and', 'its', 'sequel', 'linkffn(Harry', 'Potter', 'and', 'the', 'Chained', 'Souls', 'by', 'Theowyn', 'of', 'HPG)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'big', 'rol', 'linkffnharry', 'pot', 'enemy', 'within', 'theowyn', 'hpg', 'sequel', 'linkffnharry', 'pot', 'chain', 'soul', 'theowyn', 'hpg'], ['play', 'big', 'role', 'linkffnharry', 'potter', 'enemy', 'within', 'theowyn', 'hpg', 'sequel', 'linkffnharry', 'potter', 'chain', 'souls', 'theowyn', 'hpg'])\n",
      "original document: \n",
      "['I', 'cringed', 'when', 'I', 'read', 'it', 'but', \"didn't\", 'want', 'to', 'say', 'anything', 'because', 'people', 'get', 'really', 'defensive', 'and', 'call', 'me', 'names', 'when', 'I', 'try', 'to', 'correct', 'things', 'like', 'that.', 'D:']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cring', 'read', 'didnt', 'want', 'say', 'anyth', 'peopl', 'get', 'real', 'defend', 'cal', 'nam', 'try', 'correct', 'thing', 'lik'], ['cringe', 'read', 'didnt', 'want', 'say', 'anything', 'people', 'get', 'really', 'defensive', 'call', 'name', 'try', 'correct', 'things', 'like'])\n",
      "original document: \n",
      "['What’s', 'on', 'the', 'menu', 'for', 'tonight?', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'menu', 'tonight'], ['whats', 'menu', 'tonight'])\n",
      "original document: \n",
      "['Twice!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twic'], ['twice'])\n",
      "original document: \n",
      "['It', 'was', 'a', 'artist', 'that', 'redrew', 'it', 'in', 'his', 'own', 'style', 'he', 'credited', 'shadman']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['art', 'redrew', 'styl', 'credit', 'shadm'], ['artist', 'redrew', 'style', 'credit', 'shadman'])\n",
      "original document: \n",
      "['In', 'case', 'anyone', 'else', 'happens', 'to', 'look', 'back,', 'I', '[did', 'it](https://youtu.be/yCIiXZ0ayGM).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cas', 'anyon', 'els', 'hap', 'look', 'back', 'ithttpsyoutubeyciixz0aygm'], ['case', 'anyone', 'else', 'happen', 'look', 'back', 'ithttpsyoutubeyciixz0aygm'])\n",
      "original document: \n",
      "['I', 'got', 'depressed', 'in', 'college', 'as', 'well.', 'I', 'was', 'completely', 'overwhelmed', 'and', 'realized', 'way', 'too', 'late', 'that', 'I', \"didn't\", 'have', 'a', 'lasting', 'interest', 'in', 'what', 'I', 'was', 'studying.', 'I', 'got', 'counseling', 'and', 'medication', '(which', 'I', 'also', 'hated)', 'and', 'struggled', 'through.', 'I', 'got', 'a', 'job', 'that', 'I', 'hated,', 'played', 'with', 'my', 'medications', 'to', 'make', 'it', 'through', 'the', 'day', 'and', 'tried', 'to', 'be', 'a', 'good', 'person.\\n\\nAnd', 'one', 'day', 'I', 'was', 'done.', 'I', \"didn't\", 'want', 'to', 'drug', 'myself', 'to', 'get', 'through', 'life,', 'to', 'even', 'get', 'through', 'a', 'day.', 'Life', 'was', 'more', 'than', 'that.', 'So', 'I', 'quit', 'my', 'job', 'and', 'stopped', 'taking', 'my', 'meds.', 'Found', 'something', 'else', 'I', 'preferred', 'to', 'do', 'and', 'never', 'looked', 'back.\\n\\nAre', 'you', 'happy', 'with', 'your', 'program', 'that', 'you', 'are', 'in?', 'Are', 'you', 'happy', 'with', 'where', 'you', 'live', 'and', 'the', 'direction', 'your', 'life', 'is', 'heading?', 'Think', 'hard', 'about', 'that.', 'Especially', 'if', 'you', \"can't\", 'even', 'study', 'on', 'the', 'weekends.', 'I', 'recall', 'only', 'doing', 'that', 'on', 'weekends', 'because', 'I', 'was', 'so', 'exhausted', 'during', 'the', 'week.\\n\\nWeekends', 'are', 'supposed', 'to', 'rejuvenate', 'you.', 'Be', 'a', 'time', 'to', 'relax', 'and', 'catch', 'up', 'on', \"'you\", \"time'.\", 'Maybe', 'you', \"aren't\", 'loving', 'yourself', 'enough?\\n\\nI', 'still', 'suggest', 'that', 'you', 'get', 'up,', 'shower,', 'and', 'get', 'dressed', 'on', 'the', 'weekends.', 'Perhaps', 'at', 'least', 'go', 'for', 'a', 'walk', 'to', 'get', 'out', 'of', 'the', 'house.', 'And', 'definitely', 'keep', 'your', 'therapy', 'appointment.', 'It', 'always', 'helps', 'to', 'talk', 'to', 'someone', 'else.\\n\\nGood', 'luck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'depress', 'colleg', 'wel', 'complet', 'overwhelm', 'real', 'way', 'lat', 'didnt', 'last', 'interest', 'study', 'got', 'counsel', 'med', 'also', 'hat', 'struggled', 'got', 'job', 'hat', 'play', 'med', 'mak', 'day', 'tri', 'good', 'person\\n\\nand', 'on', 'day', 'don', 'didnt', 'want', 'drug', 'get', 'lif', 'ev', 'get', 'day', 'lif', 'quit', 'job', 'stop', 'tak', 'med', 'found', 'someth', 'els', 'prefer', 'nev', 'look', 'back\\n\\nare', 'happy', 'program', 'happy', 'liv', 'direct', 'lif', 'head', 'think', 'hard', 'espec', 'cant', 'ev', 'study', 'weekend', 'recal', 'weekend', 'exhaust', 'week\\n\\nweekends', 'suppos', 'rejuv', 'tim', 'relax', 'catch', 'tim', 'mayb', 'ar', 'lov', 'enough\\n\\ni', 'stil', 'suggest', 'get', 'show', 'get', 'dress', 'weekend', 'perhap', 'least', 'go', 'walk', 'get', 'hous', 'definit', 'keep', 'therapy', 'appoint', 'alway', 'help', 'talk', 'someon', 'else\\n\\ngood', 'luck'], ['get', 'depress', 'college', 'well', 'completely', 'overwhelm', 'realize', 'way', 'late', 'didnt', 'last', 'interest', 'study', 'get', 'counsel', 'medication', 'also', 'hat', 'struggle', 'get', 'job', 'hat', 'play', 'medications', 'make', 'day', 'try', 'good', 'person\\n\\nand', 'one', 'day', 'do', 'didnt', 'want', 'drug', 'get', 'life', 'even', 'get', 'day', 'life', 'quit', 'job', 'stop', 'take', 'meds', 'find', 'something', 'else', 'prefer', 'never', 'look', 'back\\n\\nare', 'happy', 'program', 'happy', 'live', 'direction', 'life', 'head', 'think', 'hard', 'especially', 'cant', 'even', 'study', 'weekend', 'recall', 'weekend', 'exhaust', 'week\\n\\nweekends', 'suppose', 'rejuvenate', 'time', 'relax', 'catch', 'time', 'maybe', 'arent', 'love', 'enough\\n\\ni', 'still', 'suggest', 'get', 'shower', 'get', 'dress', 'weekend', 'perhaps', 'least', 'go', 'walk', 'get', 'house', 'definitely', 'keep', 'therapy', 'appointment', 'always', 'help', 'talk', 'someone', 'else\\n\\ngood', 'luck'])\n",
      "original document: \n",
      "['Guys,', 'come', 'join', 'us', 'channel', '3', 'on', 'the', 'OpTic', 'Reddit', 'Discord!!\\n\\nhttps://discordapp.com/invite/opticgamingreddit\\n\\nEdit:', 'We', 'have', 'our', 'own', 'caster', 'who', 'takes', 'questions', 'xP\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guy', 'com', 'join', 'us', 'channel', 'three', 'opt', 'reddit', 'discord\\n\\nhttpsdiscordappcominviteopticgamingreddit\\n\\nedit', 'cast', 'tak', 'quest', 'xp\\n'], ['guy', 'come', 'join', 'us', 'channel', 'three', 'optic', 'reddit', 'discord\\n\\nhttpsdiscordappcominviteopticgamingreddit\\n\\nedit', 'caster', 'take', 'question', 'xp\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'have', '2', 'copies', 'of', 'this.', 'I', 'will', 'need', 'to', 'sell', 'one', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'cop', 'nee', 'sel', 'on', 'though'], ['two', 'copy', 'need', 'sell', 'one', 'though'])\n",
      "original document: \n",
      "['This', \"isn't\", 'a', 'report,', 'Valve', 'just', 'sends', 'everyone', 'with', 'over', '100', 'points', 'directly', 'to', 'Overwatch.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isnt', 'report', 'valv', 'send', 'everyon', 'one hundred', 'point', 'direct', 'overwatch'], ['isnt', 'report', 'valve', 'send', 'everyone', 'one hundred', 'point', 'directly', 'overwatch'])\n",
      "original document: \n",
      "['Yeh', 'ur', 'on', 'my', 'list', 'of', 'ppl', 'to', 'vote', 'for']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeh', 'ur', 'list', 'ppl', 'vot'], ['yeh', 'ur', 'list', 'ppl', 'vote'])\n",
      "original document: \n",
      "['No,', 'because', 'I', \"don't\", 'know', 'how.', 'I', 'think', 'it', 'would', 'just', 'come', 'of', 'as', '/r/fellowkids']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'think', 'would', 'com', 'rfellowkid'], ['dont', 'know', 'think', 'would', 'come', 'rfellowkids'])\n",
      "original document: \n",
      "['What', 'the', 'hell', 'am', 'I', 'looking', 'at', 'here?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hel', 'look'], ['hell', 'look'])\n",
      "original document: \n",
      "['He', 'saw', 'how', 'popular', 'the', 'crash', 'Dankquan', 'had', 'was.', \"It'll\", 'be', 'in', 'the', 'script', 'at', 'some', 'point.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'popul', 'crash', 'dankqu', 'itl', 'script', 'point'], ['saw', 'popular', 'crash', 'dankquan', 'itll', 'script', 'point'])\n",
      "original document: \n",
      "['Who', 'the', 'fuck', 'are', 'you?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck'], ['fuck'])\n",
      "original document: \n",
      "['That', 'people', \"don't\", 'realize', 'this', 'is', 'a', 'choice.', 'We', \"don't\", 'have', 'to', 'accept', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'dont', 'real', 'cho', 'dont', 'acceiv'], ['people', 'dont', 'realize', 'choice', 'dont', 'accept'])\n",
      "original document: \n",
      "['Oh', 'I', 'am', 'living.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'liv'], ['oh', 'live'])\n",
      "original document: \n",
      "['https://youtu.be/Lpq53oR94JM']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsyoutubelpq53or94jm'], ['httpsyoutubelpq53or94jm'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['They', 'are', 'both', 'whatever.', 'Henrich', 'is', 'better.', 'He', 'can', '6', 'star.', 'Those', 'two', 'can', 'only', '4', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whatev', 'henrich', 'bet', 'six', 'star', 'two', 'four'], ['whatever', 'henrich', 'better', 'six', 'star', 'two', 'four'])\n",
      "original document: \n",
      "['But', 'that', 'series', 'was', 'way', 'too', 'close,', 'HR', 'kinda', 'fucked', 'up', 'that', 'hg', 'push.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sery', 'way', 'clos', 'hr', 'kind', 'fuck', 'hg', 'push'], ['series', 'way', 'close', 'hr', 'kinda', 'fuck', 'hg', 'push'])\n",
      "original document: \n",
      "[\"I'll\", 'definitely', 'bring', 'that', 'up', 'when', 'I', 'go', 'in', 'again,', 'last', 'time', 'I', 'took', 'it', 'to', 'someone', 'they', 'said', 'that', 'one', 'of', 'the', \"engine's\", 'cylinders', 'was', 'misfiring', 'but', 'changing', 'the', 'spark', 'plug', 'seems', 'like', 'it', 'fixed', 'that', 'at', 'least']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'definit', 'bring', 'go', 'last', 'tim', 'took', 'someon', 'said', 'on', 'engin', 'cylind', 'misfir', 'chang', 'spark', 'plug', 'seem', 'lik', 'fix', 'least'], ['ill', 'definitely', 'bring', 'go', 'last', 'time', 'take', 'someone', 'say', 'one', 'engines', 'cylinders', 'misfire', 'change', 'spark', 'plug', 'seem', 'like', 'fix', 'least'])\n",
      "original document: \n",
      "['Now', 'that', 'is', 'a', 'name', 'I', \"haven't\", 'heard', 'in', 'a', 'long', 'time.', 'Beardown🐻⬇️']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nam', 'hav', 'heard', 'long', 'tim', 'beardown'], ['name', 'havent', 'hear', 'long', 'time', 'beardown'])\n",
      "original document: \n",
      "['I', 'can', 'attest', 'that', 'this', 'deck', 'is', 'quite', 'good', 'at', 'achieving', 'total', 'lock,', 'but', 'I', 'still', 'have', 'two', 'big', 'grievances', 'with', 'it.', 'First', 'is', 'that', 'without', 'White', 'it', 'can', 'have', 'a', 'hard', 'time', 'making', 'up', 'large', 'point', 'differences', 'in', 'a', 'timed', 'environment.', 'Second', 'is', 'that,', 'as', 'the', 'deck', 'is', 'presented', 'here,', 'there', \"isn't\", 'any', 'forced', 'movement', 'from', 'the', 'Purple', 'cards.', 'I', 'know', 'the', 'deck', 'is', 'straining', 'to', 'include', 'all', 'the', 'answers', 'it', 'needs,', 'but', 'without', 'movement', 'effects', 'a', 'Villain', 'farm', 'looks', 'at', 'this', 'deck,', 'shrugs', 'its', 'shoulders,', 'and', 'does', 'what', 'it', 'was', 'going', 'to', 'do', 'anyway', '(with', 'the', 'one', 'exception', 'of', 'Aloe', '&amp;', 'Lotus).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['attest', 'deck', 'quit', 'good', 'achiev', 'tot', 'lock', 'stil', 'two', 'big', 'griev', 'first', 'without', 'whit', 'hard', 'tim', 'mak', 'larg', 'point', 'diff', 'tim', 'environ', 'second', 'deck', 'pres', 'isnt', 'forc', 'mov', 'purpl', 'card', 'know', 'deck', 'straining', 'includ', 'answ', 'nee', 'without', 'mov', 'effect', 'villain', 'farm', 'look', 'deck', 'shrugs', 'should', 'going', 'anyway', 'on', 'exceiv', 'alo', 'amp', 'lot'], ['attest', 'deck', 'quite', 'good', 'achieve', 'total', 'lock', 'still', 'two', 'big', 'grievances', 'first', 'without', 'white', 'hard', 'time', 'make', 'large', 'point', 'differences', 'time', 'environment', 'second', 'deck', 'present', 'isnt', 'force', 'movement', 'purple', 'card', 'know', 'deck', 'strain', 'include', 'answer', 'need', 'without', 'movement', 'effect', 'villain', 'farm', 'look', 'deck', 'shrug', 'shoulder', 'go', 'anyway', 'one', 'exception', 'aloe', 'amp', 'lotus'])\n",
      "original document: \n",
      "['If', 'you', 'think', 'Gatsby', 'was', 'not', 'a', 'Chad', \"you're\", 'fucking', 'delusional.', \"It's\", 'clearly', 'explained', 'in', 'the', 'books', 'how', 'he', 'fucked', 'many', 'women', 'while', 'at', 'Lake.Superior', 'after', 'he', 'ran', 'away', 'from', 'college.', 'Hell', 'it', 'is', 'even', 'stated', 'in', 'the', 'book', 'that', 'he', '\"took', '[Daisy', 'Buchanan]\"', 'sexually.', 'What', 'the', 'fucking', 'story', 'shows', 'is', 'that', 'all', 'women', 'are', 'basically', 'fucking', 'cucking', 'their', 'husband.', 'Daisy', 'cheated', 'on', 'her', 'husband', 'cos', 'she', 'thought', 'Gatsby', 'was', 'better', 'but', 'then', 'went', 'back', 'to', 'her', 'husband', 'when', 'she', 'realized', 'he', 'was', 'better.', 'Fucking', 'women.', 'All', 'of', 'them.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'gatsby', 'chad', 'yo', 'fuck', 'delud', 'clear', 'explain', 'book', 'fuck', 'many', 'wom', 'lakesupery', 'ran', 'away', 'colleg', 'hel', 'ev', 'stat', 'book', 'took', 'daisy', 'buch', 'sex', 'fuck', 'story', 'show', 'wom', 'bas', 'fuck', 'cuck', 'husband', 'daisy', 'che', 'husband', 'cos', 'thought', 'gatsby', 'bet', 'went', 'back', 'husband', 'real', 'bet', 'fuck', 'wom'], ['think', 'gatsby', 'chad', 'youre', 'fuck', 'delusional', 'clearly', 'explain', 'book', 'fuck', 'many', 'women', 'lakesuperior', 'run', 'away', 'college', 'hell', 'even', 'state', 'book', 'take', 'daisy', 'buchanan', 'sexually', 'fuck', 'story', 'show', 'women', 'basically', 'fuck', 'cucking', 'husband', 'daisy', 'cheat', 'husband', 'cos', 'think', 'gatsby', 'better', 'go', 'back', 'husband', 'realize', 'better', 'fuck', 'women'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Dilemma:', \"I'm\", 'a', 'female', 'and', 'not', 'fantasizing', 'about', '\"Chad\",', 'in', 'fact', 'many', '\"Chad\"', 'qualities', '(even', 'physical)', 'are', 'not', 'appealing', 'to', 'me.\\n\\nSo,', 'am', 'I', 'not', 'a', 'female?', 'Is', 'Chad', 'a', 'lie?', 'Please,', 'save', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dilemm', 'im', 'fem', 'fantas', 'chad', 'fact', 'many', 'chad', 'qual', 'ev', 'phys', 'ap', 'me\\n\\nso', 'fem', 'chad', 'lie', 'pleas', 'sav'], ['dilemma', 'im', 'female', 'fantasize', 'chad', 'fact', 'many', 'chad', 'qualities', 'even', 'physical', 'appeal', 'me\\n\\nso', 'female', 'chad', 'lie', 'please', 'save'])\n",
      "original document: \n",
      "['Related', 'question:', 'is', 'it', 'better', 'to', 'do', 'dual', 'channel', 'at', '2933,', 'or', 'quad', 'channel', 'at', '2133?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rel', 'quest', 'bet', 'dual', 'channel', 'two thousand, nine hundred and thirty-three', 'quad', 'channel', 'two thousand, one hundred and thirty-three'], ['relate', 'question', 'better', 'dual', 'channel', 'two thousand, nine hundred and thirty-three', 'quad', 'channel', 'two thousand, one hundred and thirty-three'])\n",
      "original document: \n",
      "[\"Let's\", 'hope', 'that', 'Weber', 'flattens', 'McCormick']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'hop', 'web', 'flat', 'mccormick'], ['let', 'hope', 'weber', 'flatten', 'mccormick'])\n",
      "original document: \n",
      "['When', \"there's\", 'no', 'electricity.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'elect'], ['theres', 'electricity'])\n",
      "original document: \n",
      "['Interested', 'in', 'the', 'salty', 'banner', 'for', 'your', 'calculated', 'one?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'sal', 'ban', 'calc', 'on'], ['interest', 'salty', 'banner', 'calculate', 'one'])\n",
      "original document: \n",
      "['The', 'only', 'one', 'you', 'could', 'probably', 'run', 'non-standard', '(mostly', 'referring', 'to', 'tankless', 'runs)', 'for', 'is', 'Halatali.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'could', 'prob', 'run', 'nonstandard', 'most', 'refer', 'tankless', 'run', 'halatal'], ['one', 'could', 'probably', 'run', 'nonstandard', 'mostly', 'refer', 'tankless', 'run', 'halatali'])\n",
      "original document: \n",
      "['&gt;', 'Audrey’s', 'most', 'famous', 'moment', 'is', 'without', 'question', 'her', 'dance', 'in', 'the', 'Double', 'R', \"diner.\\n\\nAudrey's\", 'most', 'famous', 'moment', 'was', 'tied', 'the', 'cherry', 'stem.', '', 'No', 'contest.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'audrey', 'fam', 'mom', 'without', 'quest', 'dant', 'doubl', 'r', 'diner\\n\\naudreys', 'fam', 'mom', 'tied', 'cherry', 'stem', 'contest'], ['gt', 'audreys', 'famous', 'moment', 'without', 'question', 'dance', 'double', 'r', 'diner\\n\\naudreys', 'famous', 'moment', 'tie', 'cherry', 'stem', 'contest'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['&gt;', 'He', 'was', 'driving', 'me', 'home,', 'and', 'it', 'started', 'raining,', 'and', 'he', 'pushed', 'the', 'brakes,', 'to', 'the', 'floor,', 'as', 'you', 'do', 'with', 'anti-lock', 'brakes.', '\\n\\nNot', 'to', 'argue', 'but', \"aren't\", 'you', 'supposed', 'to', 'brake', 'normally,', 'feel', 'the', 'antilock', 'engage', 'and', 'then', 'press', 'to', 'the', 'floor?', '', 'Seems', 'like', 'he', \"didn't\", 'quite', 'do', 'the', 'right', 'thing', 'regardless.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'driv', 'hom', 'start', 'rain', 'push', 'brak', 'flo', 'antilock', 'brak', '\\n\\nnot', 'argu', 'ar', 'suppos', 'brak', 'norm', 'feel', 'antilock', 'eng', 'press', 'flo', 'seem', 'lik', 'didnt', 'quit', 'right', 'thing', 'regardless'], ['gt', 'drive', 'home', 'start', 'rain', 'push', 'brake', 'floor', 'antilock', 'brake', '\\n\\nnot', 'argue', 'arent', 'suppose', 'brake', 'normally', 'feel', 'antilock', 'engage', 'press', 'floor', 'seem', 'like', 'didnt', 'quite', 'right', 'thing', 'regardless'])\n",
      "original document: \n",
      "['I', \"haven't\", 'seen', 'all', 'the', 'line', 'outs', 'but', 'are', 'the', 'Pumas', 'throwing', 'down', 'the', 'middle?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hav', 'seen', 'lin', 'out', 'puma', 'throwing', 'middl'], ['havent', 'see', 'line', 'out', 'pumas', 'throw', 'middle'])\n",
      "original document: \n",
      "['[Definition', 'of', '\"Sea-star\"](http://dictionary.com/browse/Sea-star)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'seastarhttpdictionarycombrowseseast'], ['definition', 'seastarhttpdictionarycombrowseseastar'])\n",
      "original document: \n",
      "['Geez.', '2:45...', 'What', 'mode', 'was', 'that?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['geez', 'two hundred and forty-five', 'mod'], ['geez', 'two hundred and forty-five', 'mode'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Transgenders.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['transgend'], ['transgenders'])\n",
      "original document: \n",
      "['Very', 'true.', 'He', 'is', 'indeed', 'a', 'good', 'boy.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'indee', 'good', 'boy'], ['true', 'indeed', 'good', 'boy'])\n",
      "original document: \n",
      "['I', 'definitely', \"wouldn't\", 'say', 'I', 'hate', 'him', '', 'but', 'this', 'makes', 'me', 'think', 'of', 'his', 'hollywoodreporter', 'profile.', 'In', 'the', 'part', 'where', 'his', 'castmates', 'describe', 'him,', 'Patrick', 'said', 'he', 'thought', 'Ryan', 'was', 'famous', 'because', 'production', 'was', 'always', 'talking', 'to', 'him', 'more', 'than', 'everyone', 'else.', 'And', 'he', 'was', 'featured', 'prominently', 'in', 'a', 'lot', 'of', 'the', 'promotion', 'for', 'the', 'season.\\n\\nSo', 'I', 'think', \"it's\", 'more', 'of', 'a', 'backlash', 'because', 'people', 'feel', 'like', \"he's\", 'transparently', 'being', 'fed', 'to', 'them', 'as', 'the', 'face', 'of', 'the', 'season.', 'And', 'maybe', \"there's\", 'an', 'impression', 'that', \"he's\", 'aware', 'of', 'his', 'archetype', 'and', \"he's\", 'playing', 'it', 'up', 'to', 'get', 'screen', 'time', 'and', 'become', 'a', 'big', 'character.', \"\\n\\nI'm\", 'not', 'saying', \"that's\", 'true,', 'but', \"that's\", 'my', 'guess', 'as', 'to', 'what', 'people', \"don't\", 'like', 'about', 'him.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'wouldnt', 'say', 'hat', 'mak', 'think', 'hollywoodreport', 'profil', 'part', 'castm', 'describ', 'patrick', 'said', 'thought', 'ryan', 'fam', 'produc', 'alway', 'talk', 'everyon', 'els', 'feat', 'promin', 'lot', 'promot', 'season\\n\\nso', 'think', 'backlash', 'peopl', 'feel', 'lik', 'hes', 'transp', 'fed', 'fac', 'season', 'mayb', 'ther', 'impress', 'hes', 'aw', 'archetyp', 'hes', 'play', 'get', 'screen', 'tim', 'becom', 'big', 'charact', '\\n\\nim', 'say', 'that', 'tru', 'that', 'guess', 'peopl', 'dont', 'lik'], ['definitely', 'wouldnt', 'say', 'hate', 'make', 'think', 'hollywoodreporter', 'profile', 'part', 'castmates', 'describe', 'patrick', 'say', 'think', 'ryan', 'famous', 'production', 'always', 'talk', 'everyone', 'else', 'feature', 'prominently', 'lot', 'promotion', 'season\\n\\nso', 'think', 'backlash', 'people', 'feel', 'like', 'hes', 'transparently', 'feed', 'face', 'season', 'maybe', 'theres', 'impression', 'hes', 'aware', 'archetype', 'hes', 'play', 'get', 'screen', 'time', 'become', 'big', 'character', '\\n\\nim', 'say', 'thats', 'true', 'thats', 'guess', 'people', 'dont', 'like'])\n",
      "original document: \n",
      "['Great', 'pic!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'pic'], ['great', 'pic'])\n",
      "original document: \n",
      "['143417265|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'nN0OTd4c)\\n\\n&gt;&gt;143417220\\n\\nWEED\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, two hundred and sixty-fiv', 'gt', 'unit', 'stat', 'anonym', 'id', 'nn0otd4c\\n\\ngtgt143417220\\n\\nweed\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, two hundred and sixty-five', 'gt', 'unite', 'state', 'anonymous', 'id', 'nn0otd4c\\n\\ngtgt143417220\\n\\nweed\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Keep', 'doing', 'this.', 'Zombie', 'build', 'is', 'the', 'strongest', 'one', 'for', 'hero', 'league', 'in', 'my', 'opinion', 'and', 'I', 'have', 'a', 'huge', 'win', 'rate', 'with', 'it.', 'Alarak', 'is', 'one', 'of', 'the', 'best', 'heroes', 'for', 'solo', 'lane', 'and', 'you', 'only', 'need', 'to', 'ban', 'Sonya', 'and', \"don't\", 'face', 'with', 'Gazlowe', 'main.', 'Show', 'of', 'force', 'is', 'a', 'good', 'talent', 'for', 'braxis.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['keep', 'zomby', 'build', 'strongest', 'on', 'hero', 'leagu', 'opin', 'hug', 'win', 'rat', 'alarak', 'on', 'best', 'hero', 'solo', 'lan', 'nee', 'ban', 'sony', 'dont', 'fac', 'gazlow', 'main', 'show', 'forc', 'good', 'tal', 'brax'], ['keep', 'zombie', 'build', 'strongest', 'one', 'hero', 'league', 'opinion', 'huge', 'win', 'rate', 'alarak', 'one', 'best', 'heroes', 'solo', 'lane', 'need', 'ban', 'sonya', 'dont', 'face', 'gazlowe', 'main', 'show', 'force', 'good', 'talent', 'braxis'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['He', 'wasn’t', 'brought', 'back,', 'the', 'gates', 'of', 'hell', 'were', 'broken', 'and', 'the', 'living', 'were', 'aloud', 'to', 'walk', 'with', 'the', 'dead.', 'Freeza', 'just', 'came', 'to', 'the', 'living', 'world', 'because', 'he', 'could.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wasnt', 'brought', 'back', 'gat', 'hel', 'brok', 'liv', 'aloud', 'walk', 'dead', 'freez', 'cam', 'liv', 'world', 'could'], ['wasnt', 'bring', 'back', 'gate', 'hell', 'break', 'live', 'aloud', 'walk', 'dead', 'freeza', 'come', 'live', 'world', 'could'])\n",
      "original document: \n",
      "['Swim', 'to', 'freedom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['swim', 'freedom'], ['swim', 'freedom'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"Ga'head\", 'babe,', 'talk', 'at', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gahead', 'bab', 'talk'], ['gahead', 'babe', 'talk'])\n",
      "original document: \n",
      "['Well...', 'Some', 'people', 'here', 'eat', 'dogs,', 'and', \"I've\", 'heard', 'stories.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'peopl', 'eat', 'dog', 'iv', 'heard', 'story'], ['well', 'people', 'eat', 'dog', 'ive', 'hear', 'stories'])\n",
      "original document: \n",
      "['yes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['I', \"couldn't\", 'agree', 'more', 'LOL.', 'He', 'just', 'gives', 'it', 'his', 'all', 'to', 'the', 'team.', 'And', 'if', 'he', 'never', 'got', 'injured,', 'things', \"could've\", 'been', 'a', 'lot', 'different', 'in', 'the', 'play', 'offs.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['couldnt', 'agr', 'lol', 'giv', 'team', 'nev', 'got', 'ind', 'thing', 'couldv', 'lot', 'diff', 'play', 'off'], ['couldnt', 'agree', 'lol', 'give', 'team', 'never', 'get', 'injure', 'things', 'couldve', 'lot', 'different', 'play', 'off'])\n",
      "original document: \n",
      "['Because', 'you', \"don't\", 'sign', 'up', 'for', 'football,', 'run', 'in', 'a', 'pretty', 'standard', 'way', 'by', 'the', 'coach?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'sign', 'footbal', 'run', 'pretty', 'standard', 'way', 'coach'], ['dont', 'sign', 'football', 'run', 'pretty', 'standard', 'way', 'coach'])\n",
      "original document: \n",
      "['Can', 'we', 'summon', 'the', 'ghost', 'of', 'Patrick', 'Swayze', 'to', 'hash', 'this', 'all', 'out?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['summon', 'ghost', 'patrick', 'sways', 'hash'], ['summon', 'ghost', 'patrick', 'swayze', 'hash'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Do', 'you', 'even', 'know', 'what', 'a', 'walled', 'ecosystem', 'is?\\n\\nOr', 'are', 'you', 'just', 'repeating', 'it', 'because', 'you', 'think', 'it', 'makes', 'you', 'looks', 'smart?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'know', 'wal', 'ecosystem', 'is\\n\\nor', 'rep', 'think', 'mak', 'look', 'smart'], ['even', 'know', 'wall', 'ecosystem', 'is\\n\\nor', 'repeat', 'think', 'make', 'look', 'smart'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'definitely', 'see', 'a', 'ton', 'of', 'red', 'flags.', 'Your', 'mom', 'is', 'abusive,', 'for', 'sure.', 'She', 'also', 'sounds', 'super', 'unstable,', 'and', 'it', 'must', 'be', 'exhausting', 'having', 'to', 'deal', 'with', 'her', 'constant', 'up', 'and', 'down', 'fluctuations.\\n\\nYour', 'best', 'bet', 'is', 'to', 'avoid', 'her', 'when', 'possible', 'and', 'prepare', 'for', 'the', 'day', 'when', 'you', 'can', 'move', 'out', 'to', 'go', 'to', 'college', 'and', 'get', 'some', 'distance.', 'In', 'the', 'meantime,', \"don't\", 'trust', 'her', 'with', 'ANYTHING.', 'Keep', 'your', 'valuables', 'locked', 'up', 'when', 'possible.', \"Don't\", 'let', 'her', 'have', 'access', 'to', 'your', 'money.', '', \"She's\", 'already', 'threatening', 'to', 'take', 'away', 'things', 'you', 'own', 'and', 'apparently', 'feels', 'justified', 'in', 'this.', \"Don't\", 'doubt', 'that', \"she'll\", 'make', 'good', 'on', 'those', 'threats', 'one', 'of', 'these', 'days.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'see', 'ton', 'red', 'flag', 'mom', 'abud', 'sur', 'also', 'sound', 'sup', 'unst', 'must', 'exhaust', 'deal', 'const', 'fluctuations\\n\\nyour', 'best', 'bet', 'avoid', 'poss', 'prep', 'day', 'mov', 'go', 'colleg', 'get', 'dist', 'meantim', 'dont', 'trust', 'anyth', 'keep', 'valu', 'lock', 'poss', 'dont', 'let', 'access', 'money', 'she', 'already', 'threatening', 'tak', 'away', 'thing', 'app', 'feel', 'just', 'dont', 'doubt', 'shel', 'mak', 'good', 'threats', 'on', 'day'], ['definitely', 'see', 'ton', 'red', 'flag', 'mom', 'abusive', 'sure', 'also', 'sound', 'super', 'unstable', 'must', 'exhaust', 'deal', 'constant', 'fluctuations\\n\\nyour', 'best', 'bet', 'avoid', 'possible', 'prepare', 'day', 'move', 'go', 'college', 'get', 'distance', 'meantime', 'dont', 'trust', 'anything', 'keep', 'valuables', 'lock', 'possible', 'dont', 'let', 'access', 'money', 'shes', 'already', 'threaten', 'take', 'away', 'things', 'apparently', 'feel', 'justify', 'dont', 'doubt', 'shell', 'make', 'good', 'threats', 'one', 'days'])\n",
      "original document: \n",
      "['I', 'agree', '100%.', 'Truth', 'be', 'told,', 'I', \"wasn't\", 'a', 'fan', 'of', 'Susperia', '(sp.).', 'Scream', 'is', 'like', '#100', 'for', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'one hundred', 'tru', 'told', 'wasnt', 'fan', 'susper', 'sp', 'scream', 'lik', 'one hundred'], ['agree', 'one hundred', 'truth', 'tell', 'wasnt', 'fan', 'susperia', 'sp', 'scream', 'like', 'one hundred'])\n",
      "original document: \n",
      "['Yikes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yik'], ['yikes'])\n",
      "original document: \n",
      "[\"I'd\", 'actually', 'been', 'working', 'under', 'that', 'assumption', 'since', 'the', 'first', 'Vecna', 'fight', 'until', 'this', 'week.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'act', 'work', 'assum', 'sint', 'first', 'vecn', 'fight', 'week'], ['id', 'actually', 'work', 'assumption', 'since', 'first', 'vecna', 'fight', 'week'])\n",
      "original document: \n",
      "['If', 'you', \"haven't\", 'already,', 'see', 'a', 'doctor.', 'Obviously,', 'your', 'hand', 'is', 'critical', 'to', 'your', 'occupation.', \"It's\", 'also', 'much', 'easier', 'to', 'sustain', 'another', 'injury', 'when', 'you', 'have', 'to', 'compensate', 'for', 'an', 'existing', 'one.', 'You', 'may', 'have', 'a', 'fracture.', 'Either', 'way,', 'a', 'doctor', 'should', 'help', 'get', 'you', 'on', 'the', 'fastest', 'path', 'to', 'recovery.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hav', 'already', 'see', 'doct', 'obvy', 'hand', 'crit', 'occup', 'also', 'much', 'easy', 'sustain', 'anoth', 'injury', 'compens', 'ex', 'on', 'may', 'fract', 'eith', 'way', 'doct', 'help', 'get', 'fastest', 'path', 'recovery'], ['havent', 'already', 'see', 'doctor', 'obviously', 'hand', 'critical', 'occupation', 'also', 'much', 'easier', 'sustain', 'another', 'injury', 'compensate', 'exist', 'one', 'may', 'fracture', 'either', 'way', 'doctor', 'help', 'get', 'fastest', 'path', 'recovery'])\n",
      "original document: \n",
      "['He', 'is', 'a', 'shitbag.', 'I', 'despise', 'him', 'worse', 'than', 'Don', 'Lemon.', 'At', 'least', 'Lemon', 'is', 'openly', 'a', 'hater.', 'Cernavich', 'is', 'a', 'ambulance', 'chaser', 'of', 'the', 'quick', 'blog', 'views.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shitb', 'desp', 'wors', 'lemon', 'least', 'lemon', 'op', 'hat', 'cernavich', 'amb', 'chas', 'quick', 'blog', 'view'], ['shitbag', 'despise', 'worse', 'lemon', 'least', 'lemon', 'openly', 'hater', 'cernavich', 'ambulance', 'chaser', 'quick', 'blog', 'view'])\n",
      "original document: \n",
      "[\"I'm\", 'pretty', 'sure', 'there', 'is', 'a', 'physical', 'deluxe', 'version', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'pretty', 'sur', 'phys', 'delux', 'vert', 'wel'], ['im', 'pretty', 'sure', 'physical', 'deluxe', 'version', 'well'])\n",
      "original document: \n",
      "['I', 'grant', 'you', 'official', 'permission', 'to', 'eat', 'Turkey', 'on', 'our', 'Thanksgiving', 'as', 'well', 'as', 'long', 'as', 'you', 'grant', 'us', 'the', 'same', 'priviledge', 'on', 'yours.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['grant', 'off', 'permit', 'eat', 'turkey', 'thanksg', 'wel', 'long', 'grant', 'us', 'priviledg'], ['grant', 'official', 'permission', 'eat', 'turkey', 'thanksgiving', 'well', 'long', 'grant', 'us', 'priviledge'])\n",
      "original document: \n",
      "['this', 'photo', 'is', 'terribly', 'sad', 'and', 'funny', 'but', 'really', 'sad', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['photo', 'terr', 'sad', 'funny', 'real', 'sad'], ['photo', 'terribly', 'sad', 'funny', 'really', 'sad'])\n",
      "original document: \n",
      "['Please', 'note', 'that', 'having', 'a', 'few', 'narc', 'traits', \"doesn't\", 'make', 'you', 'one.', 'The', 'same', 'way', 'everyone', 'has', 'probably', 'had', 'a', 'symptoms', 'of', 'depression', 'now', 'and', 'then.', 'but', 'that', \"doesn't\", 'make', 'you', 'depressed.', '', '\\nRegardless,', 'your', 'not', 'a', 'bad', 'person', 'for', 'expressing', 'yourself', 'in', 'a', 'constructive', 'and', 'appropriate', 'manner.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'not', 'narc', 'trait', 'doesnt', 'mak', 'on', 'way', 'everyon', 'prob', 'symptom', 'depress', 'doesnt', 'mak', 'depress', '\\nregardless', 'bad', 'person', 'express', 'construct', 'appropry', 'man'], ['please', 'note', 'narc', 'traits', 'doesnt', 'make', 'one', 'way', 'everyone', 'probably', 'symptoms', 'depression', 'doesnt', 'make', 'depress', '\\nregardless', 'bad', 'person', 'express', 'constructive', 'appropriate', 'manner'])\n",
      "original document: \n",
      "['Does', 'anyone', 'know', 'where', 'I', 'could', 'possibly', 'watch', 'this', 'movie?', 'I', 'know', 'it', 'is', 'permanently', 'in', 'the', 'vault', 'for', 'good', 'reason,', 'but', 'ever', 'since', 'I', 'was', 'kid', 'I', 'wanted', 'to', 'see', 'this', 'movie.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'know', 'could', 'poss', 'watch', 'movy', 'know', 'perm', 'vault', 'good', 'reason', 'ev', 'sint', 'kid', 'want', 'see', 'movy'], ['anyone', 'know', 'could', 'possibly', 'watch', 'movie', 'know', 'permanently', 'vault', 'good', 'reason', 'ever', 'since', 'kid', 'want', 'see', 'movie'])\n",
      "original document: \n",
      "['Can', 'put', 'the', 'secondary', 'in', 'the', 'holster', 'but', 'the', 'primary', 'dissapear', 'when', 'using', 'the', 'secondary.\\nAbsolutely.\\nUnplayable.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['put', 'second', 'holst', 'prim', 'dissapear', 'us', 'secondary\\nabsolutely\\nunplayable'], ['put', 'secondary', 'holster', 'primary', 'dissapear', 'use', 'secondary\\nabsolutely\\nunplayable'])\n",
      "original document: \n",
      "['I', 'mean', \"it's\", 'hard', 'to', 'know', 'since', 'I', \"don't\", 'know', 'who', 'you', 'are', 'or', 'what', 'your', 'preferences', 'are,', 'but', 'yes', 'if', 'you', 'want', 'a', 'serious,', 'dedicated,', 'anarchist', 'communist', 'group', 'with', 'intersectional', 'politics', 'and', 'lots', 'of', 'experience', 'in', 'organising', 'then', 'sign', 'up', 'to', 'their', 'emails', 'and', 'see', 'what', 'you', 'think', 'from', 'there.', 'Also', \"there's\", 'https://www.facebook.com/WorkersSolidarityMovement', 'if', \"you're\", 'on', 'Facebook.', 'And', 'a', 'Twitter,', 'etc.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['mean', 'hard', 'know', 'sint', 'dont', 'know', 'pref', 'ye', 'want', 'sery', 'ded', 'anarch', 'commun', 'group', 'intersect', 'polit', 'lot', 'expery', 'org', 'sign', 'email', 'see', 'think', 'also', 'ther', 'httpswwwfacebookcomworkerssolidaritymovement', 'yo', 'facebook', 'twit', 'etc'], ['mean', 'hard', 'know', 'since', 'dont', 'know', 'preferences', 'yes', 'want', 'serious', 'dedicate', 'anarchist', 'communist', 'group', 'intersectional', 'politics', 'lot', 'experience', 'organise', 'sign', 'email', 'see', 'think', 'also', 'theres', 'httpswwwfacebookcomworkerssolidaritymovement', 'youre', 'facebook', 'twitter', 'etc'])\n",
      "original document: \n",
      "['I', 'did', 'fart,', 'for', 'the', 'record']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fart', 'record'], ['fart', 'record'])\n",
      "original document: \n",
      "['Politics', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['polit'], ['politics'])\n",
      "original document: \n",
      "['Evolution', 'convinced', 'you', 'that', 'evolution', 'is', 'false?', 'Why', 'not', 'just', 'admit', 'that', 'you', \"don't\", 'know', 'why', 'evolution', 'is', 'false,', 'but', 'you', 'just', 'wish', 'it', 'were?\\n\\nThose', 'threads', 'also', 'got', 'some', 'highlights', 'from', 'here.', 'But', 'there', 'are', 'still', 'plenty', 'of', 'snarky', 'comments', 'to', 'smarter', 'posts', 'on', 'those', 'threads,', 'from', 'people', 'like', 'stcordova.', 'And', 'I', 'still', 'see', 'quite', 'a', 'few', 'posts', 'that', \"aren't\", 'very', 'high,', 'but', 'are', \"correct.\\n\\nIt's\", 'not', 'demonstrably', 'false.', \"It's\", 'you', 'selecting', 'only', 'favorable', 'facts', 'to', 'support', 'your', 'case,', 'without', 'looking', 'at', 'the', 'whole', 'picture.\\n\\nHow', 'many', 'non-creationists', 'are', 'allowed', 'to', 'post', 'to', '/r/creation?', 'I', 'rest', 'my', 'case.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['evolv', 'convint', 'evolv', 'fals', 'admit', 'dont', 'know', 'evolv', 'fals', 'wish', 'were\\n\\nthos', 'threads', 'also', 'got', 'highlight', 'stil', 'plenty', 'snarky', 'com', 'smart', 'post', 'threads', 'peopl', 'lik', 'stcordova', 'stil', 'see', 'quit', 'post', 'ar', 'high', 'correct\\n\\nits', 'demonst', 'fals', 'select', 'fav', 'fact', 'support', 'cas', 'without', 'look', 'whol', 'picture\\n\\nhow', 'many', 'noncr', 'allow', 'post', 'rcreation', 'rest', 'cas'], ['evolution', 'convince', 'evolution', 'false', 'admit', 'dont', 'know', 'evolution', 'false', 'wish', 'were\\n\\nthose', 'thread', 'also', 'get', 'highlight', 'still', 'plenty', 'snarky', 'comment', 'smarter', 'post', 'thread', 'people', 'like', 'stcordova', 'still', 'see', 'quite', 'post', 'arent', 'high', 'correct\\n\\nits', 'demonstrably', 'false', 'select', 'favorable', 'facts', 'support', 'case', 'without', 'look', 'whole', 'picture\\n\\nhow', 'many', 'noncreationists', 'allow', 'post', 'rcreation', 'rest', 'case'])\n",
      "original document: \n",
      "['Somebody', 'with', 'better', 'skills', 'than', 'me...', 'Please', 'r/reallifedoodles', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['somebody', 'bet', 'skil', 'pleas', 'rreallifedoodl'], ['somebody', 'better', 'skills', 'please', 'rreallifedoodles'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['all', 'hail', 'gold']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hail', 'gold'], ['hail', 'gold'])\n",
      "original document: \n",
      "['i', 'wouldnt', 'go', 'for', 'a', 'high', 'pop', 'clan,', 'find', 'a', 'small', 'clan', 'and', 'build', 'it', 'up.', 'You', 'will', 'make', 'better', 'clanmates', 'and', 'you', 'will', 'find', 'there', 'are', 'always', 'people', 'to', 'do', 'things.', 'I', 'was', 'in', 'a', '55', 'player', 'clan', 'and', 'it', 'was', 'dead,', 'joined', 'a', '26', 'player', 'clan', 'and', 'it', 'was', 'full', 'of', 'assholes', 'that', 'wanted', 'to', 'be', 'carried', 'through', 'everything,', 'joined', 'a', '5', 'player', 'clan', 'and', 'i', 'have', 'been', 'playing', 'every', 'night', 'and', 'they', 'are', 'always', 'up', 'for', 'raids,', 'nightfalls', 'ect.', 'Small', 'clans', 'are', 'the', 'way', 'to', 'go', 'in', 'my', 'book.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'go', 'high', 'pop', 'clan', 'find', 'smal', 'clan', 'build', 'mak', 'bet', 'clanm', 'find', 'alway', 'peopl', 'thing', 'fifty-five', 'play', 'clan', 'dead', 'join', 'twenty-six', 'play', 'clan', 'ful', 'asshol', 'want', 'carry', 'everyth', 'join', 'fiv', 'play', 'clan', 'play', 'every', 'night', 'alway', 'raid', 'nightfal', 'ect', 'smal', 'clan', 'way', 'go', 'book'], ['wouldnt', 'go', 'high', 'pop', 'clan', 'find', 'small', 'clan', 'build', 'make', 'better', 'clanmates', 'find', 'always', 'people', 'things', 'fifty-five', 'player', 'clan', 'dead', 'join', 'twenty-six', 'player', 'clan', 'full', 'assholes', 'want', 'carry', 'everything', 'join', 'five', 'player', 'clan', 'play', 'every', 'night', 'always', 'raid', 'nightfalls', 'ect', 'small', 'clans', 'way', 'go', 'book'])\n",
      "original document: \n",
      "['Sack', 'incoming.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sack', 'incom'], ['sack', 'incoming'])\n",
      "original document: \n",
      "['Bless', 'her....', 'I', 'hope', \"she's\", 'the', 'kind', 'of', 'person', 'who', 'just', 'never', 'takes', 'her', 'laptop', 'out', 'in', 'public.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bless', 'hop', 'she', 'kind', 'person', 'nev', 'tak', 'laptop', 'publ'], ['bless', 'hope', 'shes', 'kind', 'person', 'never', 'take', 'laptop', 'public'])\n",
      "original document: \n",
      "[\"That's\", 'what', 'freaked', 'me', 'out', 'so', 'much', 'when', 'upper', 'management', 'thought', 'it', 'would', 'be', 'okay', 'to', 'just', 'let', 'it', 'die.', 'I', 'can', 'imagine', 'that', 'if', 'this', 'resident', 'was', 'able', 'to', 'somehow', 'get', 'better', 'and', 'came', 'back,', 'she', 'would', 'be', 'pretty', 'pissed', 'to', 'know', 'we', 'allowed', 'her', 'pet', 'to', 'die.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'freak', 'much', 'up', 'man', 'thought', 'would', 'okay', 'let', 'die', 'imagin', 'resid', 'abl', 'somehow', 'get', 'bet', 'cam', 'back', 'would', 'pretty', 'piss', 'know', 'allow', 'pet', 'die'], ['thats', 'freak', 'much', 'upper', 'management', 'think', 'would', 'okay', 'let', 'die', 'imagine', 'resident', 'able', 'somehow', 'get', 'better', 'come', 'back', 'would', 'pretty', 'piss', 'know', 'allow', 'pet', 'die'])\n",
      "original document: \n",
      "['He', 'pop', 'him', 'with', 'a', '30-30????', 'Awesome....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pop', 'three thousand and thirty', 'awesom'], ['pop', 'three thousand and thirty', 'awesome'])\n",
      "original document: \n",
      "['I', 'make', 'soap', 'and', 'lotion', 'and', \"I've\", 'thought', 'how', 'cool', \"it'd\", 'be', 'to', 'do', 'sonething', 'similar', 'to', 'this', 'with', 'lotion.', 'Almost', 'like', 'a', 'little', 'milkshake', 'kiosk.', 'You', 'could', 'pick', 'different', 'oils', 'and', 'butters', 'and', 'fragrances.', 'And', 'mix', 'and', 'package', 'right', 'there.\\n\\nAnyone', 'interested', 'in', 'DIY', 'make', 'up', 'and', 'nail', 'polish', 'TKB', 'Trading', 'is', 'highly', 'recommended.', '', 'They', 'have', 'kits', 'too.', '', 'Great', 'company,', 'reasonable', 'prices.', '\\n\\nhttps://tkbtrading.com/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'soap', 'lot', 'iv', 'thought', 'cool', 'itd', 'soneth', 'simil', 'lot', 'almost', 'lik', 'littl', 'milkshak', 'kiosk', 'could', 'pick', 'diff', 'oil', 'but', 'fragr', 'mix', 'pack', 'right', 'there\\n\\nanyon', 'interest', 'diy', 'mak', 'nail', 'pol', 'tkb', 'trad', 'high', 'recommend', 'kit', 'gre', 'company', 'reason', 'pric', '\\n\\nhttpstkbtradingcom'], ['make', 'soap', 'lotion', 'ive', 'think', 'cool', 'itd', 'sonething', 'similar', 'lotion', 'almost', 'like', 'little', 'milkshake', 'kiosk', 'could', 'pick', 'different', 'oil', 'butter', 'fragrances', 'mix', 'package', 'right', 'there\\n\\nanyone', 'interest', 'diy', 'make', 'nail', 'polish', 'tkb', 'trade', 'highly', 'recommend', 'kit', 'great', 'company', 'reasonable', 'price', '\\n\\nhttpstkbtradingcom'])\n",
      "original document: \n",
      "['Yeah.', 'So', 'far', 'both', 'shorts', 'have', 'premiered', 'at', 'a', 'convention', 'and', 'released', 'on', 'a', 'monday.', \"It's\", 'a', 'pretty', 'obvious', 'schedule', \"they've\", 'got', 'going.', \"There's\", 'not', 'much', 'to', 'speculate', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'far', 'short', 'premy', 'conv', 'releas', 'monday', 'pretty', 'obvy', 'schedule', 'theyv', 'got', 'going', 'ther', 'much', 'spec'], ['yeah', 'far', 'short', 'premier', 'convention', 'release', 'monday', 'pretty', 'obvious', 'schedule', 'theyve', 'get', 'go', 'theres', 'much', 'speculate'])\n",
      "original document: \n",
      "['What', 'do', 'you', 'get', 'when', 'you', 'cross', 'a', 'chicken', 'with', 'the', 'Terminator?', '', \"I'll\", 'be', 'bock,', 'bock,', 'bock.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'cross', 'chick', 'termin', 'il', 'bock', 'bock', 'bock'], ['get', 'cross', 'chicken', 'terminator', 'ill', 'bock', 'bock', 'bock'])\n",
      "original document: \n",
      "['he', 'did', 'and', 'tried', 'to', 'stream', 'with', 'it', 'and', \"couldn't\", 'get', 'it', 'to', 'work']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tri', 'stream', 'couldnt', 'get', 'work'], ['try', 'stream', 'couldnt', 'get', 'work'])\n",
      "original document: \n",
      "['Because', 'it', 'IS', 'hard.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hard'], ['hard'])\n",
      "original document: \n",
      "[\"It's\", 'a', 'possibility,', 'guess', \"we'll\", 'find', 'out']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poss', 'guess', 'wel', 'find'], ['possibility', 'guess', 'well', 'find'])\n",
      "original document: \n",
      "['Yeah....', '', 'no.', '', 'Just', 'like', 'everything', 'else,', 'there', 'are', 'some', 'people', 'who', 'like', 'porn', 'and', 'there', 'are', 'some', 'people', 'that', 'don’t.', '', 'Do', 'you', 'really', 'think', '*porn*', 'of', 'all', 'things', 'is', 'what', 'Dan', 'would', 'lie', 'to', 'us', 'about', 'liking?', '', 'If', 'he', 'liked', 'porn,', 'he', 'wouldn’t', 'be', 'uncomfortable', 'saying', 'it', 'on', 'YouTube.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'lik', 'everyth', 'els', 'peopl', 'lik', 'porn', 'peopl', 'dont', 'real', 'think', 'porn', 'thing', 'dan', 'would', 'lie', 'us', 'lik', 'lik', 'porn', 'wouldnt', 'uncomfort', 'say', 'youtub'], ['yeah', 'like', 'everything', 'else', 'people', 'like', 'porn', 'people', 'dont', 'really', 'think', 'porn', 'things', 'dan', 'would', 'lie', 'us', 'like', 'like', 'porn', 'wouldnt', 'uncomfortable', 'say', 'youtube'])\n",
      "original document: \n",
      "['they', 'are']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['I', 'actually', 'had', 'some', 'parents', 'refuse', 'the', 'IM', 'vitamin', 'K', 'but', 'later', 'agree', 'to', 'let', 'the', 'baby', 'have', 'it', 'orally.', \"It's\", 'the', 'same', 'medication', 'from', 'the', 'same', 'vial', 'and', 'they', 'were', 'okay', 'with', 'the', 'baby', 'receiving', 'via', 'the', 'less', 'effective', 'and,', 'arguably,', 'more', 'miserable', 'route.', 'I', 'still', \"don't\", 'understand', 'their', 'objection.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'par', 'refus', 'im', 'vitamin', 'k', 'lat', 'agr', 'let', 'baby', 'or', 'med', 'vial', 'okay', 'baby', 'receiv', 'via', 'less', 'effect', 'argu', 'mis', 'rout', 'stil', 'dont', 'understand', 'object'], ['actually', 'parent', 'refuse', 'im', 'vitamin', 'k', 'later', 'agree', 'let', 'baby', 'orally', 'medication', 'vial', 'okay', 'baby', 'receive', 'via', 'less', 'effective', 'arguably', 'miserable', 'route', 'still', 'dont', 'understand', 'objection'])\n",
      "original document: \n",
      "['The', 'Simpsons,', 'just', 'let', 'it', 'be', 'over', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['simpson', 'let', 'pleas'], ['simpsons', 'let', 'please'])\n",
      "original document: \n",
      "['Just', 'looking', 'for', 'advice', 'to', 'know', 'what', 'to', 'do.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'adv', 'know'], ['look', 'advice', 'know'])\n",
      "original document: \n",
      "['It', 'was', 'an', 'okay', 'episode,', 'but', 'the', 'reasoning', 'behind', 'it', 'was', 'strong.', '', 'Jon', 'knew', 'there', 'was', 'no', 'way', 'Cersei', 'would', 'ever', 'agree', 'to', 'any', 'alliance', 'unless', 'she', 'saw', 'a', 'wight', 'for', 'herself.', '', 'It', 'had', 'to', 'be', 'done.\\n\\nAs', 'for', 'the', 'episode', 'itself,', 'I', 'think', 'it', 'did', 'suck', 'due', 'to', 'the', 'heavy', 'use', 'of', '\"redshirts\".', '', 'Most', 'of', 'the', 'deaths', 'were', 'from', 'random', 'people', 'that', 'we', \"didn't\", 'even', 'know.', '', 'They', 'were', 'just', 'death', 'fodder.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'episod', 'reason', 'behind', 'strong', 'jon', 'knew', 'way', 'cerse', 'would', 'ev', 'agr', 'al', 'unless', 'saw', 'wight', 'done\\n\\nas', 'episod', 'think', 'suck', 'due', 'heavy', 'us', 'redshirt', 'death', 'random', 'peopl', 'didnt', 'ev', 'know', 'dea', 'fod'], ['okay', 'episode', 'reason', 'behind', 'strong', 'jon', 'know', 'way', 'cersei', 'would', 'ever', 'agree', 'alliance', 'unless', 'saw', 'wight', 'done\\n\\nas', 'episode', 'think', 'suck', 'due', 'heavy', 'use', 'redshirts', 'deaths', 'random', 'people', 'didnt', 'even', 'know', 'death', 'fodder'])\n",
      "original document: \n",
      "['I', 'saw', 'something', 'about', 'that.', 'I', 'think', 'he', 'article', 'said', 'Apple', 'knows', 'the', 'issue', 'and', 'is', 'pushing', 'out', 'an', 'update.', 'My', 'new', 'issue', 'is', 'battery', 'life', 'with', 'IOS11', 'but', 'thats', 'another', 'issue', 'altogether.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'someth', 'think', 'artic', 'said', 'appl', 'know', 'issu', 'push', 'upd', 'new', 'issu', 'battery', 'lif', 'ios11', 'that', 'anoth', 'issu', 'altogether\\n'], ['saw', 'something', 'think', 'article', 'say', 'apple', 'know', 'issue', 'push', 'update', 'new', 'issue', 'battery', 'life', 'ios11', 'thats', 'another', 'issue', 'altogether\\n'])\n",
      "original document: \n",
      "['We', \"didn't\", 'have', 'a', 'QB', 'coach', 'until', 'January', 'of', 'this', 'year.', 'No,', \"I'm\", 'not', 'joking.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'qb', 'coach', 'janu', 'year', 'im', 'jok'], ['didnt', 'qb', 'coach', 'january', 'year', 'im', 'joke'])\n",
      "original document: \n",
      "['Technically', 'a', 'sequel', 'considering', 'Star', 'Wars', 'took', 'place', 'a', 'long', 'time', 'ago.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['techn', 'sequel', 'consid', 'star', 'war', 'took', 'plac', 'long', 'tim', 'ago'], ['technically', 'sequel', 'consider', 'star', 'war', 'take', 'place', 'long', 'time', 'ago'])\n",
      "original document: \n",
      "['Both', 'sound', 'great', 'but', 'the', '92', 'has', 'insane', 'headroom.', 'It', 'can', 'handle', 'all', 'types', 'of', 'extreme', 'redlining', 'without', 'sounding', 'bad.\\n\\nI', 'think', 'the', '92', 'sounds', 'better', 'overall', '(I', 'never', 'redline', 'anything', 'anyway)', 'but', 'I', 'might', '', 'be', 'biased,', 'since', '', 'I', 'absolutely', 'adore', '', 'everything', 'about', 'the', '92.', 'I', 'would', 'be', 'interested', 'to', 'hear', 'an', 'official', 'statement', 'about', 'if', 'they', 'sound', 'identical', 'at', 'some', 'low', 'level', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'gre', 'ninety-two', 'ins', 'headroom', 'handl', 'typ', 'extrem', 'redlin', 'without', 'sound', 'bad\\n\\ni', 'think', 'ninety-two', 'sound', 'bet', 'overal', 'nev', 'redlin', 'anyth', 'anyway', 'might', 'bias', 'sint', 'absolv', 'ad', 'everyth', 'ninety-two', 'would', 'interest', 'hear', 'off', 'stat', 'sound', 'id', 'low', 'level', 'though'], ['sound', 'great', 'ninety-two', 'insane', 'headroom', 'handle', 'type', 'extreme', 'redline', 'without', 'sound', 'bad\\n\\ni', 'think', 'ninety-two', 'sound', 'better', 'overall', 'never', 'redline', 'anything', 'anyway', 'might', 'bias', 'since', 'absolutely', 'adore', 'everything', 'ninety-two', 'would', 'interest', 'hear', 'official', 'statement', 'sound', 'identical', 'low', 'level', 'though'])\n",
      "original document: \n",
      "['Why', 'is', 'it', 'bad', 'that', 'a', 'tv', 'show', 'is', 'make', 'you', 'think', 'for', 'a', 'second', 'and', 'ask', 'yourself', 'a', 'question?', \"It's\", 'not', 'mindless', 'tv.', 'That', 'means', \"it's\", 'at', 'least', 'somewhat', 'decent,', 'and', 'better', 'than', 'most', 'of', 'the', 'shows', 'out', 'there.', \"I'm\", 'sick', 'of', 'explaining', 'this', 'to', 'people.', \"It's\", 'a', 'Duplass', 'Brothers', 'production.', 'They', 'have', 'their', 'own', 'style,', \"it's\", 'very', 'indie.', \"Don't\", 'watch', 'it', 'if', 'you', \"don't\", 'like', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bad', 'tv', 'show', 'mak', 'think', 'second', 'ask', 'quest', 'mindless', 'tv', 'mean', 'least', 'somewh', 'dec', 'bet', 'show', 'im', 'sick', 'explain', 'peopl', 'duplass', 'broth', 'produc', 'styl', 'indy', 'dont', 'watch', 'dont', 'lik'], ['bad', 'tv', 'show', 'make', 'think', 'second', 'ask', 'question', 'mindless', 'tv', 'mean', 'least', 'somewhat', 'decent', 'better', 'show', 'im', 'sick', 'explain', 'people', 'duplass', 'brothers', 'production', 'style', 'indie', 'dont', 'watch', 'dont', 'like'])\n",
      "original document: \n",
      "['Gordon', 'just', 'successfully', 'beat', 'the', 'throw', 'but', 'overslid', 'the', 'bag', 'to', 'miss', '#60.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gordon', 'success', 'beat', 'throw', 'overslid', 'bag', 'miss', 'sixty'], ['gordon', 'successfully', 'beat', 'throw', 'overslid', 'bag', 'miss', 'sixty'])\n",
      "original document: \n",
      "['\"I\\'m', 'trying', 'to', 'eat', 'my', 'breakfast', 'and', \"you're\", 'just', 'talking', '*at*', 'me', 'about', 'a', 'ball.', 'Like,', 'do', 'you', 'have', 'any', 'idea', 'how', 'dull,', 'and', 'irksome', 'you', 'are?', 'What', 'is', 'the', 'point', 'in', 'spending', 'your', 'life', 'being', 'such', 'a', 'pissant?\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'try', 'eat', 'breakfast', 'yo', 'talk', 'bal', 'lik', 'ide', 'dul', 'irksom', 'point', 'spend', 'lif', 'piss'], ['im', 'try', 'eat', 'breakfast', 'youre', 'talk', 'ball', 'like', 'idea', 'dull', 'irksome', 'point', 'spend', 'life', 'pissant'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Cmon', 'man.', 'Don’t', 'you', 'know', 'that', 'girls', 'are', 'vulnerable', 'and', 'guys', 'are', 'predators', 'when', 'drunk?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cmon', 'man', 'dont', 'know', 'girl', 'vuln', 'guy', 'pred', 'drunk'], ['cmon', 'man', 'dont', 'know', 'girls', 'vulnerable', 'guy', 'predators', 'drink'])\n",
      "original document: \n",
      "['Is', 'Gears', 'of', 'War', '2', 'the', 'original', 'case?', 'If', 'so,', 'can', 'you', 'provide', 'pictures?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gear', 'war', 'two', 'origin', 'cas', 'provid', 'pict'], ['gear', 'war', 'two', 'original', 'case', 'provide', 'picture'])\n",
      "original document: \n",
      "['Profile', 'elite', 'is', 'the', 'loudest', 'hub', 'on', 'the', 'market.', '', 'Profile', 'hubs', 'have', 'almost', 'always', 'been', 'the', 'loudest,', 'back', 'in', 'the', 'day', 'when', 'my', 'friends', 'and', 'I', 'would', 'road', 'trip', 'to', 'skateparks', 'we', 'would', 'hear', 'people', 'air', 'out', 'and', 'be', 'like', '\"that', 'dude', 'has', 'a', 'profile', 'hub\".']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['profil', 'elit', 'loudest', 'hub', 'market', 'profil', 'hub', 'almost', 'alway', 'loudest', 'back', 'day', 'friend', 'would', 'road', 'trip', 'skatepark', 'would', 'hear', 'peopl', 'air', 'lik', 'dud', 'profil', 'hub'], ['profile', 'elite', 'loudest', 'hub', 'market', 'profile', 'hubs', 'almost', 'always', 'loudest', 'back', 'day', 'friends', 'would', 'road', 'trip', 'skateparks', 'would', 'hear', 'people', 'air', 'like', 'dude', 'profile', 'hub'])\n",
      "original document: \n",
      "['143414191|', '&gt;', 'None', 'Anonymous', '(ID:', 'K4S0eFk8)\\n\\nslide', 'thread\\n\\nsage\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and ninety-on', 'gt', 'non', 'anonym', 'id', 'k4s0efk8\\n\\nslide', 'thread\\n\\nsage\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and ninety-one', 'gt', 'none', 'anonymous', 'id', 'k4s0efk8\\n\\nslide', 'thread\\n\\nsage\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Tracklist:\\n\\n01.', 'Jason', 'Ross', 'feat.', 'Lauren', 'Ray', '–', 'I', 'Will', 'Be', 'There', '[ANJUNABEATS]', 'w/', 'Seven', 'Lions', '&amp;', 'Xilent', '–', 'The', 'Fall', '[OWS\\n02.', 'Jason', 'Ross', '–', 'Cairo', '[ANJUNABEATS]', 'w/', 'Seven', 'Lions', 'feat.', 'KARRA', '–', 'Silent', 'Skies', '[SEEKING', 'BLUE]\\n03.', 'Seven', 'Lions', '–', 'Steps', 'Of', 'Deep', 'Slumber', '[SEEKING', 'BLUE]', 'w/', 'Jason', 'Ross', 'feat.', 'Lauren', 'Ray', '–', 'Me', 'Tonight', '[ANJ\\n04.', 'Wrechiski', '&amp;', 'Jason', 'Ross', '–', 'Atlas', '[ANJUNABEATS]\\n05.', 'Seven', 'Lions', '–', 'Cusp', '[WHO’S', 'AFRAID', 'OF', '138]\\n06.', 'Seven', 'Lions', 'feat.', 'Skyler', 'Stonestreet', '–', 'Freesol', '[SEEKING', 'BLUE]\\n07.', 'Jason', 'Ross', '–', 'Valor', '(Seven', 'Lions', 'Edit)', '[ANJUNABEATS]\\n08.', 'Jason', 'Ross', '–', 'Mirror', 'Image', '[ANJUNABEATS]\\n09.', 'Seven', 'Lions', '&amp;', 'Jason', 'Ross', 'feat.', 'Jonathan', 'Mendelsohn', '–', 'Ocean\\n10.', 'Excision', '–', 'The', 'Paradox', '(Seven', 'Lions', '&amp;', 'Dimibo', 'Remix)', '[ROTTUN]\\n11.', 'Seven', 'Lions', '&amp;', 'Jason', 'Ross', '–', 'ID\\n12.', 'Seven', 'Lions', 'feat.', 'Ellie', 'Goulding', '–', 'Don’t', 'Leave', '[CASABLANCA]\\n13.', 'Jason', 'Ross', '–', 'Coaster', '[ANJUNABEATS]', 'w/', 'Ilan', 'Bluestone', '&amp;', 'Jason', 'Ross', '–', 'Amun', '[ANJUNABEATS]', 'w/', 'Seven', 'Lio\\n14.', 'Seven', 'Lions', 'feat.', 'Vök', '–', 'Creation', '[CASABLANCA]\\n15.', 'Seven', 'Lions', 'feat.', 'Lights', '–', 'Falling', 'Away', '(Festival', 'Mix)', '[CASABLANCA]\\n16.', 'Seven', 'Lions', '&amp;', 'Jason', 'Ross', 'feat.', 'Paul', 'Meany', '–', 'Higher', 'Love', '[ANJUNABEATS]\\n17.', 'Dirty', 'South', 'feat.', 'ANIMA!', '–', 'I', 'Swear', '(Jason', 'Ross', 'Remix)', '[ANJUNABEATS]', 'w/', 'Seven', 'Lions', '&amp;', 'Echos', '–', 'Cold', 'S\\n18.', 'Seven', 'Lions', 'feat.', 'Kerli', '–', 'Worlds', 'Apart', '(ABGT250', 'Outro', 'Edit)', '[CASABLANCA]\\ufeff']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tracklist\\n\\n01', 'jason', 'ross', 'feat', 'laur', 'ray', 'anjunab', 'w', 'sev', 'lion', 'amp', 'xil', 'fal', 'ows\\n02', 'jason', 'ross', 'cairo', 'anjunab', 'w', 'sev', 'lion', 'feat', 'karr', 'sil', 'ski', 'seek', 'blue\\n03', 'sev', 'lion', 'step', 'deep', 'slumb', 'seek', 'blu', 'w', 'jason', 'ross', 'feat', 'laur', 'ray', 'tonight', 'anj\\n04', 'wrechisk', 'amp', 'jason', 'ross', 'atla', 'anjunabeats\\n05', 'sev', 'lion', 'cusp', 'who', 'afraid', '138\\n06', 'sev', 'lion', 'feat', 'skyl', 'stonestreet', 'freesol', 'seek', 'blue\\n07', 'jason', 'ross', 'val', 'sev', 'lion', 'edit', 'anjunabeats\\n08', 'jason', 'ross', 'mir', 'im', 'anjunabeats\\n09', 'sev', 'lion', 'amp', 'jason', 'ross', 'feat', 'jonath', 'mendelsohn', 'ocean\\n10', 'excid', 'paradox', 'sev', 'lion', 'amp', 'dimibo', 'remix', 'rottun\\n11', 'sev', 'lion', 'amp', 'jason', 'ross', 'id\\n12', 'sev', 'lion', 'feat', 'el', 'gould', 'dont', 'leav', 'casablanca\\n13', 'jason', 'ross', 'coast', 'anjunab', 'w', 'il', 'blueston', 'amp', 'jason', 'ross', 'amun', 'anjunab', 'w', 'sev', 'lio\\n14', 'sev', 'lion', 'feat', 'vok', 'cre', 'casablanca\\n15', 'sev', 'lion', 'feat', 'light', 'fal', 'away', 'fest', 'mix', 'casablanca\\n16', 'sev', 'lion', 'amp', 'jason', 'ross', 'feat', 'paul', 'meany', 'high', 'lov', 'anjunabeats\\n17', 'dirty', 'sou', 'feat', 'anim', 'swear', 'jason', 'ross', 'remix', 'anjunab', 'w', 'sev', 'lion', 'amp', 'echo', 'cold', 's\\n18', 'sev', 'lion', 'feat', 'kerl', 'world', 'apart', 'abgt250', 'outro', 'edit', 'casablanc'], ['tracklist\\n\\n01', 'jason', 'ross', 'feat', 'lauren', 'ray', 'anjunabeats', 'w', 'seven', 'lions', 'amp', 'xilent', 'fall', 'ows\\n02', 'jason', 'ross', 'cairo', 'anjunabeats', 'w', 'seven', 'lions', 'feat', 'karra', 'silent', 'sky', 'seek', 'blue\\n03', 'seven', 'lions', 'step', 'deep', 'slumber', 'seek', 'blue', 'w', 'jason', 'ross', 'feat', 'lauren', 'ray', 'tonight', 'anj\\n04', 'wrechiski', 'amp', 'jason', 'ross', 'atlas', 'anjunabeats\\n05', 'seven', 'lions', 'cusp', 'whos', 'afraid', '138\\n06', 'seven', 'lions', 'feat', 'skyler', 'stonestreet', 'freesol', 'seek', 'blue\\n07', 'jason', 'ross', 'valor', 'seven', 'lions', 'edit', 'anjunabeats\\n08', 'jason', 'ross', 'mirror', 'image', 'anjunabeats\\n09', 'seven', 'lions', 'amp', 'jason', 'ross', 'feat', 'jonathan', 'mendelsohn', 'ocean\\n10', 'excision', 'paradox', 'seven', 'lions', 'amp', 'dimibo', 'remix', 'rottun\\n11', 'seven', 'lions', 'amp', 'jason', 'ross', 'id\\n12', 'seven', 'lions', 'feat', 'ellie', 'goulding', 'dont', 'leave', 'casablanca\\n13', 'jason', 'ross', 'coaster', 'anjunabeats', 'w', 'ilan', 'bluestone', 'amp', 'jason', 'ross', 'amun', 'anjunabeats', 'w', 'seven', 'lio\\n14', 'seven', 'lions', 'feat', 'vok', 'creation', 'casablanca\\n15', 'seven', 'lions', 'feat', 'light', 'fall', 'away', 'festival', 'mix', 'casablanca\\n16', 'seven', 'lions', 'amp', 'jason', 'ross', 'feat', 'paul', 'meany', 'higher', 'love', 'anjunabeats\\n17', 'dirty', 'south', 'feat', 'anima', 'swear', 'jason', 'ross', 'remix', 'anjunabeats', 'w', 'seven', 'lions', 'amp', 'echo', 'cold', 's\\n18', 'seven', 'lions', 'feat', 'kerli', 'worlds', 'apart', 'abgt250', 'outro', 'edit', 'casablanca'])\n",
      "original document: \n",
      "['Hey', 'pm', 'me', 'girl', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'pm', 'girl'], ['hey', 'pm', 'girl'])\n",
      "original document: \n",
      "['\"', 'forcible', 'suppression', 'of', 'opposition\"', '', 'Check']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['forc', 'suppress', 'opposit', 'check'], ['forcible', 'suppression', 'opposition', 'check'])\n",
      "original document: \n",
      "['Just', 'a', 'quick', 'check', 'on', 'Vivid', 'Seats', 'which', 'is', 'the', 'ticket', 'page', 'espn', 'uses.', 'Those', 'tickets', 'are', 'still', 'way', 'too', 'expensive.', 'Seats', 'around', 'that', 'price', 'are', 'going', 'for', '$30', 'per.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quick', 'check', 'vivid', 'seat', 'ticket', 'pag', 'espn', 'us', 'ticket', 'stil', 'way', 'expend', 'seat', 'around', 'pric', 'going', 'thirty', 'per'], ['quick', 'check', 'vivid', 'seat', 'ticket', 'page', 'espn', 'use', 'ticket', 'still', 'way', 'expensive', 'seat', 'around', 'price', 'go', 'thirty', 'per'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"...that's\", 'why', 'they', 'move', 'their', 'productions', 'to', 'China', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'mov', 'produc', 'chin'], ['thats', 'move', 'productions', 'china'])\n",
      "original document: \n",
      "['At', 'this', 'point', 'it’s', 'when', 'not', 'if']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['point'], ['point'])\n",
      "original document: \n",
      "[\"That's\", 'what', 'the', 'balls', 'are', 'for', 'dude.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'bal', 'dud'], ['thats', 'ball', 'dude'])\n",
      "original document: \n",
      "['Yes,', \"I'm\", 'having', 'it', 'too.', 'thank', 'you', 'for', 'letting', 'me', 'know', \"I'm\", 'not', 'alone']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'im', 'thank', 'let', 'know', 'im', 'alon'], ['yes', 'im', 'thank', 'let', 'know', 'im', 'alone'])\n",
      "original document: \n",
      "['Who', 'the', 'fuck', 'is', 'going', 'to', 'watch', \"Hangin'\", 'with', 'Mr.Cooper?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'going', 'watch', 'hangin', 'mrcooper'], ['fuck', 'go', 'watch', 'hangin', 'mrcooper'])\n",
      "original document: \n",
      "['Houston', 'Chronicle', 'articles', 'are', 'frequently', 'behind', 'a', 'metered', 'paywall.', 'This', 'link', 'may', 'let', 'you', 'view', 'the', 'article', 'if', 'you', 'have', 'reached', 'your', 'limit,', 'though', 'you', 'may', 'have', 'to', 'wait', 'a', 'few', 'hours', 'for', 'it', 'to', 'show', 'up', 'in', 'the', 'cache:\\n\\n*', '[Google', 'Cache](https://www.google.com/#q=site:http://www.houstonchronicle.com/news/houston-texas/houston/article/Harvey-laid-bare-lack-of-resources-training-at-12243556.php?cmpid=reddit-premium)\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/houston)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['houston', 'chronicle', 'artic', 'frequ', 'behind', 'met', 'paywal', 'link', 'may', 'let', 'view', 'artic', 'reach', 'limit', 'though', 'may', 'wait', 'hour', 'show', 'cache\\n\\n', 'googl', 'cachehttpswwwgooglecomqsitehttpwwwhoustonchroniclecomnewshoustontexashoustonarticleharveylaidbarelackofresourcestrainingat12243556phpcmpidredditpremium\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorhouston', 'quest', 'concern'], ['houston', 'chronicle', 'article', 'frequently', 'behind', 'meter', 'paywall', 'link', 'may', 'let', 'view', 'article', 'reach', 'limit', 'though', 'may', 'wait', 'hours', 'show', 'cache\\n\\n', 'google', 'cachehttpswwwgooglecomqsitehttpwwwhoustonchroniclecomnewshoustontexashoustonarticleharveylaidbarelackofresourcestrainingat12243556phpcmpidredditpremium\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorhouston', 'question', 'concern'])\n",
      "original document: \n",
      "['Hey,', 'this', \"isn't\", 'a', 'liberal', 'thing,', 'This', 'is', 'a', 'sovereign', 'person', 'thing.', 'They', 'tend', 'to', 'be', 'conservative/', 'libertarian', 'people', 'railing', 'against', 'any', 'taxation', 'and', 'a', 'bunch', 'of', 'other', 'weird', 'stuff.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'isnt', 'lib', 'thing', 'sovereign', 'person', 'thing', 'tend', 'conserv', 'libert', 'peopl', 'rail', 'tax', 'bunch', 'weird', 'stuff'], ['hey', 'isnt', 'liberal', 'thing', 'sovereign', 'person', 'thing', 'tend', 'conservative', 'libertarian', 'people', 'rail', 'taxation', 'bunch', 'weird', 'stuff'])\n",
      "original document: \n",
      "['Thanks!', 'Yeah,', 'figured', 'that', 'would', 'happen....I', 'honestly', \"can't\", 'tell', 'if', \"they're\", 'trolling', 'or', 'not', 'sometimes.', \"It's\", 'alright,', 'consider', 'it', 'complimentary', 'entertainment.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'yeah', 'fig', 'would', 'happen', 'honest', 'cant', 'tel', 'theyr', 'trol', 'sometim', 'alright', 'consid', 'comply', 'entertain'], ['thank', 'yeah', 'figure', 'would', 'happeni', 'honestly', 'cant', 'tell', 'theyre', 'troll', 'sometimes', 'alright', 'consider', 'complimentary', 'entertainment'])\n",
      "original document: \n",
      "['Thanks', 'for', 'this.', 'However,', \"I'm\", 'still', 'going', 'to', 'be', 'skeptical', 'until', 'the', 'orgs', 'in', 'question', 'validate', 'receipt', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'howev', 'im', 'stil', 'going', 'skept', 'org', 'quest', 'valid', 'receipt'], ['thank', 'however', 'im', 'still', 'go', 'skeptical', 'orgs', 'question', 'validate', 'receipt'])\n",
      "original document: \n",
      "['If', 'you', 'join', 'us', 'right', 'now,', 'together', 'we', 'can', 'turn', 'the', 'tiiiiiiiiiiiide']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['join', 'us', 'right', 'togeth', 'turn', 'tiiiiiiiiiiiid'], ['join', 'us', 'right', 'together', 'turn', 'tiiiiiiiiiiiide'])\n",
      "original document: \n",
      "['He', '\"literally\"', 'was,', 'though.', 'Answering', 'in', 'more', 'words', 'than', 'I', 'do', \"doesn't\", 'make', 'you', 'right.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'though', 'answ', 'word', 'doesnt', 'mak', 'right'], ['literally', 'though', 'answer', 'word', 'doesnt', 'make', 'right'])\n",
      "original document: \n",
      "['wait,', 'it', \"doesn't\", 'proc', 'off', 'of', 'the', 'cards', 'left', 'in', 'your', 'deck?', 'Two', 'Princes', '(one', 'in', 'deck,', 'one', 'in', 'hand)', 'SHOULD', 'work', 'right?', 'It', 'has', 'to', 'go', 'off', 'the', 'cards', 'that', 'are', 'actually', 'in', 'your', 'deck', 'and', 'not', 'the', 'ones', 'that', 'you', 'start', 'out', 'with', 'because', 'it', 'is', 'invalidated', 'by', 'Elise', 'packs', 'yet', 'still', 'works', 'when', 'you', 'have', 'other', '2', 'drops', 'in', 'your', 'hand', '(say', 'discover', 'a', '2', 'mana', 'card', 'off', 'of', 'lotus', 'illusionist', 'or', 'whatever).', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'doesnt', 'proc', 'card', 'left', 'deck', 'two', 'print', 'on', 'deck', 'on', 'hand', 'work', 'right', 'go', 'card', 'act', 'deck', 'on', 'start', 'invalid', 'el', 'pack', 'yet', 'stil', 'work', 'two', 'drop', 'hand', 'say', 'discov', 'two', 'man', 'card', 'lot', 'illud', 'whatev'], ['wait', 'doesnt', 'proc', 'card', 'leave', 'deck', 'two', 'princes', 'one', 'deck', 'one', 'hand', 'work', 'right', 'go', 'card', 'actually', 'deck', 'ones', 'start', 'invalidate', 'elise', 'pack', 'yet', 'still', 'work', 'two', 'drop', 'hand', 'say', 'discover', 'two', 'mana', 'card', 'lotus', 'illusionist', 'whatever'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"It's\", 'definitely', 'tricky,', 'as', 'an', 'SD', 'you', 'certainly', 'are', 'a', 'target', 'for', 'gold-digging', 'SBs,', 'lol.', '', 'I', 'would', 'never', 'want', 'my', 'SD', 'to', 'feel', 'taken', 'advantage', 'of', 'though,', 'so', \"I'm\", 'very', 'conscious', 'of', 'what', 'he', 'spends.', 'I', 'would', 'not', 'ask', 'him', 'to', 'buy', 'me', 'something,', 'but', 'I', 'do', 'appreciate', 'when', 'he', 'has', 'offered.', '', \"He's\", 'my', 'first', 'SD,', 'and', 'I', 'was', 'really', 'uncomfortable', 'with', 'the', 'money', 'aspect', 'of', 'things', 'in', 'general,', 'but', \"I've\", 'gotten', 'a', 'little', 'better', 'about', 'it.', '', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'tricky', 'sd', 'certain', 'target', 'golddig', 'sbs', 'lol', 'would', 'nev', 'want', 'sd', 'feel', 'tak', 'adv', 'though', 'im', 'conscy', 'spend', 'would', 'ask', 'buy', 'someth', 'apprecy', 'off', 'hes', 'first', 'sd', 'real', 'uncomfort', 'money', 'aspect', 'thing', 'gen', 'iv', 'got', 'littl', 'bet'], ['definitely', 'tricky', 'sd', 'certainly', 'target', 'golddigging', 'sbs', 'lol', 'would', 'never', 'want', 'sd', 'feel', 'take', 'advantage', 'though', 'im', 'conscious', 'spend', 'would', 'ask', 'buy', 'something', 'appreciate', 'offer', 'hes', 'first', 'sd', 'really', 'uncomfortable', 'money', 'aspect', 'things', 'general', 'ive', 'get', 'little', 'better'])\n",
      "original document: \n",
      "['I', 'like', 'how', 'this', 'post', \"isn't\", 'flaired']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'post', 'isnt', 'flair'], ['like', 'post', 'isnt', 'flaired'])\n",
      "original document: \n",
      "['If', 'you', 'pay', 'above', 'average,', 'you’ll', 'get', 'people', 'who', 'will', 'stay', 'with', 'the', 'company', 'longer.', '', 'I’ve', 'seen', 'countless', 'stats', 'that', 'show', 'money', 'is', 'the', 'biggest', 'driving', 'force', 'for', 'just', 'about', 'everything', 'when', 'it', 'comes', 'to', 'motivation', 'etc.', '', '$50k', 'plus', 'bonus', 'is', 'a', 'good', 'place', 'to', 'start.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pay', 'av', 'youl', 'get', 'peopl', 'stay', 'company', 'long', 'iv', 'seen', 'countless', 'stat', 'show', 'money', 'biggest', 'driv', 'forc', 'everyth', 'com', 'mot', 'etc', '50k', 'plu', 'bon', 'good', 'plac', 'start'], ['pay', 'average', 'youll', 'get', 'people', 'stay', 'company', 'longer', 'ive', 'see', 'countless', 'stats', 'show', 'money', 'biggest', 'drive', 'force', 'everything', 'come', 'motivation', 'etc', '50k', 'plus', 'bonus', 'good', 'place', 'start'])\n",
      "original document: \n",
      "['Nope,', 'just', 'super', 'bored.', 'The', 'landscape', 'loses', 'all', 'of', \"it's\", 'color', 'during', 'the', 'winter,', 'and', 'the', 'cold', 'weather', 'drives', 'me', 'insane.', 'Not', 'to', 'mention', 'that', 'damned', 'snow.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nop', 'sup', 'bor', 'landscap', 'los', 'col', 'wint', 'cold', 'weath', 'driv', 'ins', 'ment', 'damn', 'snow'], ['nope', 'super', 'bore', 'landscape', 'lose', 'color', 'winter', 'cold', 'weather', 'drive', 'insane', 'mention', 'damn', 'snow'])\n",
      "original document: \n",
      "['I', \"don't\", 'know', 'that', \"I'll\", 'ever', 'reach', 'my', 'goal', 'weight.', 'It', 'seems', 'so', 'far', 'away.....', '(I', 'have', 'to', 'lose', 'about', '25lbs)', '\\n\\nBut', 'if/when', 'I', 'do', 'get', 'there', 'I', 'think', 'I', 'could', 'be', 'satisfied', 'with', 'how', 'I', 'look', 'in', 'any', 'type', 'of', 'clothes.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['dont', 'know', 'il', 'ev', 'reach', 'goal', 'weight', 'seem', 'far', 'away', 'los', '25lbs', '\\n\\nbut', 'ifwh', 'get', 'think', 'could', 'satisfy', 'look', 'typ', 'cloth'], ['dont', 'know', 'ill', 'ever', 'reach', 'goal', 'weight', 'seem', 'far', 'away', 'lose', '25lbs', '\\n\\nbut', 'ifwhen', 'get', 'think', 'could', 'satisfy', 'look', 'type', 'clothe'])\n",
      "original document: \n",
      "['[Related', 'piece.](http://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rel', 'piecehttpslatestarcodexcom20140930icantolerateanythingexcepttheoutgroup'], ['relate', 'piecehttpslatestarcodexcom20140930icantolerateanythingexcepttheoutgroup'])\n",
      "original document: \n",
      "['It', 'was', 'my', 'tenth', 'month', 'after', 'being', 'hired', 'and', 'felt', 'like', 'literally', 'two', 'months', 'wages', 'had', 'just', 'been', 'taken', 'away', 'prior', 'to', 'going', 'on', 'holiday', 'because', 'of', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'mon', 'hir', 'felt', 'lik', 'lit', 'two', 'month', 'wag', 'tak', 'away', 'pri', 'going', 'holiday'], ['tenth', 'month', 'hire', 'felt', 'like', 'literally', 'two', 'months', 'wag', 'take', 'away', 'prior', 'go', 'holiday'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['There', 'is', 'a', '0%', 'chance', 'he', 'coaches', 'against', 'WV', 'week', '1', 'next', 'year.', 'I', 'just', 'hope', 'to', 'god', 'they', 'fire', 'him', 'within', 'in', 'the', 'next', '2', 'days.', \"I'm\", 'guessing', 'Brady', 'Hoke', 'would', 'take', 'over', 'until', 'we', 'hire', 'someone', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zero', 'chant', 'coach', 'wv', 'week', 'on', 'next', 'year', 'hop', 'god', 'fir', 'within', 'next', 'two', 'day', 'im', 'guess', 'brady', 'hok', 'would', 'tak', 'hir', 'someon'], ['zero', 'chance', 'coach', 'wv', 'week', 'one', 'next', 'year', 'hope', 'god', 'fire', 'within', 'next', 'two', 'days', 'im', 'guess', 'brady', 'hoke', 'would', 'take', 'hire', 'someone'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'propose', 'that', 'Puerto', 'Rico', 'no', 'longer', 'has', 'to', 'be', 'taxed', 'since', 'the', 'money', 'that', 'Uncle', 'Sam', 'got', 'from', 'them', 'will', 'not', 'be', 'used', 'when', 'they', 'need', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['propos', 'puerto', 'rico', 'long', 'tax', 'sint', 'money', 'unc', 'sam', 'got', 'us', 'nee'], ['propose', 'puerto', 'rico', 'longer', 'tax', 'since', 'money', 'uncle', 'sam', 'get', 'use', 'need'])\n",
      "original document: \n",
      "['IIRC', \"they're\", 'heavily', 'armored.', 'So', \"that's\", 'something', 'at', 'least']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iirc', 'theyr', 'heavy', 'arm', 'that', 'someth', 'least'], ['iirc', 'theyre', 'heavily', 'armor', 'thats', 'something', 'least'])\n",
      "original document: \n",
      "['Indeed', 'it', 'is,', 'and', 'you', 'failed', 'to', 'notice', 'that', 'this', 'is', 'one', 'of', 'the', 'oldest,', 'most', 'copied', 'text', 'on', 'here.', \"It's\", 'a', 'classic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indee', 'fail', 'not', 'on', 'oldest', 'cop', 'text', 'class'], ['indeed', 'fail', 'notice', 'one', 'oldest', 'copy', 'text', 'classic'])\n",
      "original document: \n",
      "['THICC']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thicc'], ['thicc'])\n",
      "original document: \n",
      "[\"You're\", 'getting', 'down', 'voted', 'because', 'you', 'guys', 'are', 'looking', 'impressive,', 'but', 'i', 'understand', 'that', 'halftime', 'sinking', 'feeling', 'when', \"you're\", 'at', 'the', 'game.', 'Staring', 'at', 'the', 'field', 'and', 'just', 'feeling', 'it.', \"Don't\", 'worry,', 'I', 'think', 'you', 'guys', 'are', 'competitive', 'and', 'are', 'gonna', 'fight', 'for', 'this', 'win.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'get', 'vot', 'guy', 'look', 'impress', 'understand', 'halftim', 'sink', 'feel', 'yo', 'gam', 'star', 'field', 'feel', 'dont', 'worry', 'think', 'guy', 'competit', 'gonn', 'fight', 'win'], ['youre', 'get', 'vote', 'guy', 'look', 'impressive', 'understand', 'halftime', 'sink', 'feel', 'youre', 'game', 'star', 'field', 'feel', 'dont', 'worry', 'think', 'guy', 'competitive', 'gonna', 'fight', 'win'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['You', 'are', 'the', 'fetish', 'I', 'have', 'been', 'looking', 'for']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fet', 'look'], ['fetish', 'look'])\n",
      "original document: \n",
      "['I', 'think', 'the', 'word', \"you're\", 'looking', 'for', 'is', \"'ratio'.\", 'Anyhow,', 'even', 'if', 'it', 'was', '1:100000000,', 'are', 'you', 'saying', 'that', 'one', 'is', '\"insignificant\"?\\n\\nSee,', 'I', 'can', 'do', 'fallacies', 'too.', '😉']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'word', 'yo', 'look', 'ratio', 'anyhow', 'ev', 'one billion, one hundred million', 'say', 'on', 'insignificant\\n\\nsee', 'fal'], ['think', 'word', 'youre', 'look', 'ratio', 'anyhow', 'even', 'one billion, one hundred million', 'say', 'one', 'insignificant\\n\\nsee', 'fallacies'])\n",
      "original document: \n",
      "['already', 'decorated', 'my', 'desk', 'and', 'desktop', 'a', 'bit.', 'probably', 'gonna', 'bring', 'in', 'some', 'new', 'decorations', 'as', 'usual', 'and', 'compete', 'against', 'my', 'rival', 'around', 'the', 'corner.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'dec', 'desk', 'desktop', 'bit', 'prob', 'gonn', 'bring', 'new', 'dec', 'us', 'compet', 'riv', 'around', 'corn'], ['already', 'decorate', 'desk', 'desktop', 'bite', 'probably', 'gonna', 'bring', 'new', 'decorations', 'usual', 'compete', 'rival', 'around', 'corner'])\n",
      "original document: \n",
      "['I', 'have', 'no', 'idea', 'so', \"I'm\", 'asking.', \"Wouldn't\", 'mixing', 'that', 'with', 'a', 'pregnancy', 'test', 'be', 'a', 'bad', 'idea?', 'There', 'must', 'be', '*some*', 'sort', 'of', 'chemical', 'that', 'reacts', 'with', 'urine', 'to', 'indicate', 'pregnancy', 'right?', 'How', 'safe', 'is', 'that', 'to', 'put', 'in', 'your', 'drink?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ide', 'im', 'ask', 'wouldnt', 'mix', 'pregn', 'test', 'bad', 'ide', 'must', 'sort', 'chem', 'react', 'urin', 'ind', 'pregn', 'right', 'saf', 'put', 'drink'], ['idea', 'im', 'ask', 'wouldnt', 'mix', 'pregnancy', 'test', 'bad', 'idea', 'must', 'sort', 'chemical', 'react', 'urine', 'indicate', 'pregnancy', 'right', 'safe', 'put', 'drink'])\n",
      "original document: \n",
      "[\"That's\", 'really', 'my', 'biggest', 'complaint', 'about', \"OP's\", 'first', 'point.', 'Less', 'popular', 'teams', \"won't\", 'get', 'any', 'significant', 'bigger', 'viewership', 'because', 'they', 'still', 'will', 'be', 'less', 'popular.\\n\\nIf', 'I', 'can', 'sit', 'down', 'and', 'enjoy', '3h', 'of', 'games', 'of', 'a', 'team', 'I', 'like', \"I'll\", 'do', 'that.\\n\\nBut', 'whether', 'it', 'is', '3h', 'of', 'games', 'or', '1h', 'for', 'a', 'single', 'game', 'I', \"won't\", 'sit', 'down', 'to', 'watch', '2', 'teams', 'I', \"don't\", 'care', 'about', 'playing.\\n\\nEven', 'in', 'a', 'talented', 'region', 'like', 'Korea', 'I', \"don't\", 'watch', 'teams', 'like', 'BBQ', 'Olivers', 'play', 'Afreeca', 'Freecs', 'because', 'I', \"don't\", 'care', 'about', 'either', 'of', 'those', 'teams', 'so', 'I', 'definitely', \"won't\", 'watch', 'the', 'bottom', 'NA', 'teams', 'play.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'real', 'biggest', 'complaint', 'op', 'first', 'point', 'less', 'popul', 'team', 'wont', 'get', 'sign', 'big', 'view', 'stil', 'less', 'popular\\n\\nif', 'sit', 'enjoy', '3h', 'gam', 'team', 'lik', 'il', 'that\\n\\nbut', 'wheth', '3h', 'gam', '1h', 'singl', 'gam', 'wont', 'sit', 'watch', 'two', 'team', 'dont', 'car', 'playing\\n\\neven', 'tal', 'reg', 'lik', 'kore', 'dont', 'watch', 'team', 'lik', 'bbq', 'ol', 'play', 'afreec', 'freec', 'dont', 'car', 'eith', 'team', 'definit', 'wont', 'watch', 'bottom', 'na', 'team', 'play'], ['thats', 'really', 'biggest', 'complaint', 'ops', 'first', 'point', 'less', 'popular', 'team', 'wont', 'get', 'significant', 'bigger', 'viewership', 'still', 'less', 'popular\\n\\nif', 'sit', 'enjoy', '3h', 'game', 'team', 'like', 'ill', 'that\\n\\nbut', 'whether', '3h', 'game', '1h', 'single', 'game', 'wont', 'sit', 'watch', 'two', 'team', 'dont', 'care', 'playing\\n\\neven', 'talented', 'region', 'like', 'korea', 'dont', 'watch', 'team', 'like', 'bbq', 'olivers', 'play', 'afreeca', 'freecs', 'dont', 'care', 'either', 'team', 'definitely', 'wont', 'watch', 'bottom', 'na', 'team', 'play'])\n",
      "original document: \n",
      "['I', 'think', 'she', 'just', 'might', 'be', 'banned', 'from', 'MTV.', '\\n\\nI', 'guess', 'anyone', 'with', '200+', 'followers', 'on', 'Instagram', 'is', 'considered', 'a', 'celebrity', 'now', '🙄']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'might', 'ban', 'mtv', '\\n\\ni', 'guess', 'anyon', 'two hundred', 'follow', 'instagram', 'consid', 'celebr'], ['think', 'might', 'ban', 'mtv', '\\n\\ni', 'guess', 'anyone', 'two hundred', 'followers', 'instagram', 'consider', 'celebrity'])\n",
      "original document: \n",
      "[\"What's\", 'the', 'shortest', 'song', 'for', 'grinding', 'tokens?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'shortest', 'song', 'grind', 'tok'], ['whats', 'shortest', 'song', 'grind', 'tokens'])\n",
      "original document: \n",
      "['This', 'is', 'amazing', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amaz'], ['amaze'])\n",
      "original document: \n",
      "['Estie', \"c'est\", 'profond.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['esty', 'cest', 'profond'], ['estie', 'cest', 'profond'])\n",
      "original document: \n",
      "['The', 'key', 'designers', 'could', 'have', 'benefitted', 'from', 'some', 'design', 'improvement', 'as', 'well...such', 'as', 'placing', 'picture', 'instructions', 'of', 'how', 'to', 'use', 'the', 'key', 'on', 'the', 'reverse', 'side.', '', 'Currently', 'it', 'has', 'pictures', 'of', 'all', 'the', 'family', 'brand', 'logos...which', 'is', 'really', 'worse', 'than', 'being', 'blank', 'if', 'a', 'guest', 'is', 'standing', 'there', 'getting', 'frustrated', 'because', 'then', 'they', 'associate', 'your', 'brand', 'logos', 'with', 'that', 'emotion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['key', 'design', 'could', 'benefit', 'design', 'improv', 'wellsuch', 'plac', 'pict', 'instruct', 'us', 'key', 'revers', 'sid', 'cur', 'pict', 'famy', 'brand', 'logoswhich', 'real', 'wors', 'blank', 'guest', 'stand', 'get', 'frust', 'assocy', 'brand', 'logo', 'emot'], ['key', 'designers', 'could', 'benefit', 'design', 'improvement', 'wellsuch', 'place', 'picture', 'instructions', 'use', 'key', 'reverse', 'side', 'currently', 'picture', 'family', 'brand', 'logoswhich', 'really', 'worse', 'blank', 'guest', 'stand', 'get', 'frustrate', 'associate', 'brand', 'logos', 'emotion'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'currycel.', '\\nFoids', 'who', 'are', 'attractive', 'and', 'can', 'get', 'chad', \"won't\", 'go', 'for', 'traditional', 'arranged', 'marriage,', 'average', 'and', 'below', 'foids', 'who', 'go', 'for', 'arranged', 'marriage', 'are', 'mostly', 'gold', 'diggers', 'looking', 'for', 'betabux', 'to', 'cheat', 'on.\\nArranged', 'marriage', 'works', 'differently', 'now,', 'foids', 'do', 'have', 'a', 'choice,', 'she', 'can', 'keep', 'rejecting', 'proposals', 'which', 'her', 'parents', 'find,', 'might', 'be', 'forced', 'to', 'settle', 'if', 'she', 'hits', 'her', 'late', '30s.\\nMarriage', 'is', 'cope', 'altogether,', 'every', 'married', 'wimen', 'i', 'know', 'cheats.\\n\\n\\nDowry', 'is', 'impossible', 'now,', 'might', 'even', 'be', 'falsely', 'sent', 'to', 'jail.', 'Fuck', 'feminism.', '\\n\\n\\nI', 'wish', 'escorts', 'were', 'legal', 'and', 'easy', 'to', 'find', 'in', 'india,', 'being', 'escortcel', 'is', 'way', 'better', 'than', 'becoming', 'a', 'betacuck.\\n\\n\\nYou', 'can', 'get', 'a', 'hot', 'curry', 'if', 'you', 'can', 'afford', 'to', 'bring', 'her', 'into', 'your', 'country', 'through', 'marriage', 'because', \"you're\", 'white.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'currycel', '\\nfoids', 'attract', 'get', 'chad', 'wont', 'go', 'tradit', 'arrang', 'marry', 'av', 'foid', 'go', 'arrang', 'marry', 'most', 'gold', 'dig', 'look', 'betabux', 'che', 'on\\narranged', 'marry', 'work', 'diff', 'foid', 'cho', 'keep', 'reject', 'propos', 'par', 'find', 'might', 'forc', 'settl', 'hit', 'lat', '30s\\nmarriage', 'cop', 'altogeth', 'every', 'marry', 'wim', 'know', 'cheats\\n\\n\\ndowry', 'imposs', 'might', 'ev', 'fals', 'sent', 'jail', 'fuck', 'femin', '\\n\\n\\ni', 'wish', 'escort', 'leg', 'easy', 'find', 'ind', 'escortcel', 'way', 'bet', 'becom', 'betacuck\\n\\n\\nyou', 'get', 'hot', 'curry', 'afford', 'bring', 'country', 'marry', 'yo', 'whit'], ['im', 'currycel', '\\nfoids', 'attractive', 'get', 'chad', 'wont', 'go', 'traditional', 'arrange', 'marriage', 'average', 'foids', 'go', 'arrange', 'marriage', 'mostly', 'gold', 'diggers', 'look', 'betabux', 'cheat', 'on\\narranged', 'marriage', 'work', 'differently', 'foids', 'choice', 'keep', 'reject', 'proposals', 'parent', 'find', 'might', 'force', 'settle', 'hit', 'late', '30s\\nmarriage', 'cope', 'altogether', 'every', 'marry', 'wimen', 'know', 'cheats\\n\\n\\ndowry', 'impossible', 'might', 'even', 'falsely', 'send', 'jail', 'fuck', 'feminism', '\\n\\n\\ni', 'wish', 'escort', 'legal', 'easy', 'find', 'india', 'escortcel', 'way', 'better', 'become', 'betacuck\\n\\n\\nyou', 'get', 'hot', 'curry', 'afford', 'bring', 'country', 'marriage', 'youre', 'white'])\n",
      "original document: \n",
      "['Mine', 'is', '\"Thanks', 'for', 'calling', 'X,', 'this', 'is', 'Qari,', 'can', 'I', 'get', 'your', 'name', 'and', 'company', 'please?\"\\n\\n\"uhh', 'yes', 'my', 'problem', 'is', 'X.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['min', 'thank', 'cal', 'x', 'qar', 'get', 'nam', 'company', 'please\\n\\nuhh', 'ye', 'problem', 'x'], ['mine', 'thank', 'call', 'x', 'qari', 'get', 'name', 'company', 'please\\n\\nuhh', 'yes', 'problem', 'x'])\n",
      "original document: \n",
      "['Hey,', 'but', 'Doom', 'is', 'coming', 'this', 'holiday']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'doom', 'com', 'holiday'], ['hey', 'doom', 'come', 'holiday'])\n",
      "original document: \n",
      "['Close', 'enough.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['clos', 'enough'], ['close', 'enough'])\n",
      "original document: \n",
      "['Brb', 'gonne', 'glue', 'some', 'money', 'on', 'the', 'back', 'of', 'my', 'phone', 'just', 'in', 'case']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brb', 'gon', 'glu', 'money', 'back', 'phon', 'cas'], ['brb', 'gonne', 'glue', 'money', 'back', 'phone', 'case'])\n",
      "original document: \n",
      "['Most', 'of', 'what', 'the', 'government', 'does', 'is', 'the', 'extremely', 'boring', 'shit', 'needed', 'to', 'continue', 'to', 'supply', 'basic', 'services.', 'Governments', 'maintain', 'roads,', 'regulate', 'labels', 'on', 'ketchup', 'so', 'you', 'know', 'what', \"you're\", 'buying,', 'and', 'do', 'science', 'to', 'figure', 'out', 'the', 'limit', 'on', 'the', 'number', 'of', 'ducks', 'you', 'can', 'kill', 'at', 'once', 'so', \"there's\", 'enough', 'ducks.\\n\\nA', 'very', 'small', 'amount', 'of', 'the', 'stuff', 'the', 'government', 'does', 'is', 'SCARY', 'AS', 'ALL', 'FUCKSTICKS.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['govern', 'extrem', 'bor', 'shit', 'nee', 'continu', 'supply', 'bas', 'serv', 'govern', 'maintain', 'road', 'reg', 'label', 'ketchup', 'know', 'yo', 'buy', 'sci', 'fig', 'limit', 'numb', 'duck', 'kil', 'ther', 'enough', 'ducks\\n\\na', 'smal', 'amount', 'stuff', 'govern', 'scary', 'fuckstick'], ['government', 'extremely', 'bore', 'shit', 'need', 'continue', 'supply', 'basic', 'service', 'governments', 'maintain', 'roads', 'regulate', 'label', 'ketchup', 'know', 'youre', 'buy', 'science', 'figure', 'limit', 'number', 'duck', 'kill', 'theres', 'enough', 'ducks\\n\\na', 'small', 'amount', 'stuff', 'government', 'scary', 'fucksticks'])\n",
      "original document: \n",
      "['You', 'must', 'hold', 'up', 'your', 'hand', 'a', 'lot', 'when', 'watching', 'The', 'Simpsons', 'then.', '🤔']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['must', 'hold', 'hand', 'lot', 'watch', 'simpson'], ['must', 'hold', 'hand', 'lot', 'watch', 'simpsons'])\n",
      "original document: \n",
      "['Garza', 'looking', 'like', 'he', 'needs', 'to', 'come', 'out', 'now.', 'He', 'looks', 'like', \"he's\", 'having', 'a', 'lot', 'of', 'trouble.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['garz', 'look', 'lik', 'nee', 'com', 'look', 'lik', 'hes', 'lot', 'troubl'], ['garza', 'look', 'like', 'need', 'come', 'look', 'like', 'hes', 'lot', 'trouble'])\n",
      "original document: \n",
      "['This', 'content', 'brought', 'to', 'you', 'from', '\"Spain', 'Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#off', 'site', 'feed', '\"Spain', 'Pool\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'brought', 'spain', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'sit', 'fee', 'spain', 'pool\\n'], ['content', 'bring', 'spain', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'site', 'fee', 'spain', 'pool\\n'])\n",
      "original document: \n",
      "['How', 'did', 'you', 'see', 'him?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see'], ['see'])\n",
      "original document: \n",
      "['143417408|', '&gt;', 'Switzerland', 'Anonymous', '(ID:', '/BaaYiQg)\\n\\n&gt;&gt;143416975\\nIronically', 'you', 'got', 'to', 'be', 'a', 'retard', 'if', 'you', 'choose', 'to', 'live', 'in', 'a', 'city.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, four hundred and eight', 'gt', 'switzerland', 'anonym', 'id', 'baayiqg\\n\\ngtgt143416975\\nironically', 'got', 'retard', 'choos', 'liv', 'city\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, four hundred and eight', 'gt', 'switzerland', 'anonymous', 'id', 'baayiqg\\n\\ngtgt143416975\\nironically', 'get', 'retard', 'choose', 'live', 'city\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Contingency', 'and', 'necessity', 'bit', 'is', 'pretty', 'incoherent,', 'and', 'is', 'really', 'only', 'a', 'product', 'of', 'trying', 'to', 'understand', 'the', 'Universe', 'before', 'modern', 'physics.\\n\\nThe', 'whole', '\"we', 'need', 'God', 'to', 'create', 'the', 'Universe...', 'well', 'then', 'what', 'created', 'God?', '', 'God', \"doesn't\", 'need', 'a', 'creator!', '', 'Then', 'why', 'does', 'the', 'Universe', 'need', 'God', 'to', 'create', 'it?\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'necess', 'bit', 'pretty', 'incoh', 'real', 'produc', 'try', 'understand', 'univers', 'modern', 'physics\\n\\nthe', 'whol', 'nee', 'god', 'cre', 'univers', 'wel', 'cre', 'god', 'god', 'doesnt', 'nee', 'cre', 'univers', 'nee', 'god', 'cre'], ['contingency', 'necessity', 'bite', 'pretty', 'incoherent', 'really', 'product', 'try', 'understand', 'universe', 'modern', 'physics\\n\\nthe', 'whole', 'need', 'god', 'create', 'universe', 'well', 'create', 'god', 'god', 'doesnt', 'need', 'creator', 'universe', 'need', 'god', 'create'])\n",
      "original document: \n",
      "['SAR', 'no', 'water', 'add', 'peach', 'infusion', 'and', 'white', 'tea.', \"It's\", 'not', 'that', 'big', 'if', 'a', 'pain', 'but', 'I', 'guess', 'feel', 'silly', 'ordering', 'it.', 'So', 'I', 'just', 'make', 'them', 'for', 'myself', 'before', 'a', 'ten.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sar', 'wat', 'ad', 'peach', 'infus', 'whit', 'tea', 'big', 'pain', 'guess', 'feel', 'sil', 'ord', 'mak', 'ten'], ['sar', 'water', 'add', 'peach', 'infusion', 'white', 'tea', 'big', 'pain', 'guess', 'feel', 'silly', 'order', 'make', 'ten'])\n",
      "original document: \n",
      "['I’d', 'love', 'to', 'cum', 'all', 'over', 'your', 'perfect', 'tits', 'and', 'have', 'you', 'suck', 'me', 'dry', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'lov', 'cum', 'perfect', 'tit', 'suck', 'dry'], ['id', 'love', 'cum', 'perfect', 'tits', 'suck', 'dry'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Wait', 'for', 'coffee', 'lake', 'before', 'picking', 'a', 'CPU.', 'Intel', '7700K', 'might', 'go', 'down.', 'Ryzen', '5', '1600', 'is', 'a', 'good', 'pick', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'coff', 'lak', 'pick', 'cpu', 'intel', '7700k', 'might', 'go', 'ryz', 'fiv', 'one thousand, six hundred', 'good', 'pick'], ['wait', 'coffee', 'lake', 'pick', 'cpu', 'intel', '7700k', 'might', 'go', 'ryzen', 'five', 'one thousand, six hundred', 'good', 'pick'])\n",
      "original document: \n",
      "['I', 'have', 'a', '760', 'for', '60$', 'shipped']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seven hundred and sixty', 'sixty', 'ship'], ['seven hundred and sixty', 'sixty', 'ship'])\n",
      "original document: \n",
      "['Look', 'at', 'their', 'shoes:\\n-Too', 'flashy', 'and', 'they', \"won't\", 'tip', 'generously', 'unless', 'they', 'are', 'shitfaced\\n-really', 'expensive', 'fine', 'leather', 'and', \"you've\", 'hit', 'the', 'jackpot\\n-heavily', 'worn', 'work', 'boots', 'and', 'they', 'might', 'tip', 'you', 'well', 'one', 'night', 'but', 'they', \"don't\", 'make', 'good', 'recurring', 'customers\\n\\nTeam', 'up:\\n-ask', 'customers', '\"who', 'exactly', 'is', 'your', 'favorite?\"', 'And', 'then', 'tell', 'your', 'coworker,', 'the', 'coworker', 'might', 'return', 'the', 'favor', '(but', 'do', 'not', 'expect', 'them', 'to', 'team', 'up,', 'many', 'will', 'not)\\n-tip', 'the', 'bouncers', 'to', 'send', 'generous', 'customers', 'to', 'you\\n-tip', 'the', 'bartenders', 'to', 'send', 'generous', 'customers', 'to', 'you\\n\\nBe', 'a', 'contradiction:\\n-be', 'funny', '(memorize', 'lots', 'of', 'jokes)', 'but', 'claim', 'that', 'you', \"don't\", 'know', 'any', 'jokes\\n-be', 'aggressive', 'but', 'act', 'shy\\n-act', 'drunk', 'but', 'be', 'sober', \"(don't\", 'ever', 'get', 'drunk', 'on', 'the', 'job', 'actually)\\n-claim', 'you', \"don't\", 'know', 'anything', 'but', 'have', 'intelligent', 'conversations\\n-claim', 'to', 'be', 'boring', 'but', 'be', 'super', 'entertaining\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'shoes\\ntoo', 'flashy', 'wont', 'tip', 'gen', 'unless', 'shitfaced\\nreally', 'expend', 'fin', 'leath', 'youv', 'hit', 'jackpot\\nheavily', 'worn', 'work', 'boot', 'might', 'tip', 'wel', 'on', 'night', 'dont', 'mak', 'good', 'recur', 'customers\\n\\nteam', 'up\\nask', 'custom', 'exact', 'favorit', 'tel', 'cowork', 'cowork', 'might', 'return', 'fav', 'expect', 'team', 'many', 'not\\ntip', 'bount', 'send', 'gen', 'custom', 'you\\ntip', 'bartend', 'send', 'gen', 'custom', 'you\\n\\nbe', 'contradiction\\nbe', 'funny', 'mem', 'lot', 'jok', 'claim', 'dont', 'know', 'jokes\\nbe', 'aggress', 'act', 'shy\\nact', 'drunk', 'sob', 'dont', 'ev', 'get', 'drunk', 'job', 'actually\\nclaim', 'dont', 'know', 'anyth', 'intellig', 'conversations\\nclaim', 'bor', 'sup', 'entertaining\\n'], ['look', 'shoes\\ntoo', 'flashy', 'wont', 'tip', 'generously', 'unless', 'shitfaced\\nreally', 'expensive', 'fine', 'leather', 'youve', 'hit', 'jackpot\\nheavily', 'wear', 'work', 'boot', 'might', 'tip', 'well', 'one', 'night', 'dont', 'make', 'good', 'recur', 'customers\\n\\nteam', 'up\\nask', 'customers', 'exactly', 'favorite', 'tell', 'coworker', 'coworker', 'might', 'return', 'favor', 'expect', 'team', 'many', 'not\\ntip', 'bouncers', 'send', 'generous', 'customers', 'you\\ntip', 'bartenders', 'send', 'generous', 'customers', 'you\\n\\nbe', 'contradiction\\nbe', 'funny', 'memorize', 'lot', 'joke', 'claim', 'dont', 'know', 'jokes\\nbe', 'aggressive', 'act', 'shy\\nact', 'drink', 'sober', 'dont', 'ever', 'get', 'drink', 'job', 'actually\\nclaim', 'dont', 'know', 'anything', 'intelligent', 'conversations\\nclaim', 'bore', 'super', 'entertaining\\n'])\n",
      "original document: \n",
      "['Suddenly', \"I'm\", 'reminded', 'of', 'Red', 'Dwarf.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sud', 'im', 'remind', 'red', 'dwarf'], ['suddenly', 'im', 'remind', 'red', 'dwarf'])\n",
      "original document: \n",
      "['Or', 'just', 'get', 'a', 'real', 'one', 'for', '$50', 'at', 'a', 'flea', 'market...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'real', 'on', 'fifty', 'fle', 'market'], ['get', 'real', 'one', 'fifty', 'flea', 'market'])\n",
      "original document: \n",
      "['Maybe', 'you', 'should', 'read', 'the', 'OP', 'again.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'read', 'op'], ['maybe', 'read', 'op'])\n",
      "original document: \n",
      "['Butthurt', 'China', 'topi', 'spotted', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['butthurt', 'chin', 'top', 'spot', 'lol'], ['butthurt', 'china', 'topi', 'spot', 'lol'])\n",
      "original document: \n",
      "['You', \"don't\", 'need', 'roasting,', 'being', 'a', 'Rams', 'fan', 'is', 'bad', 'enough', 'already']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'nee', 'roast', 'ram', 'fan', 'bad', 'enough', 'already'], ['dont', 'need', 'roast', 'ram', 'fan', 'bad', 'enough', 'already'])\n",
      "original document: \n",
      "['User', 'name', 'checks', 'out']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'nam', 'check'], ['user', 'name', 'check'])\n",
      "original document: \n",
      "['Hey', 'at', 'least', 'the', 'women', 'up', 'top', 'serve', 'a', 'purpose.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'least', 'wom', 'top', 'serv', 'purpos'], ['hey', 'least', 'women', 'top', 'serve', 'purpose'])\n",
      "original document: \n",
      "['What', 'about', 'luckyduck']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['luckyduck'], ['luckyduck'])\n",
      "original document: \n",
      "['I', 'have', 'stabilized', 'the', 'video', 'for', 'you:', 'https://gfycat.com/bluetartdavidstiger', '\\n\\nIt', 'took', '95', 'seconds', 'to', 'process', 'and', '15', 'seconds', 'to', 'upload.\\n___\\n[^^how', '^^to', '^^use](https://www.reddit.com/r/stabbot/comments/72irce/how_to_use_stabbot/)', '^^|', '[^^programmer](https://www.reddit.com/message/compose/?to=wotanii)', '^^|', '[^^source', '^^code](https://gitlab.com/wotanii/stabbot)', '^^|', '^^/r/ImageStabilization/', '^^|', '^^for', '^^cropped', '^^results,', '^^use', '^^\\\\/u/stabbot_crop']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stabl', 'video', 'httpsgfycatcombluetartdavidstiger', '\\n\\nit', 'took', 'ninety-five', 'second', 'process', 'fifteen', 'second', 'upload\\n___\\nhow', 'usehttpswwwredditcomrstabbotcomments72ircehow_to_use_stabbot', 'programmerhttpswwwredditcommessagecomposetowotani', 'sourc', 'codehttpsgitlabcomwotaniistabbot', 'rimagest', 'crop', 'result', 'us', 'ustabbot_crop'], ['stabilize', 'video', 'httpsgfycatcombluetartdavidstiger', '\\n\\nit', 'take', 'ninety-five', 'second', 'process', 'fifteen', 'second', 'upload\\n___\\nhow', 'usehttpswwwredditcomrstabbotcomments72ircehow_to_use_stabbot', 'programmerhttpswwwredditcommessagecomposetowotanii', 'source', 'codehttpsgitlabcomwotaniistabbot', 'rimagestabilization', 'crop', 'result', 'use', 'ustabbot_crop'])\n",
      "original document: \n",
      "['What', 'does', 'that', 'mean?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean'], ['mean'])\n",
      "original document: \n",
      "['It', 'seems', 'like', 'artists', 'have', 'been', 'using', 'this', 'as', 'an', 'easy', 'was', 'to', 'generate', 'excitement', 'in', 'the', 'UK', 'for', 'their', 'album.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seem', 'lik', 'art', 'us', 'easy', 'gen', 'excit', 'uk', 'alb'], ['seem', 'like', 'artists', 'use', 'easy', 'generate', 'excitement', 'uk', 'album'])\n",
      "original document: \n",
      "['I', 'think', 'you’re', 'too', 'upright.', 'A', 'magazine', 'once', 'noted', 'the', 'belt', 'buckle', 'should', 'be', 'angled', 'toward', 'ball.', 'A', 'more', 'athletic', 'stance,', 'with', 'more', 'knee', 'bend', 'allows', 'the', 'arms', 'and', 'core', 'to', 'turn,', 'and', 'come', 'back', 'with', 'more', 'explosive', 'contact.', 'The', 'tips', 'above', 'all', 'sound', 'good', 'to', 'me', 'as', 'well.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'yo', 'upright', 'magazin', 'not', 'belt', 'buckl', 'angl', 'toward', 'bal', 'athlet', 'stant', 'kne', 'bend', 'allow', 'arm', 'cor', 'turn', 'com', 'back', 'explod', 'contact', 'tip', 'sound', 'good', 'wel'], ['think', 'youre', 'upright', 'magazine', 'note', 'belt', 'buckle', 'angle', 'toward', 'ball', 'athletic', 'stance', 'knee', 'bend', 'allow', 'arm', 'core', 'turn', 'come', 'back', 'explosive', 'contact', 'tip', 'sound', 'good', 'well'])\n",
      "original document: \n",
      "[\"'I\", 'watch', 'Driller', \"Killer.'\", 'The', 'class', 'is', 'shocked', 'at', 'my', 'overwhelming', 'intelligence', 'and', 'taste', 'in', \"'artistic'\", 'movies.', \"'...how?\", 'I', \"can't\", 'understand', 'its', 'sheer', 'nuance', 'and', 'artistry.', 'Laser', 'Mission', 'is', 'so', 'much', 'more', \"accessible.'\", \"'Well...OOP\", 'IS', 'THE', \"PUNCH.'\", 'One', 'student', 'laughs,', 'and', 'I', 'turn', 'to', 'see', 'who', 'the', 'fellow', 'artist', 'is.', 'Its', 'none', 'other', 'than', 'Rob', \"'Alpacapatrol'\", 'Robert.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['watch', 'dril', 'kil', 'class', 'shock', 'overwhelm', 'intellig', 'tast', 'art', 'movy', 'cant', 'understand', 'she', 'nuant', 'art', 'las', 'miss', 'much', 'access', 'welloop', 'punch', 'on', 'stud', 'laugh', 'turn', 'see', 'fellow', 'art', 'non', 'rob', 'alpacapatrol', 'robert'], ['watch', 'driller', 'killer', 'class', 'shock', 'overwhelm', 'intelligence', 'taste', 'artistic', 'movies', 'cant', 'understand', 'sheer', 'nuance', 'artistry', 'laser', 'mission', 'much', 'accessible', 'welloop', 'punch', 'one', 'student', 'laugh', 'turn', 'see', 'fellow', 'artist', 'none', 'rob', 'alpacapatrol', 'robert'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['L-lewd', ':o']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['llewd'], ['llewd'])\n",
      "original document: \n",
      "['I', \"don't\", 'know', 'why', 'you', 'were', 'down', 'vote', 'so', 'much.', 'You', \"aren't\", 'wrong.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'vot', 'much', 'ar', 'wrong'], ['dont', 'know', 'vote', 'much', 'arent', 'wrong'])\n",
      "original document: \n",
      "['I', 'appreciate', 'the', 'honest', 'thirst', 'in', 'this', 'comment.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['apprecy', 'honest', 'thirst', 'com'], ['appreciate', 'honest', 'thirst', 'comment'])\n",
      "original document: \n",
      "['Chelsea', 'reminds', 'me', 'of', 'Michael', 'Cera', 'a', 'bit', 'in', 'this', 'picture', 'and', 'now', 'I', 'really', 'really', 'want', 'to', 'see', 'Michael', 'Cera', 'in', 'a', 'wig', 'gangbanged', 'by', 'a', 'bunch', 'of', 'muscular,', 'strong', 'military', 'men', '', '\\nNo', 'homo,', 'of', 'course.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chelse', 'remind', 'michael', 'cer', 'bit', 'pict', 'real', 'real', 'want', 'see', 'michael', 'cer', 'wig', 'gangbang', 'bunch', 'muscul', 'strong', 'milit', 'men', '\\nno', 'homo', 'cours'], ['chelsea', 'remind', 'michael', 'cera', 'bite', 'picture', 'really', 'really', 'want', 'see', 'michael', 'cera', 'wig', 'gangbanged', 'bunch', 'muscular', 'strong', 'military', 'men', '\\nno', 'homo', 'course'])\n",
      "original document: \n",
      "['Got', 'the', 'same', 'thing.', 'I', 'love', 'this', 'generation’s', 'version', 'of', 'Space', 'Gray.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'thing', 'lov', 'gen', 'vert', 'spac', 'gray'], ['get', 'thing', 'love', 'generations', 'version', 'space', 'gray'])\n",
      "original document: \n",
      "['Me,', 'you', 'and', 'one', 'other', 'person', 'at', 'least.', 'I', 'finally', 'got', 'them', '6', 'hours', 'ago.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'person', 'least', 'fin', 'got', 'six', 'hour', 'ago'], ['one', 'person', 'least', 'finally', 'get', 'six', 'hours', 'ago'])\n",
      "original document: \n",
      "['Too', 'much', 'water', '7.8/10']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'wat', 'seven thousand, eight hundred and t'], ['much', 'water', 'seven thousand, eight hundred and ten'])\n",
      "original document: \n",
      "['Oh,', 'I', 'am', 'sorry', 'I', 'was', 'unaware', 'that', 'you', 'personally', 'knew', 'these', 'two', 'young', 'men.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sorry', 'unaw', 'person', 'knew', 'two', 'young', 'men'], ['oh', 'sorry', 'unaware', 'personally', 'know', 'two', 'young', 'men'])\n",
      "original document: \n",
      "['They', \"didn't\", 'nerf', 'or', 'buff', 'anything.', 'They', 'changed', 'a', 'formula', 'that', 'unfairly', 'punished', 'Pokémon', 'with', 'unbalanced', 'stats.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'nerf', 'buff', 'anyth', 'chang', 'formul', 'unfair', 'pun', 'pokemon', 'unb', 'stat'], ['didnt', 'nerf', 'buff', 'anything', 'change', 'formula', 'unfairly', 'punish', 'pokemon', 'unbalance', 'stats'])\n",
      "original document: \n",
      "['We', 'fucking', 'NEED', 'a', 'bot', 'that', 'comments', 'thisemojipasta', 'every', 'single', 'time', 'it', 'scans', 'a', 'word', 'of', 'this', 'in', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'nee', 'bot', 'com', 'thisemojipast', 'every', 'singl', 'tim', 'scan', 'word'], ['fuck', 'need', 'bot', 'comment', 'thisemojipasta', 'every', 'single', 'time', 'scan', 'word'])\n",
      "original document: \n",
      "['Sucks', 'when', 'grandma', \"isn't\", 'streaming.', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suck', 'grandm', 'isnt', 'streaming'], ['suck', 'grandma', 'isnt', 'stream'])\n",
      "original document: \n",
      "['Bro.', \"It's\", 'Tina.', 'She', 'cray.', 'Gtfo.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bro', 'tin', 'cray', 'gtfo'], ['bro', 'tina', 'cray', 'gtfo'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Heinz', 'only.', 'Pickles', 'are', 'great.', 'At', 'least', 'you’re', '1', 'for', '2.', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heinz', 'pickl', 'gre', 'least', 'yo', 'on', 'two'], ['heinz', 'pickle', 'great', 'least', 'youre', 'one', 'two'])\n",
      "original document: \n",
      "['But', \"you'd\", 'be', 'wrong.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youd', 'wrong'], ['youd', 'wrong'])\n",
      "original document: \n",
      "['Only', 'conservatives', 'are', 'allowed', 'to', 'be', 'offended', 'these', 'days.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['conserv', 'allow', 'offend', 'day'], ['conservatives', 'allow', 'offend', 'days'])\n",
      "original document: \n",
      "['Then', 'Lee', 'or', 'White.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lee', 'whit'], ['lee', 'white'])\n",
      "original document: \n",
      "['I', 'disagree', 'with', 'it,', 'unless', 'they', 'start', 'charging', 'families', 'PER', 'child.', '', 'I', \"don't\", 'mind', 'tax', 'money', 'to', 'go', 'to', 'paying', 'for', 'schools,', 'but', 'Utah', 'is', 'in', 'a', 'unique', 'situation', 'where', 'our', 'local', 'culture', 'encourages', 'oversize', 'families', 'yet', 'everyone', 'picks', 'up', 'the', 'tab', 'equally.', '', \"I'm\", 'all', 'for', '2', 'kids', 'included', 'with', 'your', 'taxes,', 'beyond', 'that', '$1000', 'a', 'year', '(or', 'whatever', 'a', 'reasonable', 'price', 'is).', '', 'Until', 'then', 'NO', 'NO', 'NO.\\n\\nI', \"don't\", 'have', 'kids', 'if', 'that', 'was', 'no', 'clear', 'already,', 'and', 'like', 'I', 'said,', 'I', \"don't\", 'mind', 'paying', 'taxes,', 'but', 'the', 'system', 'is', 'currently', 'woefully', 'unfair', 'and', 'is', 'not', 'able', 'to', 'deal', 'with', 'said', '\"culture\".', '', 'Maybe', 'the', '\"CULTure\"', 'that', 'encourages', 'the', 'ravenous', 'growth', 'could', 'chip', 'in', 'so', 'these', 'bonds', 'would', 'not', 'be', 'needed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['disagr', 'unless', 'start', 'charg', 'famy', 'per', 'child', 'dont', 'mind', 'tax', 'money', 'go', 'pay', 'schools', 'utah', 'un', 'situ', 'loc', 'cult', 'enco', 'overs', 'famy', 'yet', 'everyon', 'pick', 'tab', 'eq', 'im', 'two', 'kid', 'includ', 'tax', 'beyond', 'one thousand', 'year', 'whatev', 'reason', 'pric', 'no\\n\\ni', 'dont', 'kid', 'clear', 'already', 'lik', 'said', 'dont', 'mind', 'pay', 'tax', 'system', 'cur', 'woe', 'unfair', 'abl', 'deal', 'said', 'cult', 'mayb', 'cult', 'enco', 'rav', 'grow', 'could', 'chip', 'bond', 'would', 'nee'], ['disagree', 'unless', 'start', 'charge', 'families', 'per', 'child', 'dont', 'mind', 'tax', 'money', 'go', 'pay', 'school', 'utah', 'unique', 'situation', 'local', 'culture', 'encourage', 'oversize', 'families', 'yet', 'everyone', 'pick', 'tab', 'equally', 'im', 'two', 'kid', 'include', 'tax', 'beyond', 'one thousand', 'year', 'whatever', 'reasonable', 'price', 'no\\n\\ni', 'dont', 'kid', 'clear', 'already', 'like', 'say', 'dont', 'mind', 'pay', 'tax', 'system', 'currently', 'woefully', 'unfair', 'able', 'deal', 'say', 'culture', 'maybe', 'culture', 'encourage', 'ravenous', 'growth', 'could', 'chip', 'bond', 'would', 'need'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'meant', 'public', 'in', 'the', 'sense', 'that', 'it', \"wasn't\", 'just', 'between', 'a', 'group', 'of', 'close', 'friends', 'but', 'between', 'a', 'group', 'of', 'people', 'from', 'across', 'the', 'country', 'who', \"didn't\", 'necessarily', 'know', 'each', 'other', 'outside', 'of', 'the', 'group', 'or', 'organisation', 'it', 'was', 'affiliated', 'with.', 'Seems', 'as', 'though', 'anyone', 'could', 'have', 'been', 'a', 'member', 'of', 'the', 'group.\\n\\nWhy', 'does', 'hanging', 'a', 'banner', 'on', 'a', 'bridge', 'make', 'it', 'worse', 'that', 'you', 'have', 'those', 'views?', 'It', 'might', 'make', 'you', 'a', 'bit', 'more', 'annoying', 'than', 'if', 'you', 'keep', 'it', 'quiet', 'but', \"I'm\", 'not', 'sure', 'that', 'matters', 'much', 'in', 'the', 'context', 'of', 'this', 'discussion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meant', 'publ', 'sens', 'wasnt', 'group', 'clos', 'friend', 'group', 'peopl', 'across', 'country', 'didnt', 'necess', 'know', 'outsid', 'group', 'org', 'affy', 'seem', 'though', 'anyon', 'could', 'memb', 'group\\n\\nwhy', 'hang', 'ban', 'bridg', 'mak', 'wors', 'view', 'might', 'mak', 'bit', 'annoy', 'keep', 'quiet', 'im', 'sur', 'mat', 'much', 'context', 'discuss'], ['mean', 'public', 'sense', 'wasnt', 'group', 'close', 'friends', 'group', 'people', 'across', 'country', 'didnt', 'necessarily', 'know', 'outside', 'group', 'organisation', 'affiliate', 'seem', 'though', 'anyone', 'could', 'member', 'group\\n\\nwhy', 'hang', 'banner', 'bridge', 'make', 'worse', 'view', 'might', 'make', 'bite', 'annoy', 'keep', 'quiet', 'im', 'sure', 'matter', 'much', 'context', 'discussion'])\n",
      "original document: \n",
      "['You', \"can't\", 'just', 'run', 'stage', '4', 'on', 'LR', 'Goku', 'event', 'to', 'get', 'dupes?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'run', 'stag', 'four', 'lr', 'goku', 'ev', 'get', 'dup'], ['cant', 'run', 'stage', 'four', 'lr', 'goku', 'event', 'get', 'dupe'])\n",
      "original document: \n",
      "['rip', 'lsu']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rip', 'lsu'], ['rip', 'lsu'])\n",
      "original document: \n",
      "['Not', 'gonna', 'lie,', 'kinda', 'looks', 'like', 'Winnie', 'the', 'Pooh', 'with', 'bigger', 'ears']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gonn', 'lie', 'kind', 'look', 'lik', 'winny', 'pooh', 'big', 'ear'], ['gonna', 'lie', 'kinda', 'look', 'like', 'winnie', 'pooh', 'bigger', 'ears'])\n",
      "original document: \n",
      "['Hail', \"Mary's?\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hail', 'mary'], ['hail', 'marys'])\n",
      "original document: \n",
      "['Good', 'points.', 'As', 'bad', 'as', 'they', 'handled', 'Oliver', 'and', \"Laurel's\", 'relationship,', 'it', 'could', 'have', 'been', 'much', 'worse.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'point', 'bad', 'handl', 'ol', 'laurel', 'rel', 'could', 'much', 'wors'], ['good', 'point', 'bad', 'handle', 'oliver', 'laurels', 'relationship', 'could', 'much', 'worse'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[+Llim](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoblhz/):\\n\\nWhat', 'are', 'giraffes', 'even', 'trying', 'to', 'do?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['llimhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoblhz\\n\\nwhat', 'giraff', 'ev', 'try'], ['llimhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoblhz\\n\\nwhat', 'giraffes', 'even', 'try'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['&gt;', 'So', 'in', 'your', 'mind,', 'it', 'would', 'be', 'more', 'humane', 'to', 'simply', 'eradicate', 'the', 'species', 'in', 'what', 'tantamounts', 'to', 'genocide,', 'than', 'to', 'simply', 'continue', 'sheering', 'them', 'as', \"we've\", 'done', 'for', 'millennia.\\n&gt;', 'For', 'somebody', 'pretending', 'to', 'love', 'animals,', 'you', 'sure', 'seem', 'to', 'want', 'them', 'dead.\\n\\nI', 'will', 'never', 'not', 'be', 'surprised', 'by', 'how', 'stupid', 'the', 'mental', 'gymanstics', 'people', 'like', 'you', 'will', 'come', 'up', 'with!', 'Ha!\\n\\nNext', 'up', 'wanting', 'to', 'ban', 'bullfighting', 'is', 'genocide', 'to', 'Spanish', 'Bullfighting', 'Bulls,', 'wanting', 'to', 'ban', 'pugs', 'is', 'genocide', 'to', 'pugs,', 'wanting', 'to', 'ban', 'puppy', 'mills', 'is', 'also', 'genocide', 'too!\\n\\nYay', 'I', 'can', 'play', 'this']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'mind', 'would', 'hum', 'simply', 'erad', 'specy', 'tantamount', 'genocid', 'simply', 'continu', 'she', 'wev', 'don', 'millennia\\ngt', 'somebody', 'pretend', 'lov', 'anim', 'sur', 'seem', 'want', 'dead\\n\\ni', 'nev', 'surpr', 'stupid', 'ment', 'gymanst', 'peopl', 'lik', 'com', 'ha\\n\\nnext', 'want', 'ban', 'bullfight', 'genocid', 'span', 'bullfight', 'bul', 'want', 'ban', 'pug', 'genocid', 'pug', 'want', 'ban', 'puppy', 'mil', 'also', 'genocid', 'too\\n\\nyay', 'play'], ['gt', 'mind', 'would', 'humane', 'simply', 'eradicate', 'species', 'tantamounts', 'genocide', 'simply', 'continue', 'sheer', 'weve', 'do', 'millennia\\ngt', 'somebody', 'pretend', 'love', 'animals', 'sure', 'seem', 'want', 'dead\\n\\ni', 'never', 'surprise', 'stupid', 'mental', 'gymanstics', 'people', 'like', 'come', 'ha\\n\\nnext', 'want', 'ban', 'bullfighting', 'genocide', 'spanish', 'bullfighting', 'bull', 'want', 'ban', 'pugs', 'genocide', 'pugs', 'want', 'ban', 'puppy', 'mill', 'also', 'genocide', 'too\\n\\nyay', 'play'])\n",
      "original document: \n",
      "['I', 'just', 'said', 'that', 'I', 'liked', 'her.', \"\\n\\nIt's\", 'easier', 'if', 'you', 'have', 'a', 'crush', 'on', 'someone', 'you', 'already', 'know,', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'lik', '\\n\\nits', 'easy', 'crush', 'someon', 'already', 'know', 'though'], ['say', 'like', '\\n\\nits', 'easier', 'crush', 'someone', 'already', 'know', 'though'])\n",
      "original document: \n",
      "['Central', 'air?\\n\\nHello', 'Azrael.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cent', 'air\\n\\nhello', 'azrael'], ['central', 'air\\n\\nhello', 'azrael'])\n",
      "original document: \n",
      "['I', 'bet', 'it', 'will', 'leak', 'in', 'a', 'few', 'weeks', 'about', 'how', 'Trump', \"didn't\", 'know', 'the', 'United', 'States', 'was', 'responsible', 'for', 'Puerto', 'Rico.', 'And', '*then*', 'his', 'supporters', 'will', 'defend', 'that', 'revelation', 'as', '\"SO', 'ALPHA.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'leak', 'week', 'trump', 'didnt', 'know', 'unit', 'stat', 'respons', 'puerto', 'rico', 'support', 'defend', 'revel', 'alph'], ['bet', 'leak', 'weeks', 'trump', 'didnt', 'know', 'unite', 'state', 'responsible', 'puerto', 'rico', 'supporters', 'defend', 'revelation', 'alpha'])\n",
      "original document: \n",
      "['Nothing', 'has', 'been', 'confirmed', 'about', 'celebrity', 'big', 'brother', 'outside', 'of', 'the', 'fact', 'that', 'it’s', 'happening.', 'Everything', 'going', 'around', 'about', 'potential', 'houseguests', 'has', 'come', 'from', 'completely', 'unreliable', 'sources.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noth', 'confirm', 'celebr', 'big', 'broth', 'outsid', 'fact', 'hap', 'everyth', 'going', 'around', 'pot', 'houseguest', 'com', 'complet', 'unrely', 'sourc'], ['nothing', 'confirm', 'celebrity', 'big', 'brother', 'outside', 'fact', 'happen', 'everything', 'go', 'around', 'potential', 'houseguests', 'come', 'completely', 'unreliable', 'source'])\n",
      "original document: \n",
      "[\"I'm\", 'just', 'going', 'to', 'assume', \"you're\", 'trying', 'to', '\"troll\",', 'so', 'have', 'a', 'nice', 'day.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'going', 'assum', 'yo', 'try', 'trol', 'nic', 'day'], ['im', 'go', 'assume', 'youre', 'try', 'troll', 'nice', 'day'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['What', 'are', 'the', 'minimum', 'requirements', 'to', 'complete', 'the', '7th', 'tier', 'battle', 'gear', 'wise?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['minim', 'requir', 'complet', '7th', 'tier', 'battl', 'gear', 'wis'], ['minimum', 'requirements', 'complete', '7th', 'tier', 'battle', 'gear', 'wise'])\n",
      "original document: \n",
      "['That', 'same', 'story', 'is', 'true', 'of', 'a', 'whole', 'lot', 'of', 'Americans', 'too.', 'Hey,', \"I'm\", 'poor,', 'I', 'live', 'on', 'benefits', 'but', 'my', 'rent', 'is', '$3', 'a', 'month', 'and', 'I', 'can', 'cheat', 'like', 'crazy', 'on', 'my', 'Section', '8', 'housing!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['story', 'tru', 'whol', 'lot', 'am', 'hey', 'im', 'poor', 'liv', 'benefit', 'rent', 'three', 'mon', 'che', 'lik', 'crazy', 'sect', 'eight', 'hous'], ['story', 'true', 'whole', 'lot', 'americans', 'hey', 'im', 'poor', 'live', 'benefit', 'rent', 'three', 'month', 'cheat', 'like', 'crazy', 'section', 'eight', 'house'])\n",
      "original document: \n",
      "['Cat.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cat'], ['cat'])\n",
      "original document: \n",
      "[\"There's\", 'no', 'real', 'way', 'to', 'compare', 'them.', '', \"I'd\", 'assume', 'that', 'Cell', 'is', 'significantly', 'stronger', 'when', 'Ganos', 'first', 'transforms,', 'but', 'given', 'enough', 'time,', 'Ganos', 'could', 'surpass', 'him', 'assuming', 'that', 'his', 'strength', 'would', 'keep', 'increasing.', '(hence', 'why', 'Roshi', 'took', 'him', 'out', 'ASAP)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'real', 'way', 'comp', 'id', 'assum', 'cel', 'sign', 'stronger', 'gano', 'first', 'transform', 'giv', 'enough', 'tim', 'gano', 'could', 'surpass', 'assum', 'strength', 'would', 'keep', 'increas', 'hent', 'rosh', 'took', 'asap'], ['theres', 'real', 'way', 'compare', 'id', 'assume', 'cell', 'significantly', 'stronger', 'ganos', 'first', 'transform', 'give', 'enough', 'time', 'ganos', 'could', 'surpass', 'assume', 'strength', 'would', 'keep', 'increase', 'hence', 'roshi', 'take', 'asap'])\n",
      "original document: \n",
      "['Well', 'everything', 'is', 'entirely', 'dependent', 'on', 'your', 'situation.', 'I', 'just', 'got', 'done', 'reading', 'through', 'this', 'thread', 'and', 'a', '3.9', 'GPA', 'is', 'extremely', 'strong', 'for', 'a', 'community', 'college.', 'Most', \"CC's(Community\", \"College's)\", 'will', 'have', 'you', 'take', 'a', 'placement', 'tests', 'and', 'that', 'will', 'determine', 'which', 'class', 'levels', 'to', 'take.', 'In', 'an', 'ideal', 'world', 'you', 'would', 'have', 'a', 'good', 'chunk', 'of', 'change', 'saved', 'up', 'for', 'life', 'after', 'the', 'corps', 'and', 'use', 'that', 'to', 'pay', 'for', 'CC', 'Courses', 'and', 'you', 'would', 'crash', 'at', 'your', 'parents', 'place.', 'Once', 'your', 'done', 'with', 'your', 'Core', 'Classes', 'and', 'begin', 'your', 'transfer', 'to', 'a', 'University', 'and', 'start', 'using', 'your', 'GI', 'bill.', 'If', 'you', 'apply', 'for', 'FAFSA', 'you', 'can', 'also', 'opt', 'to', 'do', 'a', 'work', 'study', 'which', 'gives', 'you', 'a', 'chance', 'to', 'work', '&lt;20', 'hours', 'per', 'week', 'at', 'the', 'University', 'as', 'a', 'side', 'job.', 'Being', 'a', 'MCWIS', 'you', 'should', 'be', 'able', 'to', 'land', 'a', 'life', 'guard', 'job', 'no', 'problem.', '\\n\\n\\nPaying', 'for', 'Community', 'College', 'with', 'the', 'GI', 'Bill', 'can', 'seem', 'like', 'a', 'waste', 'if', 'you', 'can', 'pay', 'for', 'it', 'out', 'of', 'pocket.', 'But', 'if', 'you', 'dont', 'have', 'a', 'choice', 'then', 'it', 'can', 'be', 'a', 'lifesaver.\\n\\nNot', 'to', 'get', 'too', 'personal', 'but', 'the', 'more', 'info', 'you', 'give', 'about', 'your', 'situation', 'the', 'better', 'we', 'can', 'help', 'you', 'out', 'brother.', 'For', 'example', 'if', 'you', 'dont', 'have', 'dependents', 'then', 'you', 'will', 'have', 'a', 'greater', 'degree', 'in', 'freedom,', 'what', 'state', 'your', 'from/', 'plan', 'to', 'go', 'to', 'school', 'in,', 'or', 'if', 'Grad', 'school', 'is', 'something', 'your', 'looking', 'into', 'for', 'the', 'future?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'everyth', 'entir', 'depend', 'situ', 'got', 'don', 'read', 'thread', 'thirty-nine', 'gpa', 'extrem', 'strong', 'commun', 'colleg', 'ccscommunity', 'colleg', 'tak', 'plac', 'test', 'determin', 'class', 'level', 'tak', 'id', 'world', 'would', 'good', 'chunk', 'chang', 'sav', 'lif', 'corp', 'us', 'pay', 'cc', 'cours', 'would', 'crash', 'par', 'plac', 'don', 'cor', 'class', 'begin', 'transf', 'univers', 'start', 'us', 'gi', 'bil', 'apply', 'fafs', 'also', 'opt', 'work', 'study', 'giv', 'chant', 'work', 'lt20', 'hour', 'per', 'week', 'univers', 'sid', 'job', 'mcwis', 'abl', 'land', 'lif', 'guard', 'job', 'problem', '\\n\\n\\npaying', 'commun', 'colleg', 'gi', 'bil', 'seem', 'lik', 'wast', 'pay', 'pocket', 'dont', 'cho', 'lifesaver\\n\\nnot', 'get', 'person', 'info', 'giv', 'situ', 'bet', 'help', 'broth', 'exampl', 'dont', 'depend', 'gre', 'degr', 'freedom', 'stat', 'plan', 'go', 'school', 'grad', 'school', 'someth', 'look', 'fut'], ['well', 'everything', 'entirely', 'dependent', 'situation', 'get', 'do', 'read', 'thread', 'thirty-nine', 'gpa', 'extremely', 'strong', 'community', 'college', 'ccscommunity', 'colleges', 'take', 'placement', 'test', 'determine', 'class', 'level', 'take', 'ideal', 'world', 'would', 'good', 'chunk', 'change', 'save', 'life', 'corps', 'use', 'pay', 'cc', 'course', 'would', 'crash', 'parent', 'place', 'do', 'core', 'class', 'begin', 'transfer', 'university', 'start', 'use', 'gi', 'bill', 'apply', 'fafsa', 'also', 'opt', 'work', 'study', 'give', 'chance', 'work', 'lt20', 'hours', 'per', 'week', 'university', 'side', 'job', 'mcwis', 'able', 'land', 'life', 'guard', 'job', 'problem', '\\n\\n\\npaying', 'community', 'college', 'gi', 'bill', 'seem', 'like', 'waste', 'pay', 'pocket', 'dont', 'choice', 'lifesaver\\n\\nnot', 'get', 'personal', 'info', 'give', 'situation', 'better', 'help', 'brother', 'example', 'dont', 'dependents', 'greater', 'degree', 'freedom', 'state', 'plan', 'go', 'school', 'grad', 'school', 'something', 'look', 'future'])\n",
      "original document: \n",
      "['Yeah', \"that'd\", 'be', 'the', 'easier', 'way', 'to', 'explain', 'it.', \"Didn't\", 'think', 'to', 'say', 'it', 'like', 'that', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'thatd', 'easy', 'way', 'explain', 'didnt', 'think', 'say', 'lik', 'lol'], ['yeah', 'thatd', 'easier', 'way', 'explain', 'didnt', 'think', 'say', 'like', 'lol'])\n",
      "original document: \n",
      "['No', 'problem.', 'I', 'would', 'ask', 'AFPC', 'if', 'it', 'had', 'to', 'be', 'signed', 'by', 'your', 'commander.', 'If', 'they', 'say', 'yes,', 'then', 'you', 'could', 'talk', 'to', 'the', 'IG', 'or', 'legal,', 'I', 'have', 'no', 'idea', 'whom,', 'but', 'there', 'should', 'be', 'a', 'way', 'around', 'it.', 'Your', 'commander', \"doesn't\", 'have', 'to', 'approve', 'it,', 'hence', 'the', 'approval/disapproval', 'format', 'of', 'the', 'memo.', 'You', 'just', 'need', 'his', 'signature', 'stating', 'whether', 'he', 'does', 'or', 'not.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'would', 'ask', 'afpc', 'sign', 'command', 'say', 'ye', 'could', 'talk', 'ig', 'leg', 'ide', 'way', 'around', 'command', 'doesnt', 'approv', 'hent', 'approvaldisapprov', 'form', 'memo', 'nee', 'sign', 'stat', 'wheth'], ['problem', 'would', 'ask', 'afpc', 'sign', 'commander', 'say', 'yes', 'could', 'talk', 'ig', 'legal', 'idea', 'way', 'around', 'commander', 'doesnt', 'approve', 'hence', 'approvaldisapproval', 'format', 'memo', 'need', 'signature', 'state', 'whether'])\n",
      "original document: \n",
      "['143413367|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'msELbNeP)\\n\\n&gt;&gt;143412250', '(OP)\\nTrump\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and sixty-seven', 'gt', 'unit', 'stat', 'anonym', 'id', 'mselbnep\\n\\ngtgt143412250', 'op\\ntrump\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and sixty-seven', 'gt', 'unite', 'state', 'anonymous', 'id', 'mselbnep\\n\\ngtgt143412250', 'op\\ntrump\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['well', 'he', 'applied', 'and', 'will', 'be', 'working', 'in', 'Chicago', 'proper', 'so', 'idk', '😶']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'apply', 'work', 'chicago', 'prop', 'idk'], ['well', 'apply', 'work', 'chicago', 'proper', 'idk'])\n",
      "original document: \n",
      "['All', '6', 'gear', 'slots', 'have', 'to', 'be', 'classified.', 'To', 'unlock', 'the', '6th', 'talent.', 'Sorry', 'to', 'burst', 'your', 'bubble']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['six', 'gear', 'slot', 'class', 'unlock', '6th', 'tal', 'sorry', 'burst', 'bubbl'], ['six', 'gear', 'slot', 'classify', 'unlock', '6th', 'talent', 'sorry', 'burst', 'bubble'])\n",
      "original document: \n",
      "['Alright,', 'cool.', 'And', 'thank', 'you', 'for', 'doing', 'this', 'for', 'us', 'poor', 'IB', 'kids!', ':D', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alright', 'cool', 'thank', 'us', 'poor', 'ib', 'kid'], ['alright', 'cool', 'thank', 'us', 'poor', 'ib', 'kid'])\n",
      "original document: \n",
      "['Great', 'taste!', '', 'The', 'General', 'Lee', 'is', 'always', 'awesome', 'when', 'done', 'right!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'tast', 'gen', 'lee', 'alway', 'awesom', 'don', 'right'], ['great', 'taste', 'general', 'lee', 'always', 'awesome', 'do', 'right'])\n",
      "original document: \n",
      "['They', 'surely', 'got', 'way', 'more', 'money', 'from', 'SCR', 'than', 'what', 'they', 'invested', 'into', 'retexturing.', 'They', 'need', 'to', 'hire', 'more', 'people', 'to', 'work', 'on', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'got', 'way', 'money', 'scr', 'invest', 'retext', 'nee', 'hir', 'peopl', 'work'], ['surely', 'get', 'way', 'money', 'scr', 'invest', 'retexturing', 'need', 'hire', 'people', 'work'])\n",
      "original document: \n",
      "['When', 'your', 'low', 'is', '80', 'and', 'your', 'high', 'is', '100,', 'opening', 'windows', 'at', 'night', 'may', 'not', 'be', 'useful.', 'If', 'you', 'have', 'a', 'two', 'story', 'house,', 'the', 'upper', 'area', 'will', 'be', 'warmer', 'at', 'least', '10', 'degrees.', 'Running', 'fans', 'while', \"you're\", 'in', 'the', 'room', 'will', 'enhance', 'cooling;', 'you', 'will', 'want', 'to', 'turn', 'the', 'fans', 'off', 'when', 'the', 'room', 'is', 'empty,', 'because', \"it's\", 'wasting', 'electricity.', '\\nWhich', 'part', 'of', 'Texas', 'are', 'you', 'in?', 'I', 'mainly', 'lived', 'in', 'DFW,', 'before', 'I', 'escaped.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['low', 'eighty', 'high', 'one hundred', 'op', 'window', 'night', 'may', 'us', 'two', 'story', 'hous', 'up', 'are', 'warm', 'least', 'ten', 'degr', 'run', 'fan', 'yo', 'room', 'enh', 'cool', 'want', 'turn', 'fan', 'room', 'empty', 'wast', 'elect', '\\nwhich', 'part', 'texa', 'main', 'liv', 'dfw', 'escap'], ['low', 'eighty', 'high', 'one hundred', 'open', 'windows', 'night', 'may', 'useful', 'two', 'story', 'house', 'upper', 'area', 'warmer', 'least', 'ten', 'degrees', 'run', 'fan', 'youre', 'room', 'enhance', 'cool', 'want', 'turn', 'fan', 'room', 'empty', 'waste', 'electricity', '\\nwhich', 'part', 'texas', 'mainly', 'live', 'dfw', 'escape'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['That', \"doesn't\", 'follow.', 'The', 'construction', 'workers', 'could', 'work', 'a', 'little', 'faster', 'but', \"they're\", 'still', 'restricted', 'by', 'their', 'materials.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'follow', 'construct', 'work', 'could', 'work', 'littl', 'fast', 'theyr', 'stil', 'restrict', 'mat'], ['doesnt', 'follow', 'construction', 'workers', 'could', 'work', 'little', 'faster', 'theyre', 'still', 'restrict', 'materials'])\n",
      "original document: \n",
      "['Other', 'than', 'Watergate,', 'Nixon', 'was', 'actually', 'a', 'pretty', 'good', 'president.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['waterg', 'nixon', 'act', 'pretty', 'good', 'presid'], ['watergate', 'nixon', 'actually', 'pretty', 'good', 'president'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Hi', 'Guys!\\n\\nSorry', 'to', 'disappoint,', 'but', \"I'm\", 'not', 'KD.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'guys\\n\\nsorry', 'disappoint', 'im', 'kd'], ['hi', 'guys\\n\\nsorry', 'disappoint', 'im', 'kd'])\n",
      "original document: \n",
      "['DELET', 'THIS']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delet'])\n",
      "original document: \n",
      "['So', 'yeah.', 'Late', 'last', 'year', 'I', 'was', 'travelling', 'through', 'Japan', 'with', 'some', 'friends', 'of', 'mine', '(I', 'tend', 'to', 'goto', 'Japan', 'for', 'a', 'few', 'weeks', 'every', 'year)', '\\n\\nOn', 'this', 'occasion', 'I', 'met', 'this', 'lovely', '38', 'year', 'old', 'woman', 'named', 'Hiroko', 'at', 'a', 'cocktail', 'bar', 'in', 'Kyoto.', 'I', 'noticed', 'her', 'looking', 'my', 'way', 'a', 'few', 'times', 'but', 'I', \"couldn't\", 'tell', 'if', 'it', 'was', 'good', 'or', 'bad', 'because,', 'well', \"I'm\", 'a', 'bigger', 'guy', 'and', 'in', 'Japan', \"it's\", 'already', 'a', 'big', 'disadvantage', 'lol', '\\n\\nWith', 'the', 'power', 'of', 'a', 'little', 'liquid', 'courage', 'I', 'managed', 'to', 'walk', 'past', 'her', 'as', 'I', 'went', 'to', 'the', 'to', 'the', 'bar', 'and', 'as', 'I', 'walked', 'back', 'I', 'managed', 'to', 'make', 'a', 'little', 'joke', 'about', 'the', 'drink', 'she', 'had', '(wish', 'I', 'did', 'in', 'English,', 'that', \"could've\", 'been', 'a', 'waste', 'of', 'time', 'lol)', 'thankfully', 'she', 'saw', 'the', 'humor', 'to', 'it', 'and', 'asked', 'where', 'I', 'am', 'from', '\\n\\nSo', 'we', 'sat', 'and', 'spoke', 'for', 'a', 'bit,', 'found', 'out', 'she', 'actually', 'visited', 'my', 'city', 'with', 'her', 'now', 'ex', 'husband', 'a', 'few', 'years', 'ago.', 'So', 'we', 'already', 'had', 'some', 'common', 'ground.', 'In', 'the', 'middle', 'of', 'chatting', 'causally', 'we', 'were', 'making', 'cheeky', 'jabs', 'at', 'each', 'other', 'too', 'so', 'I', 'could', 'tell', 'there', 'was', 'something', 'there.', 'Her', 'girlfriends', 'arrived', 'and', 'so', 'we', 'exchanged', 'Line', 'IDs', 'and', 'went', 'our', 'seperate', 'ways.\\n\\nNow', 'I', 'didnt', 'think', 'this', 'would', 'go', 'anywhere,', 'because', 'I', 'was', 'going', 'to', 'Osaka', 'the', 'next', 'day.', 'But', 'we', 'spoke', 'for', 'a', 'couple', 'days', 'on', 'Line', 'before', 'she', 'asked', 'me', 'straight', 'up', 'if', 'I', 'wanted', 'to', 'spend', 'some', 'time', 'with', 'her', 'on', 'the', 'weekend.', 'Of', 'course', 'I', 'jumped', 'at', 'the', 'opportunity', 'and', 'booked', 'us', 'an', 'Airbnb.\\n\\nShe', 'caught', 'the', 'train', 'in', 'from', 'Kyoto', 'and', 'she', 'was', 'wearing', 'this', 'cute', 'little', 'outfit', 'that', 'and', 'a', 'little', 'bow', 'tie', 'lol', 'but', 'she', 'made', 'it', 'look', 'cute.', 'We', 'headed', 'back', 'the', 'Airbnb', 'where', 'wow', 'it', 'had', 'an', 'incredible', 'view', 'of', 'Osaka', 'Castle.', '\\n\\nWe', 'sat', 'down', 'on', 'the', 'couch', 'to', 'rest', 'our', 'feet', 'and', 'she', 'didnt', 'waste', 'much', 'time.', 'She', 'told', 'me', 'she', 'was', 'excited', 'because', 'she', 'had', 'never', 'been', 'with', 'a', '\"western', 'man\"', 'before', '(whether', 'thats', 'true', 'or', 'not,', 'i', 'dont', 'give', 'a', 'shit)', 'and', 'went', 'straight', 'to', 'unzipping', 'my', 'pants', 'and', 'reaching', 'into', 'grab', 'my', 'cock.', 'With', 'a', 'smile', 'on', 'her', 'face', 'she', 'looked', 'and', 'me', 'and', 'said', '\"very', 'big\"', '(now', 'for', 'record,', \"she's\", 'clear', 'fan', 'servicing.', 'Im', 'not', 'overly', 'big', 'in', 'that', 'area', 'at', 'all.', 'Probably', 'just', 'abit', 'over', 'average', 'in', 'length,', 'pretty', 'girthy', 'though)\\n\\nShe', 'told', 'me', 'stand', 'up', 'and', 'when', 'i', 'did', 'she', 'ripped', 'my', 'pants', 'down', 'and', 'got', 'to', 'her', 'knees', 'infront', 'of', 'me', 'and', 'started', 'service', 'my', 'cock.', 'Her', 'gorgeous', 'dark', 'eyes', 'staring', 'up', 'at', 'me', 'while', 'she', 'tried', 'her', 'best', 'to', 'deep', 'throat', 'my', 'cock', 'was', 'mind', 'blowing.', 'Not', 'only', 'this,', 'she', 'was', 'very', 'attentive', 'to', 'my', 'sensitive', 'spots', 'and', 'would', 'torture', 'me', 'with', 'them', 'and', 'giggle', 'at', 'the', 'sight', 'of', 'me', 'shaking.', '\\n\\nAfter', 'I', 'came', 'I', 'was', 'hard', 'again', 'a', 'few', 'minutes', 'later', 'and', 'ready', 'to', 'feel', 'her', 'little', 'Japanese', 'pussy.', 'She', 'layed', 'out', 'on', 'the', 'bed', 'and', 'I', 'got', 'the', 'tip', 'of', 'my', 'cock', 'slightly', 'in', 'before', 'she', 'pushed', 'me', 'back', 'and', 'told', 'me', 'no!', 'No..no?', 'I', 'couldnt', 'understand,', 'she', 'then', 'smirked', 'and', 'stood', 'up', 'and', 'whispered', 'in', 'my', 'earth', '\"you', 'must', 'wait\"', 'she', 'got', 'dressed', 'and', 'told', 'me', 'to', 'get', 'my', 'clothes', 'on', 'too', 'because', 'she', 'wanted', 'to', 'have', 'dinner.\\n\\nI', 'stood', 'there', 'with', 'my', 'throbbing', 'cock,', 'confused', 'and', 'aroused', 'haha', 'this', 'woman', 'was', 'teasing', 'the', 'fuck', 'out', 'of', 'me', 'and', 'I', 'loved', 'it.', 'So', 'we', 'went', 'back', 'out', 'for', 'a', 'couple', 'of', 'hours', 'and', 'had', 'dinner', 'before', 'heading', 'back', 'to', 'the', 'airbnb.', 'The', 'whole', 'time', 'all', 'I', 'could', 'think', 'about', 'we', 'fucking', 'her', 'brains', 'out,', 'I', 'think', 'this', 'is', 'exactly', 'what', 'she', 'wanted.\\n\\nWe', 'finally', 'get', 'back.', 'She', 'has', 'a', 'shower', 'straight', 'away', 'and', 'tells', 'me', 'to', 'have', 'one', 'after', 'her.', 'After', 'mine,', 'i', 'come', 'out', 'and', 'see', 'her', 'on', 'the', 'bed', 'wearing', 'thigh', 'highs', 'and', 'nothing', 'else.', 'They', 'were', 'pinching', 'in', 'her', 'skin', 'ever', 'so', 'much', 'that', 'it', 'caused', 'a', 'slight', 'dimple', 'in', 'the', 'skin,', 'so', 'hot!\\n\\nShe', 'sits', 'down', 'on', 'the', 'bed', 'and', 'gestures', 'me', 'over', 'and', 'stops', 'me', 'when', 'im', 'leaning', 'down', 'to', 'kiss', 'her.', 'She', 'puts', 'her', 'hand', 'on', 'my', 'head', 'and', 'pushes', 'me', 'down', 'so', 'i', 'am', 'level', 'with', 'her', 'pussy', 'and', 'demands', 'me', 'to', 'eat', 'it', 'and', 'you', 'bet', 'I', 'did.', 'Her', 'pussy', 'tasted', 'incredible', 'and', 'I', 'could', 'tell', 'from', 'sliding', 'a', 'finger', 'into', 'her', 'pussy', 'that', 'it', 'was', 'gonna', 'be', 'tough', 'when', 'my', 'cock', 'was', 'going', 'to', 'enter.\\n\\nAfter', 'willingly', 'complying', 'to', 'her', 'demands', 'and', 'eating', 'her', 'pussy', 'she', 'told', 'me', 'that', 'i', 'had', 'waited', 'long', 'enough', 'and', 'now', 'i', 'was', 'allowed', 'to', 'enter', 'her.', 'We', 'started', 'with', 'her', 'on', 'her', 'back', 'and', 'her', 'holding', 'her', 'legs', 'back.', 'It', 'took', 'abit', 'of', 'work', 'and', 'some', 'lube,', 'but', 'that', 'moment', 'of', 'penetration', 'where', 'her', 'eyes', 'shut', 'tight,', 'mouth', 'gasped,', 'head', 'back', 'and', 'hands', 'gripped', 'the', 'blanket', 'beneath', 'her', 'is', 'an', 'image', 'and', 'feeling', \"i'll\", 'never', 'forget.\\n\\nI', 'had', 'to', 'work', 'her', 'tight', 'pussy', 'slowly', 'to', 'begin', 'with', 'though', 'and', 'as', 'she', 'became', 'more', 'used', 'to', 'my', 'girth', 'she', 'started', 'to', 'push', 'against', 'me', 'and', 'as', 'my', 'cock', 'pushed', 'a', 'little', 'further', 'in', 'each', 'time', 'she', 'would', 'let', 'out', 'a', 'sigh.', 'When', 'we', 'finally', 'worked', 'it', 'in', 'and', 'we', 'could', 'build', 'up', 'a', 'good', 'rhythm', 'it', 'was', 'amazing.', 'This', 'gorgeous', 'Japanese', 'woman', 'taking', 'my', 'cock', 'and', 'shaking', 'and', 'one', 'put', 'she', 'was', 'moaning', 'so', 'hard', 'i', 'could', 'see', 'a', 'vessel', 'form', 'on', 'her', 'forehead', 'before', 'she', 'finally', 'gasped', 'for', 'air', 'and', 'let', 'go', 'of', 'the', 'blankets.\\n\\nAfter', 'seeing', 'her', 'let', 'go', 'i', 'decided', 'to', 'lean', 'down', 'and', 'scoop', 'her', 'up.', 'Her', 'legs', 'dangling', 'over', 'my', 'arms', 'and', 'i', 'held', 'onto', 'her', 'cute', 'little', 'ass.', 'She', 'put', 'her', 'arms', 'around', 'my', 'neck', 'and', 'held', 'on', 'while', 'I', 'pounded', 'fuck', 'out', 'of', 'her', 'pussy', 'but', 'she', 'could', 'only', 'handle', 'this', 'for', 'a', 'couple', 'of', 'minutes', 'because', 'it', 'was', 'beginning', 'to', 'hurt.\\n\\nSo', 'I', 'placed', 'her', 'back', 'onto', 'the', 'bed', 'on', 'all', 'fours', 'and', 'teased', 'her', 'pussy', 'and', 'clit', 'from', 'behind', 'with', 'my', 'cock', 'before', 'slowling', 're-entering', 'her', 'incredible', 'pussy.', 'With', 'her', 'thrust', 'in', 'she', 'would', 'push', 'back.', 'Her', 'ass', 'was', 'the', 'tight', 'little', 'thing', 'but', 'enough', 'meat', 'for', 'me', 'to', 'grab', 'a', 'handful', 'with', 'each', 'hand', 'and', 'pound', 'away', 'her.', '\\n\\nThere', 'was', 'a', 'moment', 'I', 'looked', 'up', 'and', 'realised', 'how', 'cool', 'everything', 'was', 'about', 'this', 'too', 'as', 'i', 'looked', 'out', 'the', 'window', 'and', 'saw', 'osaka', 'castle', 'lit', 'up', 'and', 'i', 'stopped', 'for', 'a', 'moment,', 'still', 'inside', 'her', 'and', 'told', 'her', 'to', 'look', 'too.', 'She', 'shared', 'the', 'same', 'enthusiasm', 'of', 'how', 'cool', 'this', 'experience', 'was.', 'We', 'went', 'back', 'to', 'push', 'each', 'others', 'sexual', 'limits', 'and', 'fucking', 'like', 'animals', 'for', 'the', 'rest', 'of', 'night.\\n\\nWe', 'lazed', 'around', 'in', 'bed', 'together', 'the', 'next', 'morning', 'before', 'we', 'had', 'a', 'couple', 'more', 'rounds', 'of', 'sex', 'and', 'then', 'check', 'out.', 'We', 'got', 'brunch.', 'Went', 'to', 'some', 'cool', 'bamboo', 'forest', 'and', 'shrines.', 'We', 'said', 'our', 'goodbyes', 'and', 'that', 'was', 'that.', 'We', 'still', 'keep', 'in', 'touch', 'but', 'we', 'are', 'both', 'busy.', 'Hopefully', 'when', \"I'm\", 'in', 'Japan', 'next', \"we'll\", 'be', 'able', 'to', 'hook', 'up', 'again', ':)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'lat', 'last', 'year', 'travel', 'jap', 'friend', 'min', 'tend', 'goto', 'jap', 'week', 'every', 'year', '\\n\\non', 'occas', 'met', 'lov', 'thirty-eight', 'year', 'old', 'wom', 'nam', 'hiroko', 'cocktail', 'bar', 'kyoto', 'not', 'look', 'way', 'tim', 'couldnt', 'tel', 'good', 'bad', 'wel', 'im', 'big', 'guy', 'jap', 'already', 'big', 'disadv', 'lol', '\\n\\nwith', 'pow', 'littl', 'liquid', 'cour', 'man', 'walk', 'past', 'went', 'bar', 'walk', 'back', 'man', 'mak', 'littl', 'jok', 'drink', 'wish', 'engl', 'couldv', 'wast', 'tim', 'lol', 'thank', 'saw', 'hum', 'ask', '\\n\\nso', 'sat', 'spok', 'bit', 'found', 'act', 'visit', 'city', 'ex', 'husband', 'year', 'ago', 'already', 'common', 'ground', 'middl', 'chat', 'caus', 'mak', 'cheeky', 'jab', 'could', 'tel', 'someth', 'girlfriend', 'ar', 'exchang', 'lin', 'id', 'went', 'sep', 'ways\\n\\nnow', 'didnt', 'think', 'would', 'go', 'anywh', 'going', 'osak', 'next', 'day', 'spok', 'coupl', 'day', 'lin', 'ask', 'straight', 'want', 'spend', 'tim', 'weekend', 'cours', 'jump', 'opportun', 'book', 'us', 'airbnb\\n\\nshe', 'caught', 'train', 'kyoto', 'wear', 'cut', 'littl', 'outfit', 'littl', 'bow', 'tie', 'lol', 'mad', 'look', 'cut', 'head', 'back', 'airbnb', 'wow', 'incred', 'view', 'osak', 'castl', '\\n\\nwe', 'sat', 'couch', 'rest', 'feet', 'didnt', 'wast', 'much', 'tim', 'told', 'excit', 'nev', 'western', 'man', 'wheth', 'that', 'tru', 'dont', 'giv', 'shit', 'went', 'straight', 'unzip', 'pant', 'reach', 'grab', 'cock', 'smil', 'fac', 'look', 'said', 'big', 'record', 'she', 'clear', 'fan', 'serv', 'im', 'ov', 'big', 'are', 'prob', 'abit', 'av', 'leng', 'pretty', 'girthy', 'though\\n\\nshe', 'told', 'stand', 'rip', 'pant', 'got', 'kne', 'infront', 'start', 'serv', 'cock', 'gorg', 'dark', 'ey', 'star', 'tri', 'best', 'deep', 'throat', 'cock', 'mind', 'blow', 'at', 'sensit', 'spot', 'would', 'tort', 'giggl', 'sight', 'shak', '\\n\\nafter', 'cam', 'hard', 'minut', 'lat', 'ready', 'feel', 'littl', 'japanes', 'pussy', 'lay', 'bed', 'got', 'tip', 'cock', 'slight', 'push', 'back', 'told', 'nono', 'couldnt', 'understand', 'smirk', 'stood', 'whisp', 'ear', 'must', 'wait', 'got', 'dress', 'told', 'get', 'cloth', 'want', 'dinner\\n\\ni', 'stood', 'throbbing', 'cock', 'confus', 'ar', 'hah', 'wom', 'teas', 'fuck', 'lov', 'went', 'back', 'coupl', 'hour', 'din', 'head', 'back', 'airbnb', 'whol', 'tim', 'could', 'think', 'fuck', 'brain', 'think', 'exact', 'wanted\\n\\nwe', 'fin', 'get', 'back', 'show', 'straight', 'away', 'tel', 'on', 'min', 'com', 'see', 'bed', 'wear', 'thigh', 'high', 'noth', 'els', 'pinch', 'skin', 'ev', 'much', 'caus', 'slight', 'dimpl', 'skin', 'hot\\n\\nshe', 'sit', 'bed', 'gest', 'stop', 'im', 'lean', 'kiss', 'put', 'hand', 'head', 'push', 'level', 'pussy', 'demand', 'eat', 'bet', 'pussy', 'tast', 'incred', 'could', 'tel', 'slid', 'fing', 'pussy', 'gonn', 'tough', 'cock', 'going', 'enter\\n\\naft', 'wil', 'comply', 'demand', 'eat', 'pussy', 'told', 'wait', 'long', 'enough', 'allow', 'ent', 'start', 'back', 'hold', 'leg', 'back', 'took', 'abit', 'work', 'lub', 'mom', 'penet', 'ey', 'shut', 'tight', 'mou', 'gasp', 'head', 'back', 'hand', 'grip', 'blanket', 'benea', 'im', 'feel', 'il', 'nev', 'forget\\n\\ni', 'work', 'tight', 'pussy', 'slow', 'begin', 'though', 'becam', 'us', 'gir', 'start', 'push', 'cock', 'push', 'littl', 'tim', 'would', 'let', 'sigh', 'fin', 'work', 'could', 'build', 'good', 'rhythm', 'amaz', 'gorg', 'japanes', 'wom', 'tak', 'cock', 'shak', 'on', 'put', 'moan', 'hard', 'could', 'see', 'vessel', 'form', 'forehead', 'fin', 'gasp', 'air', 'let', 'go', 'blankets\\n\\nafter', 'see', 'let', 'go', 'decid', 'lean', 'scoop', 'leg', 'dangl', 'arm', 'held', 'onto', 'cut', 'littl', 'ass', 'put', 'arm', 'around', 'neck', 'held', 'pound', 'fuck', 'pussy', 'could', 'handl', 'coupl', 'minut', 'begin', 'hurt\\n\\nso', 'plac', 'back', 'onto', 'bed', 'four', 'teas', 'pussy', 'clit', 'behind', 'cock', 'slowl', 'reent', 'incred', 'pussy', 'thrust', 'would', 'push', 'back', 'ass', 'tight', 'littl', 'thing', 'enough', 'meat', 'grab', 'hand', 'hand', 'pound', 'away', '\\n\\nthere', 'mom', 'look', 'real', 'cool', 'everyth', 'look', 'window', 'saw', 'osak', 'castl', 'lit', 'stop', 'mom', 'stil', 'insid', 'told', 'look', 'shar', 'enthusiasm', 'cool', 'expery', 'went', 'back', 'push', 'oth', 'sex', 'limit', 'fuck', 'lik', 'anim', 'rest', 'night\\n\\nwe', 'laz', 'around', 'bed', 'togeth', 'next', 'morn', 'coupl', 'round', 'sex', 'check', 'got', 'brunch', 'went', 'cool', 'bamboo', 'forest', 'shrines', 'said', 'goodby', 'stil', 'keep', 'touch', 'busy', 'hop', 'im', 'jap', 'next', 'wel', 'abl', 'hook'], ['yeah', 'late', 'last', 'year', 'travel', 'japan', 'friends', 'mine', 'tend', 'goto', 'japan', 'weeks', 'every', 'year', '\\n\\non', 'occasion', 'meet', 'lovely', 'thirty-eight', 'year', 'old', 'woman', 'name', 'hiroko', 'cocktail', 'bar', 'kyoto', 'notice', 'look', 'way', 'time', 'couldnt', 'tell', 'good', 'bad', 'well', 'im', 'bigger', 'guy', 'japan', 'already', 'big', 'disadvantage', 'lol', '\\n\\nwith', 'power', 'little', 'liquid', 'courage', 'manage', 'walk', 'past', 'go', 'bar', 'walk', 'back', 'manage', 'make', 'little', 'joke', 'drink', 'wish', 'english', 'couldve', 'waste', 'time', 'lol', 'thankfully', 'saw', 'humor', 'ask', '\\n\\nso', 'sit', 'speak', 'bite', 'find', 'actually', 'visit', 'city', 'ex', 'husband', 'years', 'ago', 'already', 'common', 'grind', 'middle', 'chat', 'causally', 'make', 'cheeky', 'jab', 'could', 'tell', 'something', 'girlfriends', 'arrive', 'exchange', 'line', 'ids', 'go', 'seperate', 'ways\\n\\nnow', 'didnt', 'think', 'would', 'go', 'anywhere', 'go', 'osaka', 'next', 'day', 'speak', 'couple', 'days', 'line', 'ask', 'straight', 'want', 'spend', 'time', 'weekend', 'course', 'jump', 'opportunity', 'book', 'us', 'airbnb\\n\\nshe', 'catch', 'train', 'kyoto', 'wear', 'cute', 'little', 'outfit', 'little', 'bow', 'tie', 'lol', 'make', 'look', 'cute', 'head', 'back', 'airbnb', 'wow', 'incredible', 'view', 'osaka', 'castle', '\\n\\nwe', 'sit', 'couch', 'rest', 'feet', 'didnt', 'waste', 'much', 'time', 'tell', 'excite', 'never', 'western', 'man', 'whether', 'thats', 'true', 'dont', 'give', 'shit', 'go', 'straight', 'unzip', 'pant', 'reach', 'grab', 'cock', 'smile', 'face', 'look', 'say', 'big', 'record', 'shes', 'clear', 'fan', 'service', 'im', 'overly', 'big', 'area', 'probably', 'abit', 'average', 'length', 'pretty', 'girthy', 'though\\n\\nshe', 'tell', 'stand', 'rip', 'pant', 'get', 'knees', 'infront', 'start', 'service', 'cock', 'gorgeous', 'dark', 'eye', 'star', 'try', 'best', 'deep', 'throat', 'cock', 'mind', 'blow', 'attentive', 'sensitive', 'spot', 'would', 'torture', 'giggle', 'sight', 'shake', '\\n\\nafter', 'come', 'hard', 'minutes', 'later', 'ready', 'feel', 'little', 'japanese', 'pussy', 'lay', 'bed', 'get', 'tip', 'cock', 'slightly', 'push', 'back', 'tell', 'nono', 'couldnt', 'understand', 'smirk', 'stand', 'whisper', 'earth', 'must', 'wait', 'get', 'dress', 'tell', 'get', 'clothe', 'want', 'dinner\\n\\ni', 'stand', 'throb', 'cock', 'confuse', 'arouse', 'haha', 'woman', 'tease', 'fuck', 'love', 'go', 'back', 'couple', 'hours', 'dinner', 'head', 'back', 'airbnb', 'whole', 'time', 'could', 'think', 'fuck', 'brain', 'think', 'exactly', 'wanted\\n\\nwe', 'finally', 'get', 'back', 'shower', 'straight', 'away', 'tell', 'one', 'mine', 'come', 'see', 'bed', 'wear', 'thigh', 'highs', 'nothing', 'else', 'pinch', 'skin', 'ever', 'much', 'cause', 'slight', 'dimple', 'skin', 'hot\\n\\nshe', 'sit', 'bed', 'gesture', 'stop', 'im', 'lean', 'kiss', 'put', 'hand', 'head', 'push', 'level', 'pussy', 'demand', 'eat', 'bet', 'pussy', 'taste', 'incredible', 'could', 'tell', 'slide', 'finger', 'pussy', 'gonna', 'tough', 'cock', 'go', 'enter\\n\\nafter', 'willingly', 'comply', 'demand', 'eat', 'pussy', 'tell', 'wait', 'long', 'enough', 'allow', 'enter', 'start', 'back', 'hold', 'legs', 'back', 'take', 'abit', 'work', 'lube', 'moment', 'penetration', 'eye', 'shut', 'tight', 'mouth', 'gasp', 'head', 'back', 'hand', 'grip', 'blanket', 'beneath', 'image', 'feel', 'ill', 'never', 'forget\\n\\ni', 'work', 'tight', 'pussy', 'slowly', 'begin', 'though', 'become', 'use', 'girth', 'start', 'push', 'cock', 'push', 'little', 'time', 'would', 'let', 'sigh', 'finally', 'work', 'could', 'build', 'good', 'rhythm', 'amaze', 'gorgeous', 'japanese', 'woman', 'take', 'cock', 'shake', 'one', 'put', 'moan', 'hard', 'could', 'see', 'vessel', 'form', 'forehead', 'finally', 'gasp', 'air', 'let', 'go', 'blankets\\n\\nafter', 'see', 'let', 'go', 'decide', 'lean', 'scoop', 'legs', 'dangle', 'arm', 'hold', 'onto', 'cute', 'little', 'ass', 'put', 'arm', 'around', 'neck', 'hold', 'pound', 'fuck', 'pussy', 'could', 'handle', 'couple', 'minutes', 'begin', 'hurt\\n\\nso', 'place', 'back', 'onto', 'bed', 'fours', 'tease', 'pussy', 'clit', 'behind', 'cock', 'slowling', 'reentering', 'incredible', 'pussy', 'thrust', 'would', 'push', 'back', 'ass', 'tight', 'little', 'thing', 'enough', 'meat', 'grab', 'handful', 'hand', 'pound', 'away', '\\n\\nthere', 'moment', 'look', 'realise', 'cool', 'everything', 'look', 'window', 'saw', 'osaka', 'castle', 'light', 'stop', 'moment', 'still', 'inside', 'tell', 'look', 'share', 'enthusiasm', 'cool', 'experience', 'go', 'back', 'push', 'others', 'sexual', 'limit', 'fuck', 'like', 'animals', 'rest', 'night\\n\\nwe', 'laze', 'around', 'bed', 'together', 'next', 'morning', 'couple', 'round', 'sex', 'check', 'get', 'brunch', 'go', 'cool', 'bamboo', 'forest', 'shrine', 'say', 'goodbyes', 'still', 'keep', 'touch', 'busy', 'hopefully', 'im', 'japan', 'next', 'well', 'able', 'hook'])\n",
      "original document: \n",
      "[\"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnocfl4/):\\n\\nI'm\", 'not', 'sure,', 'but', \"I'm\", 'glad', \"they're\", 'trying', 'to', 'do', 'it.', '\\n\\nFun', 'fact:', 'I', 'got', 'to', 'feed', 'giraffes', 'once', 'and', 'I', 'cried']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnocfl4\\n\\nim', 'sur', 'im', 'glad', 'theyr', 'try', '\\n\\nfun', 'fact', 'got', 'fee', 'giraff', 'cri'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnocfl4\\n\\nim', 'sure', 'im', 'glad', 'theyre', 'try', '\\n\\nfun', 'fact', 'get', 'fee', 'giraffes', 'cry'])\n",
      "original document: \n",
      "[\"Wasn't\", 'there', 'a', 'thread', 'with', 'the', 'opposite', 'happening?', 'The', \"poster's\", 'dog', 'kept', 'attacking', 'some', 'other', \"guy's\", 'dog,', 'and', 'that', 'owner', 'kicked', 'the', 'attacking', 'dog', 'and', 'the', 'poster', 'was', 'indignant', 'that', 'someone', 'would', 'dare', 'to', 'try', 'to', 'prevent', 'his', 'dog', 'from', 'attacking', 'another', 'one,', 'because', 'it', 'was', 'just', 'playing', 'or', 'whatnot.\\n', '', '\\nAnyway,', 'the', 'reality', 'is', 'that', 'while', 'dogs', 'are', 'great,', 'other', 'owners', \"aren't.\", 'They', 'often', 'let', 'bad', 'behavior', 'go', 'on', 'and', 'sometimes', 'even', 'encourage', 'it.\\n\\n', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wasnt', 'thread', 'opposit', 'hap', 'post', 'dog', 'kept', 'attack', 'guy', 'dog', 'own', 'kick', 'attack', 'dog', 'post', 'indign', 'someon', 'would', 'dar', 'try', 'prev', 'dog', 'attack', 'anoth', 'on', 'play', 'whatnot\\n', '\\nanyway', 'real', 'dog', 'gre', 'own', 'ar', 'oft', 'let', 'bad', 'behavy', 'go', 'sometim', 'ev', 'enco', 'it\\n\\n'], ['wasnt', 'thread', 'opposite', 'happen', 'posters', 'dog', 'keep', 'attack', 'guy', 'dog', 'owner', 'kick', 'attack', 'dog', 'poster', 'indignant', 'someone', 'would', 'dare', 'try', 'prevent', 'dog', 'attack', 'another', 'one', 'play', 'whatnot\\n', '\\nanyway', 'reality', 'dog', 'great', 'owners', 'arent', 'often', 'let', 'bad', 'behavior', 'go', 'sometimes', 'even', 'encourage', 'it\\n\\n'])\n",
      "original document: \n",
      "['I', 'mean', \"it's\", 'just', 'as', 'nice', 'during', 'the', 'day', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'nic', 'day', 'wel'], ['mean', 'nice', 'day', 'well'])\n",
      "original document: \n",
      "['Mercy!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mercy'], ['mercy'])\n",
      "original document: \n",
      "['That', 'is', 'an', 'immoral', 'act', 'and', 'should', 'be', 'censored', 'to', 'protect', 'the', 'children.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'act', 'cens', 'protect', 'childr'], ['immoral', 'act', 'censor', 'protect', 'children'])\n",
      "original document: \n",
      "['At', 'least', 'they', 'should', 'have', 'made', 'it', 'an', 'option', 'to', 'rub', 'two', 'sticks', 'together', 'to', 'make', 'a', 'fire.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'mad', 'opt', 'rub', 'two', 'stick', 'togeth', 'mak', 'fir'], ['least', 'make', 'option', 'rub', 'two', 'stick', 'together', 'make', 'fire'])\n",
      "original document: \n",
      "['Another', 'scene', 'that', 'really', 'pays', 'homage', 'to', 'it', 'is', 'when', 'the', 'super', 'battle', 'droids', 'take', 'Fives', 'from', 'the', 'republic', 'shuttle', 'in', 'season', '6.', 'Such', 'a', 'great', 'scene.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anoth', 'scen', 'real', 'pay', 'hom', 'sup', 'battl', 'droid', 'tak', 'fiv', 'republ', 'shuttl', 'season', 'six', 'gre', 'scen'], ['another', 'scene', 'really', 'pay', 'homage', 'super', 'battle', 'droids', 'take', 'fives', 'republic', 'shuttle', 'season', 'six', 'great', 'scene'])\n",
      "original document: \n",
      "['I', 'just', 'said', 'the', 'same', 'thing', 'about', 'this', 'sub']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'thing', 'sub'], ['say', 'thing', 'sub'])\n",
      "original document: \n",
      "['#**SportsHD**\\n\\n\\n📽**HD**', '[Mariners', 'at', 'Angels', '|', 'Home', '|', 'English', '|', 'ad', '1', '|', '📲', 'Mobile:', 'Yes](http://sportshd.me/mlb/492512/h)\\n\\n\\n📽**HD**', '[Mariners', 'at', 'Angels', '|', 'Away', '|', 'English', '|', 'ad', '1', '|', '📲', 'Mobile:', 'Yes](http://sportshd.me/mlb/492512/a)\\n\\n\\niOS', 'Uncertain']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sportshd\\n\\n\\nhd', 'marin', 'angel', 'hom', 'engl', 'ad', 'on', 'mobl', 'yeshttpsportshdmemlb492512h\\n\\n\\nhd', 'marin', 'angel', 'away', 'engl', 'ad', 'on', 'mobl', 'yeshttpsportshdmemlb492512a\\n\\n\\nios', 'uncertain'], ['sportshd\\n\\n\\nhd', 'mariners', 'angels', 'home', 'english', 'ad', 'one', 'mobile', 'yeshttpsportshdmemlb492512h\\n\\n\\nhd', 'mariners', 'angels', 'away', 'english', 'ad', 'one', 'mobile', 'yeshttpsportshdmemlb492512a\\n\\n\\nios', 'uncertain'])\n",
      "original document: \n",
      "['Man,', \"that's\", 'rough.', '\\n\\nI', 'cringe', 'that', \"they're\", 'drinking', 'while', 'on', 'medication.', \"That's\", 'extremely', 'risky', 'behavior', 'with', 'antidepressants', 'because', \"you're\", 'giving', 'two', 'types', 'of', 'mood', 'changing', 'drugs.', \"I'm\", 'on', 'antidepressants', 'and', 'am', 'now', 'abstaining', 'from', 'alcohol.', 'I', \"don't\", 'know', 'anything', 'about', 'stroke', 'meds', 'but', 'I', \"can't\", 'see', 'that', 'being', 'a', 'greenlight', 'on', 'consuming', 'ethynol,', 'which', 'is', 'the', 'scientific', 'name', 'of', 'alcohol.\\n\\nI', 'would', 'bring', 'it', 'up', 'very', 'delicately', 'and', 'with', 'a', 'heaping', 'amount', 'of', 'support.', 'An', 'unexpected', 'job', 'change', 'after', '30', 'years', 'is', 'a', 'very', 'stressful', 'thing', 'to', 'happen.', \"It's\", 'right', 'up', 'there', 'with', 'school,', 'marriage,', 'and', 'funerals.', 'Being', 'depressed', 'is', 'hard', 'and', 'requires', 'a', 'lot', 'of', 'unrequited', 'support.', '', 'Try', 'doing', 'activities', 'or', 'planned', 'days', 'where', 'you', 'spend', 'most', 'of', 'the', 'day', 'together.', '\\n\\nEven', 'with', 'the', 'best', 'intentions', 'they', \"aren't\", 'going', 'to', 'change', 'unless', 'they', 'want', 'to.', '\\n\\nA', 'local', 'clinic', 'or', 'health', 'center', 'could', 'have', 'listings', 'for', 'a', 'support', 'group', 'for', 'family', 'of', 'alcoholics', 'anonymous.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['man', 'that', 'rough', '\\n\\ni', 'cring', 'theyr', 'drink', 'med', 'that', 'extrem', 'risky', 'behavy', 'antidepress', 'yo', 'giv', 'two', 'typ', 'mood', 'chang', 'drug', 'im', 'antidepress', 'abstain', 'alcohol', 'dont', 'know', 'anyth', 'stroke', 'med', 'cant', 'see', 'greenlight', 'consum', 'ethynol', 'sci', 'nam', 'alcohol\\n\\ni', 'would', 'bring', 'del', 'heap', 'amount', 'support', 'unexpect', 'job', 'chang', 'thirty', 'year', 'stressful', 'thing', 'hap', 'right', 'school', 'marry', 'fun', 'depress', 'hard', 'requir', 'lot', 'unrequit', 'support', 'try', 'act', 'plan', 'day', 'spend', 'day', 'togeth', '\\n\\neven', 'best', 'int', 'ar', 'going', 'chang', 'unless', 'want', '\\n\\na', 'loc', 'clin', 'heal', 'cent', 'could', 'list', 'support', 'group', 'famy', 'alcohol', 'anonym'], ['man', 'thats', 'rough', '\\n\\ni', 'cringe', 'theyre', 'drink', 'medication', 'thats', 'extremely', 'risky', 'behavior', 'antidepressants', 'youre', 'give', 'two', 'type', 'mood', 'change', 'drug', 'im', 'antidepressants', 'abstain', 'alcohol', 'dont', 'know', 'anything', 'stroke', 'meds', 'cant', 'see', 'greenlight', 'consume', 'ethynol', 'scientific', 'name', 'alcohol\\n\\ni', 'would', 'bring', 'delicately', 'heap', 'amount', 'support', 'unexpected', 'job', 'change', 'thirty', 'years', 'stressful', 'thing', 'happen', 'right', 'school', 'marriage', 'funerals', 'depress', 'hard', 'require', 'lot', 'unrequited', 'support', 'try', 'activities', 'plan', 'days', 'spend', 'day', 'together', '\\n\\neven', 'best', 'intentions', 'arent', 'go', 'change', 'unless', 'want', '\\n\\na', 'local', 'clinic', 'health', 'center', 'could', 'list', 'support', 'group', 'family', 'alcoholics', 'anonymous'])\n",
      "original document: \n",
      "['This', 'content', 'brought', 'to', 'you', 'from', '\"Spain', 'Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#off', 'site', 'feed', '\"Spain', 'Pool\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'brought', 'spain', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'sit', 'fee', 'spain', 'pool\\n'], ['content', 'bring', 'spain', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'site', 'fee', 'spain', 'pool\\n'])\n",
      "original document: \n",
      "['Well,', 'yeah', \"that's\", 'how', \"it's\", 'normally', 'done', 'lmao.', 'There', 'are', 'certain', 'people', 'known', 'to', 'have', 'really', 'good', 'combinations', 'of', 'gut', 'flora', 'and', 'no', 'health', 'issues', 'and', 'they', 'are', 'paid', 'handsomely', 'for', 'their', 'poop.', \"It's\", 'all', 'been', 'extensively', 'studied', 'for', 'health', 'risks', 'and', 'all', 'that', 'before', 'they', 'let', 'it', 'loose', 'on', 'the', 'public']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'yeah', 'that', 'norm', 'don', 'lmao', 'certain', 'peopl', 'known', 'real', 'good', 'combin', 'gut', 'flor', 'heal', 'issu', 'paid', 'handsom', 'poop', 'extend', 'study', 'heal', 'risk', 'let', 'loos', 'publ'], ['well', 'yeah', 'thats', 'normally', 'do', 'lmao', 'certain', 'people', 'know', 'really', 'good', 'combinations', 'gut', 'flora', 'health', 'issue', 'pay', 'handsomely', 'poop', 'extensively', 'study', 'health', 'risk', 'let', 'loose', 'public'])\n",
      "original document: \n",
      "['i', 'payed', '25', 'for', 'mines', 'and', 'depends', 'on', 'what', 'crates', 'they', 'are']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pay', 'twenty-five', 'min', 'depend', 'crat'], ['pay', 'twenty-five', 'mine', 'depend', 'crate'])\n",
      "original document: \n",
      "['The', 'CE', 'of', 'Shirou,', 'Rin,', 'and', 'Sakura', 'at', 'their', 'full', 'potential', 'is', 'a', 'good', 'sight', 'to', 'see']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ce', 'shirou', 'rin', 'sakur', 'ful', 'pot', 'good', 'sight', 'see'], ['ce', 'shirou', 'rin', 'sakura', 'full', 'potential', 'good', 'sight', 'see'])\n",
      "original document: \n",
      "['Ahhh', 'because', \"it's\", 'Solo,', 'ie', \"doesn't\", 'need', 'to', 'be', 'attached', 'to', 'anything', 'else', 'or', 'online.', 'Thank', 'you.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahhh', 'solo', 'ie', 'doesnt', 'nee', 'attach', 'anyth', 'els', 'onlin', 'thank'], ['ahhh', 'solo', 'ie', 'doesnt', 'need', 'attach', 'anything', 'else', 'online', 'thank'])\n",
      "original document: \n",
      "['You', 'seem', 'to', 'be', 'a', 'bright', 'guy', 'and', 'I', \"don't\", 'mean', 'to', 'insult', 'your', 'intelligence,', 'but', 'have', 'you', 'verified', 'game', 'cache', 'through', 'Steam', 'on', 'the', 'off', 'chance', 'a', 'spider', 'file', 'went', 'missing?', 'Long', 'shot,', 'but...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seem', 'bright', 'guy', 'dont', 'mean', 'insult', 'intellig', 'ver', 'gam', 'cach', 'steam', 'chant', 'spid', 'fil', 'went', 'miss', 'long', 'shot'], ['seem', 'bright', 'guy', 'dont', 'mean', 'insult', 'intelligence', 'verify', 'game', 'cache', 'steam', 'chance', 'spider', 'file', 'go', 'miss', 'long', 'shoot'])\n",
      "original document: \n",
      "['Thank', 'you.', 'Good', 'that', 'at', 'least', 'some', 'people', 'know', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'good', 'least', 'peopl', 'know'], ['thank', 'good', 'least', 'people', 'know'])\n",
      "original document: \n",
      "['Please', 'I', 'really', 'need', 'to', 'find', \"this.\\n\\nI'm\", 'looking', 'for', 'a', 'doujin', 'where', \"There's\", 'a', 'guy', 'that', 'tells', 'his', 'friend', \"he's\", 'going', 'to', 'confess', 'to', 'a', 'girl.', 'Then', 'the', 'guy', 'is', 'alone', 'in', 'a', 'coffee', 'shop', 'and', 'the', 'girl', 'is', 'the', 'barista.', 'He', 'orders', 'a', 'coffee', 'and', 'tries', 'to', 'make', 'small', 'talk,', 'from', 'time', 'to', 'time', 'the', 'girl', 'goes', 'to', 'the', 'kitchen', 'were', 'he', \"can't\", 'see', 'her.\\n\\nWe', 'see', 'that', 'the', 'girl', 'goes', 'out', 'from', 'the', 'back', 'exit', 'and', 'has', 'sex', 'with', 'the', \"guy's\", 'friend.', 'She', 'knows', 'that', 'the', 'guy', 'wants', 'to', 'confess', 'to', 'her', 'and', 'she', 'wants', 'to', 'be', 'with', 'him.', 'The', 'friend', 'comes', 'inside', 'her', 'so', 'he', 'uses', 'an', 'special', 'cork-like', 'thing', 'to', 'close', 'her', 'pussy', 'so', 'the', 'semen', \"doesn't\", 'come', 'out,', 'he', 'promises', 'to', 'remove', 'it', 'when', 'the', 'guy', 'confesses.\\n\\nAs', 'time', 'passes,', 'the', 'guy', \"doesn't\", 'confess,', 'the', 'girl', 'keeps', 'coming', 'out', 'and', 'the', 'friend', 'fucks', 'her', 'ass.\\n\\nIn', 'the', 'end', 'she', \"can't\", 'take', 'it', 'anymore', 'and', 'she', 'leaves', 'the', 'guy', 'hanging', 'asking', 'the', 'friend', 'to', 'remove', 'the', 'cork', 'and', 'fuck', 'her.\\n\\nThe', 'doujin', 'ends', 'with', 'the', 'friend', 'lamenting', 'that', 'the', 'guy', 'never', 'confessed,', 'and', 'that', 'he', 'would', 'definitely', 'enjoy', 'her', 'while', 'the', 'girl', 'is', 'filled', 'with', 'semen', 'on', 'all', 'holes', 'and', 'discarded', 'on', 'the', 'ground.\\n\\nThe', 'doujin', 'also', 'had', 'a', 'mini', '2', 'pages', 'story', 'at', 'the', 'beginning', 'where', 'the', 'girl', 'has', 'sex', 'with', 'the', 'friend', 'in', 'a', 'fitting', 'room', 'at', 'a', 'clothing', 'story.', '\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'real', 'nee', 'find', 'this\\n\\nim', 'look', 'doujin', 'ther', 'guy', 'tel', 'friend', 'hes', 'going', 'confess', 'girl', 'guy', 'alon', 'coff', 'shop', 'girl', 'barist', 'ord', 'coff', 'tri', 'mak', 'smal', 'talk', 'tim', 'tim', 'girl', 'goe', 'kitch', 'cant', 'see', 'her\\n\\nwe', 'see', 'girl', 'goe', 'back', 'exit', 'sex', 'guy', 'friend', 'know', 'guy', 'want', 'confess', 'want', 'friend', 'com', 'insid', 'us', 'spec', 'corklik', 'thing', 'clos', 'pussy', 'sem', 'doesnt', 'com', 'prom', 'remov', 'guy', 'confesses\\n\\na', 'tim', 'pass', 'guy', 'doesnt', 'confess', 'girl', 'keep', 'com', 'friend', 'fuck', 'ass\\n\\nin', 'end', 'cant', 'tak', 'anym', 'leav', 'guy', 'hang', 'ask', 'friend', 'remov', 'cork', 'fuck', 'her\\n\\nthe', 'doujin', 'end', 'friend', 'lam', 'guy', 'nev', 'confess', 'would', 'definit', 'enjoy', 'girl', 'fil', 'sem', 'hol', 'discard', 'ground\\n\\nthe', 'doujin', 'also', 'min', 'two', 'pag', 'story', 'begin', 'girl', 'sex', 'friend', 'fit', 'room', 'cloth', 'story', '\\n\\n'], ['please', 'really', 'need', 'find', 'this\\n\\nim', 'look', 'doujin', 'theres', 'guy', 'tell', 'friend', 'hes', 'go', 'confess', 'girl', 'guy', 'alone', 'coffee', 'shop', 'girl', 'barista', 'order', 'coffee', 'try', 'make', 'small', 'talk', 'time', 'time', 'girl', 'go', 'kitchen', 'cant', 'see', 'her\\n\\nwe', 'see', 'girl', 'go', 'back', 'exit', 'sex', 'guy', 'friend', 'know', 'guy', 'want', 'confess', 'want', 'friend', 'come', 'inside', 'use', 'special', 'corklike', 'thing', 'close', 'pussy', 'semen', 'doesnt', 'come', 'promise', 'remove', 'guy', 'confesses\\n\\nas', 'time', 'pass', 'guy', 'doesnt', 'confess', 'girl', 'keep', 'come', 'friend', 'fuck', 'ass\\n\\nin', 'end', 'cant', 'take', 'anymore', 'leave', 'guy', 'hang', 'ask', 'friend', 'remove', 'cork', 'fuck', 'her\\n\\nthe', 'doujin', 'end', 'friend', 'lament', 'guy', 'never', 'confess', 'would', 'definitely', 'enjoy', 'girl', 'fill', 'semen', 'hole', 'discard', 'ground\\n\\nthe', 'doujin', 'also', 'mini', 'two', 'page', 'story', 'begin', 'girl', 'sex', 'friend', 'fit', 'room', 'clothe', 'story', '\\n\\n'])\n",
      "original document: \n",
      "['I', 'literally', 'finished', 'it', 'yesterday.', 'I', 'really', 'liked', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'fin', 'yesterday', 'real', 'lik'], ['literally', 'finish', 'yesterday', 'really', 'like'])\n",
      "original document: \n",
      "['Nty', 'dude.', 'Not', 'a', 'fan', 'of', 'Helios.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nty', 'dud', 'fan', 'helio'], ['nty', 'dude', 'fan', 'helios'])\n",
      "original document: \n",
      "['LED,', 'resistor(s),', 'momentary', 'switch,', 'recharge', 'port,', 'soundboard,', 'speaker,', 'wire.\\n\\nCheck', 'out', 'the', 'custom', 'saber', 'shop', \"they'll\", 'have', 'everything', 'you', 'need,', 'but', 'they', 'only', 'carry', 'plecter', 'labs', 'soundboards,', 'NEC', 'is', 'the', 'other', 'main', 'soundboard', 'brand', 'and', 'they', 'have', 'their', 'own', 'site.', '\\n\\nWhat', 'kind', 'of', 'budget', 'are', 'you', 'playing', 'with?\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['led', 'resist', 'mom', 'switch', 'recharg', 'port', 'soundboard', 'speak', 'wire\\n\\ncheck', 'custom', 'sab', 'shop', 'theyl', 'everyth', 'nee', 'carry', 'plect', 'lab', 'soundboard', 'nec', 'main', 'soundboard', 'brand', 'sit', '\\n\\nwhat', 'kind', 'budget', 'play', 'with\\n'], ['lead', 'resistors', 'momentary', 'switch', 'recharge', 'port', 'soundboard', 'speaker', 'wire\\n\\ncheck', 'custom', 'saber', 'shop', 'theyll', 'everything', 'need', 'carry', 'plecter', 'labs', 'soundboards', 'nec', 'main', 'soundboard', 'brand', 'site', '\\n\\nwhat', 'kind', 'budget', 'play', 'with\\n'])\n",
      "original document: \n",
      "['Makes', 'sense.', 'Whatever', 'works!', 'It', 'looks', 'like', 'a', 'freaking', 'beast', 'and', \"I'm\", 'hopeful', \"I'll\", 'give', 'it', 'a', 'shot', 'someday', '-', \"I'll\", 'just', 'lust', 'after', 'you', 'alls', 'for', 'now', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'sens', 'whatev', 'work', 'look', 'lik', 'freak', 'beast', 'im', 'hop', 'il', 'giv', 'shot', 'someday', 'il', 'lust', 'al'], ['make', 'sense', 'whatever', 'work', 'look', 'like', 'freak', 'beast', 'im', 'hopeful', 'ill', 'give', 'shoot', 'someday', 'ill', 'lust', 'alls'])\n",
      "original document: \n",
      "['When', 'we', 'switched', 'email', 'systems', 'at', 'work', 'one', 'of', 'my', 'employees', 'emailed', 'me', 'asking', 'for', 'instructions', 'on', 'how', 'to', 'send', 'an', 'email.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['switch', 'email', 'system', 'work', 'on', 'employ', 'email', 'ask', 'instruct', 'send', 'email'], ['switch', 'email', 'systems', 'work', 'one', 'employees', 'email', 'ask', 'instructions', 'send', 'email'])\n",
      "original document: \n",
      "['Then', 'maybe', 'they', 'need', 'better', 'advertisers?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'nee', 'bet', 'advert'], ['maybe', 'need', 'better', 'advertisers'])\n",
      "original document: \n",
      "['What', 'can', 'i', 'do', 'about', 'that?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['A', 'brain', 'filled', 'only', 'with', 'Reddit', 'comments', 'and', 'left', 'wing', 'media', 'headlines.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brain', 'fil', 'reddit', 'com', 'left', 'wing', 'med', 'headlin'], ['brain', 'fill', 'reddit', 'comment', 'leave', 'wing', 'media', 'headline'])\n",
      "original document: \n",
      "['Brock', 'Lesnar.\\n\\nBoom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brock', 'lesnar\\n\\nboom'], ['brock', 'lesnar\\n\\nboom'])\n",
      "original document: \n",
      "['Do', 'you', 'wanna', 'see', 'spiders?', 'Man', 'get', 'some', '1p', 'if', 'your', 'getting', 'RCs', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wann', 'see', 'spid', 'man', 'get', '1p', 'get', 'rcs'], ['wanna', 'see', 'spiders', 'man', 'get', '1p', 'get', 'rcs'])\n",
      "original document: \n",
      "['Thank', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank'], ['thank'])\n",
      "original document: \n",
      "['i', 'also', 'kinda', 'agree', ',', 'but', 'mostly', 'becouse', 'i', 'played', 'like', '4', 'hours', 'of', 'the', 'vn', ',', 'and', 'it', 'felt', 'like', 'not', 'that', 'intersting', '.', 'So', 'if', 'HF', 'is', 'the', 'Good', 'part', ',', 'and', 'you', 'needed', 'to', 'play', 'like', '50', 'hours', 'to', 'get', 'there', ',', 'you', 'want', 'to', 'hide', 'from', 'yourself', 'that', 'you', 'wasted', 'that', 'much', 'time', ',', 'so', 'you', 'go', 'all', 'the', 'way', 'in', 'the', '', 'fate', 'fan', 'rute', '', 'and', 'watch', 'and', 'play', 'everything', 'fate', '(that', 'i', 'actually', 'admire', 'like', 'its', 'so', 'vaste', 'that', 'knowing', 'all', 'the', 'stuff', 'is', 'like', 'a', 'really', 'hard', 'achivment', ').\\n\\nBut', 'yeah', 'im', 'talking', 'as', 'a', 'person', 'that', 'doesnt', 'really', 'like', 'battle', 'royales', ',', 'nor', 'likes', 'sabers', 'disgns', ',', 'so', 'im', 'totaly', 'not', 'the', 'target', 'XD', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'kind', 'agr', 'most', 'bec', 'play', 'lik', 'four', 'hour', 'vn', 'felt', 'lik', 'interst', 'hf', 'good', 'part', 'nee', 'play', 'lik', 'fifty', 'hour', 'get', 'want', 'hid', 'wast', 'much', 'tim', 'go', 'way', 'fat', 'fan', 'rut', 'watch', 'play', 'everyth', 'fat', 'act', 'admir', 'lik', 'vast', 'know', 'stuff', 'lik', 'real', 'hard', 'ach', '\\n\\nbut', 'yeah', 'im', 'talk', 'person', 'doesnt', 'real', 'lik', 'battl', 'roy', 'lik', 'sab', 'disgn', 'im', 'tota', 'target', 'xd'], ['also', 'kinda', 'agree', 'mostly', 'becouse', 'play', 'like', 'four', 'hours', 'vn', 'felt', 'like', 'intersting', 'hf', 'good', 'part', 'need', 'play', 'like', 'fifty', 'hours', 'get', 'want', 'hide', 'waste', 'much', 'time', 'go', 'way', 'fate', 'fan', 'rute', 'watch', 'play', 'everything', 'fate', 'actually', 'admire', 'like', 'vaste', 'know', 'stuff', 'like', 'really', 'hard', 'achivment', '\\n\\nbut', 'yeah', 'im', 'talk', 'person', 'doesnt', 'really', 'like', 'battle', 'royales', 'like', 'saber', 'disgns', 'im', 'totaly', 'target', 'xd'])\n",
      "original document: \n",
      "['Looks', 'like', 'the', 'kind', 'of', 'place', 'people', 'get', 'murdered', 'in', 'in', 'movies.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'kind', 'plac', 'peopl', 'get', 'murd', 'movy'], ['look', 'like', 'kind', 'place', 'people', 'get', 'murder', 'movies'])\n",
      "original document: \n",
      "['crash', 'vs', 'ratchet?\\n\\nwhich', 'one?\\n\\ni', 'have', 'no', 'idea', 'what', 'the', 'objective', 'is', 'on', 'either', 'of', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['crash', 'vs', 'ratchet\\n\\nwhich', 'one\\n\\ni', 'ide', 'object', 'eith'], ['crash', 'vs', 'ratchet\\n\\nwhich', 'one\\n\\ni', 'idea', 'objective', 'either'])\n",
      "original document: \n",
      "['Money', 'in', 'the', 'back', 'of', 'their', 'phone?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['money', 'back', 'phon'], ['money', 'back', 'phone'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'think', 'you', 'should', 'post', 'to', 'r/sex.', 'Also,', 'I', 'am', 'in', 'the', 'same', 'situation', 'as', 'you', 'so', 'if', 'you', 'find', 'any', 'good', 'advice', 'let', 'me', 'know!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'post', 'rsex', 'also', 'situ', 'find', 'good', 'adv', 'let', 'know'], ['think', 'post', 'rsex', 'also', 'situation', 'find', 'good', 'advice', 'let', 'know'])\n",
      "original document: \n",
      "['You', 'have', 'proof?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['proof'], ['proof'])\n",
      "original document: \n",
      "[\"That's\", 'absolutely', 'true', ':/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'absolv', 'tru'], ['thats', 'absolutely', 'true'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['And', 'the', 'best', 'part?', '', 'Ken', 'Penders', 'apparently', 'intended', 'to', 'use', 'the', 'crossover', 'to', 'launch', 'his', 'own', 'series,', '*The', 'Lost', 'Ones*,', 'which', 'bombed', 'and', 'has', 'yet', 'to', 'get', 'past', 'its', 'first', 'issue.', '', '\\n\\nI', \"can't\", 'quite', 'remember', 'the', 'exact', 'details,', 'but', 'it', 'involved', 'people', 'getting', 'superpowers', 'from', 'the', 'atomic', 'bombing', 'of', 'Hiroshima.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'part', 'ken', 'pend', 'app', 'intend', 'us', 'crossov', 'launch', 'sery', 'lost', 'on', 'bomb', 'yet', 'get', 'past', 'first', 'issu', '\\n\\ni', 'cant', 'quit', 'rememb', 'exact', 'detail', 'involv', 'peopl', 'get', 'superpow', 'atom', 'bomb', 'hiroshim'], ['best', 'part', 'ken', 'penders', 'apparently', 'intend', 'use', 'crossover', 'launch', 'series', 'lose', 'ones', 'bomb', 'yet', 'get', 'past', 'first', 'issue', '\\n\\ni', 'cant', 'quite', 'remember', 'exact', 'detail', 'involve', 'people', 'get', 'superpowers', 'atomic', 'bomb', 'hiroshima'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"It's\", 'always', 'top', 'news.', 'The', 'fact', 'that', 'republicans', 'freaked', 'out', \"doesn't\", 'mean', 'anything,', 'because', 'then', 'a', 'republican', 'does', 'something,', 'and', 'the', 'democrats', 'lose', 'their', 'shit.', '\\nThings', \"haven't\", 'changed;', \"that's\", 'how', 'the', 'news', 'operates.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'top', 'new', 'fact', 'republ', 'freak', 'doesnt', 'mean', 'anyth', 'republ', 'someth', 'democr', 'los', 'shit', '\\nthings', 'hav', 'chang', 'that', 'new', 'op'], ['always', 'top', 'news', 'fact', 'republicans', 'freak', 'doesnt', 'mean', 'anything', 'republican', 'something', 'democrats', 'lose', 'shit', '\\nthings', 'havent', 'change', 'thats', 'news', 'operate'])\n",
      "original document: \n",
      "['25', 'lb', '≈', '11', 'kg\\n\\n^metric', '^units', '^bot', '^|', '^[feedback](https://www.reddit.com/r/metric_units/comments/73edn2/constructive_feedback_thread/)', '^|', '^[source](https://github.com/cannawen/metric_units_reddit_bot)', '^|', '^[hacktoberfest](https://www.reddit.com/r/metric_units/comments/73ef7e/contribute_to_metric_units/)', '^|', '^[block](https://www.reddit.com/message/compose?to=metric_units&amp;subject=stop&amp;message=If%20you%20would%20like%20to%20stop%20seeing%20this%20bot%27s%20comments%2C%20please%20send%20this%20private%20message%20with%20the%20subject%20%27stop%27.%20If%20you%20are%20a%20moderator%2C%20please%20go%20to%20https%3A%2F%2Fwww.reddit.com%2Fr%2FproED%2Fabout%2Fbanned%2F)', '^|', '^v0.11.2']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty-five', 'lb', 'elev', 'kg\\n\\nmetric', 'unit', 'bot', 'feedbackhttpswwwredditcomrmetric_unitscomments73edn2constructive_feedback_thread', 'sourcehttpsgithubcomcannawenmetric_units_reddit_bot', 'hacktoberfesthttpswwwredditcomrmetric_unitscomments73ef7econtribute_to_metric_units', 'blockhttpswwwredditcommessagecomposetometric_unitsampsubjectstopampmessageif20you20would20like20to20stop20seeing20this20bot27s20comments2c20please20send20this20private20message20with20the20subject2027stop2720if20you20are20a20moderator2c20please20go20to20https3a2f2fwwwredditcom2fr2fproed2fabout2fbanned2f', 'v0112'], ['twenty-five', 'lb', 'eleven', 'kg\\n\\nmetric', 'units', 'bot', 'feedbackhttpswwwredditcomrmetric_unitscomments73edn2constructive_feedback_thread', 'sourcehttpsgithubcomcannawenmetric_units_reddit_bot', 'hacktoberfesthttpswwwredditcomrmetric_unitscomments73ef7econtribute_to_metric_units', 'blockhttpswwwredditcommessagecomposetometric_unitsampsubjectstopampmessageif20you20would20like20to20stop20seeing20this20bot27s20comments2c20please20send20this20private20message20with20the20subject2027stop2720if20you20are20a20moderator2c20please20go20to20https3a2f2fwwwredditcom2fr2fproed2fabout2fbanned2f', 'v0112'])\n",
      "original document: \n",
      "['Two', 'wrongs', \"don't\", 'make', 'a', 'right.', 'And', \"it's\", 'worse', 'when', \"it's\", 'done', 'deliberately.', \"It's\", 'not', 'so', 'much', 'the', 'act', 'itself', 'but', 'the', 'reason', 'behind', 'it.', 'People', 'that', 'get', 'paid', 'millions', 'of', 'dollars', 'a', 'year', 'to', 'play', 'a', 'sport', \"that's\", 'only', 'played', 'in', 'the', 'US,', 'supported', 'by', 'tax', 'payers,', 'and', 'are', 'examples', 'of', 'how', 'far', 'liberty', 'and', 'freedom', 'can', 'get', 'you', 'if', 'you', 'work', 'hard', 'and', 'work', 'well', 'with', 'others,', 'and', 'then', 'protest', 'against', 'that', 'country', \"doesn't\", 'really', 'offend', 'me,', 'but', 'it', 'is', 'really', 'contradictory', 'and', 'hypocritical.', 'All', 'those', 'guys', 'could', 'instead', 'declare', 'that', 'all', 'their', 'pay', 'goes', 'to', 'support', 'poor', 'communities.', 'Instead', 'they', 'just', 'want', 'to', 'feel', 'special', 'without', 'actually', 'sacrificing', 'anything.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'wrong', 'dont', 'mak', 'right', 'wors', 'don', 'delib', 'much', 'act', 'reason', 'behind', 'peopl', 'get', 'paid', 'mil', 'doll', 'year', 'play', 'sport', 'that', 'play', 'us', 'support', 'tax', 'pay', 'exampl', 'far', 'liberty', 'freedom', 'get', 'work', 'hard', 'work', 'wel', 'oth', 'protest', 'country', 'doesnt', 'real', 'offend', 'real', 'contradict', 'hypocrit', 'guy', 'could', 'instead', 'decl', 'pay', 'goe', 'support', 'poor', 'commun', 'instead', 'want', 'feel', 'spec', 'without', 'act', 'sacr', 'anyth'], ['two', 'wrong', 'dont', 'make', 'right', 'worse', 'do', 'deliberately', 'much', 'act', 'reason', 'behind', 'people', 'get', 'pay', 'millions', 'dollars', 'year', 'play', 'sport', 'thats', 'play', 'us', 'support', 'tax', 'payers', 'examples', 'far', 'liberty', 'freedom', 'get', 'work', 'hard', 'work', 'well', 'others', 'protest', 'country', 'doesnt', 'really', 'offend', 'really', 'contradictory', 'hypocritical', 'guy', 'could', 'instead', 'declare', 'pay', 'go', 'support', 'poor', 'communities', 'instead', 'want', 'feel', 'special', 'without', 'actually', 'sacrifice', 'anything'])\n",
      "original document: \n",
      "['Its', 'his', 'own', 'fault.', '', 'He', 'said', 'yesterday', 'when', 'he', 'started', 'the', 'stream', 'that', 'he', 'had', 'woken', 'up', 'at', '9AM', 'that', 'morning', 'and', \"hadn't\", 'done', 'shit', 'and', 'how', 'he', 'had', 'broken', 'phones', 'and', 'stuff', '(use', 'that', 'time', 'he', \"didn't\", 'do', 'shit/went', 'to', 'whataburger', 'with', 'Greek', 'and', 'go', 'get', 'your', 'phones', 'fixed', 'maybe?)', 'so', 'thats', 'on', 'him', 'for', 'not', 'doing', 'shit', 'about', 'it.', '', 'Teradek', 'trash?', '', 'Why', \"hasn't\", 'he', 'gone', 'to', 'Irvine', 'again', 'and', 'seen', 'what', 'was', 'up', 'with', 'his', 'setup?', '', 'He', 'just', 'keeps', 'making', 'excuse', 'after', 'excuse', 'for', 'scuffed', 'content/not', 'streaming', 'and', 'you', 'normies', 'just', 'eat', 'that', 'shit', 'up.', '', 'I', 'call', 'a', 'spade', 'a', 'spade', 'and', 'this', 'dude', 'is', 'a', 'fucking', 'victim-card', 'playing', 'idiot.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fault', 'said', 'yesterday', 'start', 'stream', 'wok', '9am', 'morn', 'hadnt', 'don', 'shit', 'brok', 'phon', 'stuff', 'us', 'tim', 'didnt', 'shitw', 'whataburg', 'greek', 'go', 'get', 'phon', 'fix', 'mayb', 'that', 'shit', 'teradek', 'trash', 'hasnt', 'gon', 'irvin', 'seen', 'setup', 'keep', 'mak', 'excus', 'excus', 'scuff', 'contentnot', 'streaming', 'normy', 'eat', 'shit', 'cal', 'spad', 'spad', 'dud', 'fuck', 'victimcard', 'play', 'idiot'], ['fault', 'say', 'yesterday', 'start', 'stream', 'wake', '9am', 'morning', 'hadnt', 'do', 'shit', 'break', 'phone', 'stuff', 'use', 'time', 'didnt', 'shitwent', 'whataburger', 'greek', 'go', 'get', 'phone', 'fix', 'maybe', 'thats', 'shit', 'teradek', 'trash', 'hasnt', 'go', 'irvine', 'see', 'setup', 'keep', 'make', 'excuse', 'excuse', 'scuff', 'contentnot', 'stream', 'normies', 'eat', 'shit', 'call', 'spade', 'spade', 'dude', 'fuck', 'victimcard', 'play', 'idiot'])\n",
      "original document: \n",
      "['I', \"didn't\", 'realize', 'I', 'had', 'this', 'for', 'free', 'after', 'already', 'owning', 'the', 'UC4', 'digital', 'deluxe', 'version', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'real', 'fre', 'already', 'own', 'uc4', 'digit', 'delux', 'vert'], ['didnt', 'realize', 'free', 'already', 'own', 'uc4', 'digital', 'deluxe', 'version'])\n",
      "original document: \n",
      "[\"That's\", 'very', 'helpful', 'thank', 'you.', \"There's\", 'a', 'lot', 'of', 'peer', 'pressure', 'to', 'get', 'this', 'game,', 'but', \"I'm\", 'on', 'the', 'fence', 'because', \"I'm\", 'neither', 'into', 'golf', 'or', 'RPGs.', 'I', \"haven't\", 'read', 'proper', 'answers', 'about', 'the', 'RPG', 'side', 'of', 'things', 'until', 'now.', 'And', 'I', 'do', 'realize', 'that', 'a', 'lot', 'of', 'good', 'stories', 'and', 'gameplay', 'hide', 'behind', 'themes', 'and', 'subject', 'matter', \"I'm\", 'not', 'a', 'fan', 'off', 'like', 'sports,', 'but', 'this', 'game', 'looks', 'really', 'charming', 'and', 'delightful.', 'Now', 'the', 'only', 'thing', 'keeping', 'me', 'from', 'buying', 'it', 'is', 'the', 'need', 'to', 'finish', 'Thimbleweed', 'Park', 'and', 'Shovel', 'Knight.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'help', 'thank', 'ther', 'lot', 'peer', 'press', 'get', 'gam', 'im', 'fent', 'im', 'neith', 'golf', 'rpgs', 'hav', 'read', 'prop', 'answ', 'rpg', 'sid', 'thing', 'real', 'lot', 'good', 'story', 'gameplay', 'hid', 'behind', 'them', 'subject', 'mat', 'im', 'fan', 'lik', 'sport', 'gam', 'look', 'real', 'charm', 'delight', 'thing', 'keep', 'buy', 'nee', 'fin', 'thimblewee', 'park', 'shovel', 'knight'], ['thats', 'helpful', 'thank', 'theres', 'lot', 'peer', 'pressure', 'get', 'game', 'im', 'fence', 'im', 'neither', 'golf', 'rpgs', 'havent', 'read', 'proper', 'answer', 'rpg', 'side', 'things', 'realize', 'lot', 'good', 'stories', 'gameplay', 'hide', 'behind', 'theme', 'subject', 'matter', 'im', 'fan', 'like', 'sport', 'game', 'look', 'really', 'charm', 'delightful', 'thing', 'keep', 'buy', 'need', 'finish', 'thimbleweed', 'park', 'shovel', 'knight'])\n",
      "original document: \n",
      "['This', 'is', 'more', 'than', 'just', 'lines.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lin'], ['line'])\n",
      "original document: \n",
      "['Your', 'offense', 'only', 'put', 'up', '31.', 'Most', 'of', 'your', 'points', 'came', 'from', 'giveaways', 'by', 'Indiana', 'on', 'their', 'side', 'of', 'the', 'field.', 'Only', '~~two~~3', 'of', 'your', 'point', 'scoring', 'drives', 'were', 'longer', 'than', '50', 'yards.', '\\n\\nEdit:', 'it', 'was', '3', 'scoring', 'drives', 'of', '50+.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['offens', 'put', 'thirty-one', 'point', 'cam', 'giveaway', 'indian', 'sid', 'field', 'two3', 'point', 'scor', 'driv', 'long', 'fifty', 'yard', '\\n\\nedit', 'three', 'scor', 'driv', 'fifty'], ['offense', 'put', 'thirty-one', 'point', 'come', 'giveaways', 'indiana', 'side', 'field', 'two3', 'point', 'score', 'drive', 'longer', 'fifty', 'yards', '\\n\\nedit', 'three', 'score', 'drive', 'fifty'])\n",
      "original document: \n",
      "['Go', 'get', 'em!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'get', 'em'], ['go', 'get', 'em'])\n",
      "original document: \n",
      "['JRR', 'Tolkien.', 'Gandalf,', 'Aragorn,', 'Frodo,', 'Bilbo', 'Baggins,', 'Gollum...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jrr', 'tolky', 'gandalf', 'aragorn', 'frodo', 'bilbo', 'baggin', 'goll'], ['jrr', 'tolkien', 'gandalf', 'aragorn', 'frodo', 'bilbo', 'baggins', 'gollum'])\n",
      "original document: \n",
      "['Thanks', 'for', 'the', 'answer.', 'Makes', 'sense', 'to', 'me.', \"I'm\", 'in', 'engineering,', 'so', 'my', 'use', 'of', 'terminology', 'is', 'definitely', 'different', 'from', 'how', 'scientific', 'fields', 'use', 'it,', 'and', \"there's\", 'differences', 'even', 'among', 'fields', 'of', 'science.', 'I', 'was', 'thinking', 'that', \"'data'\", 'is', 'almost', 'like', 'a', 'unit', 'of', 'measurement.', 'How', 'much', 'data', 'supports', 'a', 'proposition/theory', 'is', 'often', 'a', 'measure', 'of', 'its', 'likely', 'accuracy.', 'So', 'in', 'this', 'way', 'I', 'wish', 'it', 'were', 'standardized', 'so', 'we', 'could', 'use', 'data', 'vocabulary', 'to', 'communicate', 'how', 'much', 'confidence', 'we', 'have.', 'Saying', '\"we', 'have', 'datum', 'supporting', 'it\"', 'should', 'be', 'higher', 'confidence', 'than', '\"we', 'have', 'data', 'supporting', 'it\".', 'But', 'obviously', 'context', 'would', 'make', 'this', 'apparent', 'most', 'of', 'the', 'time.', 'Anyway,', 'good', 'answer.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'answ', 'mak', 'sens', 'im', 'engin', 'us', 'terminolog', 'definit', 'diff', 'sci', 'field', 'us', 'ther', 'diff', 'ev', 'among', 'field', 'sci', 'think', 'dat', 'almost', 'lik', 'unit', 'meas', 'much', 'dat', 'support', 'propositionth', 'oft', 'meas', 'lik', 'acc', 'way', 'wish', 'standard', 'could', 'us', 'dat', 'vocab', 'commun', 'much', 'confid', 'say', 'dat', 'support', 'high', 'confid', 'dat', 'support', 'obvy', 'context', 'would', 'mak', 'app', 'tim', 'anyway', 'good', 'answ'], ['thank', 'answer', 'make', 'sense', 'im', 'engineer', 'use', 'terminology', 'definitely', 'different', 'scientific', 'field', 'use', 'theres', 'differences', 'even', 'among', 'field', 'science', 'think', 'data', 'almost', 'like', 'unit', 'measurement', 'much', 'data', 'support', 'propositiontheory', 'often', 'measure', 'likely', 'accuracy', 'way', 'wish', 'standardize', 'could', 'use', 'data', 'vocabulary', 'communicate', 'much', 'confidence', 'say', 'datum', 'support', 'higher', 'confidence', 'data', 'support', 'obviously', 'context', 'would', 'make', 'apparent', 'time', 'anyway', 'good', 'answer'])\n",
      "original document: \n",
      "['Main', 'saucey', 'ingredients\\n\\nhttps://imgur.com/a/Bsgob\\n\\nhttps://np.reddit.com/r/MakeupAddiction/comments/73geqz/first_post_subculture_apocalypse/\\n\\nSrs,', 'This', 'was', 'quite..', 'a', 'look', '\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['main', 'saucey', 'ingredients\\n\\nhttpsimgurcomabsgob\\n\\nhttpsnpredditcomrmakeupaddictioncomments73geqzfirst_post_subculture_apocalypse\\n\\nsr', 'quit', 'look', '\\n'], ['main', 'saucey', 'ingredients\\n\\nhttpsimgurcomabsgob\\n\\nhttpsnpredditcomrmakeupaddictioncomments73geqzfirst_post_subculture_apocalypse\\n\\nsrs', 'quite', 'look', '\\n'])\n",
      "original document: \n",
      "['I', 'got', 'a', 'lot', 'of', 'good', 'advice', 'when', 'I', 'asked', 'a', 'few', 'months', 'ago.', '', '[Link](https://www.reddit.com/r/Lightroom/comments/5laag8/transferring_to_a_new_laptop_nows_the_time_to_do/)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'lot', 'good', 'adv', 'ask', 'month', 'ago', 'linkhttpswwwredditcomrlightroomcomments5laag8transferring_to_a_new_laptop_nows_the_time_to_do'], ['get', 'lot', 'good', 'advice', 'ask', 'months', 'ago', 'linkhttpswwwredditcomrlightroomcomments5laag8transferring_to_a_new_laptop_nows_the_time_to_do'])\n",
      "original document: \n",
      "['A', 'reference', 'to', 'the', 'new', 'dropship', 'troop', 'in', 'bh', 'I', 'suppose']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ref', 'new', 'drop', 'troop', 'bh', 'suppos'], ['reference', 'new', 'dropship', 'troop', 'bh', 'suppose'])\n",
      "original document: \n",
      "['Not', 'updated', 'as', 'of', 'yet.', \"GOG's\", 'really', 'slow', 'to', 'greenlight', 'updates', 'though;', 'it', 'could', 'quite', 'possibly', 'be', 'several', 'weeks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['upd', 'yet', 'gog', 'real', 'slow', 'greenlight', 'upd', 'though', 'could', 'quit', 'poss', 'sev', 'week'], ['update', 'yet', 'gogs', 'really', 'slow', 'greenlight', 'update', 'though', 'could', 'quite', 'possibly', 'several', 'weeks'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Hahahahaaha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahahahaah'], ['hahahahaaha'])\n",
      "original document: \n",
      "['Thanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank'], ['thank'])\n",
      "original document: \n",
      "['Ditto', 'on', 'the', 'wall/sun', 'comment.', 'Even', 'your', 'latitude', 'will', 'have', 'limited', 'sun', 'in', 'winterPlant', 'your', 'tallest', 'vegetables', 'nearest', 'the', 'wall', 'to', 'give', 'the', 'shorter', 'ones', 'more', 'rays.', 'As', 'for', 'what', 'you', 'plant,', 'plant', 'what', \"you'll\", 'eat!', 'Getting', 'excited', 'about', 'eating', 'the', 'stuff', 'is', 'what', 'gives', 'you', 'the', 'energy', 'to', 'maintain', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ditto', 'wallsun', 'com', 'ev', 'latitud', 'limit', 'sun', 'winterpl', 'tallest', 'veget', 'nearest', 'wal', 'giv', 'short', 'on', 'ray', 'plant', 'plant', 'youl', 'eat', 'get', 'excit', 'eat', 'stuff', 'giv', 'energy', 'maintain'], ['ditto', 'wallsun', 'comment', 'even', 'latitude', 'limit', 'sun', 'winterplant', 'tallest', 'vegetables', 'nearest', 'wall', 'give', 'shorter', 'ones', 'ray', 'plant', 'plant', 'youll', 'eat', 'get', 'excite', 'eat', 'stuff', 'give', 'energy', 'maintain'])\n",
      "original document: \n",
      "['There', 'are', 'multiple', 'ways', 'it', 'could', 'have', 'been', 'made,', 'but', \"let's\", 'even', 'pretend', 'it', 'was', 'something', 'as', '\"advanced\"', 'as', 'silk', 'screening.', 'The', 'way', 'people', 'keep', 'whining', 'about', 'this', \"you'd\", 'think', 'they', 'thought', 'silk', 'screening', 'process', 'was', 'this', 'crazy,', 'futuristic', 'technology.', 'Try', 'visiting', 'one', 'of', 'these', 'shops:', 'it', \"isn't.\", 'And', 'even', 'what', 'the', 'good', 'ones', 'do', 'can', 'be', 'done', 'on', 'the', 'cheap', 'if', 'wanted', '--', 'this', 'shirt', 'is', 'block', 'white', 'letters.', \"It's\", 'like', 'being', 'mystified', 'over', 'someone', 'drawing', 'a', 'poster', 'with', 'a', \"sharpie.\\n\\nI'm\", 'not', 'sure', 'why', \"it's\", 'beyond', 'belief', 'that', 'some', 'person', 'who', 'owns', 'a', 'shirt', 'making', 'business', 'took', 'the', 'time', 'to', 'make', 'this', 'because', 'he', 'thought', \"it'd\", 'get', \"someone's\", 'attention.', 'It', 'was', 'probably', 'something', 'the', 'creator', 'thought', 'he', 'or', 'she', 'could', 'do', 'with', 'their', 'skillset.', 'And', 'so', 'what?', 'I', 'know', 'you', \"don't\", 'actually', 'care', 'about', 'this', 'and', 'are', 'deflecting', 'from', 'the', 'real', 'issue,', 'but', 'seeing', 'this', 'argument', 'for', 'the', '100th', 'time', 'is', 'mind-numbing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['multipl', 'way', 'could', 'mad', 'let', 'ev', 'pretend', 'someth', 'adv', 'silk', 'screening', 'way', 'peopl', 'keep', 'whin', 'youd', 'think', 'thought', 'silk', 'screening', 'process', 'crazy', 'fut', 'technolog', 'try', 'visit', 'on', 'shop', 'isnt', 'ev', 'good', 'on', 'don', 'cheap', 'want', 'shirt', 'block', 'whit', 'let', 'lik', 'myst', 'someon', 'draw', 'post', 'sharpie\\n\\nim', 'sur', 'beyond', 'believ', 'person', 'own', 'shirt', 'mak', 'busy', 'took', 'tim', 'mak', 'thought', 'itd', 'get', 'someon', 'at', 'prob', 'someth', 'cre', 'thought', 'could', 'skillset', 'know', 'dont', 'act', 'car', 'deflect', 'real', 'issu', 'see', 'argu', '100th', 'tim', 'mindnumb'], ['multiple', 'ways', 'could', 'make', 'let', 'even', 'pretend', 'something', 'advance', 'silk', 'screen', 'way', 'people', 'keep', 'whine', 'youd', 'think', 'think', 'silk', 'screen', 'process', 'crazy', 'futuristic', 'technology', 'try', 'visit', 'one', 'shop', 'isnt', 'even', 'good', 'ones', 'do', 'cheap', 'want', 'shirt', 'block', 'white', 'letter', 'like', 'mystify', 'someone', 'draw', 'poster', 'sharpie\\n\\nim', 'sure', 'beyond', 'belief', 'person', 'own', 'shirt', 'make', 'business', 'take', 'time', 'make', 'think', 'itd', 'get', 'someones', 'attention', 'probably', 'something', 'creator', 'think', 'could', 'skillset', 'know', 'dont', 'actually', 'care', 'deflect', 'real', 'issue', 'see', 'argument', '100th', 'time', 'mindnumbing'])\n",
      "original document: \n",
      "['&gt;', 'Dennis', 'told', 'The', 'Sumter', 'Item', 'that', 'people', 'think', 'the', 'church', 'which', 'was', 'founded', 'by', 'Scotch-Irish', 'settlers', 'in', '1759', 'is', 'haunted,', 'but', 'he', 'doesn’t', 'know', 'why.\\n\\nHe', \"doesn't\", 'know', 'why', 'people', 'think', 'that', 'the', '**Salem', 'Black', 'River**', 'Presbyterian', 'Church', 'is', 'haunted?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'den', 'told', 'sumt', 'item', 'peopl', 'think', 'church', 'found', 'scotchir', 'settl', 'one thousand, seven hundred and fifty-nin', 'haunt', 'doesnt', 'know', 'why\\n\\nhe', 'doesnt', 'know', 'peopl', 'think', 'salem', 'black', 'riv', 'presbyt', 'church', 'haunt'], ['gt', 'dennis', 'tell', 'sumter', 'item', 'people', 'think', 'church', 'found', 'scotchirish', 'settlers', 'one thousand, seven hundred and fifty-nine', 'haunt', 'doesnt', 'know', 'why\\n\\nhe', 'doesnt', 'know', 'people', 'think', 'salem', 'black', 'river', 'presbyterian', 'church', 'haunt'])\n",
      "original document: \n",
      "['Yea', 'it', \"isn't\", 'the', 'best', 'for', 'durability', 'but', 'it', 'works!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'isnt', 'best', 'dur', 'work'], ['yea', 'isnt', 'best', 'durability', 'work'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'], ['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'])\n",
      "original document: \n",
      "['He', 'was', 'running', 'like', 'a', 'damn', 'good', 'back', 'last', 'night.', 'A', 'little', 'bit', 'of', 'flashes', 'of', 'Helu.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['run', 'lik', 'damn', 'good', 'back', 'last', 'night', 'littl', 'bit', 'flash', 'helu'], ['run', 'like', 'damn', 'good', 'back', 'last', 'night', 'little', 'bite', 'flash', 'helu'])\n",
      "original document: \n",
      "['First', 'avenue', 'has', 'room', 'for', 'improvement....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'avenu', 'room', 'improv'], ['first', 'avenue', 'room', 'improvement'])\n",
      "original document: \n",
      "['143415817|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'nN0OTd4c)\\n\\n&gt;&gt;143412250', '(OP)\\n\\n2008:', 'Primary', '-', 'Ron', 'Paul', '|', 'General', '-', 'Nobody\\n2012:', 'Primary', '-', 'Ron', 'Paul', '|', 'General', '-', 'Nobody\\n2016:', 'Primary', '-', 'Bernie', 'Sanders', '|', 'General', '-', 'Hillary', 'Clinton\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fifteen thousand, eight hundred and seventeen', 'gt', 'unit', 'stat', 'anonym', 'id', 'nn0otd4c\\n\\ngtgt143412250', 'op\\n\\n2008', 'prim', 'ron', 'paul', 'gen', 'nobody\\n2012', 'prim', 'ron', 'paul', 'gen', 'nobody\\n2016', 'prim', 'berny', 'sand', 'gen', 'hil', 'clinton\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fifteen thousand, eight hundred and seventeen', 'gt', 'unite', 'state', 'anonymous', 'id', 'nn0otd4c\\n\\ngtgt143412250', 'op\\n\\n2008', 'primary', 'ron', 'paul', 'general', 'nobody\\n2012', 'primary', 'ron', 'paul', 'general', 'nobody\\n2016', 'primary', 'bernie', 'sanders', 'general', 'hillary', 'clinton\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Up', 'until', 'this', 'post', 'I', 'had', 'thought', 'the', 'X', 'and', 'the', '8', 'were', 'the', 'exact', 'same', 'width', 'screen']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'thought', 'x', 'eight', 'exact', 'wid', 'screen'], ['post', 'think', 'x', 'eight', 'exact', 'width', 'screen'])\n",
      "original document: \n",
      "['Three', 'spots,', '42,', '35', 'and', '16']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['three', 'spot', 'forty-two', 'thirty-five', 'sixteen'], ['three', 'spot', 'forty-two', 'thirty-five', 'sixteen'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['True', 'to', 'some', 'extent.', '', 'But', 'for', 'default', 'costumes,', 'it', 'might', 'be', 'better', 'off', 'to', 'blend', 'them', 'now', 'if', 'you', 'only', 'have', '1-2', 'of', 'them', '(will', 'depend', 'how', 'many', 'costumes', 'you', 'already', 'own).', '', 'If', 'you', 'have', '3+,', 'probably', 'better', 'off', 'to', 'keep', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'ext', 'default', 'costum', 'might', 'bet', 'blend', 'twelv', 'depend', 'many', 'costum', 'already', 'three', 'prob', 'bet', 'keep'], ['true', 'extent', 'default', 'costume', 'might', 'better', 'blend', 'twelve', 'depend', 'many', 'costume', 'already', 'three', 'probably', 'better', 'keep'])\n",
      "original document: \n",
      "['This', 'is', 'the', 'top', 'comment,', 'so', \"I'm\", 'going', 'to', 'apologize', 'here', 'because', 'I', 'just', 'want', 'views.', '', 'This', 'was', 'a', 'breakdown', 'I', 'did', 'of', 'this', 'a', 'while', 'back', '', 'It', 'still', 'works.\\n\\nhttps://www.reddit.com/r/nba/comments/6x7t89/a_rant_on_russell_westbrook/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['top', 'com', 'im', 'going', 'apolog', 'want', 'view', 'breakdown', 'back', 'stil', 'works\\n\\nhttpswwwredditcomrnbacomments6x7t89a_rant_on_russell_westbrook'], ['top', 'comment', 'im', 'go', 'apologize', 'want', 'view', 'breakdown', 'back', 'still', 'works\\n\\nhttpswwwredditcomrnbacomments6x7t89a_rant_on_russell_westbrook'])\n",
      "original document: \n",
      "['292', 'hunter,', 'psn:', 'chemicologist']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two hundred and ninety-two', 'hunt', 'psn', 'chemicolog'], ['two hundred and ninety-two', 'hunter', 'psn', 'chemicologist'])\n",
      "original document: \n",
      "['All', 'good', 'things', 'take', 'time,', 'my', 'son...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'thing', 'tak', 'tim', 'son'], ['good', 'things', 'take', 'time', 'son'])\n",
      "original document: \n",
      "['[+AxezCore](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoku7m/):\\n\\nDid', 'you', 'know', 'swans', 'can', 'be', 'gay?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['axezcorehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoku7m\\n\\ndid', 'know', 'swan', 'gay'], ['axezcorehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoku7m\\n\\ndid', 'know', 'swan', 'gay'])\n",
      "original document: \n",
      "['jesus', 'that', 'price', 'on', 'the', 'tt...', 'y', 'dont', 'i', 'have', 'keys']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jes', 'pric', 'tt', 'dont', 'key'], ['jesus', 'price', 'tt', 'dont', 'key'])\n",
      "original document: \n",
      "['For', 'you', 'kids', 'who', \"don't\", 'remember,', 'Grant', 'Roberts', 'smoked', 'some', 'weed', 'in', 'the', 'Shea', 'Stadium', 'parking', 'lot', 'in', '2002', 'or', 'so.', 'Also,', 'the', 'Mets', 'had', 'a', 'press', 'conference', 'at', 'Shea', 'Stadium', 'for', 'Mike', 'Piazza', 'to', 'address', 'those', 'gay', 'rumors.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kid', 'dont', 'rememb', 'grant', 'robert', 'smok', 'wee', 'she', 'stad', 'park', 'lot', 'two thousand and two', 'also', 'met', 'press', 'conf', 'she', 'stad', 'mik', 'piazz', 'address', 'gay', 'rum'], ['kid', 'dont', 'remember', 'grant', 'roberts', 'smoke', 'weed', 'shea', 'stadium', 'park', 'lot', 'two thousand and two', 'also', 'mets', 'press', 'conference', 'shea', 'stadium', 'mike', 'piazza', 'address', 'gay', 'rumor'])\n",
      "original document: \n",
      "['I', 'hope', 'to', 'be', 'there', 'by', '1:30-2']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'one thousand, three hundred and two'], ['hope', 'one thousand, three hundred and two'])\n",
      "original document: \n",
      "['Romans', 'bugger', 'off,\\nSaxons', 'and', 'Danes', 'arrive', 'then', 'stab', 'each', 'other', 'until', 'only', 'one', 'Saxon', 'kingdom', 'remains,', 'Decides', 'to', 'call', 'this', 'England,', 'Gets', 'a', 'fancy', 'hat.\\n\\nFrench', 'vikings', 'decide', 'fancy', 'hat', 'would', 'look', 'better', 'on', 'them,', 'Become', 'kings', 'of', 'England.\\n\\nLast', 'welsh', 'kingdoms', 'decide', 'now', 'would', 'be', 'a', 'great', 'time', 'to', 'attack', '(I', 'mean', 'what', 'could', 'go', 'wrong)\\nOnly', 'border', 'with', 'Scotland', 'left', 'on', 'island,', 'England', 'and', 'Scotland', 'then', 'proceed', 'to', 'try', 'kill', 'each', 'other', 'for', 'the', 'next', 'few', \"century's.\\n\\nEngland\", 'runs', 'out', 'of', 'people', 'to', 'wear', 'fancy', 'hat', 'so', 'Scottish', 'king', 'gets', 'both.', 'Procced', 'to', 'have', 'civil', 'war,', 'Get', 'Totally', 'not', 'dictator', 'for', 'life', '\"Lord', 'protector', 'Cromwell\"', 'he', 'bans', 'fun,', 'When', 'he', 'dies', 'we', 'decide', 'kings', \"aren't\", 'that', 'bad', 'and', 'some', 'dutch', 'guy', 'comes', 'to', 'wear', 'the', 'fancy', 'hat.\\nHas', 'no', 'children', 'so', 'we', 'import', 'Germans.\\n\\nTry', 'to', 'take', 'over', 'planet', 'starting', 'with', 'America,', 'America', 'escapes.\\nTry', 'to', 'take', 'over', 'the', 'planet', 'again', 'this', 'time', 'much', 'more', 'successfully.\\nFrench', 'midget', 'annexes', 'Europe,', 'we', 'ruin', 'his', 'day,', 'Twice.\\n\\nSteal', \"everybody's\", 'stuff', 'for', 'a', 'century', 'without', 'much', 'bother,', 'Germans', 'unite', 'finally', 'and', 'decide', 'world', 'would', 'be', 'better', 'with', 'them', 'in', 'charge', '(I', 'mean', 'what', 'could', 'go', 'wrong)', 'we', 'get', 'colony', 'as', 'spoils.\\nGermans', 'decide', 'again', 'they', 'should', 'rule', 'world,', 'we', 'laugh,', 'the', 'french', 'laugh,', 'the', 'Russians', 'laugh', 'and', 'then', 'the', 'Germans', 'kick', 'all', 'our', 'arses.', 'America', 'saves', 'the', 'day', 'but', 'say', '\"no', 'empire', 'any', 'more\"\\n\\nWe', 'lose', 'the', 'empire', 'really', 'quickly,', 'then', 'go', 'broke.\\nJoin', 'this', 'new', 'European', 'trading', 'thingy', 'get', 'rich', 'again.', 'then', 'a', 'few', 'decades', 'later', 'leave', 'again']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rom', 'bug', 'off\\nsaxons', 'dan', 'ar', 'stab', 'on', 'saxon', 'kingdom', 'remain', 'decid', 'cal', 'england', 'get', 'fant', 'hat\\n\\nfrench', 'vik', 'decid', 'fant', 'hat', 'would', 'look', 'bet', 'becom', 'king', 'england\\n\\nlast', 'welsh', 'kingdom', 'decid', 'would', 'gre', 'tim', 'attack', 'mean', 'could', 'go', 'wrong\\nonly', 'bord', 'scotland', 'left', 'island', 'england', 'scotland', 'process', 'try', 'kil', 'next', 'centurys\\n\\nengland', 'run', 'peopl', 'wear', 'fant', 'hat', 'scot', 'king', 'get', 'procc', 'civil', 'war', 'get', 'tot', 'dict', 'lif', 'lord', 'protect', 'cromwel', 'ban', 'fun', 'die', 'decid', 'king', 'ar', 'bad', 'dutch', 'guy', 'com', 'wear', 'fant', 'hat\\nhas', 'childr', 'import', 'germans\\n\\ntry', 'tak', 'planet', 'start', 'americ', 'americ', 'escapes\\ntry', 'tak', 'planet', 'tim', 'much', 'successfully\\nfrench', 'midget', 'annex', 'europ', 'ruin', 'day', 'twice\\n\\nsteal', 'everybody', 'stuff', 'century', 'without', 'much', 'both', 'germ', 'unit', 'fin', 'decid', 'world', 'would', 'bet', 'charg', 'mean', 'could', 'go', 'wrong', 'get', 'colony', 'spoils\\ngerman', 'decid', 'rul', 'world', 'laugh', 'french', 'laugh', 'russ', 'laugh', 'germ', 'kick', 'ars', 'americ', 'sav', 'day', 'say', 'empir', 'more\\n\\nw', 'los', 'empir', 'real', 'quick', 'go', 'broke\\njoin', 'new', 'europ', 'trad', 'thingy', 'get', 'rich', 'decad', 'lat', 'leav'], ['romans', 'bugger', 'off\\nsaxons', 'danes', 'arrive', 'stab', 'one', 'saxon', 'kingdom', 'remain', 'decide', 'call', 'england', 'get', 'fancy', 'hat\\n\\nfrench', 'vikings', 'decide', 'fancy', 'hat', 'would', 'look', 'better', 'become', 'kings', 'england\\n\\nlast', 'welsh', 'kingdoms', 'decide', 'would', 'great', 'time', 'attack', 'mean', 'could', 'go', 'wrong\\nonly', 'border', 'scotland', 'leave', 'island', 'england', 'scotland', 'proceed', 'try', 'kill', 'next', 'centurys\\n\\nengland', 'run', 'people', 'wear', 'fancy', 'hat', 'scottish', 'king', 'get', 'procced', 'civil', 'war', 'get', 'totally', 'dictator', 'life', 'lord', 'protector', 'cromwell', 'ban', 'fun', 'die', 'decide', 'kings', 'arent', 'bad', 'dutch', 'guy', 'come', 'wear', 'fancy', 'hat\\nhas', 'children', 'import', 'germans\\n\\ntry', 'take', 'planet', 'start', 'america', 'america', 'escapes\\ntry', 'take', 'planet', 'time', 'much', 'successfully\\nfrench', 'midget', 'annex', 'europe', 'ruin', 'day', 'twice\\n\\nsteal', 'everybodys', 'stuff', 'century', 'without', 'much', 'bother', 'germans', 'unite', 'finally', 'decide', 'world', 'would', 'better', 'charge', 'mean', 'could', 'go', 'wrong', 'get', 'colony', 'spoils\\ngermans', 'decide', 'rule', 'world', 'laugh', 'french', 'laugh', 'russians', 'laugh', 'germans', 'kick', 'arses', 'america', 'save', 'day', 'say', 'empire', 'more\\n\\nwe', 'lose', 'empire', 'really', 'quickly', 'go', 'broke\\njoin', 'new', 'european', 'trade', 'thingy', 'get', 'rich', 'decades', 'later', 'leave'])\n",
      "original document: \n",
      "['HOLD', 'THREE,', 'THE', 'SASKATCHEWAN', 'SPINNING', 'NERVE', 'HOLD\\n\\n\"Fucking', 'Ridiculous\"', 'Cried', 'Tony', 'Ferguson.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hold', 'three', 'saskatchew', 'spin', 'nerv', 'hold\\n\\nfucking', 'ridic', 'cri', 'tony', 'ferguson'], ['hold', 'three', 'saskatchewan', 'spin', 'nerve', 'hold\\n\\nfucking', 'ridiculous', 'cry', 'tony', 'ferguson'])\n",
      "original document: \n",
      "['Sent', 'invite.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sent', 'invit'], ['send', 'invite'])\n",
      "original document: \n",
      "['I', 'grabbed', 'the', 'red', 'rye', 'today.', 'Gotta', 'say', 'it’s', 'pretty', 'enjoyable.', 'Might', 'have', 'to', 'grab', 'some', 'more', 'before', 'they', 'sell', 'out!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['grab', 'red', 'rye', 'today', 'gott', 'say', 'pretty', 'enjoy', 'might', 'grab', 'sel'], ['grab', 'red', 'rye', 'today', 'gotta', 'say', 'pretty', 'enjoyable', 'might', 'grab', 'sell'])\n",
      "original document: \n",
      "['1.', '', '782', '(TransUnion', 'on', 'Credit', 'Karma),', '776', '(Equifax', 'on', 'Credit', 'Karma)\\n\\n2.', 'Chase', 'Sapphire', 'Reserve', '(1/17),', 'JetBlue', '(no', 'annual', 'fee)\\n\\n3.', 'Currently', 'w/', '15k', 'AA', 'miles;', '100k', 'UR;', '10k', 'Jetblue\\n\\n5.', 'Flying', 'out', 'of', 'BOS,', 'but', 'LA', 'starting', 'in', 'December\\n\\n6.', 'Tokyo,', 'Hong', 'Kong,', 'SE', 'Asia\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'seven hundred and eighty-two', 'transun', 'credit', 'karm', 'seven hundred and seventy-six', 'equifax', 'credit', 'karma\\n\\n2', 'chas', 'sapphir', 'reserv', 'one hundred and seventeen', 'jetblu', 'an', 'fee\\n\\n3', 'cur', 'w', '15k', 'aa', 'mil', '100k', 'ur', '10k', 'jetblue\\n\\n5', 'fly', 'bos', 'la', 'start', 'december\\n\\n6', 'tokyo', 'hong', 'kong', 'se', 'asia\\n'], ['one', 'seven hundred and eighty-two', 'transunion', 'credit', 'karma', 'seven hundred and seventy-six', 'equifax', 'credit', 'karma\\n\\n2', 'chase', 'sapphire', 'reserve', 'one hundred and seventeen', 'jetblue', 'annual', 'fee\\n\\n3', 'currently', 'w', '15k', 'aa', 'miles', '100k', 'ur', '10k', 'jetblue\\n\\n5', 'fly', 'bos', 'la', 'start', 'december\\n\\n6', 'tokyo', 'hong', 'kong', 'se', 'asia\\n'])\n",
      "original document: \n",
      "['Did', 'you', 'order', 'on', 'Amazon?', '', 'When', 'were', 'you', 'given', 'a', 'shipping', 'estimate', 'and', 'all', 'that?', '', 'My', 'estimated', 'delivery', 'date', 'is', 'monday', 'but', \"I'm\", 'past', 'the', 'estimated', 'ship', 'date', 'now', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ord', 'amazon', 'giv', 'ship', 'estim', 'estim', 'delivery', 'dat', 'monday', 'im', 'past', 'estim', 'ship', 'dat'], ['order', 'amazon', 'give', 'ship', 'estimate', 'estimate', 'delivery', 'date', 'monday', 'im', 'past', 'estimate', 'ship', 'date'])\n",
      "original document: \n",
      "['that', 'spider', 'is', 'terrifying']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spid', 'terr'], ['spider', 'terrify'])\n",
      "original document: \n",
      "['The', 'interior', 'of', 'that', 'vehicle', 'is', 'no', 'more']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['intery', 'vehic'], ['interior', 'vehicle'])\n",
      "original document: \n",
      "[\"Who's\", '17?', 'He', 'looks', 'a', 'lot', 'like', 'Allan', 'Houston']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['who', 'seventeen', 'look', 'lot', 'lik', 'al', 'houston'], ['whos', 'seventeen', 'look', 'lot', 'like', 'allan', 'houston'])\n",
      "original document: \n",
      "['Climate', 'change', 'is', 'a', 'phenomenon', 'that', 'is', 'studied', 'by', 'science.', 'And', 'the', 'science', 'on', 'it', 'is', 'settled.', 'Denying', 'it', 'is', 'denying', 'science.', 'The', 'only', 'political', 'aspect', 'of', 'it', 'is', 'how', 'we', 'should', 'go', 'about', 'alleviating', 'the', 'effects', 'of', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['clim', 'chang', 'phenomenon', 'study', 'sci', 'sci', 'settl', 'deny', 'deny', 'sci', 'polit', 'aspect', 'go', 'allevy', 'effect'], ['climate', 'change', 'phenomenon', 'study', 'science', 'science', 'settle', 'deny', 'deny', 'science', 'political', 'aspect', 'go', 'alleviate', 'effect'])\n",
      "original document: \n",
      "['&gt;', 'Xbox', 'One\\n\\nY', 'u', 'do', 'dis', 'to', 'me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'xbox', 'one\\n\\ny', 'u', 'dis'], ['gt', 'xbox', 'one\\n\\ny', 'u', 'dis'])\n",
      "original document: \n",
      "['I', 'saw', 'a', 'documentary', 'called', 'CROPSY', 'on', 'Netflix', 'awhile', 'ago', 'and', 'it', 'really', 'creeped', 'me', 'out.', 'Not', 'sure', 'if', 'it’s', 'still', 'there.', 'Beware', 'the', 'Slenderman', 'is', 'also', 'really', 'unnerving.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'docu', 'cal', 'cropsy', 'netflix', 'awhil', 'ago', 'real', 'creep', 'sur', 'stil', 'bew', 'slenderm', 'also', 'real', 'unnerv'], ['saw', 'documentary', 'call', 'cropsy', 'netflix', 'awhile', 'ago', 'really', 'creep', 'sure', 'still', 'beware', 'slenderman', 'also', 'really', 'unnerve'])\n",
      "original document: \n",
      "['See,', 'this', 'is', 'why', 'they', 'all', 'need', 'to', 'be', 'hooked', 'into', 'this', 'central', 'Ai.', 'So', 'it', 'can', 'keep', 'them', 'and', 'any', 'infiltrators', 'apart.', 'Nothing', 'could', 'ever', 'go', 'wrong', 'and', 'no', 'this', 'is', 'not', 'a', 'sinister', 'plot', 'to', 'become', 'an', 'even', 'more', 'autocratic', 'ruler.', 'Now', 'have', 'the', 'damn', 'ship', 'installed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'nee', 'hook', 'cent', 'ai', 'keep', 'infilt', 'apart', 'noth', 'could', 'ev', 'go', 'wrong', 'sin', 'plot', 'becom', 'ev', 'autocr', 'rul', 'damn', 'ship', 'instal'], ['see', 'need', 'hook', 'central', 'ai', 'keep', 'infiltrators', 'apart', 'nothing', 'could', 'ever', 'go', 'wrong', 'sinister', 'plot', 'become', 'even', 'autocratic', 'ruler', 'damn', 'ship', 'instal'])\n",
      "original document: \n",
      "['Like?', 'Nope.', 'More', 'like', 'LOVE!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'nop', 'lik', 'lov'], ['like', 'nope', 'like', 'love'])\n",
      "original document: \n",
      "['At', 'that', 'point', 'I', 'like', 'to', 'just', 'do', 'the', 'homework', 'in', 'class', 'so', 'at', 'least', 'my', 'time', \"isn't\", 'a', 'complete', 'loss']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['point', 'lik', 'homework', 'class', 'least', 'tim', 'isnt', 'complet', 'loss'], ['point', 'like', 'homework', 'class', 'least', 'time', 'isnt', 'complete', 'loss'])\n",
      "original document: \n",
      "['Nah']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah'], ['nah'])\n",
      "original document: \n",
      "['If', \"you're\", 'focused', 'on', 'Beermoney', 'by', 'referrals,', 'use', 'RobinHood', 'instead.', '', 'Their', 'referral', 'program', 'is', 'amazing.', 'You', 'and', 'your', 'referral', 'each', 'get', 'a', 'full', 'share', 'of', 'stock', 'like', 'apple,', 'ford,', 'bank', 'of', 'america,', 'sirius', 'XM.', '', 'Also,', 'no', 'fees', 'to', 'cash', 'out', 'or', 'trade.', '', 'No', 'DRIP', 'or', 'partial', 'shares', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'focus', 'beermoney', 'refer', 'us', 'robin', 'instead', 'refer', 'program', 'amaz', 'refer', 'get', 'ful', 'shar', 'stock', 'lik', 'appl', 'ford', 'bank', 'americ', 'siri', 'xm', 'also', 'fee', 'cash', 'trad', 'drip', 'part', 'shar', 'though'], ['youre', 'focus', 'beermoney', 'referrals', 'use', 'robinhood', 'instead', 'referral', 'program', 'amaze', 'referral', 'get', 'full', 'share', 'stock', 'like', 'apple', 'ford', 'bank', 'america', 'sirius', 'xm', 'also', 'fee', 'cash', 'trade', 'drip', 'partial', 'share', 'though'])\n",
      "original document: \n",
      "['In', 'my', 'head', 'canon', 'it', 'still', 'happens', 'before', 'the', 'actual', 'best', 'ending']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['head', 'canon', 'stil', 'hap', 'act', 'best', 'end'], ['head', 'canon', 'still', 'happen', 'actual', 'best', 'end'])\n",
      "original document: \n",
      "['Yeah', 'I', 'keep', 'hearing', 'that', 'rumor', 'I', 'would', 'like', 'to', 'see', 'evidence', 'but', 'he', 'does', 'look', 'much', 'older.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'keep', 'hear', 'rum', 'would', 'lik', 'see', 'evid', 'look', 'much', 'old'], ['yeah', 'keep', 'hear', 'rumor', 'would', 'like', 'see', 'evidence', 'look', 'much', 'older'])\n",
      "original document: \n",
      "['Yay!', 'Small', 'battles!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yay', 'smal', 'battl'], ['yay', 'small', 'battle'])\n",
      "original document: \n",
      "['Hopefully', 'this', 'will', 'help', 'reduce', 'the', 'number', 'of', '“we’ve', 'reached”', 'screenshot', 'posts', 'of', 'memes', 'in', 'other', 'subs.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'help', 'reduc', 'numb', 'wev', 'reach', 'screenshot', 'post', 'mem', 'sub'], ['hopefully', 'help', 'reduce', 'number', 'weve', 'reach', 'screenshot', 'post', 'memes', 'sub'])\n",
      "original document: \n",
      "['Him', 'and', 'his', 'adDicktions']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['addickt'], ['addicktions'])\n",
      "original document: \n",
      "[\"That's\", 'silly', 'and', 'irrational', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'sil', 'ir'], ['thats', 'silly', 'irrational'])\n",
      "original document: \n",
      "['Hey', 'folks,\\n\\nPS4', 'on', 'the', 'west', 'coast', 'with', 'a', 'mic.', '2', 'wins', 'so', 'far', 'and', 'own', 'a', 'house', 'near', 'retail', 'row.\\n\\nLongwaywest', 'is', 'my', 'tag.', 'Feel', 'free', 'to', 'add', 'me', 'if', 'you', 'want', 'to', 'play.', \"\\n\\nLet's\", 'go', 'bag', 'some', 'definitely', 'not', 'chicken', 'dinners.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'folks\\n\\nps4', 'west', 'coast', 'mic', 'two', 'win', 'far', 'hous', 'near', 'retail', 'row\\n\\nlongwaywest', 'tag', 'feel', 'fre', 'ad', 'want', 'play', '\\n\\nlets', 'go', 'bag', 'definit', 'chick', 'din'], ['hey', 'folks\\n\\nps4', 'west', 'coast', 'mic', 'two', 'win', 'far', 'house', 'near', 'retail', 'row\\n\\nlongwaywest', 'tag', 'feel', 'free', 'add', 'want', 'play', '\\n\\nlets', 'go', 'bag', 'definitely', 'chicken', 'dinners'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol72o/):\\n\\nI', 'did!', 'I', 'think', 'its', 'lovely', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol72o\\n\\ni', 'think', 'lov'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol72o\\n\\ni', 'think', 'lovely'])\n",
      "original document: \n",
      "['Oh', 'god,', 'see', 'to', 'be', 'triggered', 'is', 'to', 'be', 'offended', 'and', 'filled', 'with', 'hate', 'about', 'something', 'someone', 'says.', 'The', 'only', 'thing', \"I'm\", 'doing', 'is', 'making', 'fun', 'of', 'your', 'retarded', 'point', 'of', 'view.', 'Also,', 'good', 'job', 'ignoring', 'the', 'Asian', 'thing', 'which', 'proved', 'you', 'unequivocally', 'wrong.', \"I'm\", 'done', 'mocking', 'you', 'now,', \"it's\", 'getting', 'boring.', 'Bye', 'faggot!', '\\n\\nSorry', 'to', 'use', 'another', 'ad', 'hominem', 'attack!', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'god', 'see', 'trig', 'offend', 'fil', 'hat', 'someth', 'someon', 'say', 'thing', 'im', 'mak', 'fun', 'retard', 'point', 'view', 'also', 'good', 'job', 'ign', 'as', 'thing', 'prov', 'unequivoc', 'wrong', 'im', 'don', 'mock', 'get', 'bor', 'bye', 'faggot', '\\n\\nsorry', 'us', 'anoth', 'ad', 'hominem', 'attack'], ['oh', 'god', 'see', 'trigger', 'offend', 'fill', 'hate', 'something', 'someone', 'say', 'thing', 'im', 'make', 'fun', 'retard', 'point', 'view', 'also', 'good', 'job', 'ignore', 'asian', 'thing', 'prove', 'unequivocally', 'wrong', 'im', 'do', 'mock', 'get', 'bore', 'bye', 'faggot', '\\n\\nsorry', 'use', 'another', 'ad', 'hominem', 'attack'])\n",
      "original document: \n",
      "['Your', 'comment', 'has', 'been', 'removed', 'because:\\n\\nGendered', 'slurs', 'are', 'strictly', 'scrutinized;', 'please', 'see', 'our', '[gendered', 'slurs', 'policy', 'guide.](/r/askwomen/w/genderedslurs)', '', '\\nIf', 'you', 'edit', 'your', 'comment,', 'let', 'us', 'know', 'and', 'it', 'may', 'be', 'reinstated.', '\\n\\n\\n\\n**[Have', 'questions', 'about', 'this', 'moderator', 'action?', 'CLICK', 'HERE!](http://www.reddit.com/message/compose/?to=/r/AskWomen&amp;subject=Why+was+this+removed?&amp;message=\\\\[My+comment\\\\]\\\\(https://www.reddit.com/r/AskWomen/comments/73i7xi/what_are_your_thoughts_on_the_idea_behind_male/dnqhbc1/\\\\)+was+removed+and+I+do+not+understand+the+reason+given+by+the+mod+who+acted', 'upon+it.)**', '', '', '', '\\n\\n\\n[AskWomen', 'rules](/r/askwomen/w/rules)', '|', '[AskWomen', 'FAQ](/r/askwomen/w/index)', '', '\\n[reddit', 'rules](http://www.reddit.com/rules/)', '|', '[reddiquette](http://www.reddit.com/wiki/reddiquette)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['com', 'remov', 'because\\n\\ngendered', 'slur', 'strictly', 'scrutinized', 'pleas', 'see', 'gend', 'slur', 'policy', 'guideraskwomenwgenderedsl', '\\nif', 'edit', 'com', 'let', 'us', 'know', 'may', 'reinst', '\\n\\n\\n\\nhave', 'quest', 'mod', 'act', 'click', 'herehttpwwwredditcommessagecomposetoraskwomenampsubjectwhywasthisremovedampmessagemycommenthttpswwwredditcomraskwomencomments73i7xiwhat_are_your_thoughts_on_the_idea_behind_malednqhbc1wasremovedandidonotunderstandthereasongivenbythemodwhoacted', 'uponit', '\\n\\n\\naskwomen', 'rulesraskwomenwr', 'askwom', 'faqraskwomenwindex', '\\nreddit', 'ruleshttpwwwredditcomr', 'reddiquettehttpwwwredditcomwikireddiquet'], ['comment', 'remove', 'because\\n\\ngendered', 'slur', 'strictly', 'scrutinize', 'please', 'see', 'gendered', 'slur', 'policy', 'guideraskwomenwgenderedslurs', '\\nif', 'edit', 'comment', 'let', 'us', 'know', 'may', 'reinstate', '\\n\\n\\n\\nhave', 'question', 'moderator', 'action', 'click', 'herehttpwwwredditcommessagecomposetoraskwomenampsubjectwhywasthisremovedampmessagemycommenthttpswwwredditcomraskwomencomments73i7xiwhat_are_your_thoughts_on_the_idea_behind_malednqhbc1wasremovedandidonotunderstandthereasongivenbythemodwhoacted', 'uponit', '\\n\\n\\naskwomen', 'rulesraskwomenwrules', 'askwomen', 'faqraskwomenwindex', '\\nreddit', 'ruleshttpwwwredditcomrules', 'reddiquettehttpwwwredditcomwikireddiquette'])\n",
      "original document: \n",
      "['No', 'because', 'you', 'are', 'selling', 'them', 'for', 'a', 'loss']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sel', 'loss'], ['sell', 'loss'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['This', 'posting', 'aroused', 'my', 'interest,', 'alas', 'there', 'is', 'no', 'details.', 'Gonegirl27', 'I', 'want', 'all', 'the', 'juicy', 'details', 'of', 'your', 'past', 'debauchery', '...', 'please,', 'please,', 'please', 'do', 'share.\\n\\nJust', 'kidding', '!!!', '...', 'I', 'am', 'more', 'interested', 'in', 'the', 'rock', 'n', 'roll.', 'I', 'know', 'you', 'are', 'a', 'big', 'fan', 'of', 'the', '[Grateful', 'Dead](https://www.youtube.com/watch?v=yOTWLpSUBM8)', 'which', 'other', 'bands', 'inspired', 'you', '?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'ar', 'interest', 'ala', 'detail', 'gonegirl27', 'want', 'juicy', 'detail', 'past', 'debauchery', 'pleas', 'pleas', 'pleas', 'share\\n\\njust', 'kid', 'interest', 'rock', 'n', 'rol', 'know', 'big', 'fan', 'grat', 'deadhttpswwwyoutubecomwatchvyotwlpsubm8', 'band', 'inspir'], ['post', 'arouse', 'interest', 'alas', 'detail', 'gonegirl27', 'want', 'juicy', 'detail', 'past', 'debauchery', 'please', 'please', 'please', 'share\\n\\njust', 'kid', 'interest', 'rock', 'n', 'roll', 'know', 'big', 'fan', 'grateful', 'deadhttpswwwyoutubecomwatchvyotwlpsubm8', 'band', 'inspire'])\n",
      "original document: \n",
      "['Your', 'post', 'has', 'been', 'removed', 'because', 'it', 'appears', 'to', 'be', 'about', 'a', 'survey', 'or', 'poll,', 'which', 'belongs', 'in', '/r/samplesize.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/cars)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'remov', 'appear', 'survey', 'pol', 'belong', 'rsamplesize\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorc', 'quest', 'concern'], ['post', 'remove', 'appear', 'survey', 'poll', 'belong', 'rsamplesize\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorcars', 'question', 'concern'])\n",
      "original document: \n",
      "['Rowlet', 'plushie', 'fer', 'sure', 'yoo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rowlet', 'plushy', 'fer', 'sur', 'yoo'], ['rowlet', 'plushie', 'fer', 'sure', 'yoo'])\n",
      "original document: \n",
      "['Always', 'so', 'dramatic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'dram'], ['always', 'dramatic'])\n",
      "original document: \n",
      "['Dabo', 'looks', 'like', \"he'll\", 'change', 'your', 'oil', 'right', 'quick\\n\\nFuente', 'looks', 'like', \"he'll\", 'be', 'the', '+1', 'your', 'rebellious', 'daughter', 'brings', 'home', 'at', 'her', 'second', 'Thanksgiving', 'Break', 'home', 'from', 'college']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dabo', 'look', 'lik', 'hel', 'chang', 'oil', 'right', 'quick\\n\\nfuente', 'look', 'lik', 'hel', 'on', 'rebel', 'daught', 'bring', 'hom', 'second', 'thanksg', 'break', 'hom', 'colleg'], ['dabo', 'look', 'like', 'hell', 'change', 'oil', 'right', 'quick\\n\\nfuente', 'look', 'like', 'hell', 'one', 'rebellious', 'daughter', 'bring', 'home', 'second', 'thanksgiving', 'break', 'home', 'college'])\n",
      "original document: \n",
      "['&gt;it', 'ended', 'up', 'being', 'added', 'to', 'the', 'pool', '', '', '', '\\n', '', '', '', '\\nWow', 'they', 'were!', 'I', 'never', 'noticed', 'that', 'before', 'but', 'I', 'clearly', 'see', 'Brave', 'Lyn', 'listed', 'under', 'colorless.', 'So', 'I', 'have', 'hope', 'for', 'another', 'Sacred', 'Sparrow', '2.', '', '\\n', '', '', '', '', '', '\\n&gt;And', 'it', 'never', 'hurts', 'to', 'have', 'dancers', 'for', 'arena', 'assult', '', '', '', '\\n', '', '', '', '\\nFor', 'sure', 'since', \"they're\", 'viable', 'even', 'at', 'level', '1.', 'Chain', 'Challenge', 'missions', 'too.', 'Is', 'stopping', 'me', 'from', 'merging', 'my', '4x', 'Olivia', 'together', 'at', '4', 'stars.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtit', 'end', 'ad', 'pool', '\\n', '\\nwow', 'nev', 'not', 'clear', 'see', 'brav', 'lyn', 'list', 'colorless', 'hop', 'anoth', 'sacr', 'sparrow', 'two', '\\n', '\\ngtand', 'nev', 'hurt', 'dant', 'aren', 'assult', '\\n', '\\nfor', 'sur', 'sint', 'theyr', 'viabl', 'ev', 'level', 'on', 'chain', 'challeng', 'miss', 'stop', 'merg', '4x', 'oliv', 'togeth', 'four', 'star'], ['gtit', 'end', 'add', 'pool', '\\n', '\\nwow', 'never', 'notice', 'clearly', 'see', 'brave', 'lyn', 'list', 'colorless', 'hope', 'another', 'sacred', 'sparrow', 'two', '\\n', '\\ngtand', 'never', 'hurt', 'dancers', 'arena', 'assult', '\\n', '\\nfor', 'sure', 'since', 'theyre', 'viable', 'even', 'level', 'one', 'chain', 'challenge', 'missions', 'stop', 'merge', '4x', 'olivia', 'together', 'four', 'star'])\n",
      "original document: \n",
      "['A', 'spot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spot'], ['spot'])\n",
      "original document: \n",
      "['Yeah', 'good', 'thing', 'you', 'choke', 'In', 'the', 'FA', 'cup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'good', 'thing', 'chok', 'fa', 'cup'], ['yeah', 'good', 'thing', 'choke', 'fa', 'cup'])\n",
      "original document: \n",
      "['The', 'video', 'is', 'over', 'five', 'minutes,', 'so', 'here', 'is', 'the', 'mandatory', 'brief', 'description.', 'Curtis', 'explains', 'that', 'swapping', 'far-left', 'messages', 'with', 'far-right', 'messages', 'isn’t', 'going', 'to', 'de-politicize', 'comics', 'or', 'result', 'in', 'better', 'stories.', 'Instead', 'it', 'will', 'only', 'exacerbate', 'the', 'problem', 'and', 'result', 'in', 'more', 'shitty,', 'heavily', 'politicized', 'comics.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['video', 'fiv', 'minut', 'mand', 'brief', 'describ', 'curt', 'explain', 'swap', 'farleft', 'mess', 'farright', 'mess', 'isnt', 'going', 'depolit', 'com', 'result', 'bet', 'story', 'instead', 'exacerb', 'problem', 'result', 'shitty', 'heavy', 'polit', 'com'], ['video', 'five', 'minutes', 'mandatory', 'brief', 'description', 'curtis', 'explain', 'swap', 'farleft', 'message', 'farright', 'message', 'isnt', 'go', 'depoliticize', 'comics', 'result', 'better', 'stories', 'instead', 'exacerbate', 'problem', 'result', 'shitty', 'heavily', 'politicize', 'comics'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"It's\", 'ok', 'Emmitt', 'we', 'all', 'make', 'mistakes', 'and', 'post', 'being', 'a', 'Gaylord', 'on', 'the', 'internet', 'for', 'sympathy', 'points.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'emmit', 'mak', 'mistak', 'post', 'gaylord', 'internet', 'sympathy', 'point'], ['ok', 'emmitt', 'make', 'mistake', 'post', 'gaylord', 'internet', 'sympathy', 'point'])\n",
      "original document: \n",
      "['Dammit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dammit'], ['dammit'])\n",
      "original document: \n",
      "['For', 'fucks', 'sake', 'this', 'is', 'like', 'watching', 'Wales', 'Women', 'trying', 'to', 'attack.', 'Or', 'just', 'Wales,', 'really.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'sak', 'lik', 'watch', 'wal', 'wom', 'try', 'attack', 'wal', 'real'], ['fuck', 'sake', 'like', 'watch', 'wales', 'women', 'try', 'attack', 'wales', 'really'])\n",
      "original document: \n",
      "['Goose', 'bumps', '👍']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goos', 'bump'], ['goose', 'bump'])\n",
      "original document: \n",
      "['[If', 'you', 'want', 'to', 'watch', 'in,', \"there's\", 'a', 'fairly', 'recent', 'miniseries', 'about', 'it.](http://www.imdb.com/title/tt2372220/)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'watch', 'ther', 'fair', 'rec', 'minisery', 'ithttpwwwimdbcomtitlett2372220'], ['want', 'watch', 'theres', 'fairly', 'recent', 'miniseries', 'ithttpwwwimdbcomtitlett2372220'])\n",
      "original document: \n",
      "['Growth', 'rates', 'have', 'crashed', 'in', 'the', 'western', 'world', 'but', 'places', 'like', 'Ethiopia', 'it', 'has', 'surged.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['grow', 'rat', 'crash', 'western', 'world', 'plac', 'lik', 'ethiop', 'surg'], ['growth', 'rat', 'crash', 'western', 'world', 'place', 'like', 'ethiopia', 'surge'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['i', 'like', 'the', 'name', 'a', 'lot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'nam', 'lot'], ['like', 'name', 'lot'])\n",
      "original document: \n",
      "['WHERE!?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['143413305|', '&gt;', 'Canada', 'Anonymous', '(ID:', '8h4XA49X)\\n\\n&gt;&gt;143413258\\nk', 'sorry', 'lol\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and fiv', 'gt', 'canad', 'anonym', 'id', '8h4xa49x\\n\\ngtgt143413258\\nk', 'sorry', 'lol\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and five', 'gt', 'canada', 'anonymous', 'id', '8h4xa49x\\n\\ngtgt143413258\\nk', 'sorry', 'lol\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"It's\", '2am', 'here', 'and', 'I', 'literally', 'went', '\"OMG', 'this', 'is', 'GENIUS\"', 'in', 'front', 'of', 'my', 'computer']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['2am', 'lit', 'went', 'omg', 'geni', 'front', 'comput'], ['2am', 'literally', 'go', 'omg', 'genius', 'front', 'computer'])\n",
      "original document: \n",
      "['If', 'Nantes', 'could', 'somehow', 'finish', 'first', 'I', 'would', 'probably', 'nut', 'for', '3', 'days', 'straight.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nant', 'could', 'somehow', 'fin', 'first', 'would', 'prob', 'nut', 'three', 'day', 'straight'], ['nantes', 'could', 'somehow', 'finish', 'first', 'would', 'probably', 'nut', 'three', 'days', 'straight'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Well', 'as', 'Moira', 'would', 'say:', 'there', 'could', 'be', 'no', 'other', 'end', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'moir', 'would', 'say', 'could', 'end'], ['well', 'moira', 'would', 'say', 'could', 'end'])\n",
      "original document: \n",
      "['they', 'changed', 'it', 'you', 'cant', 'get', 'the', 'gamma', 'from', 'the', 'quest', 'anymore', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chang', 'cant', 'get', 'gamm', 'quest', 'anym'], ['change', 'cant', 'get', 'gamma', 'quest', 'anymore'])\n",
      "original document: \n",
      "['Spectre', 'has', 'fallen', 'and', \"isn't\", 'nearly', 'as', 'desirable', 'as', 'it', 'was.', 'I', 'value', 'the', 'like', 'octane', 'at', 'a', 'lab', 'and', 'honestly', 'I', 'value', 'the', 'halo', 'around', '2', 'slips', 'right', 'now.', \"I'd\", 'probably', 'take', '20xx', 'for', 'both', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spect', 'fal', 'isnt', 'near', 'desir', 'valu', 'lik', 'oct', 'lab', 'honest', 'valu', 'halo', 'around', 'two', 'slip', 'right', 'id', 'prob', 'tak', '20xx'], ['spectre', 'fall', 'isnt', 'nearly', 'desirable', 'value', 'like', 'octane', 'lab', 'honestly', 'value', 'halo', 'around', 'two', 'slip', 'right', 'id', 'probably', 'take', '20xx'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['did', 'he', 'eat', 'it', 'all?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eat'], ['eat'])\n",
      "original document: \n",
      "['&gt;', 'Rhopalogaster', 'transversarium\\n\\n[NEAT!!!](http://mushroomobserver.org/observer/index_observation?q=9fXu)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'rhopalogast', 'transversarium\\n\\nneathttpmushroomobserverorgobserverindex_observationq9fxu'], ['gt', 'rhopalogaster', 'transversarium\\n\\nneathttpmushroomobserverorgobserverindex_observationq9fxu'])\n",
      "original document: \n",
      "['This', 'content', 'brought', 'to', 'you', 'from', '\"Argentina', 'Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#off', 'site', 'feed', '\"Argentina', 'Pool\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'brought', 'argentin', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'sit', 'fee', 'argentin', 'pool\\n'], ['content', 'bring', 'argentina', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'site', 'fee', 'argentina', 'pool\\n'])\n",
      "original document: \n",
      "['The', 'stock', 'pickups', 'are', 'nice.', 'They', 'can', 'be', 'split', 'into', 'single', 'coils', 'and', 'sound', 'good', 'both', 'as', 'singles', 'and', 'humbuckers.', 'Plus', 'with', 'pickups,', 'the', 'higher', 'the', 'output,', 'the', 'less', 'they', 'clean', 'up.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stock', 'pickup', 'nic', 'split', 'singl', 'coil', 'sound', 'good', 'singl', 'humbuck', 'plu', 'pickup', 'high', 'output', 'less', 'cle'], ['stock', 'pickups', 'nice', 'split', 'single', 'coil', 'sound', 'good', 'single', 'humbuckers', 'plus', 'pickups', 'higher', 'output', 'less', 'clean'])\n",
      "original document: \n",
      "['As', 'in', 'a', 'single', '11x', 'draw?', 'Because', 'if', 'it', 'were', 'single', 'pulls', 'then', \"that's\", 'incredible', 'luck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['singl', '11x', 'draw', 'singl', 'pul', 'that', 'incred', 'luck'], ['single', '11x', 'draw', 'single', 'pull', 'thats', 'incredible', 'luck'])\n",
      "original document: \n",
      "['Looking', 'for', 'a', 'winner', 'of', 'Hey', 'Arnold', 'cast', 'signing', 'that', \"doesn't\", 'want', 'to', 'attend', 'or', 'has', 'an', 'extra', 'spot.', 'I', 'am', 'willing', 'to', 'pay', 'you', 'handsomely', 'for', 'the', 'trouble', '!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'win', 'hey', 'arnold', 'cast', 'sign', 'doesnt', 'want', 'attend', 'extr', 'spot', 'wil', 'pay', 'handsom', 'troubl'], ['look', 'winner', 'hey', 'arnold', 'cast', 'sign', 'doesnt', 'want', 'attend', 'extra', 'spot', 'will', 'pay', 'handsomely', 'trouble'])\n",
      "original document: \n",
      "['Why', \"don't\", 'you', 'try', 'it?', 'There', 'is', 'a', 'guide', 'in', 'my', 'answer:', 'https://blog.brave.com/loading-chrome-extensions-in-brave/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'try', 'guid', 'answ', 'httpsblogbravecomloadingchromeextensionsinbrave'], ['dont', 'try', 'guide', 'answer', 'httpsblogbravecomloadingchromeextensionsinbrave'])\n",
      "original document: \n",
      "['Not', 'at', 'all.', 'I', 'enjoy', 'being', 'an', 'Engineer', 'and', 'think', 'playing', 'games', 'for', 'a', 'living', 'would', 'actually', 'ruin', 'gaming', 'as', 'a', 'hobby', 'for', 'me.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['enjoy', 'engin', 'think', 'play', 'gam', 'liv', 'would', 'act', 'ruin', 'gam', 'hobby'], ['enjoy', 'engineer', 'think', 'play', 'game', 'live', 'would', 'actually', 'ruin', 'game', 'hobby'])\n",
      "original document: \n",
      "['true,', 'my', 'bad']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'bad'], ['true', 'bad'])\n",
      "original document: \n",
      "['How', 'appropriate', 'that', 'this', 'was', 'posted', 'on', 'conference', 'weekend', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['appropry', 'post', 'conf', 'weekend', 'lt3'], ['appropriate', 'post', 'conference', 'weekend', 'lt3'])\n",
      "original document: \n",
      "['', 'Should', 'have', 'definitley,', 'but', \"it's\", 'so', 'common', 'out', 'here', 'I', 'considered', 'it', 'pointless.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definitley', 'common', 'consid', 'pointless'], ['definitley', 'common', 'consider', 'pointless'])\n",
      "original document: \n",
      "['Can', 'you', 'not', 'please', \"I'm\", 'in', 'public', 'I', \"can't\", 'cry', 'rn', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'im', 'publ', 'cant', 'cry', 'rn'], ['please', 'im', 'public', 'cant', 'cry', 'rn'])\n",
      "original document: \n",
      "['Not', 'in', 'IB', 'but', 'in', 'a', 'similar', 'program', 'so', 'I', 'can', 'relate\\n\\nMy', 'friend', 'just', 'graduated', 'IB', 'and', 'she', 'says', 'her', 'class', 'of', '60', 'people', 'in', 'grade', '9', 'turned', 'into', '15', 'people', 'in', 'grade', '12,', 'so', 'good', 'job', 'for', 'making', 'it', 'this', 'far.\\n\\nThe', 'only', 'advice', 'I', 'have', 'motivation-wise', 'is', 'to', 'go', 'somewhere', 'where', 'lots', 'of', 'people', 'are', 'studying', 'and', 'working.', 'I', 'like', 'to', 'go', 'to', 'a', 'big', 'library', 'in', 'my', 'city', 'and', 'just', 'sit', 'there', 'studying', 'with', 'everyone', 'else', 'until', 'very', 'late', 'in', 'the', 'evening.', 'I', 'never', 'get', 'work', 'done', 'at', 'home', 'but', 'I', 'know', 'lots', 'of', 'people', 'like', 'me', 'find', 'it', 'easier', 'to', 'go', 'out', 'and', 'study.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ib', 'simil', 'program', 'relate\\n\\nmy', 'friend', 'gradu', 'ib', 'say', 'class', 'sixty', 'peopl', 'grad', 'nin', 'turn', 'fifteen', 'peopl', 'grad', 'twelv', 'good', 'job', 'mak', 'far\\n\\nthe', 'adv', 'motivationw', 'go', 'somewh', 'lot', 'peopl', 'study', 'work', 'lik', 'go', 'big', 'libr', 'city', 'sit', 'study', 'everyon', 'els', 'lat', 'ev', 'nev', 'get', 'work', 'don', 'hom', 'know', 'lot', 'peopl', 'lik', 'find', 'easy', 'go', 'study'], ['ib', 'similar', 'program', 'relate\\n\\nmy', 'friend', 'graduate', 'ib', 'say', 'class', 'sixty', 'people', 'grade', 'nine', 'turn', 'fifteen', 'people', 'grade', 'twelve', 'good', 'job', 'make', 'far\\n\\nthe', 'advice', 'motivationwise', 'go', 'somewhere', 'lot', 'people', 'study', 'work', 'like', 'go', 'big', 'library', 'city', 'sit', 'study', 'everyone', 'else', 'late', 'even', 'never', 'get', 'work', 'do', 'home', 'know', 'lot', 'people', 'like', 'find', 'easier', 'go', 'study'])\n",
      "original document: \n",
      "['That', 'Cut-throat', 'bitch!', '(and', 'about', '15', 'other', 'juicy', 'successful-series', 'repeating', 'character', 'roles).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cutthro', 'bitch', 'fifteen', 'juicy', 'successfulsery', 'rep', 'charact', 'rol'], ['cutthroat', 'bitch', 'fifteen', 'juicy', 'successfulseries', 'repeat', 'character', 'roles'])\n",
      "original document: \n",
      "['bruh', 'lmao', 'ima', 'have', 'to', 'try', 'this']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bruh', 'lmao', 'im', 'try'], ['bruh', 'lmao', 'ima', 'try'])\n",
      "original document: \n",
      "['Til', 'how', 'fucking', 'toxic', 'online', 'dating', 'is', 'and', 'why', 'no', 'one', 'on', 'Reddit', 'has', 'success.\\n\\nOh', 'every', 'profile', 'is', 'the', 'exact', 'same.', 'Not', 'like', 'I', \"haven't\", 'heard', 'the', 'same', 'meme', 'in', 'every', 'comment', 'thread', 'for', 'years', 'now.\\n\\nPeople', 'afraid', 'to', 'open', 'up', 'and', 'give', 'shitty', 'one', 'word', 'answers?', 'Naaaah', 'not', 'on', 'Reddit', 'full', 'of', 'confident', 'as', 'fuck', 'people', 'all', 'the', 'time.\\n\\nPictures', 'that', \"don't\", 'really', 'show', 'what', 'a', 'person', 'looks', 'Like!?', 'Jesus', 'Christ', 'not', 'on', 'my', 'Photoshop', 'free', 'Reddit.', '\\n\\nHave', 'some', 'patients,', 'first', 'dates', 'are', 'nerve', 'racking', 'So', 'will', 'the', 'first', 'day', 'or', 'two', 'of', 'messages.', 'People', 'definitely', \"aren't\", 'uploading', 'their', 'worst', 'photos,', 'who', 'would', 'have', 'thought,', 'every', 'person', 'you', 'meet', \"isn't\", 'some', 'insanely', 'original', 'human', 'when', 'all', 'they', 'have', 'to', 'go', 'off', 'of', 'is', 'a', 'paragraph?', 'Say', 'it', \"ain't\", 'so.', '\\n\\nI', 'get', 'the', 'joke,', 'but', \"y'all\", 'take', 'it', 'a', 'little', 'too', 'far.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['til', 'fuck', 'tox', 'onlin', 'dat', 'on', 'reddit', 'success\\n\\noh', 'every', 'profil', 'exact', 'lik', 'hav', 'heard', 'mem', 'every', 'com', 'thread', 'year', 'now\\n\\npeople', 'afraid', 'op', 'giv', 'shitty', 'on', 'word', 'answ', 'naaaah', 'reddit', 'ful', 'confid', 'fuck', 'peopl', 'time\\n\\npictures', 'dont', 'real', 'show', 'person', 'look', 'lik', 'jes', 'christ', 'photoshop', 'fre', 'reddit', '\\n\\nhave', 'paty', 'first', 'dat', 'nerv', 'rack', 'first', 'day', 'two', 'mess', 'peopl', 'definit', 'ar', 'upload', 'worst', 'photo', 'would', 'thought', 'every', 'person', 'meet', 'isnt', 'ins', 'origin', 'hum', 'go', 'paragraph', 'say', 'aint', '\\n\\ni', 'get', 'jok', 'yal', 'tak', 'littl', 'far'], ['til', 'fuck', 'toxic', 'online', 'date', 'one', 'reddit', 'success\\n\\noh', 'every', 'profile', 'exact', 'like', 'havent', 'hear', 'meme', 'every', 'comment', 'thread', 'years', 'now\\n\\npeople', 'afraid', 'open', 'give', 'shitty', 'one', 'word', 'answer', 'naaaah', 'reddit', 'full', 'confident', 'fuck', 'people', 'time\\n\\npictures', 'dont', 'really', 'show', 'person', 'look', 'like', 'jesus', 'christ', 'photoshop', 'free', 'reddit', '\\n\\nhave', 'patients', 'first', 'date', 'nerve', 'rack', 'first', 'day', 'two', 'message', 'people', 'definitely', 'arent', 'upload', 'worst', 'photos', 'would', 'think', 'every', 'person', 'meet', 'isnt', 'insanely', 'original', 'human', 'go', 'paragraph', 'say', 'aint', '\\n\\ni', 'get', 'joke', 'yall', 'take', 'little', 'far'])\n",
      "original document: \n",
      "['As', 'a', 'borderline', 'Aspergers-type', 'person,', 'I', 'have', 'to', 'say,', 'the', 'thing', 'that', 'keeps', 'Aspies', 'from', 'being', 'religious', 'is', 'a', 'quality', 'that', 'EVERYONE', 'should', 'have', 'more', 'of,', 'or', 'at', 'least', 'learn', 'it', 'as', 'a', 'skill', 'that', 'they', 'are', 'able', 'to', 'turn', 'on', 'at', 'will.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['borderlin', 'aspergerstyp', 'person', 'say', 'thing', 'keep', 'aspy', 'religy', 'qual', 'everyon', 'least', 'learn', 'skil', 'abl', 'turn'], ['borderline', 'aspergerstype', 'person', 'say', 'thing', 'keep', 'aspies', 'religious', 'quality', 'everyone', 'least', 'learn', 'skill', 'able', 'turn'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Ikr.', 'Like,', 'damn,', \"I'm\", 'gonna', 'be', 'SOOO', 'pissed', 'when', 'I', 'get', 'killed', 'because', 'some', 'dude', 'had', 'a', '2', 'second', 'quicker', 'cooldown', 'on', 'his', 'Star', 'card..\\nAnd', 'holy', 'crap,', \"I'm\", 'really', 'gonna', 'notice', 'it', 'when', 'I', 'get', 'killed', 'by', 'a', 'grenade', 'from', '5.5metres', 'instead', 'of', '5metres...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ikr', 'lik', 'damn', 'im', 'gonn', 'sooo', 'piss', 'get', 'kil', 'dud', 'two', 'second', 'quick', 'cooldown', 'star', 'card\\nand', 'holy', 'crap', 'im', 'real', 'gonn', 'not', 'get', 'kil', 'grenad', '55metres', 'instead', '5metres'], ['ikr', 'like', 'damn', 'im', 'gonna', 'sooo', 'piss', 'get', 'kill', 'dude', 'two', 'second', 'quicker', 'cooldown', 'star', 'card\\nand', 'holy', 'crap', 'im', 'really', 'gonna', 'notice', 'get', 'kill', 'grenade', '55metres', 'instead', '5metres'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Way', 'better', 'to', 'have', 'octogenarians', 'running', 'things,', 'who', 'still', 'think', 'the', 'world', 'can', 'be', 'run', 'the', 'same', 'as', 'if', \"it's\", 'the', 'same', 'as', 'it', 'was', 'when', 'they', 'were', 'young-', 'leaving', 'a', 'mess', 'that', 'they', 'conveniently', 'are', 'too', 'dead', 'to', 'have', 'to', 'deal', 'with.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'bet', 'octogen', 'run', 'thing', 'stil', 'think', 'world', 'run', 'young', 'leav', 'mess', 'conveny', 'dead', 'deal'], ['way', 'better', 'octogenarians', 'run', 'things', 'still', 'think', 'world', 'run', 'young', 'leave', 'mess', 'conveniently', 'dead', 'deal'])\n",
      "original document: \n",
      "[\"[+Will_dance_for_bells](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokaxw/):\\n\\nThere's\", 'a', 'drive', 'through', 'safari', 'park', 'here', 'in', 'the', 'UK', 'where', 'I', 'live.', 'You', 'can', 'buy', 'a', 'box', 'of', 'food', 'and', 'the', 'giraffes', 'come', 'and', 'stick', 'their', 'heads', 'in', 'the', 'car', 'to', 'get', 'some', 'eats.', '', \"It's\", 'so', 'much', 'fun.', '\\n\\nCongratulations', 'on', 'the', 'book!', 'Think', 'my', 'daughter', 'will', 'love', 'it.\\n\\nEDIT:', 'For', 'those', 'of', 'you', 'that', 'asked', \"it's\", 'West', 'Midlands', 'Safari', 'Park', 'in', 'Kidderminster,', 'UK.', 'I', \"haven't\", 'been', 'for', 'years', 'but', 'at', 'one', 'point', 'went', 'every', 'birthday', 'for', 'a', 'five', 'year', 'stretch.', '\\nThere', 'used', 'to', 'be', 'a', 'monkey', 'section', 'too', 'but', 'they', 'had', 'to', 'close', 'it', 'due', 'to', 'the', 'monkeys', 'royally', 'screwing', 'up', 'cars.', '', '\\n', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['will_dance_for_bellshttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokaxw\\n\\ntheres', 'driv', 'safar', 'park', 'uk', 'liv', 'buy', 'box', 'food', 'giraff', 'com', 'stick', 'head', 'car', 'get', 'eat', 'much', 'fun', '\\n\\ncongratulations', 'book', 'think', 'daught', 'lov', 'it\\n\\nedit', 'ask', 'west', 'midland', 'safar', 'park', 'kidderminst', 'uk', 'hav', 'year', 'on', 'point', 'went', 'every', 'birthday', 'fiv', 'year', 'stretch', '\\nthere', 'us', 'monkey', 'sect', 'clos', 'due', 'monkey', 'roy', 'screwing', 'car', '\\n'], ['will_dance_for_bellshttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokaxw\\n\\ntheres', 'drive', 'safari', 'park', 'uk', 'live', 'buy', 'box', 'food', 'giraffes', 'come', 'stick', 'head', 'car', 'get', 'eat', 'much', 'fun', '\\n\\ncongratulations', 'book', 'think', 'daughter', 'love', 'it\\n\\nedit', 'ask', 'west', 'midlands', 'safari', 'park', 'kidderminster', 'uk', 'havent', 'years', 'one', 'point', 'go', 'every', 'birthday', 'five', 'year', 'stretch', '\\nthere', 'use', 'monkey', 'section', 'close', 'due', 'monkey', 'royally', 'screw', 'cars', '\\n'])\n",
      "original document: \n",
      "['Academics', 'are', 'easy', 'as', 'long', 'as', 'you', 'study.', 'The', 'nervousness', 'will', 'eventually', 'start', 'to', 'go', 'away.', 'Good', 'luck.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['academ', 'easy', 'long', 'study', 'nerv', 'ev', 'start', 'go', 'away', 'good', 'luck'], ['academics', 'easy', 'long', 'study', 'nervousness', 'eventually', 'start', 'go', 'away', 'good', 'luck'])\n",
      "original document: \n",
      "['The', 'sens', 'are', 'so', 'irrelevant', 'yet', 'we', 'all', 'hate', 'them', \"\\n\\nThat's\", 'how', 'you', 'know', 'they', 'are', 'trash.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sen', 'irrelev', 'yet', 'hat', '\\n\\nthats', 'know', 'trash'], ['sens', 'irrelevant', 'yet', 'hate', '\\n\\nthats', 'know', 'trash'])\n",
      "original document: \n",
      "['Day', 'and', 'a', 'half']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['day', 'half'], ['day', 'half'])\n",
      "original document: \n",
      "['#SELLER']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sel'], ['seller'])\n",
      "original document: \n",
      "['This', 'is', 'really', 'high', 'quality', 'stuff.', '', 'From', 'the', 'shrieks', 'of', 'terror', 'to', 'the', 'guy', 'pointing', 'at', 'and', 'yelling', 'about', 'his', 'upside', 'down', 'sign', 'this', 'really', 'has', 'it', 'all.', '', 'Shame', 'the', 'cops', 'gave', 'into', 'the', 'mob', 'though.', '', 'Not', 'a', 'good', 'idea.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'high', 'qual', 'stuff', 'shrieks', 'ter', 'guy', 'point', 'yel', 'upsid', 'sign', 'real', 'sham', 'cop', 'gav', 'mob', 'though', 'good', 'ide'], ['really', 'high', 'quality', 'stuff', 'shriek', 'terror', 'guy', 'point', 'yell', 'upside', 'sign', 'really', 'shame', 'cop', 'give', 'mob', 'though', 'good', 'idea'])\n",
      "original document: \n",
      "['What', 'do', 'you', 'mean', 'Marvel', 'vs', 'Capcom?\\n\\n...all', 'I', 'see', 'is', 'Ryu...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'marvel', 'vs', 'capcom\\n\\nall', 'see', 'ryu'], ['mean', 'marvel', 'vs', 'capcom\\n\\nall', 'see', 'ryu'])\n",
      "original document: \n",
      "['I', 'have', 'had', 'phone', 'calls', 'from', 'Eastons', 'in', 'PA,', 'MA,', 'OH,', 'and', 'TX.', \"We're\", 'located', 'in', 'Talbot', 'County,', 'Maryland', 'and', 'VERY', 'OFTEN', 'the', 'SO', 'will', 'get', 'phone', 'calls', 'from', 'Talbot', 'County,', 'Georgia.\\n\\nAlso,', 'I', 'will', 'trade', 'you', 'an', 'Easton', 'MD', 'patch', 'for', 'an', 'Easton', 'MA', 'patch...', 'pm', 'me', 'if', 'interested.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['phon', 'cal', 'easton', 'pa', 'oh', 'tx', 'loc', 'talbot', 'county', 'maryland', 'oft', 'get', 'phon', 'cal', 'talbot', 'county', 'georgia\\n\\nalso', 'trad', 'easton', 'md', 'patch', 'easton', 'patch', 'pm', 'interest'], ['phone', 'call', 'eastons', 'pa', 'oh', 'tx', 'locate', 'talbot', 'county', 'maryland', 'often', 'get', 'phone', 'call', 'talbot', 'county', 'georgia\\n\\nalso', 'trade', 'easton', 'md', 'patch', 'easton', 'patch', 'pm', 'interest'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'see.', '\\n\\nSFV', 'is', 'just', 'too', 'trash', 'for', 'me', 'to', 'go', 'back.', 'If', 'they', 'actually', 'do', 'SuperSFV', 'maybeeeeee\\n\\nEveryone', 'knew', 'what', 'Capcom', 'was', 'doing', 'w/', 'SFV,', 'and', 'the', 'casuals(well,', 'most', 'of', 'them)', \"didn't\", 'buy', 'it.', \"We're\", 'seeing', 'Capcom', 'double', 'down', 'on', 'stupidity', 'and', 'ESPORTS', 'greed', 'w/', 'MVCI.\\n\\nI', 'was', 'soooooo', 'done', 'with', 'the', 'game', 'when', 'i', 'realized', 'alex', 'was', 'in', 'the', 'same', 'tier(trash)', 'he', 'was', 'in', 'SFIII.', 'All', 'those', 'alex', 'avatars', 'and', 'alex', 'hype', 'and', 'he', \"can't\", 'be', 'played.', \"I'm\", 'not', 'even', 'a', 'juri', 'fan', 'but', \"it's\", 'a', 'tragedy', 'what', 'they', 'did', 'to', 'her', '-', 'and', 'a', 'good', 'example', 'of', 'some', 'of', 'the', 'idiocy', 'of', 'capcom\\n\\nI', 'hate', 'hate', 'hate', 'hate', 'hate', 'hate', 'how', \"mika's\", 'v-skill', 'is', 'useless', 'and', 'like', 'u', 'said', 'v-system', 'is', 'unbalanced.', \"It's\", 'like', 'Ultras', 'but', 'wayyyy', \"worse.\\n\\nI'm\", 'not', 'a', 'real', 'technical', 'guy', 'but', 'someone', 'put', 'it', 'well', 'like', 'u', 'said', 'about', 'the', 'v-system', 'but', 'also', 'the', 'defensive', 'options', 'in', 'this', 'game', 'are', 'so', 'fucking', 'low.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', '\\n\\nsfv', 'trash', 'go', 'back', 'act', 'supersfv', 'maybeeeeee\\n\\neveryon', 'knew', 'capcom', 'w', 'sfv', 'casualswel', 'didnt', 'buy', 'see', 'capcom', 'doubl', 'stupid', 'esport', 'gree', 'w', 'mvci\\n\\ni', 'soooooo', 'don', 'gam', 'real', 'alex', 'tiertrash', 'sfii', 'alex', 'avat', 'alex', 'hyp', 'cant', 'play', 'im', 'ev', 'jur', 'fan', 'tragedy', 'good', 'exampl', 'idiocy', 'capcom\\n\\ni', 'hat', 'hat', 'hat', 'hat', 'hat', 'hat', 'mika', 'vskill', 'useless', 'lik', 'u', 'said', 'vsystem', 'unb', 'lik', 'ultra', 'wayyyy', 'worse\\n\\nim', 'real', 'techn', 'guy', 'someon', 'put', 'wel', 'lik', 'u', 'said', 'vsystem', 'also', 'defend', 'opt', 'gam', 'fuck', 'low'], ['see', '\\n\\nsfv', 'trash', 'go', 'back', 'actually', 'supersfv', 'maybeeeeee\\n\\neveryone', 'know', 'capcom', 'w', 'sfv', 'casualswell', 'didnt', 'buy', 'see', 'capcom', 'double', 'stupidity', 'esports', 'greed', 'w', 'mvci\\n\\ni', 'soooooo', 'do', 'game', 'realize', 'alex', 'tiertrash', 'sfiii', 'alex', 'avatars', 'alex', 'hype', 'cant', 'play', 'im', 'even', 'juri', 'fan', 'tragedy', 'good', 'example', 'idiocy', 'capcom\\n\\ni', 'hate', 'hate', 'hate', 'hate', 'hate', 'hate', 'mikas', 'vskill', 'useless', 'like', 'u', 'say', 'vsystem', 'unbalance', 'like', 'ultras', 'wayyyy', 'worse\\n\\nim', 'real', 'technical', 'guy', 'someone', 'put', 'well', 'like', 'u', 'say', 'vsystem', 'also', 'defensive', 'options', 'game', 'fuck', 'low'])\n",
      "original document: \n",
      "['Most', 'likely.', 'He', 'can', 'check', 'if', 'he', 'goes', 'through', 'transaction', 'details.', 'Not', 'going', 'to', 'bother', 'emailing', 'PayPal,', \"don't\", 'really', 'know', 'why', 'that', 'would', 'be', 'of', 'help.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'check', 'goe', 'transact', 'detail', 'going', 'both', 'email', 'payp', 'dont', 'real', 'know', 'would', 'help'], ['likely', 'check', 'go', 'transaction', 'detail', 'go', 'bother', 'email', 'paypal', 'dont', 'really', 'know', 'would', 'help'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['MXPlayer,', 'VLC', 'or', 'Kodi']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mxplayer', 'vlc', 'kod'], ['mxplayer', 'vlc', 'kodi'])\n",
      "original document: \n",
      "['Thats', 'going', 'to', 'be', 'the', 'interesting', 'part', 'with', 'the', 'new', 'patcher,', 'not', 'having', 'to', 'load', '30+', 'almost', 'daily', 'its', 'hopefully', 'becoming', 'more', 'and', 'more', 'playable.\\n\\nWhat', 'i', 'personally', 'wonder', 'is', 'chris', 'statement', 'on', 'not', 'wanting', 'crashes', 'every', '5-10minutes', 'for', 'evocati,', 'as', 'i', 'still', 'think', 'to', 'recall', 'that', 'was', 'exactly', 'how', '2.0', 'ptu', 'was', 'like.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'going', 'interest', 'part', 'new', 'patch', 'load', 'thirty', 'almost', 'dai', 'hop', 'becom', 'playable\\n\\nwhat', 'person', 'wond', 'chris', 'stat', 'want', 'crash', 'every', '510minutes', 'evocat', 'stil', 'think', 'recal', 'exact', 'twenty', 'ptu', 'lik'], ['thats', 'go', 'interest', 'part', 'new', 'patcher', 'load', 'thirty', 'almost', 'daily', 'hopefully', 'become', 'playable\\n\\nwhat', 'personally', 'wonder', 'chris', 'statement', 'want', 'crash', 'every', '510minutes', 'evocati', 'still', 'think', 'recall', 'exactly', 'twenty', 'ptu', 'like'])\n",
      "original document: \n",
      "['kuh', 'lick', 'bait']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kuh', 'lick', 'bait'], ['kuh', 'lick', 'bait'])\n",
      "original document: \n",
      "['Perfect.', 'You', 'got', 'a', 'mic?', 'And', 'not', 'a', 'dick?', 'Or', \"you're\", 'a', 'dick', 'but', 'funny?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['perfect', 'got', 'mic', 'dick', 'yo', 'dick', 'funny'], ['perfect', 'get', 'mic', 'dick', 'youre', 'dick', 'funny'])\n",
      "original document: \n",
      "['If', 'questions', 'are', 'frequent,', 'you', 'can', 'redirect', 'inquiries', 'to', 'a', 'FAQ', 'rather', 'than', 'banning...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quest', 'frequ', 'redirect', 'inquiry', 'faq', 'rath', 'ban'], ['question', 'frequent', 'redirect', 'inquiries', 'faq', 'rather', 'ban'])\n",
      "original document: \n",
      "['Thanks', 'mom.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'mom'], ['thank', 'mom'])\n",
      "original document: \n",
      "['**Off-Topic', 'Discussion**:', 'All', 'top-level', 'comments', 'must', 'be', 'a', 'story', 'or', 'poem.', 'Reply', 'here', 'for', 'other', 'comments.\\n\\n#####Reminder', 'for', 'Writers', 'and', 'Readers:\\n*', 'Prompts', 'are', 'meant', 'to', 'inspire', 'new', 'writing.', '', 'Responses', \"don't\", 'have', 'to', 'fulfill', 'every', 'detail.\\n\\n*', 'Please', 'remember', 'to', '[be', 'civil](https://www.reddit.com/r/WritingPrompts/wiki/rules#wiki_rule_10.3A_be_civil)', 'in', 'any', 'feedback.\\n\\n---\\n\\n[](#icon-help)[^(What', 'Is', 'This?)](https://www.reddit.com/r/WritingPrompts/wiki/off_topic)\\n[](#icon-information)[^(First', 'Time', 'Here?)](https://www.reddit.com/r/WritingPrompts/wiki/user_guide)\\n[](#icon-exclamation)[^(Special', 'Announcements)](https://www.reddit.com/r/WritingPrompts/wiki/announcements)\\n[](#icon-comments)[^(Click', 'For', 'Our', 'Chatrooms)](https://www.reddit.com/r/WritingPrompts/wiki/chat)\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['offtop', 'discuss', 'toplevel', 'com', 'must', 'story', 'poem', 'reply', 'comments\\n\\nreminder', 'writ', 'readers\\n', 'prompt', 'meant', 'inspir', 'new', 'writ', 'respons', 'dont', 'fulfil', 'every', 'detail\\n\\n', 'pleas', 'rememb', 'civilhttpswwwredditcomrwritingpromptswikiruleswiki_rule_103a_be_civil', 'feedback\\n\\n\\n\\niconhelpwhat', 'thishttpswwwredditcomrwritingpromptswikioff_topic\\niconinformationfirst', 'tim', 'herehttpswwwredditcomrwritingpromptswikiuser_guide\\niconexclamationspecial', 'announcementshttpswwwredditcomrwritingpromptswikiannouncements\\niconcommentsclick', 'chatroomshttpswwwredditcomrwritingpromptswikichat\\n'], ['offtopic', 'discussion', 'toplevel', 'comment', 'must', 'story', 'poem', 'reply', 'comments\\n\\nreminder', 'writers', 'readers\\n', 'prompt', 'mean', 'inspire', 'new', 'write', 'responses', 'dont', 'fulfill', 'every', 'detail\\n\\n', 'please', 'remember', 'civilhttpswwwredditcomrwritingpromptswikiruleswiki_rule_103a_be_civil', 'feedback\\n\\n\\n\\niconhelpwhat', 'thishttpswwwredditcomrwritingpromptswikioff_topic\\niconinformationfirst', 'time', 'herehttpswwwredditcomrwritingpromptswikiuser_guide\\niconexclamationspecial', 'announcementshttpswwwredditcomrwritingpromptswikiannouncements\\niconcommentsclick', 'chatroomshttpswwwredditcomrwritingpromptswikichat\\n'])\n",
      "original document: \n",
      "[\"I'm\", 'in', 'the', 'same', 'boat', 'as', 'you.', 'I', 'really', 'like', 'a', 'lot', 'of', 'things', 'about', 'the', 'game,', 'but', 'the', 'content', 'and', 'complexity', 'are', 'seriously', 'lacking.', 'I', 'feel', 'like', 'everything', \"it's\", 'lacking', 'could', 'be', 'added', 'later.', 'Crafting', 'is', 'really', 'the', 'only', 'thing', 'that', 'needs', 'serious', 'rework.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'boat', 'real', 'lik', 'lot', 'thing', 'gam', 'cont', 'complex', 'sery', 'lack', 'feel', 'lik', 'everyth', 'lack', 'could', 'ad', 'lat', 'craft', 'real', 'thing', 'nee', 'sery', 'rework'], ['im', 'boat', 'really', 'like', 'lot', 'things', 'game', 'content', 'complexity', 'seriously', 'lack', 'feel', 'like', 'everything', 'lack', 'could', 'add', 'later', 'craft', 'really', 'thing', 'need', 'serious', 'rework'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['R10', 'per', 'week?', 'What', 'a', 'lucky', 'little', 'person', 'you', 'were!', \"I'd\", 'get', 'R2', 'when', 'my', 'dad', 'had', 'some', 'extra', 'cash', 'to', 'spare.', 'Then', 'I', \"couldn't\", 'even', 'afford', 'a', 'packet', 'of', 'lays', 'and', 'would', 'need', 'to', 'settle', 'for', 'nik', 'naks', 'or', 'fritos', '😒']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['r10', 'per', 'week', 'lucky', 'littl', 'person', 'id', 'get', 'r2', 'dad', 'extr', 'cash', 'spar', 'couldnt', 'ev', 'afford', 'packet', 'lay', 'would', 'nee', 'settl', 'nik', 'nak', 'frito'], ['r10', 'per', 'week', 'lucky', 'little', 'person', 'id', 'get', 'r2', 'dad', 'extra', 'cash', 'spare', 'couldnt', 'even', 'afford', 'packet', 'lay', 'would', 'need', 'settle', 'nik', 'naks', 'fritos'])\n",
      "original document: \n",
      "['Is', 'that', 'warning', 'label', 'basically', 'saying', '\"stop', 'now\"?', 'Seems', 'kinda', 'blunt', 'haha.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['warn', 'label', 'bas', 'say', 'stop', 'seem', 'kind', 'blunt', 'hah'], ['warn', 'label', 'basically', 'say', 'stop', 'seem', 'kinda', 'blunt', 'haha'])\n",
      "original document: \n",
      "['We', 'had', 'them', 'when', 'I', 'lived', 'in', 'Costa', 'Rica.', 'They', 'would', 'climb', 'up', 'on', 'our', 'porch', 'to', 'eat', 'our', 'potted', 'plants,', 'run', 'around', 'on', 'roofs', 'and', 'in', 'the', 'gutters', 'scaring', 'people.', 'I', 'had', 'a', 'couple', 'climb', 'in', 'my', 'windows', 'and', 'I', 'would', 'have', 'to', 'shoo', 'them', 'out.', 'they', 'bite.', 'The', 'locals', 'call', 'them', 'tree', 'chickens.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['liv', 'cost', 'ric', 'would', 'climb', 'porch', 'eat', 'pot', 'plant', 'run', 'around', 'roof', 'gut', 'scar', 'peopl', 'coupl', 'climb', 'window', 'would', 'shoo', 'bit', 'loc', 'cal', 'tre', 'chick'], ['live', 'costa', 'rica', 'would', 'climb', 'porch', 'eat', 'pot', 'plant', 'run', 'around', 'roof', 'gutter', 'scar', 'people', 'couple', 'climb', 'windows', 'would', 'shoo', 'bite', 'locals', 'call', 'tree', 'chickens'])\n",
      "original document: \n",
      "['Mocha', 'Uson']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['moch', 'uson'], ['mocha', 'uson'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', \"don't\", 'really', 'trust', 'the', 'save', 'feature', 'on', 'the', 'app.', 'So', \"I'm\", 'replying', 'to', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'real', 'trust', 'sav', 'feat', 'ap', 'im', 'reply'], ['dont', 'really', 'trust', 'save', 'feature', 'app', 'im', 'reply'])\n",
      "original document: \n",
      "['if', 'king', 'takes', '-', 'Nb6\\n\\nif', 'queen', 'takes', '-', '', 'Ne5', 'dxe5', 'Ne5\\n\\nif', 'king', 'to', 'e4', '-', 'Nf6\\n\\nif', 'king', 'to', 'c2', '-', 'Ne3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['king', 'tak', 'nb6\\n\\nif', 'queen', 'tak', 'ne5', 'dxe5', 'ne5\\n\\nif', 'king', 'e4', 'nf6\\n\\nif', 'king', 'c2', 'ne3'], ['king', 'take', 'nb6\\n\\nif', 'queen', 'take', 'ne5', 'dxe5', 'ne5\\n\\nif', 'king', 'e4', 'nf6\\n\\nif', 'king', 'c2', 'ne3'])\n",
      "original document: \n",
      "['Yea', 'i', 'know', 'what', 'you', 'mean,', 'but', 'if', 'they', 'did', 'do', 'something', 'stupid', 'like', 'that', 'regarding', 'your', 'pay,', 'it', 'would', 'be', 'lawsuit', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'know', 'mean', 'someth', 'stupid', 'lik', 'regard', 'pay', 'would', 'lawsuit', 'tim'], ['yea', 'know', 'mean', 'something', 'stupid', 'like', 'regard', 'pay', 'would', 'lawsuit', 'time'])\n",
      "original document: \n",
      "['As', 'a', 'foreign', 'it', 'seemed', 'as', 'all', 'the', 'news', 'were', 'pro', 'Hillary']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['foreign', 'seem', 'new', 'pro', 'hil'], ['foreign', 'seem', 'news', 'pro', 'hillary'])\n",
      "original document: \n",
      "['There', 'is', 'a', 'whole', 'system', 'in', 'place', 'that', 'forces', 'farmers', 'out', 'of', 'business', 'unless', 'they', 'use', 'big', 'pharma', 'services.', 'It’s', 'not', 'like', 'you', 'are', 'going', 'to', 'survive', 'by', 'producing', 'small', 'scale', 'organic', 'when', 'a', 'neighboring', 'farmer', 'gets', 'all', 'the', 'financial', 'benefits', 'by', 'producing', 'allot', 'through', 'the', 'use', 'of', 'chemicals.', 'Teaching', 'farmers', 'is', 'not', 'going', 'to', 'do', 'anything,', 'it', 'is', 'the', 'consumers', 'who', 'need', 'to', 'be', 'thought.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whol', 'system', 'plac', 'forc', 'farm', 'busy', 'unless', 'us', 'big', 'pharm', 'serv', 'lik', 'going', 'surv', 'produc', 'smal', 'scal', 'org', 'neighb', 'farm', 'get', 'fin', 'benefit', 'produc', 'allot', 'us', 'chem', 'teach', 'farm', 'going', 'anyth', 'consum', 'nee', 'thought'], ['whole', 'system', 'place', 'force', 'farmers', 'business', 'unless', 'use', 'big', 'pharma', 'service', 'like', 'go', 'survive', 'produce', 'small', 'scale', 'organic', 'neighbor', 'farmer', 'get', 'financial', 'benefit', 'produce', 'allot', 'use', 'chemicals', 'teach', 'farmers', 'go', 'anything', 'consumers', 'need', 'think'])\n",
      "original document: \n",
      "['143413258|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413069\\npls', \"don't\", 'assume', 'my', 'gender.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, two hundred and fifty-eight', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413069\\npls', 'dont', 'assum', 'gender\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, two hundred and fifty-eight', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413069\\npls', 'dont', 'assume', 'gender\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol6ic/):\\n\\nI', 'might', 'have', 'to', 'visit', 'where', 'you', 'live', 'then.', '.', '.', '\\n\\nThank', 'you!', 'I', 'hope', 'she', 'does', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol6ic\\n\\ni', 'might', 'visit', 'liv', '\\n\\nthank', 'hop'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol6ic\\n\\ni', 'might', 'visit', 'live', '\\n\\nthank', 'hope'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"I'm\", 'in', 'interested', 'in', 'it,', 'but', \"it's\", 'not', 'cheap', 'fosho.', 'Is', 'it', 'a', 'textbook?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'interest', 'cheap', 'fosho', 'textbook'], ['im', 'interest', 'cheap', 'fosho', 'textbook'])\n",
      "original document: \n",
      "['If', 'you', 'click', 'the', 'link,', 'there', 'is', 'a', 'video', 'of', 'him', 'saying', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['click', 'link', 'video', 'say'], ['click', 'link', 'video', 'say'])\n",
      "original document: \n",
      "['one', 'was', 'yellow', 'stairs', 'and', 'the', 'other', 'was', 'by', 'white', 'van.', 'none', 'of', 'them', 'were', 'near', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'yellow', 'stair', 'whit', 'van', 'non', 'near'], ['one', 'yellow', 'stairs', 'white', 'van', 'none', 'near'])\n",
      "original document: \n",
      "['Can', 'count', 'the', 'dozens', 'of', 'times', 'ENFP', 'and', 'INTJ', 'get', 'back', 'together,', 'and', 'can', 'count', 'the', 'dozens', 'of', 'times', 'they', 'never', 'get', 'back', 'together,', 'and', 'I', 'can', 'count', 'the', 'dozens', 'of', 'times', 'where', 'they', 'get', 'back', 'together', 'only', 'to', 'break', 'up', 'again.\\n\\nI', 'am', 'not', 'exaggerating.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['count', 'doz', 'tim', 'enfp', 'ints', 'get', 'back', 'togeth', 'count', 'doz', 'tim', 'nev', 'get', 'back', 'togeth', 'count', 'doz', 'tim', 'get', 'back', 'togeth', 'break', 'again\\n\\ni', 'exag'], ['count', 'dozens', 'time', 'enfp', 'intj', 'get', 'back', 'together', 'count', 'dozens', 'time', 'never', 'get', 'back', 'together', 'count', 'dozens', 'time', 'get', 'back', 'together', 'break', 'again\\n\\ni', 'exaggerate'])\n",
      "original document: \n",
      "['Include', 'bendy', 'and', 'have', 'the', 'title', 'be', 'a', 'joke', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['includ', 'bendy', 'titl', 'jok'], ['include', 'bendy', 'title', 'joke'])\n",
      "original document: \n",
      "['I', 'never', 'understood', 'that', 'until', 'now.', 'The', 'game', 'and', 'DLC', 'are', 'the', 'greatest', \"I've\", 'ever', 'played', 'in', 'my', 'life', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'understood', 'gam', 'dlc', 'greatest', 'iv', 'ev', 'play', 'lif'], ['never', 'understand', 'game', 'dlc', 'greatest', 'ive', 'ever', 'play', 'life'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Depends', 'on', 'the', 'crate', 'really.\\n\\nWhat', 'do', 'you', 'have?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'crat', 'really\\n\\nwhat'], ['depend', 'crate', 'really\\n\\nwhat'])\n",
      "original document: \n",
      "['It', 'populates', 'to', 'your', 'account', 'automatically', 'for', 'some', 'reason', 'without', 'a', 'download.', 'So', 'it’s', 'there,', 'but', 'for', 'some', 'reason', 'doesn’t', 'need', 'to', 'download.', 'Was', 'wondering', 'about', 'this', 'for', 'the', 'longest', 'time.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pop', 'account', 'autom', 'reason', 'without', 'download', 'reason', 'doesnt', 'nee', 'download', 'wond', 'longest', 'tim'], ['populate', 'account', 'automatically', 'reason', 'without', 'download', 'reason', 'doesnt', 'need', 'download', 'wonder', 'longest', 'time'])\n",
      "original document: \n",
      "['WARNING:', '\\n\\nLower', 'the', 'volume!', 'Sound', 'effects', 'are', 'loud', 'and', 'may', 'cause', 'jumpscare.', 'I', 'do', 'not', 'intend', 'to', 'jumpscare', 'you.', 'Sorry', 'about', 'that.\\n\\n\\nABOUT:\\n\\n\\nTank', 'and', 'Camo', 'are', 'having', 'a', 'serious', 'money', 'problems.', 'They', 'have', 'no', 'choice', 'but', 'to', 'rob', 'a', 'bank.', 'Will', 'they', 'succeed?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['warn', '\\n\\nlower', 'volum', 'sound', 'effect', 'loud', 'may', 'caus', 'jumpsc', 'intend', 'jumpsc', 'sorry', 'that\\n\\n\\nabout\\n\\n\\ntank', 'camo', 'sery', 'money', 'problem', 'cho', 'rob', 'bank', 'success'], ['warn', '\\n\\nlower', 'volume', 'sound', 'effect', 'loud', 'may', 'cause', 'jumpscare', 'intend', 'jumpscare', 'sorry', 'that\\n\\n\\nabout\\n\\n\\ntank', 'camo', 'serious', 'money', 'problems', 'choice', 'rob', 'bank', 'succeed'])\n",
      "original document: \n",
      "['screw', 'FCC,', 'its', 'all', 'about', 'Miami', 'FC', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['screw', 'fcc', 'miam', 'fc'], ['screw', 'fcc', 'miami', 'fc'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['One', 'random', 'spot', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'spot', 'pleas'], ['one', 'random', 'spot', 'please'])\n",
      "original document: \n",
      "['\"What', 'are', 'Saturdays', 'for?\"\\n\\n\"The', 'boys!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saturday', 'for\\n\\nthe', 'boy'], ['saturdays', 'for\\n\\nthe', 'boys'])\n",
      "original document: \n",
      "['Saved', 'for', 'later.', \"You're\", 'super', 'smokin', 'hot', 'baby']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sav', 'lat', 'yo', 'sup', 'smokin', 'hot', 'baby'], ['save', 'later', 'youre', 'super', 'smokin', 'hot', 'baby'])\n",
      "original document: \n",
      "['&gt;', 'With', 'favorites', 'ready,', 'guard', 'your', 'side—\\n\\nGuys,', \"it's\", 'been', 'confirmed,', 'all', 'of', 'our', 'favorites', 'are', 'coming!', 'That', 'means', 'literally', 'everyone', 'gets', 'a', 'spot!', 'All', 'well', 'over', '1K', 'characters!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'favorit', 'ready', 'guard', 'side\\n\\nguys', 'confirm', 'favorit', 'com', 'mean', 'lit', 'everyon', 'get', 'spot', 'wel', '1k', 'charact'], ['gt', 'favorites', 'ready', 'guard', 'side\\n\\nguys', 'confirm', 'favorites', 'come', 'mean', 'literally', 'everyone', 'get', 'spot', 'well', '1k', 'character'])\n",
      "original document: \n",
      "['no,', 'but', 'it', 'also', \"doesn't\", 'make', 'it', 'the', 'wrong', 'way', 'either.\\n\\n\\n\\nif', 'I', 'can', 'shoot', 'the', '3pt', 'at', '40%', 'success', 'rate', 'with', 'good', 'release,', 'what', 'the', \"he'll\", 'does', 'my', 'attribute', 'number', 'really', 'matter?', 'like', 'who', 'cares', 'if', 'it', 'was', '10,', 'if', 'I', 'can', 'shoot', 'at', '40%', '', 'it', \"doesn't\", 'matter', 'if', 'that', 'number', 'is', '19,', '50,', '200,', '5656.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'doesnt', 'mak', 'wrong', 'way', 'either\\n\\n\\n\\nif', 'shoot', '3pt', 'forty', 'success', 'rat', 'good', 'releas', 'hel', 'attribut', 'numb', 'real', 'mat', 'lik', 'car', 'ten', 'shoot', 'forty', 'doesnt', 'mat', 'numb', 'nineteen', 'fifty', 'two hundred', 'five thousand, six hundred and fifty-six'], ['also', 'doesnt', 'make', 'wrong', 'way', 'either\\n\\n\\n\\nif', 'shoot', '3pt', 'forty', 'success', 'rate', 'good', 'release', 'hell', 'attribute', 'number', 'really', 'matter', 'like', 'care', 'ten', 'shoot', 'forty', 'doesnt', 'matter', 'number', 'nineteen', 'fifty', 'two hundred', 'five thousand, six hundred and fifty-six'])\n",
      "original document: \n",
      "['TGT.', 'I', 'got', 'legend', 'two', 'times,', 'but', 'only', 'because', 'usually', 'I', 'get', 'to', 'rank', '5', 'and', 'then', 'stop', 'playing', 'on', 'ladder', 'because', 'I', 'prefer', 'arena.', \"I'm\", 'confident', 'that', 'I', 'could', 'reach', 'legend', 'almost', 'every', 'month', 'if', 'I', 'would', 'like', 'to']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tgt', 'got', 'legend', 'two', 'tim', 'us', 'get', 'rank', 'fiv', 'stop', 'play', 'lad', 'pref', 'aren', 'im', 'confid', 'could', 'reach', 'legend', 'almost', 'every', 'mon', 'would', 'lik'], ['tgt', 'get', 'legend', 'two', 'time', 'usually', 'get', 'rank', 'five', 'stop', 'play', 'ladder', 'prefer', 'arena', 'im', 'confident', 'could', 'reach', 'legend', 'almost', 'every', 'month', 'would', 'like'])\n",
      "original document: \n",
      "['Yeah', 'and', 'all', 'muslims', 'are', 'terrorists.', 'The', 'pot', 'calling', 'the', 'kettle', 'black.', 'Come', 'on..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'muslim', 'ter', 'pot', 'cal', 'kettl', 'black', 'com'], ['yeah', 'muslims', 'terrorists', 'pot', 'call', 'kettle', 'black', 'come'])\n",
      "original document: \n",
      "['0.5', 'PPR', 'standard\\n\\nMartavis', 'Bryant', '@', 'BAL\\n\\nWill', 'Fuller', 'vs.', 'TEN\\n\\nJason', 'Witten', 'vs.', 'LAR\\n\\nDerrick', 'Henry', 'vs.', 'HOU']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fiv', 'ppr', 'standard\\n\\nmartavis', 'bry', 'bal\\n\\nwil', 'ful', 'vs', 'ten\\n\\njason', 'wit', 'vs', 'lar\\n\\nderrick', 'henry', 'vs', 'hou'], ['five', 'ppr', 'standard\\n\\nmartavis', 'bryant', 'bal\\n\\nwill', 'fuller', 'vs', 'ten\\n\\njason', 'witten', 'vs', 'lar\\n\\nderrick', 'henry', 'vs', 'hou'])\n",
      "original document: \n",
      "['&gt;The', 'RES', 'spam', 'tracker.\\n\\nIs', 'it', 'possible', 'to', 'learn', 'this', 'power?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthe', 'res', 'spam', 'tracker\\n\\nis', 'poss', 'learn', 'pow'], ['gtthe', 'res', 'spam', 'tracker\\n\\nis', 'possible', 'learn', 'power'])\n",
      "original document: \n",
      "[\"What's\", 'the', 'song', 'playing', 'when', 'he', 'tests', 'out', 'the', 'speakers?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'song', 'play', 'test', 'speak'], ['whats', 'song', 'play', 'test', 'speakers'])\n",
      "original document: \n",
      "['I', 'had', 'no', 'idea.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ide'], ['idea'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'], ['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'])\n",
      "original document: \n",
      "['What', 'do', 'you', 'mean,', 'man,', 'aghs', 'is', 'super', 'good', 'on', 'Bloodseeker', 'in', 'almost', 'all', 'cases.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'man', 'agh', 'sup', 'good', 'bloodseek', 'almost', 'cas'], ['mean', 'man', 'aghs', 'super', 'good', 'bloodseeker', 'almost', 'case'])\n",
      "original document: \n",
      "['that', 'was', 'a', 'misclick', 'i', 'meant', 'to', 'reply', 'that', 'to', 'the', 'tread', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['misclick', 'meant', 'reply', 'tread', 'lol'], ['misclick', 'mean', 'reply', 'tread', 'lol'])\n",
      "original document: \n",
      "['Would', 'you', 'be', 'interested', 'in', 'a', 'straight', 'trade', 'for', 'the', 'Eventide', 'TimeFactor?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'interest', 'straight', 'trad', 'eventid', 'timefact'], ['would', 'interest', 'straight', 'trade', 'eventide', 'timefactor'])\n",
      "original document: \n",
      "['No', 'real', 'explanation', 'aside', 'that', 'he', 'seems', 'to', 'play', 'jump', 'rope', 'with', 'his', 'little', 'sister', 'a', 'lot', 'and', 'ride', 'his', 'bike', 'up', 'and', 'down', 'a', 'mountain', 'everyday.', \"He's\", 'got', 'a', 'lot', 'of', 'energy', 'which', 'also', 'puts', 'more', 'spring', 'in', 'his', 'step.', '\\n\\nAs', 'to', 'whether', \"it's\", 'realistic', 'to', 'jump', 'that', 'high,', 'I', \"don't\", 'know.', 'But', 'he', 'does', 'at', 'least', 'seem', 'to', 'have', 'strong', 'legs', 'due', 'to', 'cycling', 'and', 'always', 'being', 'physically', 'active', 'since', 'middle', 'school.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'expl', 'asid', 'seem', 'play', 'jump', 'rop', 'littl', 'sist', 'lot', 'rid', 'bik', 'mountain', 'everyday', 'hes', 'got', 'lot', 'energy', 'also', 'put', 'spring', 'step', '\\n\\nas', 'wheth', 'real', 'jump', 'high', 'dont', 'know', 'least', 'seem', 'strong', 'leg', 'due', 'cyc', 'alway', 'phys', 'act', 'sint', 'middl', 'school'], ['real', 'explanation', 'aside', 'seem', 'play', 'jump', 'rope', 'little', 'sister', 'lot', 'ride', 'bike', 'mountain', 'everyday', 'hes', 'get', 'lot', 'energy', 'also', 'put', 'spring', 'step', '\\n\\nas', 'whether', 'realistic', 'jump', 'high', 'dont', 'know', 'least', 'seem', 'strong', 'legs', 'due', 'cycle', 'always', 'physically', 'active', 'since', 'middle', 'school'])\n",
      "original document: \n",
      "['I', 'always', 'try', 'and', 'pick', 'up', '1', 'cheap', 'foil', 'for', 'my', 'zada', 'EDH', 'deck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'try', 'pick', 'on', 'cheap', 'foil', 'zad', 'edh', 'deck'], ['always', 'try', 'pick', 'one', 'cheap', 'foil', 'zada', 'edh', 'deck'])\n",
      "original document: \n",
      "['If', 'healers', 'straight', 'up', 'out-healed', 'the', 'DPS,', 'it', 'would', 'be', 'a', 'game', 'of', 'which', 'support', 'dies', 'first.', 'Overwatch', 'would', 'never', 'work', 'if', 'healers', 'did', 'more', 'HPS', 'than', 'damage', 'did', 'DPS.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heal', 'straight', 'outh', 'dps', 'would', 'gam', 'support', 'die', 'first', 'overwatch', 'would', 'nev', 'work', 'heal', 'hps', 'dam', 'dps'], ['healers', 'straight', 'outhealed', 'dps', 'would', 'game', 'support', 'die', 'first', 'overwatch', 'would', 'never', 'work', 'healers', 'hps', 'damage', 'dps'])\n",
      "original document: \n",
      "['That', 'ship', 'has', 'sailed', 'my', 'friend.', 'When', 'the', 'FDA’s', 'swat', 'teams', 'started', 'swarming', 'buildings,', 'weapons', 'hot,', 'to', 'bust', 'the', 'unpasteurized', 'milk', 'syndicates', 'we', 'were', 'already', 'lost.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ship', 'sail', 'friend', 'fda', 'swat', 'team', 'start', 'swarm', 'build', 'weapon', 'hot', 'bust', 'unpast', 'milk', 'synd', 'already', 'lost'], ['ship', 'sail', 'friend', 'fdas', 'swat', 'team', 'start', 'swarm', 'build', 'weapons', 'hot', 'bust', 'unpasteurized', 'milk', 'syndicate', 'already', 'lose'])\n",
      "original document: \n",
      "[\"Isn't\", 'this', 'the', 'same', 'question', 'as', '\"Smart', 'people,', 'do', 'you', 'get', 'frustrated', 'at', 'people', 'who', 'are', 'slow', 'to', 'understand', 'how', 'things', 'work?\"', '--', 'in', 'which', 'case', 'the', 'answer', 'is,', 'as', 'always,', '\"it', 'depends\".', '', \"I'm\", 'reasonably', 'mechanically', 'inclined.', '', 'Would', 'I', 'get', 'frustrated', 'at', 'someone', 'who', 'saw', 'the', 'inside', 'of', 'a', 'transmission', 'and', \"didn't\", 'immediately', 'understand', 'it?', 'No,', 'of', 'course', 'not.', '', 'Would', 'I', 'get', 'frustrated', 'at', 'someone', 'who', \"didn't\", 'understand', 'an', \"Archimedes'\", 'screw', 'after', '2', 'hours', 'of', 'detailed', 'explanation?', 'Yes.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isnt', 'quest', 'smart', 'peopl', 'get', 'frust', 'peopl', 'slow', 'understand', 'thing', 'work', 'cas', 'answ', 'alway', 'depend', 'im', 'reason', 'mech', 'inclin', 'would', 'get', 'frust', 'someon', 'saw', 'insid', 'transmit', 'didnt', 'immedy', 'understand', 'cours', 'would', 'get', 'frust', 'someon', 'didnt', 'understand', 'archim', 'screw', 'two', 'hour', 'detail', 'expl', 'ye'], ['isnt', 'question', 'smart', 'people', 'get', 'frustrate', 'people', 'slow', 'understand', 'things', 'work', 'case', 'answer', 'always', 'depend', 'im', 'reasonably', 'mechanically', 'incline', 'would', 'get', 'frustrate', 'someone', 'saw', 'inside', 'transmission', 'didnt', 'immediately', 'understand', 'course', 'would', 'get', 'frustrate', 'someone', 'didnt', 'understand', 'archimedes', 'screw', 'two', 'hours', 'detail', 'explanation', 'yes'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Safe', 'on', 'paper', 'yes.', 'For', 'coin', 'splitting', 'ask', 'around.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saf', 'pap', 'ye', 'coin', 'splitting', 'ask', 'around'], ['safe', 'paper', 'yes', 'coin', 'split', 'ask', 'around'])\n",
      "original document: \n",
      "['The', 'next', 'time', 'you', 'fill', 'up', 'your', 'water', 'bottle', 'from', 'the', 'tap,', 'receive', 'something', 'from', 'usps,', 'or', 'use', 'countless', 'other', 'services', 'you', 'can', 'thank', 'our', 'mixed', 'economy.', 'http://www.investopedia.com/ask/answers/031815/united-states-considered-market-economy-or-mixed-economy.asp.', '\\n', '', '\\n', '', '\\nNo', 'market', 'type', 'is', 'flawless', 'but', 'ours', 'has', 'made', 'the', 'USA', 'an', 'absolute', 'powerhouse.', 'If', 'not', 'for', 'our', 'market', 'we', 'would', 'very', 'likely', 'be', 'behind', 'China', 'and', 'Russia', 'in', 'many', 'respects']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['next', 'tim', 'fil', 'wat', 'bottl', 'tap', 'receiv', 'someth', 'usp', 'us', 'countless', 'serv', 'thank', 'mix', 'econom', 'httpwwwinvestopediacomaskanswers031815unitedstatesconsideredmarketeconomyormixedeconomyasp', '\\n', '\\n', '\\nno', 'market', 'typ', 'flawless', 'mad', 'us', 'absolv', 'powerh', 'market', 'would', 'lik', 'behind', 'chin', 'russ', 'many', 'respect'], ['next', 'time', 'fill', 'water', 'bottle', 'tap', 'receive', 'something', 'usps', 'use', 'countless', 'service', 'thank', 'mix', 'economy', 'httpwwwinvestopediacomaskanswers031815unitedstatesconsideredmarketeconomyormixedeconomyasp', '\\n', '\\n', '\\nno', 'market', 'type', 'flawless', 'make', 'usa', 'absolute', 'powerhouse', 'market', 'would', 'likely', 'behind', 'china', 'russia', 'many', 'respect'])\n",
      "original document: \n",
      "['Well', 'then...', 'you', 'provide', 'the', 'costume', 'and', 'let', 'me', 'know', 'what', 'you', 'want', ':O']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'provid', 'costum', 'let', 'know', 'want'], ['well', 'provide', 'costume', 'let', 'know', 'want'])\n",
      "original document: \n",
      "['Good', 'to', 'know', 'for', 'the', 'future!', 'Haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'know', 'fut', 'hah'], ['good', 'know', 'future', 'haha'])\n",
      "original document: \n",
      "[\"Don't\", 'you', 'still', 'have', 'to', 'pay', 'for', 'the', 'Nazi', 'tattoo', 'to', 'begin', 'with?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'stil', 'pay', 'naz', 'tattoo', 'begin'], ['dont', 'still', 'pay', 'nazi', 'tattoo', 'begin'])\n",
      "original document: \n",
      "['He', 'started', 'acting', 'very', 'strange,', 'hiding', 'from', 'us', 'and', 'seemed', 'to', 'be', 'absolutely', 'terrified', 'of', 'something.', 'He', \"wasn't\", 'eating,', 'drinking,', 'or', 'going', 'to', 'the', 'bathroom.', 'When', 'we', 'took', 'him', 'in', 'they', 'noticed', 'he', 'was', 'displaying', 'unusual', 'rapid', 'eye', 'movements', 'and', 'ran', 'an', 'MRI', 'which', 'displayed', 'swelling.', \"I've\", 'also', 'been', 'told', 'some', 'animals', 'display', 'this', 'by', 'placing', 'their', 'head', 'against', 'an', 'object', 'but', 'we', \"didn't\", 'experience', 'that.', 'Basically', 'things', 'went', 'really', 'wrong', 'really', 'fast.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'act', 'strange', 'hid', 'us', 'seem', 'absolv', 'terr', 'someth', 'wasnt', 'eat', 'drink', 'going', 'bathroom', 'took', 'not', 'display', 'unus', 'rapid', 'ey', 'mov', 'ran', 'mri', 'display', 'swel', 'iv', 'also', 'told', 'anim', 'display', 'plac', 'head', 'object', 'didnt', 'expery', 'bas', 'thing', 'went', 'real', 'wrong', 'real', 'fast'], ['start', 'act', 'strange', 'hide', 'us', 'seem', 'absolutely', 'terrify', 'something', 'wasnt', 'eat', 'drink', 'go', 'bathroom', 'take', 'notice', 'display', 'unusual', 'rapid', 'eye', 'movements', 'run', 'mri', 'display', 'swell', 'ive', 'also', 'tell', 'animals', 'display', 'place', 'head', 'object', 'didnt', 'experience', 'basically', 'things', 'go', 'really', 'wrong', 'really', 'fast'])\n",
      "original document: \n",
      "['I', 'have', 'four', 'reasons', 'why', 'I', 'cant', 'use', 'pams', 'in', 'this', 'pack.', '\\n\\n*', \"It's\", 'in', 'literally', 'every', 'pack', 'in', 'the', 'modded', 'world.', 'I', 'use', 'mostly', 'mods', 'that', 'go', 'unused', 'or', 'underrated.\\n*', 'Recent', 'pams', 'changes', 'made', 'the', 'game', 'insanely', 'hard.\\n*', 'Pams', 'is', 'in', 'a', 'very', 'transitional', 'stage', 'right', 'now', 'for', 'balance', 'and', 'recipes.', 'If', 'I', 'had', 'it', 'I', 'would', 'have', 'to', 'push', 'updates', 'like', 'every', '3', 'days.\\n*', 'Pams', 'adds', 'too', 'much', 'for', 'this', 'type', 'of', 'pack.', \"I'm\", 'trying', 'to', 'reach', 'out', 'to', 'people', 'that', 'have', 'not', 'played', 'mods', 'before', 'and', 'having', 'over', '200', 'food', 'items', 'is', 'a', 'tad', 'overwhelming.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['four', 'reason', 'cant', 'us', 'pam', 'pack', '\\n\\n', 'lit', 'every', 'pack', 'mod', 'world', 'us', 'most', 'mod', 'go', 'unus', 'underrated\\n', 'rec', 'pam', 'chang', 'mad', 'gam', 'ins', 'hard\\n', 'pam', 'transit', 'stag', 'right', 'bal', 'recip', 'would', 'push', 'upd', 'lik', 'every', 'three', 'days\\n', 'pam', 'ad', 'much', 'typ', 'pack', 'im', 'try', 'reach', 'peopl', 'play', 'mod', 'two hundred', 'food', 'item', 'tad', 'overwhelm'], ['four', 'reason', 'cant', 'use', 'pams', 'pack', '\\n\\n', 'literally', 'every', 'pack', 'modded', 'world', 'use', 'mostly', 'mods', 'go', 'unused', 'underrated\\n', 'recent', 'pams', 'change', 'make', 'game', 'insanely', 'hard\\n', 'pams', 'transitional', 'stage', 'right', 'balance', 'recipes', 'would', 'push', 'update', 'like', 'every', 'three', 'days\\n', 'pams', 'add', 'much', 'type', 'pack', 'im', 'try', 'reach', 'people', 'play', 'mods', 'two hundred', 'food', 'items', 'tad', 'overwhelm'])\n",
      "original document: \n",
      "['Your', 'submission', 'has', 'been', 'removed', 'for', 'the', 'following', 'reason(s):\\n\\n\\n*', 'Rule', '6', '-', 'No', 'Fireteam,', 'Friend', 'Request,', 'or', 'Clan', 'threads.', 'Please', 'use', '/r/Fireteams', 'for', 'generic', 'requests', 'and', 'clan', 'posts,', '/r/DestinySherpa', 'for', 'help', 'teaching/learning', 'the', 'raids,', '/r/CrucibleSherpa', 'for', 'teaching/learning', 'the', 'Crucible,', 'or', 'the', 'Team', 'Up', 'Tuesday', 'Megathread', 'instead.\\n\\n\\n\\n---\\n\\nFor', 'more', 'information,', 'see', '[our', 'detailed', 'rules', 'page.](http://www.reddit.com/r/destinythegame/wiki/rules)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'remov', 'follow', 'reasons\\n\\n\\n', 'rul', 'six', 'fireteam', 'friend', 'request', 'clan', 'threads', 'pleas', 'us', 'rfireteam', 'gen', 'request', 'clan', 'post', 'rdestinysherp', 'help', 'teachinglearn', 'raid', 'rcruciblesherpa', 'teachinglearn', 'cruc', 'team', 'tuesday', 'megathread', 'instead\\n\\n\\n\\n\\n\\nfor', 'inform', 'see', 'detail', 'rul', 'pagehttpwwwredditcomrdestinythegamewikir'], ['submission', 'remove', 'follow', 'reasons\\n\\n\\n', 'rule', 'six', 'fireteam', 'friend', 'request', 'clan', 'thread', 'please', 'use', 'rfireteams', 'generic', 'request', 'clan', 'post', 'rdestinysherpa', 'help', 'teachinglearning', 'raid', 'rcruciblesherpa', 'teachinglearning', 'crucible', 'team', 'tuesday', 'megathread', 'instead\\n\\n\\n\\n\\n\\nfor', 'information', 'see', 'detail', 'rule', 'pagehttpwwwredditcomrdestinythegamewikirules'])\n",
      "original document: \n",
      "[\"It's\", 'on', 'regular', 'fox', 'here.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['regul', 'fox'], ['regular', 'fox'])\n",
      "original document: \n",
      "['I', 'wanna', 'fuck', 'her', 'tits,', 'but', 'she', 'need', 'to', 'stfu', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wann', 'fuck', 'tit', 'nee', 'stfu'], ['wanna', 'fuck', 'tits', 'need', 'stfu'])\n",
      "original document: \n",
      "['This', 'season', 'ash/IQ', 'and', 'ela/jager', 'but', 'I', 'usually', 'play', 'what', 'the', 'team', 'needs']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['season', 'ashiq', 'elas', 'us', 'play', 'team', 'nee'], ['season', 'ashiq', 'elajager', 'usually', 'play', 'team', 'need'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['So', 'weird', 'hack', 'at', 'Best', 'buy.', 'If', 'you', 'buy', 'the', 'GoPro6', 'and', 'three', '\"camrea\"', 'accessories.', 'You', 'get', '10%', 'off', 'each', 'acsessorie.', 'So', 'Karma', 'drone,', 'Grip,', 'and', 'SD', 'card', 'count', 'as', 'three', 'accessories.', 'But', 'you', 'have', 'to', 'find', 'them', 'individually.', 'Lol', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weird', 'hack', 'best', 'buy', 'buy', 'gopro6', 'three', 'camre', 'access', 'get', 'ten', 'acsess', 'karm', 'dron', 'grip', 'sd', 'card', 'count', 'three', 'access', 'find', 'individ', 'lol'], ['weird', 'hack', 'best', 'buy', 'buy', 'gopro6', 'three', 'camrea', 'accessories', 'get', 'ten', 'acsessorie', 'karma', 'drone', 'grip', 'sd', 'card', 'count', 'three', 'accessories', 'find', 'individually', 'lol'])\n",
      "original document: \n",
      "['Did', 'not', 'excpect', 'to', 'see', 'wongraven', 'on', 'this', 'thread', 'thats', 'awesome,', 'ive', 'Been', 'inte', 'alot', 'of', 'at', 'The', 'gates,', 'dissection,', '', 'Skitarg', 'latley', 'and', 'i', 'dont', 'know', 'if', 'you', 'can', 'call', 'skitarg', 'deathmetal', 'but', \"they're\", 'awesome', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['excpect', 'see', 'wongrav', 'thread', 'that', 'awesom', 'iv', 'int', 'alot', 'gat', 'dissect', 'skitarg', 'latley', 'dont', 'know', 'cal', 'skitarg', 'deathmet', 'theyr', 'awesom'], ['excpect', 'see', 'wongraven', 'thread', 'thats', 'awesome', 'ive', 'inte', 'alot', 'gate', 'dissection', 'skitarg', 'latley', 'dont', 'know', 'call', 'skitarg', 'deathmetal', 'theyre', 'awesome'])\n",
      "original document: \n",
      "['&gt;[**Tyler', 'Joseph', 'talking', 'about', 'song', 'Blasphemy', '[1:40]**](http://youtu.be/Lpq53oR94JM)\\n\\n&gt;', '[*^JohnyBoy*](https://www.youtube.com/channel/UCkwxnLofimoJRF0DvGeErQw)', '^in', '^Music\\n\\n&gt;*^8,554', '^views', '^since', '^Jun', '^2017*\\n\\n[^bot', '^info](/r/youtubefactsbot/wiki/index)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gttyler', 'joseph', 'talk', 'song', 'blasphemy', '140httpyoutubelpq53or94jm\\n\\ngt', 'johnyboyhttpswwwyoutubecomchanneluckwxnlofimojrf0dvgeerqw', 'music\\n\\ngt8554', 'view', 'sint', 'jun', '2017\\n\\nbot', 'inforyoutubefactsbotwikiindex'], ['gttyler', 'joseph', 'talk', 'song', 'blasphemy', '140httpyoutubelpq53or94jm\\n\\ngt', 'johnyboyhttpswwwyoutubecomchanneluckwxnlofimojrf0dvgeerqw', 'music\\n\\ngt8554', 'view', 'since', 'jun', '2017\\n\\nbot', 'inforyoutubefactsbotwikiindex'])\n",
      "original document: \n",
      "[\"She's\", 'hot', 'AF']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['she', 'hot', 'af'], ['shes', 'hot', 'af'])\n",
      "original document: \n",
      "['Well', 'what', 'are', 'they', 'going', 'to', 'do?', 'Wait', 'until', \"it's\", 'a', 'good', 'time', '?', 'They', \"don't\", 'have', 'a', 'lot', 'of', 'options.\\n\\nI', 'understand', 'their', 'decision', 'and', 'in', 'many', 'ways', \"it's\", 'great', 'life', 'experience', 'that', 'is', 'irreplaceable.', 'But', \"let's\", 'not', 'pretend', 'like', 'they', 'were', 'only', 'thinking', 'of', 'other', 'people', 'when', 'they', 'joined.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'going', 'wait', 'good', 'tim', 'dont', 'lot', 'options\\n\\ni', 'understand', 'decid', 'many', 'way', 'gre', 'lif', 'expery', 'irreplac', 'let', 'pretend', 'lik', 'think', 'peopl', 'join'], ['well', 'go', 'wait', 'good', 'time', 'dont', 'lot', 'options\\n\\ni', 'understand', 'decision', 'many', 'ways', 'great', 'life', 'experience', 'irreplaceable', 'let', 'pretend', 'like', 'think', 'people', 'join'])\n",
      "original document: \n",
      "['Err', 'dungeon.', 'Thanks', 'for', 'the', 'correction.', 'I', 'meant', 'that.', 'Any', 'idea', 'when', 'that', 'second', 'one', 'will', 'occur?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['er', 'dungeon', 'thank', 'correct', 'meant', 'ide', 'second', 'on', 'occ'], ['err', 'dungeon', 'thank', 'correction', 'mean', 'idea', 'second', 'one', 'occur'])\n",
      "original document: \n",
      "['Please', 'refer', 'to', '[this', 'thread](https://www.reddit.com/r/gopro/comments/4wac7w/lets_talk_about_sd_cards_again/)', 'for', 'information', 'and', 'discussion', 'regarding', 'SD', 'cards', 'for', 'GoPro\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/gopro)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'ref', 'threadhttpswwwredditcomrgoprocomments4wac7wlets_talk_about_sd_cards_again', 'inform', 'discuss', 'regard', 'sd', 'card', 'gopro\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorgopro', 'quest', 'concern'], ['please', 'refer', 'threadhttpswwwredditcomrgoprocomments4wac7wlets_talk_about_sd_cards_again', 'information', 'discussion', 'regard', 'sd', 'card', 'gopro\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorgopro', 'question', 'concern'])\n",
      "original document: \n",
      "['To', 'be', 'fair', 'though', 'typology', 'among', 'other', 'things', 'appears', 'as', 'early', 'as', 'Genesis.', 'The', 'difference', 'is', 'in', 'whoever', 'is', 'claiming', 'to', 'interpret', 'said', 'prophecy.', 'If', 'Matthew', 'does', 'not', 'have', 'any', 'authority,', 'then', 'you', 'pretty', 'much', 'can', 'throw', 'out', 'almost', 'of', 'all', 'Christianity.', '\\n\\nSecondly', 'who', 'knows', 'maybe', 'you', 'could', 'reinterpret', 'it,', 'but', 'the', 'reason', 'they', 'would', 'get', 'laughed', 'at', 'is', 'because', 'there', 'would', 'be', 'reasons', 'to', 'go', 'against', 'that', 'belief.', 'People', 'also', 'did', 'interpret', 'Daniel,', 'and', 'Revelation', 'for', 'thinking', 'the', 'end', 'times', 'were', 'in', 'the', '70s-80s', 'because', 'of', 'the', 'cold', 'war.', 'And', 'actually', 'that', 'belief', 'was', 'a', 'big', 'reason', 'the', 'modern', 'version', 'of', 'the', 'rapture', 'is', 'still', 'professed', 'today,', 'so', 'they', 'were', 'not', 'laughed', \"at.\\n\\nIt's\", 'very', 'important', 'to', 'look', 'at', 'the', 'context', 'behind', 'the', 'interpretations.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'though', 'typolog', 'among', 'thing', 'appear', 'ear', 'genes', 'diff', 'whoev', 'claim', 'interpret', 'said', 'prophecy', 'matthew', 'auth', 'pretty', 'much', 'throw', 'almost', 'christianity', '\\n\\nsecondly', 'know', 'mayb', 'could', 'reinterpret', 'reason', 'would', 'get', 'laugh', 'would', 'reason', 'go', 'believ', 'peopl', 'also', 'interpret', 'daniel', 'revel', 'think', 'end', 'tim', '70s80s', 'cold', 'war', 'act', 'believ', 'big', 'reason', 'modern', 'vert', 'rapt', 'stil', 'profess', 'today', 'laugh', 'at\\n\\nits', 'import', 'look', 'context', 'behind', 'interpret'], ['fair', 'though', 'typology', 'among', 'things', 'appear', 'early', 'genesis', 'difference', 'whoever', 'claim', 'interpret', 'say', 'prophecy', 'matthew', 'authority', 'pretty', 'much', 'throw', 'almost', 'christianity', '\\n\\nsecondly', 'know', 'maybe', 'could', 'reinterpret', 'reason', 'would', 'get', 'laugh', 'would', 'reason', 'go', 'belief', 'people', 'also', 'interpret', 'daniel', 'revelation', 'think', 'end', 'time', '70s80s', 'cold', 'war', 'actually', 'belief', 'big', 'reason', 'modern', 'version', 'rapture', 'still', 'profess', 'today', 'laugh', 'at\\n\\nits', 'important', 'look', 'context', 'behind', 'interpretations'])\n",
      "original document: \n",
      "['I', 'think', 'it', \"wouldn't\", 'even', 'boot', 'without', 'a', 'gpu.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'wouldnt', 'ev', 'boot', 'without', 'gpu'], ['think', 'wouldnt', 'even', 'boot', 'without', 'gpu'])\n",
      "original document: \n",
      "['Officially', 'licensed', 'by', 'Capcom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['off', 'licens', 'capcom'], ['officially', 'license', 'capcom'])\n",
      "original document: \n",
      "['2', 'randoms', 'please', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'random', 'pleas'], ['two', 'randoms', 'please'])\n",
      "original document: \n",
      "['When', 'we', 'talk', 'the', 'middle', 'east,', 'sadly', 'we', \"can't\", 'hold', 'everybody', 'to', 'the', 'western', 'standard.', 'So', 'we', 'have', 'to', 'look', 'at', 'it', 'in', 'context', 'of', 'brutal', 'leaders', 'who', 'often', 'imprison', 'or', 'kill', '', 'opposition.', '\\n\\n\\nYet', 'you', 'kinda', 'further', 'compounded', 'the', 'point', 'of', 'my', 'post:', 'you', 'happily', 'embrace', 'things', 'that', 'are', 'fully', 'supportive', 'of', 'your', 'view,', 'and', 'stop', 'caring,', 'or', 'ignore', 'everything', 'you', 'dislike.', '\\n\\n\\nI', 'would', 'seriously', 'suggest', 'recognizing', 'that', 'your', 'side', 'in', 'anything', 'may', 'not', 'be', 'perfect,', 'and', 'that', 'you', 'need', 'to', 'take', 'on', 'challenging', 'views', 'otherwise', 'you', 'risk', 'falling', 'into', 'the', 'same', 'trap', 'that', 'the', 'FSA', 'fell', 'into', 'where', 'they', 'became', 'so', 'full', 'of', 'hatred', 'and', 'willing', 'blindness', 'to', 'the', 'evil', 'that', 'claimed', 'to', 'support', 'their', 'revolution,', 'that', 'they', 'allowed', 'themselves', 'to', 'be', 'so', 'distanced', 'from', 'reality', 'that', 'none', 'saw', 'how', 'the', 'evil', 'had', 'long', 'usurped', 'their', 'cause.', '\\n\\n\\nSo', 'my', 'question', 'to', 'you:\\n\\nIs', 'it', 'justified', 'for', 'you', 'to', 'be', 'lied', 'to,', 'and', 'to', 'completely', 'believe', 'their', 'lies,', 'then', 'stop', 'caring', 'about', 'this', 'the', 'moment', 'you', 'find', 'out', 'the', 'person', 'who', 'committed', 'this', 'murder', 'was', 'on', 'the', 'side', 'that', 'you', 'support,', 'and', 'to', 'disregard', 'how', 'people', 'exploited', 'your', 'anger', 'for', 'their', 'own', 'gain?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['talk', 'middl', 'east', 'sad', 'cant', 'hold', 'everybody', 'western', 'standard', 'look', 'context', 'brut', 'lead', 'oft', 'imprison', 'kil', 'opposit', '\\n\\n\\nyet', 'kind', 'compound', 'point', 'post', 'happy', 'embrac', 'thing', 'ful', 'support', 'view', 'stop', 'car', 'ign', 'everyth', 'dislik', '\\n\\n\\ni', 'would', 'sery', 'suggest', 'recogn', 'sid', 'anyth', 'may', 'perfect', 'nee', 'tak', 'challeng', 'view', 'otherw', 'risk', 'fal', 'trap', 'fsa', 'fel', 'becam', 'ful', 'hat', 'wil', 'blind', 'evil', 'claim', 'support', 'revolv', 'allow', 'dist', 'real', 'non', 'saw', 'evil', 'long', 'usurp', 'caus', '\\n\\n\\nso', 'quest', 'you\\n\\nis', 'just', 'lied', 'complet', 'believ', 'lie', 'stop', 'car', 'mom', 'find', 'person', 'commit', 'murd', 'sid', 'support', 'disregard', 'peopl', 'exploit', 'ang', 'gain'], ['talk', 'middle', 'east', 'sadly', 'cant', 'hold', 'everybody', 'western', 'standard', 'look', 'context', 'brutal', 'leaders', 'often', 'imprison', 'kill', 'opposition', '\\n\\n\\nyet', 'kinda', 'compound', 'point', 'post', 'happily', 'embrace', 'things', 'fully', 'supportive', 'view', 'stop', 'care', 'ignore', 'everything', 'dislike', '\\n\\n\\ni', 'would', 'seriously', 'suggest', 'recognize', 'side', 'anything', 'may', 'perfect', 'need', 'take', 'challenge', 'view', 'otherwise', 'risk', 'fall', 'trap', 'fsa', 'fell', 'become', 'full', 'hatred', 'will', 'blindness', 'evil', 'claim', 'support', 'revolution', 'allow', 'distance', 'reality', 'none', 'saw', 'evil', 'long', 'usurp', 'cause', '\\n\\n\\nso', 'question', 'you\\n\\nis', 'justify', 'lie', 'completely', 'believe', 'lie', 'stop', 'care', 'moment', 'find', 'person', 'commit', 'murder', 'side', 'support', 'disregard', 'people', 'exploit', 'anger', 'gain'])\n",
      "original document: \n",
      "['Thanks,', 'me', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank'], ['thank'])\n",
      "original document: \n",
      "['The', 'problem', \"isn't\", 'your', 'personality,', \"it's\", 'your', 'husband.', 'He', \"doesn't\", 'pull', 'his', 'weight,', 'complains', 'when', 'you', 'do', 'it,', 'complains', 'when', 'you', 'ask', 'him', 'to', 'do', 'it,', 'and', 'is', 'an', 'asshole', 'when', 'you', 'point', 'out', 'the', 'problem.', 'And', \"he's\", 'found', 'a', 'way', 'to', 'blame', 'this', 'all', 'on', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'isnt', 'person', 'husband', 'doesnt', 'pul', 'weight', 'complain', 'complain', 'ask', 'asshol', 'point', 'problem', 'hes', 'found', 'way', 'blam'], ['problem', 'isnt', 'personality', 'husband', 'doesnt', 'pull', 'weight', 'complain', 'complain', 'ask', 'asshole', 'point', 'problem', 'hes', 'find', 'way', 'blame'])\n",
      "original document: \n",
      "['It', 'was', 'made', 'by', 'PDP', '(Then', 'known', 'as', 'Pelican)', 'to', 'interact', 'with', 'Rock', 'Band.', 'Every', 'song', 'made', 'from', 'Rock', 'Band', '1', 'through', '2', 'plus', 'all', 'the', 'DLC', 'contains', 'Stage', 'Kit', 'commands', 'to', 'go', 'along', 'with', 'the', 'song.', 'Rock', 'Band', '3', 'actually', 'did', 'not', 'support', 'the', 'stage', 'kit', 'at', 'launch', '(due', 'to', \"it's\", 'low', 'sales),', 'but', 'a', 'later', 'patch', 'added', 'it', 'back', 'in,', 'although', 'I', 'think', 'all', 'RB3', 'DLC', 'and', 'disc', 'songs', 'just', 'had', 'a', 'sort', 'of', '\"generic\"', 'stage', 'kit', 'support', 'based', 'off', 'BPM', 'or', 'something.\\n\\nThe', 'reason', 'it', 'was', '360', 'only', 'is', 'the', 'stage', 'kit', 're-uses', 'the', 'rumble', 'information', 'channel', 'for', 'a', 'standard', 'controller', 'to', 'send', 'the', 'stage', 'kit', 'controls', 'for', 'what', 'it', 'does.', 'Since', 'the', 'PS3', 'did', 'not', 'support', 'rumble', 'at', 'all', 'at', 'the', 'time,', 'it', 'never', 'got', 'the', 'stage', 'kit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mad', 'pdp', 'known', 'pel', 'interact', 'rock', 'band', 'every', 'song', 'mad', 'rock', 'band', 'on', 'two', 'plu', 'dlc', 'contain', 'stag', 'kit', 'command', 'go', 'along', 'song', 'rock', 'band', 'three', 'act', 'support', 'stag', 'kit', 'launch', 'due', 'low', 'sal', 'lat', 'patch', 'ad', 'back', 'although', 'think', 'rb3', 'dlc', 'disc', 'song', 'sort', 'gen', 'stag', 'kit', 'support', 'bas', 'bpm', 'something\\n\\nthe', 'reason', 'three hundred and sixty', 'stag', 'kit', 'reus', 'rumbl', 'inform', 'channel', 'standard', 'control', 'send', 'stag', 'kit', 'control', 'sint', 'ps3', 'support', 'rumbl', 'tim', 'nev', 'got', 'stag', 'kit'], ['make', 'pdp', 'know', 'pelican', 'interact', 'rock', 'band', 'every', 'song', 'make', 'rock', 'band', 'one', 'two', 'plus', 'dlc', 'contain', 'stage', 'kit', 'command', 'go', 'along', 'song', 'rock', 'band', 'three', 'actually', 'support', 'stage', 'kit', 'launch', 'due', 'low', 'sales', 'later', 'patch', 'add', 'back', 'although', 'think', 'rb3', 'dlc', 'disc', 'songs', 'sort', 'generic', 'stage', 'kit', 'support', 'base', 'bpm', 'something\\n\\nthe', 'reason', 'three hundred and sixty', 'stage', 'kit', 'reuse', 'rumble', 'information', 'channel', 'standard', 'controller', 'send', 'stage', 'kit', 'control', 'since', 'ps3', 'support', 'rumble', 'time', 'never', 'get', 'stage', 'kit'])\n",
      "original document: \n",
      "['Unless', 'your', 'teacher', 'said', 'that', 'before', '1995,', 'he', 'was', 'mistaken.', '', 'On', '23', 'March', '1995,', 'a', 'federal-level', 'fatwa', 'was', 'issued', 'saying', 'cigarettes', 'are', 'haram.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unless', 'teach', 'said', 'one thousand, nine hundred and ninety-fiv', 'mistak', 'twenty-three', 'march', 'one thousand, nine hundred and ninety-fiv', 'federallevel', 'fatw', 'issu', 'say', 'cigaret', 'haram'], ['unless', 'teacher', 'say', 'one thousand, nine hundred and ninety-five', 'mistake', 'twenty-three', 'march', 'one thousand, nine hundred and ninety-five', 'federallevel', 'fatwa', 'issue', 'say', 'cigarettes', 'haram'])\n",
      "original document: \n",
      "['**HD**|\\n[**AWAY', 'MultiBitrate**](http://www.streams4sport.com/index.php?/topic/331-mariners-vs-angels-away/)', '|', '\\nMobile', 'Compatible', ':', 'Yes', '📱', '|', 'Ad', 'Overlays:', '0', '|\\n\\n**HD**|\\n[**HOME', 'MultiBitrate**](http://www.streams4sport.com/index.php?/topic/330-mariners-vs-angels-home/)', '|', '\\nMobile', 'Compatible', ':', 'Yes', '📱', '|', 'Ad', 'Overlays:', '0', '|\\n\\nUpvote', 'if', 'you', 'like', '.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hd\\naway', 'multibitratehttpwwwstreams4sportcomindexphptopic331marinersvsangelsaway', '\\nmobile', 'compat', 'ye', 'ad', 'overlay', 'zero', '\\n\\nhd\\nhome', 'multibitratehttpwwwstreams4sportcomindexphptopic330marinersvsangelshome', '\\nmobile', 'compat', 'ye', 'ad', 'overlay', 'zero', '\\n\\nupvote', 'lik'], ['hd\\naway', 'multibitratehttpwwwstreams4sportcomindexphptopic331marinersvsangelsaway', '\\nmobile', 'compatible', 'yes', 'ad', 'overlay', 'zero', '\\n\\nhd\\nhome', 'multibitratehttpwwwstreams4sportcomindexphptopic330marinersvsangelshome', '\\nmobile', 'compatible', 'yes', 'ad', 'overlay', 'zero', '\\n\\nupvote', 'like'])\n",
      "original document: \n",
      "['Bamboo', 'House', 'in', 'Hollywood.', 'Get', 'there', 'before', '2am', 'if', 'you', 'want', 'to', 'have', 'a', 'seat', 'or', 'table']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bamboo', 'hous', 'hollywood', 'get', '2am', 'want', 'seat', 'tabl'], ['bamboo', 'house', 'hollywood', 'get', '2am', 'want', 'seat', 'table'])\n",
      "original document: \n",
      "['Ditto!!', 'R1', 'is', 'the', 'truth']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ditto', 'r1', 'tru'], ['ditto', 'r1', 'truth'])\n",
      "original document: \n",
      "['I', 'was', 'thinking', 'specialization', 'within', 'a', 'class,', 'not', 'switching', 'between', 'classes', 'entirely.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'spec', 'within', 'class', 'switch', 'class', 'entir'], ['think', 'specialization', 'within', 'class', 'switch', 'class', 'entirely'])\n",
      "original document: \n",
      "['[Enemy', 'Intel:', 'The', 'Power', 'Of', 'Play-Action', '(3:28)](http://www.philadelphiaeagles.com/videos/videos/Enemy-Intel-The-Power-Of-Play-Action/0787dc83-4832-45e2-a527-e38aa57ffa75)\\n\\nThe', 'Chargers', 'boast', 'one', 'of', 'the', 'best', 'pass', 'rush', 'duos', 'in', 'the', 'entire', 'NFL.', 'Greg', 'Cosell', 'shows', 'us', 'what', 'makes', 'them', 'so', 'good,', 'and', 'John', 'Clark', 'joins', 'Mike', 'Quick', 'to', 'explain', 'how', 'the', 'Eagles', 'can', 'negate', 'their', 'effectiveness.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['enemy', 'intel', 'pow', 'playact', '328httpwwwphiladelphiaeaglescomvideosvideosenemyintelthepowerofplayaction0787dc83483245e2a527e38aa57ffa75\\n\\nthe', 'charg', 'boast', 'on', 'best', 'pass', 'rush', 'duo', 'entir', 'nfl', 'greg', 'cosel', 'show', 'us', 'mak', 'good', 'john', 'clark', 'join', 'mik', 'quick', 'explain', 'eagl', 'neg', 'effect'], ['enemy', 'intel', 'power', 'playaction', '328httpwwwphiladelphiaeaglescomvideosvideosenemyintelthepowerofplayaction0787dc83483245e2a527e38aa57ffa75\\n\\nthe', 'chargers', 'boast', 'one', 'best', 'pass', 'rush', 'duos', 'entire', 'nfl', 'greg', 'cosell', 'show', 'us', 'make', 'good', 'john', 'clark', 'join', 'mike', 'quick', 'explain', 'eagle', 'negate', 'effectiveness'])\n",
      "original document: \n",
      "['[+AgaveNeomexicana](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokl00/):\\n\\nGet', 'to', 'those', 'really', 'delicious', 'leaves', 'just', 'up', 'there', 'a', 'little', 'higher', '.', '.', '.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agaveneomexicanahttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokl00\\n\\nget', 'real', 'delicy', 'leav', 'littl', 'high'], ['agaveneomexicanahttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokl00\\n\\nget', 'really', 'delicious', 'leave', 'little', 'higher'])\n",
      "original document: \n",
      "['He’s', 'talking', 'about', 'being', 'killed', 'mentally', '.The', 'pain', 'from', 'knowing', 'he', 'is', 'just', 'like', 'his', 'dad', 'is', 'killing', 'him', 'mentally.', 'Edit:', 'not', 'sure', 'if', 'that', 'made', 'sense']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'talk', 'kil', 'ment', 'pain', 'know', 'lik', 'dad', 'kil', 'ment', 'edit', 'sur', 'mad', 'sens'], ['hes', 'talk', 'kill', 'mentally', 'pain', 'know', 'like', 'dad', 'kill', 'mentally', 'edit', 'sure', 'make', 'sense'])\n",
      "original document: \n",
      "['143412560|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'xfBAecFs)\\n\\nI', 'voted', 'for', 'DRUMPF', 'and', 'I', 'hate', 'myself', 'every', 'day', 'for', 'it.\\nIT', 'WAS', 'HER', 'TURN\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, five hundred and sixty', 'gt', 'unit', 'stat', 'anonym', 'id', 'xfbaecfs\\n\\ni', 'vot', 'drumpf', 'hat', 'every', 'day', 'it\\nit', 'turn\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, five hundred and sixty', 'gt', 'unite', 'state', 'anonymous', 'id', 'xfbaecfs\\n\\ni', 'vote', 'drumpf', 'hate', 'every', 'day', 'it\\nit', 'turn\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Given', 'that', 'race', 'is', 'still', 'the', 'primary', 'social', 'divide', 'in', 'the', 'world,', 'we', \"shouldn't\", 'point', 'out', 'when', 'people', 'have', 'beliefs', 'that', 'our', 'unusual', 'within', 'their', 'group?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'rac', 'stil', 'prim', 'soc', 'divid', 'world', 'shouldnt', 'point', 'peopl', 'believ', 'unus', 'within', 'group'], ['give', 'race', 'still', 'primary', 'social', 'divide', 'world', 'shouldnt', 'point', 'people', 'beliefs', 'unusual', 'within', 'group'])\n",
      "original document: \n",
      "['this', 'dude', 'is', 'in', 'every', 'single', 'thread', 'complaining', 'about', 'the', 'sub']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dud', 'every', 'singl', 'thread', 'complain', 'sub'], ['dude', 'every', 'single', 'thread', 'complain', 'sub'])\n",
      "original document: \n",
      "['Playing', 'as', 'American', 'on', 'Argonne.', 'Run', 'into', 'a', 'squadmate', 'capping', 'B', 'from', 'the', 'other', 'direction', 'and', 'mine', '(or', 'his)', 'character', 'goes,', '', '\"Hey,', 'good', 'to', 'see', 'ya!\".']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'am', 'argon', 'run', 'squadm', 'cap', 'b', 'direct', 'min', 'charact', 'goe', 'hey', 'good', 'see', 'ya'], ['play', 'american', 'argonne', 'run', 'squadmate', 'cap', 'b', 'direction', 'mine', 'character', 'go', 'hey', 'good', 'see', 'ya'])\n",
      "original document: \n",
      "['Hehe.', 'Thank', 'you.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heh', 'thank'], ['hehe', 'thank'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['This', 'has', 'been', 'happening', 'to', 'me', 'as', 'well', 'especially', 'with', 'Eagles', 'and', 'Phoenixes,', 'and', \"it's\", 'starting', 'to', 'get', 'rather', 'frustrating.', \"I've\", 'been', 'playing', 'as', 'Teclis.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hap', 'wel', 'espec', 'eagl', 'phoenix', 'start', 'get', 'rath', 'frust', 'iv', 'play', 'tec'], ['happen', 'well', 'especially', 'eagle', 'phoenixes', 'start', 'get', 'rather', 'frustrate', 'ive', 'play', 'teclis'])\n",
      "original document: \n",
      "['Do', 'you', 'know', 'which', 'one?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'on'], ['know', 'one'])\n",
      "original document: \n",
      "['We', 'have', 'a', 'support', 'group,', 'if', 'you', 'need', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['support', 'group', 'nee'], ['support', 'group', 'need'])\n",
      "original document: \n",
      "['[Found', 'a', 'dotard', 'tweet!', '', 'Straight', 'from', 'the', 'golf', 'course.](https://twitter.com/AndrewOnSeeAIR/status/914227871010447360)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['found', 'dotard', 'tweet', 'straight', 'golf', 'coursehttpstwittercomandrewonseeairstatus914227871010447360'], ['find', 'dotard', 'tweet', 'straight', 'golf', 'coursehttpstwittercomandrewonseeairstatus914227871010447360'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Great', 'job.', 'The', 'deck', 'looks', 'sweet.', 'I', 'love', 'Deeproot', 'Waters', 'and', \"Shaper's\", 'Sanctuary', 'as', 'sideboard', 'tech.', 'So', 'much', 'resilience.', \"I'm\", 'going', 'to', 'keep', 'them', 'in', 'mind', 'as', 'the', 'Modern', 'UG', 'brews', 'develop.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'job', 'deck', 'look', 'sweet', 'lov', 'deeproot', 'wat', 'shap', 'sanctu', 'sideboard', 'tech', 'much', 'resy', 'im', 'going', 'keep', 'mind', 'modern', 'ug', 'brew', 'develop'], ['great', 'job', 'deck', 'look', 'sweet', 'love', 'deeproot', 'water', 'shapers', 'sanctuary', 'sideboard', 'tech', 'much', 'resilience', 'im', 'go', 'keep', 'mind', 'modern', 'ug', 'brew', 'develop'])\n",
      "original document: \n",
      "['so', 'it', 'is', 'legit?', 'BTW', \"doesn't\", 'thunder', 'road', 'own', 'john', 'wick?', 'since', 'that', 'is', 'canon.', 'So', 'thunder', 'road', 'and', 'starbreeze', 'would', 'be', 'making', 'it', 'even', 'if', 'john', \"isn't\", 'in', 'it', 'i', 'am', 'assuming.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['legit', 'btw', 'doesnt', 'thund', 'road', 'john', 'wick', 'sint', 'canon', 'thund', 'road', 'starbreez', 'would', 'mak', 'ev', 'john', 'isnt', 'assum'], ['legit', 'btw', 'doesnt', 'thunder', 'road', 'john', 'wick', 'since', 'canon', 'thunder', 'road', 'starbreeze', 'would', 'make', 'even', 'john', 'isnt', 'assume'])\n",
      "original document: \n",
      "['You', 'want', 'to', 'go', 'to', 'a', 'unit', 'on', 'a', 'Defense', 'Support', 'of', 'Civil', 'Authorities', '(DSCA)', 'tasking', 'but', 'if', \"you're\", 'not', 'already', 'in', 'one', 'it', 'would', 'be', 'hard', 'to', 'get', 'reassigned', 'to', 'one', 'in', 'time', 'to', 'help', 'with', 'the', 'hurricanes.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'go', 'unit', 'defens', 'support', 'civil', 'auth', 'dsca', 'task', 'yo', 'already', 'on', 'would', 'hard', 'get', 'reassign', 'on', 'tim', 'help', 'hur'], ['want', 'go', 'unit', 'defense', 'support', 'civil', 'authorities', 'dsca', 'task', 'youre', 'already', 'one', 'would', 'hard', 'get', 'reassign', 'one', 'time', 'help', 'hurricanes'])\n",
      "original document: \n",
      "[\"He's\", 'allowed', 'to', 'protest', 'however', 'the', 'heck', 'he', 'wants,', 'so', 'long', 'as', 'he', \"doesn't\", 'break', 'any', 'laws', '(which', 'he', \"hasn't),\", 'but', 'people', 'are', 'also', 'allowed', 'to', 'call', 'him', 'out', 'for', 'being', 'a', 'fool', 'who', \"can't\", 'even', 'articulate', 'what', \"he's\", 'actually', 'protesting.', 'Considering', 'he', 'wore', 'cop-as-pig', 'socks', 'and', 'made', 'some', 'ridiculous', 'statements', 'admiring', 'Che', 'Guevara,', \"it's\", 'pretty', 'clear', 'he', 'has', 'absolutely', 'no', 'freaking', 'idea', 'what', \"he's\", 'talking', 'about.', 'Some', 'would', 'say', 'its', 'a', 'publicity', 'play', 'to', 'go', 'out', 'as', 'a', 'martyr', 'for', 'social', 'injustice', 'when', 'he', \"wasn't\", 'good', 'enough', 'to', 'justify', 'his', 'massive', 'contract', 'or', 'hold', 'on', 'to', 'an', 'NFL', 'job.', 'He', 'found', 'a', 'way', 'to', 'stay', 'relevant.', 'People', 'forget', 'that', 'pretty', 'much', 'everyone', 'in', 'the', 'SF', 'locker', 'room', 'hated', 'this', 'dude', 'before', 'he', 'ever', 'did', 'anything', 'political,', 'even', 'going', 'back', 'to', 'their', '2013', 'SB', 'run.', 'Now', 'he', 'thinks', \"he's\", 'some', 'kind', 'of', 'MLK-style', 'civil', 'rights', 'intellectual?', 'Puh-leeze.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'allow', 'protest', 'howev', 'heck', 'want', 'long', 'doesnt', 'break', 'law', 'hasnt', 'peopl', 'also', 'allow', 'cal', 'fool', 'cant', 'ev', 'artic', 'hes', 'act', 'protest', 'consid', 'wor', 'copaspig', 'sock', 'mad', 'ridic', 'stat', 'admir', 'che', 'guevar', 'pretty', 'clear', 'absolv', 'freak', 'ide', 'hes', 'talk', 'would', 'say', 'publ', 'play', 'go', 'martyr', 'soc', 'injust', 'wasnt', 'good', 'enough', 'just', 'mass', 'contract', 'hold', 'nfl', 'job', 'found', 'way', 'stay', 'relev', 'peopl', 'forget', 'pretty', 'much', 'everyon', 'sf', 'lock', 'room', 'hat', 'dud', 'ev', 'anyth', 'polit', 'ev', 'going', 'back', 'two thousand and thirteen', 'sb', 'run', 'think', 'hes', 'kind', 'mlkstyle', 'civil', 'right', 'intellect', 'puhleez'], ['hes', 'allow', 'protest', 'however', 'heck', 'want', 'long', 'doesnt', 'break', 'laws', 'hasnt', 'people', 'also', 'allow', 'call', 'fool', 'cant', 'even', 'articulate', 'hes', 'actually', 'protest', 'consider', 'wear', 'copaspig', 'sock', 'make', 'ridiculous', 'statements', 'admire', 'che', 'guevara', 'pretty', 'clear', 'absolutely', 'freak', 'idea', 'hes', 'talk', 'would', 'say', 'publicity', 'play', 'go', 'martyr', 'social', 'injustice', 'wasnt', 'good', 'enough', 'justify', 'massive', 'contract', 'hold', 'nfl', 'job', 'find', 'way', 'stay', 'relevant', 'people', 'forget', 'pretty', 'much', 'everyone', 'sf', 'locker', 'room', 'hat', 'dude', 'ever', 'anything', 'political', 'even', 'go', 'back', 'two thousand and thirteen', 'sb', 'run', 'think', 'hes', 'kind', 'mlkstyle', 'civil', 'right', 'intellectual', 'puhleeze'])\n",
      "original document: \n",
      "['Its', 'easy,', 'just', 'setup', 'ports', '80-10080', 'as', 'open!!', 'Security', 'is', 'for', '🅱ussies']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['easy', 'setup', 'port', 'eight million, ten thousand and eighty', 'op', 'sec', 'ussy'], ['easy', 'setup', 'port', 'eight million, ten thousand and eighty', 'open', 'security', 'ussies'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'I', 'want', 'to', 'see', 'your', 'goggle', 'history.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'want', 'see', 'goggl', 'hist'], ['dont', 'think', 'want', 'see', 'goggle', 'history'])\n",
      "original document: \n",
      "['he', 'gave', 'up', 'bc', 'he’s', 'a', 'failure', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gav', 'bc', 'hes', 'fail'], ['give', 'bc', 'hes', 'failure'])\n",
      "original document: \n",
      "['Yeah,', 'those', 'two', 'senators', 'are', 'reserved', 'for', '20', 'fucked', 'up', 'farmers', 'in', 'Wyoming.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'two', 'sen', 'reserv', 'twenty', 'fuck', 'farm', 'wyom'], ['yeah', 'two', 'senators', 'reserve', 'twenty', 'fuck', 'farmers', 'wyoming'])\n",
      "original document: \n",
      "['Durant', \"can't\", 'lead', 'a', 'team', 'by', 'himself', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dur', 'cant', 'lead', 'team'], ['durant', 'cant', 'lead', 'team'])\n",
      "original document: \n",
      "['&gt;', \"I'm\", 'pretty', 'hungover', 'right', 'now', 'so', \"I'll\", 'update', 'this', 'later.\\n\\nSo,', \"how's\", 'the', 'saving', 'the', 'booze', 'money', 'going?', 'XD']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'im', 'pretty', 'hungov', 'right', 'il', 'upd', 'later\\n\\nso', 'how', 'sav', 'booz', 'money', 'going', 'xd'], ['gt', 'im', 'pretty', 'hungover', 'right', 'ill', 'update', 'later\\n\\nso', 'hows', 'save', 'booze', 'money', 'go', 'xd'])\n",
      "original document: \n",
      "['When', 'I', 'feel', 'like', 'that', 'with', 'deads', 'I', 'switch', 'it', 'up', 'for', 'the', 'day.', 'I', 'pull', 'conventional', 'but', 'if', 'I', 'feel', 'off', 'then', 'I', 'switch', 'to', 'sumo', 'or', 'stiff', 'leg.', 'Or', \"I'll\", 'do', 'low', 'weight', 'high', 'reps', 'super', 'slow', 'to', 'focus', '100%', 'on', 'form.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'dead', 'switch', 'day', 'pul', 'conv', 'feel', 'switch', 'sumo', 'stiff', 'leg', 'il', 'low', 'weight', 'high', 'rep', 'sup', 'slow', 'foc', 'one hundred', 'form'], ['feel', 'like', 'deads', 'switch', 'day', 'pull', 'conventional', 'feel', 'switch', 'sumo', 'stiff', 'leg', 'ill', 'low', 'weight', 'high', 'reps', 'super', 'slow', 'focus', 'one hundred', 'form'])\n",
      "original document: \n",
      "['Especially', 'during', 'festivities', 'and', 'holidays', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['espec', 'fest', 'holiday'], ['especially', 'festivities', 'holiday'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol7tj/):\\n\\nThis', 'is', 'a', 'good', 'answer,', \"that's\", 'what', 'I', 'would', 'be', 'doing', 'if', 'I', 'were', 'a', 'giraffe.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol7tj\\n\\nth', 'good', 'answ', 'that', 'would', 'giraff'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol7tj\\n\\nthis', 'good', 'answer', 'thats', 'would', 'giraffe'])\n",
      "original document: \n",
      "['If', 'I', 'have', 'an', 'FLB', 'Benedia', 'at', 'Lv150,', 'which', 'weapon', 'should', 'I', 'drop?', '(For', 'the', 'f2p', 'grid,', 'specifically)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flb', 'bened', 'lv150', 'weapon', 'drop', 'f2p', 'grid', 'spec'], ['flb', 'benedia', 'lv150', 'weapon', 'drop', 'f2p', 'grid', 'specifically'])\n",
      "original document: \n",
      "['fucking', 'preach', 'my', 'nigga...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'preach', 'nigg'], ['fuck', 'preach', 'nigga'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['But', 'even', 'if', 'you', 'do', 'a', 'degree', 'that', 'has', 'weight.', 'You', \"can't\", 'guarantee', 'everyone', 'who', 'goes', 'on', 'the', 'course', 'will', 'pass,', 'nor', 'can', 'you', 'guarantee', 'they', 'will', 'actually', 'get', 'a', 'job.\\n\\nBesides.', '\"Pointless\"', 'degree', 'is', 'very', 'subjective.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'degr', 'weight', 'cant', 'guar', 'everyon', 'goe', 'cours', 'pass', 'guar', 'act', 'get', 'job\\n\\nbesides', 'pointless', 'degr', 'subject'], ['even', 'degree', 'weight', 'cant', 'guarantee', 'everyone', 'go', 'course', 'pass', 'guarantee', 'actually', 'get', 'job\\n\\nbesides', 'pointless', 'degree', 'subjective'])\n",
      "original document: \n",
      "['Spotted', 'the', 'no-nothing,', 'never', 'been', 'outside', 'of', 'his', 'own', 'bubble,', 'coastal', 'city', 'dwelling,', 'liberal.', '\\n\\nKiddo,', 'you', \"don't\", 'understand', 'logistics.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spot', 'nonoth', 'nev', 'outsid', 'bubbl', 'coast', 'city', 'dwel', 'lib', '\\n\\nkiddo', 'dont', 'understand', 'log'], ['spot', 'nonothing', 'never', 'outside', 'bubble', 'coastal', 'city', 'dwell', 'liberal', '\\n\\nkiddo', 'dont', 'understand', 'logistics'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Shit', 'man,', \"I'm\", 'not', 'even', 'sure', 'about', 'almost', '1300', 'games', \"I'd\", 'want', 'to', 'try']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shit', 'man', 'im', 'ev', 'sur', 'almost', 'one thousand, three hundred', 'gam', 'id', 'want', 'try'], ['shit', 'man', 'im', 'even', 'sure', 'almost', 'one thousand, three hundred', 'game', 'id', 'want', 'try'])\n",
      "original document: \n",
      "['First', 'of', 'all,', 'Brigham', 'Young', 'NEVER', 'said', 'that.', '', 'Second', 'of', 'all,', 'every', 'Prophet', 'and', 'Apostle', 'I', 'know', 'of', 'has', 'already', 'stated', 'how', 'to', 'gain', 'a', 'testimony.........by', 'the', 'Spirit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'brigham', 'young', 'nev', 'said', 'second', 'every', 'prophet', 'apostl', 'know', 'already', 'stat', 'gain', 'testimonyby', 'spirit'], ['first', 'brigham', 'young', 'never', 'say', 'second', 'every', 'prophet', 'apostle', 'know', 'already', 'state', 'gain', 'testimonyby', 'spirit'])\n",
      "original document: \n",
      "['\"Oh', '2.147m', \"that's\", 'not', 'bad....wait', 'a', 'minute\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', '2147m', 'that', 'badwait', 'minut'], ['oh', '2147m', 'thats', 'badwait', 'minute'])\n",
      "original document: \n",
      "['Bias', 'or', 'something', 'else?', 'Cos', 'I', \"don't\", 'feel', 'like', \"they've\", 'been', 'biased.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bia', 'someth', 'els', 'cos', 'dont', 'feel', 'lik', 'theyv', 'bias'], ['bias', 'something', 'else', 'cos', 'dont', 'feel', 'like', 'theyve', 'bias'])\n",
      "original document: \n",
      "['☑', 'mucho\\n\\n☑', 'trabajo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mucho\\n\\n', 'trabajo'], ['mucho\\n\\n', 'trabajo'])\n",
      "original document: \n",
      "['#2', 'is', 'the', 'only', 'one', 'that', 'looks', 'like', 'it', 'belongs', 'on', 'a', 'runway', 'to', 'me,', '', 'so', \"that's\", 'my', 'vote.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'on', 'look', 'lik', 'belong', 'runway', 'that', 'vot'], ['two', 'one', 'look', 'like', 'belong', 'runway', 'thats', 'vote'])\n",
      "original document: \n",
      "['When', 'rebuilding', 'StoreFront', 'servers', 'you', 'must', 'ensure', 'to', 'use', 'the', 'same', 'Hostbase', 'URL,', 'same', 'store', 'path', 'and', 'the', 'same', 'SRID', 'in', 'order', 'to', 'avoid', 'this', 'particular', 'issue', 'with', 'Windows', 'Receivers.\\n\\nhttps://discussions.citrix.com/topic/357226-receiver-failover-between-storefront-server-groups/?p=1856858']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rebuild', 'storefront', 'serv', 'must', 'ens', 'us', 'hostbas', 'url', 'stor', 'path', 'srid', 'ord', 'avoid', 'particul', 'issu', 'window', 'receivers\\n\\nhttpsdiscussionscitrixcomtopic357226receiverfailoverbetweenstorefrontservergroupsp1856858'], ['rebuild', 'storefront', 'servers', 'must', 'ensure', 'use', 'hostbase', 'url', 'store', 'path', 'srid', 'order', 'avoid', 'particular', 'issue', 'windows', 'receivers\\n\\nhttpsdiscussionscitrixcomtopic357226receiverfailoverbetweenstorefrontservergroupsp1856858'])\n",
      "original document: \n",
      "['I', 'do', 'my', 'best,', 'lol.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'lol'], ['best', 'lol'])\n",
      "original document: \n",
      "['The', 'amount', 'of', 'pressure', 'needed', 'to', 'create', 'a', 'star', 'in', 'a', 'universe', 'in', 'which', 'is,', 'in', 'the', '\"big', 'freeze\"', 'scenario,', 'is', 'trying', 'to', 'grow', 'faster', 'apart', 'would', 'be', 'too', 'high', 'for', 'anything', 'to', 'get', 'done.', 'Regardless', 'of', 'that,', 'in', 'the', '\"big', 'freeze\"', 'scenario,', 'that', 'are', 'many', 'dwarf', 'stars', 'that', 'remain', 'for', '-', 'theoretically', '-', 'trillions', 'of', 'years.', 'We', 'could', 'build', 'a', 'Dyson', 'sphere', 'around', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amount', 'press', 'nee', 'cre', 'star', 'univers', 'big', 'freez', 'scenario', 'try', 'grow', 'fast', 'apart', 'would', 'high', 'anyth', 'get', 'don', 'regardless', 'big', 'freez', 'scenario', 'many', 'dwarf', 'star', 'remain', 'theoret', 'tril', 'year', 'could', 'build', 'dyson', 'sphere', 'around'], ['amount', 'pressure', 'need', 'create', 'star', 'universe', 'big', 'freeze', 'scenario', 'try', 'grow', 'faster', 'apart', 'would', 'high', 'anything', 'get', 'do', 'regardless', 'big', 'freeze', 'scenario', 'many', 'dwarf', 'star', 'remain', 'theoretically', 'trillions', 'years', 'could', 'build', 'dyson', 'sphere', 'around'])\n",
      "original document: \n",
      "['\"I', 'thought', \"you'd\", 'never', 'ask!', '&lt;3\"']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'youd', 'nev', 'ask', 'lt3'], ['think', 'youd', 'never', 'ask', 'lt3'])\n",
      "original document: \n",
      "['Je', 'suis', 'enrhubé.', 'Arrive', 'bô', 'à', 'dormir.', '', '', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['je', 'sui', 'enrhub', 'ar', 'bo', 'dormir'], ['je', 'suis', 'enrhube', 'arrive', 'bo', 'dormir'])\n",
      "original document: \n",
      "['Thank', 'you', 'heaps', 'for', 'the', 'trade', '&lt;:\\n\\nTrade', 'summary', 'for', 'rules:\\n\\n-', 'Me:', 'WIN2011', 'Suicune,', 'Gedatzu', '-&gt;', 'me\\n-', 'You:', 'SUM2013', 'Palkia', '+', 'XY', 'torchic,', 'obtained', 'by', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'heap', 'trad', 'lt\\n\\ntrade', 'sum', 'rules\\n\\n', 'win2011', 'suicun', 'gedatzu', 'gt', 'me\\n', 'sum2013', 'palk', 'xy', 'torch', 'obtain'], ['thank', 'heap', 'trade', 'lt\\n\\ntrade', 'summary', 'rules\\n\\n', 'win2011', 'suicune', 'gedatzu', 'gt', 'me\\n', 'sum2013', 'palkia', 'xy', 'torchic', 'obtain'])\n",
      "original document: \n",
      "['1.', 'Nightcap', '\\n2.', 'Quiet', 'Nights', 'or', 'Pembroke\\n3.', '1Q', '(had', 'to', 'throw', 'an', 'aro', 'in', 'there)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'nightcap', '\\n2', 'quiet', 'night', 'pembroke\\n3', '1q', 'throw', 'aro'], ['one', 'nightcap', '\\n2', 'quiet', 'nights', 'pembroke\\n3', '1q', 'throw', 'aro'])\n",
      "original document: \n",
      "['So', 'is', 'this', 'the', 'Binging', 'with', 'Babish,', 'but', 'for', 'TV', 'drinks', 'instead', 'of', 'food?', 'Nice!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bing', 'bab', 'tv', 'drink', 'instead', 'food', 'nic'], ['binge', 'babish', 'tv', 'drink', 'instead', 'food', 'nice'])\n",
      "original document: \n",
      "['Guzan', 'is', 'a', 'god']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guz', 'god'], ['guzan', 'god'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Yeah', \"he's\", 'one', 'of', 'the', 'best', '(in', 'terms', 'of', 'usernames', 'and', 'in', 'shitposts)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'hes', 'on', 'best', 'term', 'usernam', 'shitpost'], ['yeah', 'hes', 'one', 'best', 'term', 'usernames', 'shitposts'])\n",
      "original document: \n",
      "[\"Don't\", 'think', 'so.', \"Can't\", 'see', 'any', 'connection.', 'Also', 'that', 'site', 'is', 'awful', 'and', 'not', 'reliable.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'cant', 'see', 'connect', 'also', 'sit', 'aw', 'rely'], ['dont', 'think', 'cant', 'see', 'connection', 'also', 'site', 'awful', 'reliable'])\n",
      "original document: \n",
      "['I', \"haven't\", 'actually', 'seen', 'any,', 'I', 'just', 'want', 'to.', '\\n\\nExcept', 'a', 'death', 'count', 'at', 'the', 'beginning', 'of', 'a', 'Naked', 'hunt', 'against', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hav', 'act', 'seen', 'want', '\\n\\nexcept', 'dea', 'count', 'begin', 'nak', 'hunt', 'on'], ['havent', 'actually', 'see', 'want', '\\n\\nexcept', 'death', 'count', 'begin', 'naked', 'hunt', 'one'])\n",
      "original document: \n",
      "['Breadman', 'gave', 'Murray', 'a', 'yeast', 'infection.', 'Murray', 'better', 'get', 'used', 'to', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['breadm', 'gav', 'murray', 'yeast', 'infect', 'murray', 'bet', 'get', 'us'], ['breadman', 'give', 'murray', 'yeast', 'infection', 'murray', 'better', 'get', 'use'])\n",
      "original document: \n",
      "['Depends', 'on', 'how', 'patient', 'I', 'am', 'willing', 'to', 'be.', 'Usually', \"I'm\", 'pretty', 'lax', 'about', 'it', 'and', 'can', 'explain.', 'There', 'are', 'a', 'few', 'rarer', 'times', 'where', 'frustration', 'will', 'set', 'in', 'pretty', 'easily,', 'generally', 'if', \"I'm\", 'already', 'agitated', 'by', 'something', 'or', 'other.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'paty', 'wil', 'us', 'im', 'pretty', 'lax', 'explain', 'rar', 'tim', 'frust', 'set', 'pretty', 'easy', 'gen', 'im', 'already', 'agit', 'someth'], ['depend', 'patient', 'will', 'usually', 'im', 'pretty', 'lax', 'explain', 'rarer', 'time', 'frustration', 'set', 'pretty', 'easily', 'generally', 'im', 'already', 'agitate', 'something'])\n",
      "original document: \n",
      "['#', '**[PC]**', 'Paypal', '{Friends', 'and', 'Family},', 'Prices', 'are', 'in', 'USD.', '\\n\\n#', 'I', 'also', 'accept', 'most', 'cryptos', '(5%', 'discount).', '\\n\\n#**[H]**', '16', 'Accelerator', 'Crates', '**[W]**', '$1', 'each!', '($15', 'for', 'all)!!!\\n\\n~~**[H]**', 'Jager', '619', 'RS', '**[W]**', '$7~~\\n\\n**[H]**', 'Apex', '', '**[W]**', '$2\\n\\n**[H]**', 'Breakout', 'Type-S/Dominus', 'GT/Octane', 'ZSR/Forest', 'Green', 'Breakout**[W]**', '$1', 'each\\n\\n**[H]', 'Purple', 'Thermal', '(Sweeper)', '', '[W]', '$2**', '\\n\\n[H]', 'Sky', 'Blue', 'Thermal/Crimson', 'Unicorn', 'Horn/Black', 'Falco', '[W]', '$1', 'each\\n\\n[H]', 'Turbine/Zomba/Looper/Roulette/Kalos/Photon/', '[W]', '$0.75', 'each', '\\n\\n[H]', '**RLCS**', 'Decals', '(Type-S,', 'Dominus', 'GT,', 'Breakout)/', 'Mount', 'Champ[W]', '$0.75', 'each\\n\\nIf', 'you', 'have', 'any', 'questions/requests', 'feel', 'free', 'to', 'add', 'me', 'on', 'Steam!\\n\\n[**Steam', ':)', ':)**](http://steamcommunity.com/profiles/76561198184805093/)**|**\\n[**myrep', ':)', ':)**](https://www.reddit.com/r/RocketLeagueExchange/comments/52k9le/meta_successful_tradereputation_thread/da9vudm/)**|**\\n[**myrep2', ':)', ':)**](https://www.reddit.com/r/RocketLeagueExchange/comments/5yzsc4/meta_successful_tradereputation_thread_20/dhcjm85/)\\n[**myrep3', ':)', ':)**](https://www.reddit.com/r/RocketLeagueExchange/comments/6ywnpi/meta_successful_tradereputation_post_30/dmsopus/)\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pc', 'payp', 'friend', 'famy', 'pric', 'usd', '\\n\\n', 'also', 'acceiv', 'crypto', 'fiv', 'discount', '\\n\\nh', 'sixteen', 'accel', 'crat', 'w', 'on', 'fifteen', 'all\\n\\nh', 'jag', 'six hundred and nineteen', 'rs', 'w', '7\\n\\nh', 'apex', 'w', '2\\n\\nh', 'breakout', 'typesdomin', 'gtoct', 'zsrforest', 'green', 'breakoutw', 'on', 'each\\n\\nh', 'purpl', 'therm', 'sweep', 'w', 'two', '\\n\\nh', 'sky', 'blu', 'thermalcrimson', 'unicorn', 'hornblack', 'falco', 'w', 'on', 'each\\n\\nh', 'turbinezombalooperroulettekalosphoton', 'w', 'seventy-five', '\\n\\nh', 'rlcs', 'dec', 'typ', 'domin', 'gt', 'breakout', 'mount', 'champw', 'seventy-five', 'each\\n\\nif', 'questionsrequest', 'feel', 'fre', 'ad', 'steam\\n\\nsteam', 'httpsteamcommunitycomprofiles76561198184805093\\nmyrep', 'httpswwwredditcomrrocketleagueexchangecomments52k9lemeta_successful_tradereputation_threadda9vudm\\nmyrep2', 'httpswwwredditcomrrocketleagueexchangecomments5yzsc4meta_successful_tradereputation_thread_20dhcjm85\\nmyrep3', 'httpswwwredditcomrrocketleagueexchangecomments6ywnpimeta_successful_tradereputation_post_30dmsopus\\n\\n'], ['pc', 'paypal', 'friends', 'family', 'price', 'usd', '\\n\\n', 'also', 'accept', 'cryptos', 'five', 'discount', '\\n\\nh', 'sixteen', 'accelerator', 'crate', 'w', 'one', 'fifteen', 'all\\n\\nh', 'jager', 'six hundred and nineteen', 'rs', 'w', '7\\n\\nh', 'apex', 'w', '2\\n\\nh', 'breakout', 'typesdominus', 'gtoctane', 'zsrforest', 'green', 'breakoutw', 'one', 'each\\n\\nh', 'purple', 'thermal', 'sweeper', 'w', 'two', '\\n\\nh', 'sky', 'blue', 'thermalcrimson', 'unicorn', 'hornblack', 'falco', 'w', 'one', 'each\\n\\nh', 'turbinezombalooperroulettekalosphoton', 'w', 'seventy-five', '\\n\\nh', 'rlcs', 'decals', 'type', 'dominus', 'gt', 'breakout', 'mount', 'champw', 'seventy-five', 'each\\n\\nif', 'questionsrequests', 'feel', 'free', 'add', 'steam\\n\\nsteam', 'httpsteamcommunitycomprofiles76561198184805093\\nmyrep', 'httpswwwredditcomrrocketleagueexchangecomments52k9lemeta_successful_tradereputation_threadda9vudm\\nmyrep2', 'httpswwwredditcomrrocketleagueexchangecomments5yzsc4meta_successful_tradereputation_thread_20dhcjm85\\nmyrep3', 'httpswwwredditcomrrocketleagueexchangecomments6ywnpimeta_successful_tradereputation_post_30dmsopus\\n\\n'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'seamstress,', 'do', 'mostly', 'costuming', 'and', 'reenactment', 'garb.', 'Being', 'an', 'empath', 'in', 'this', 'line', 'of', 'work', 'is', 'incredibly', 'helpful', 'because', 'most', 'people', 'know', 'what', 'they', 'want,', 'but', 'have', 'zero', 'idea', 'how', 'to', 'put', 'words', 'to', 'the', 'vision', 'in', 'their', 'heads.', 'Just', 'through', 'conversation', '', '(in', 'person,', 'of', 'course!),', 'I', 'can', 'get', 'a', 'solid', 'feel', 'for', 'my', \"clients'\", 'costuming', 'desires.', 'They', 'seem', 'to', 'think', \"it's\", 'magic', 'that', 'I', '\"just', 'get', 'them.\"', ':)', '', 'Oddly', 'enough,', \"I'm\", 'certain', 'my', 'empath', 'abilities', 'also', 'translate', 'into', 'an', 'ability', 'to', 'make', 'things', 'that', 'fit', 'perfectly', 'the', 'first', 'time', 'they', 'try', 'them', 'on.\\n\\n(Just', 'a', 'note:', 'I', 'did', '*not*', 'go', 'to', 'school', 'for', 'fashion', 'design', 'or', 'tailoring.', \"I'm\", 'self-taught.)\\n\\nAnyway,', 'the', 'point', 'here', 'is', 'that', 'your', 'coworker', \"doesn't\", 'necessarily', 'need', 'to', 'go', 'into', 'typical', '\"healer\"', 'type', 'work.', 'If', 'she', 'enjoys', 'working', 'with', 'her', 'hands', 'and', 'being', 'creative,', 'there', 'are', 'jobs', 'that', 'involve', 'working', 'with', 'the', 'public', '*and*', 'creative', 'activities.', 'Interior', 'design,', 'home', 'remodeling,', 'custom', 'sewing,', 'commissioned', 'art...', 'heck,', 'even', 'designer', 'cakes.', 'And,', 'if', 'her', 'parents', 'are', 'dead-set', 'on', 'her', 'going', 'to', 'college,', 'these', 'are', 'all', 'things', 'that', 'she', 'can', 'get', 'an', 'Associate', 'or', 'Bachelor', 'degree', 'in.\\n\\n(Re:', 'student', 'debt...', 'tell', 'her', 'she', 'can', 'start', 'her', 'higher', 'education', 'at', 'a', 'community', 'college', 'for', 'her', '\"basic\"', 'courses', 'and', 'then', 'transfer', 'to', 'a', '4-year', 'school.', 'Saves', 'a', 'ton', 'of', 'money!', 'I', 'wish', 'it', 'had', 'been', 'an', 'option', '25', 'years', 'ago!)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'seamstress', 'most', 'costum', 'reenact', 'garb', 'empa', 'lin', 'work', 'incred', 'help', 'peopl', 'know', 'want', 'zero', 'ide', 'put', 'word', 'vis', 'head', 'convers', 'person', 'cours', 'get', 'solid', 'feel', 'cli', 'costum', 'desir', 'seem', 'think', 'mag', 'get', 'od', 'enough', 'im', 'certain', 'empa', 'abl', 'also', 'transl', 'abl', 'mak', 'thing', 'fit', 'perfect', 'first', 'tim', 'try', 'on\\n\\njust', 'not', 'go', 'school', 'fash', 'design', 'tail', 'im', 'selftaught\\n\\nanyway', 'point', 'cowork', 'doesnt', 'necess', 'nee', 'go', 'typ', 'heal', 'typ', 'work', 'enjoy', 'work', 'hand', 'cre', 'job', 'involv', 'work', 'publ', 'cre', 'act', 'intery', 'design', 'hom', 'remodel', 'custom', 'sew', 'commit', 'art', 'heck', 'ev', 'design', 'cak', 'par', 'deadset', 'going', 'colleg', 'thing', 'get', 'assocy', 'bachel', 'degr', 'in\\n\\nre', 'stud', 'debt', 'tel', 'start', 'high', 'educ', 'commun', 'colleg', 'bas', 'cours', 'transf', '4year', 'school', 'sav', 'ton', 'money', 'wish', 'opt', 'twenty-five', 'year', 'ago'], ['im', 'seamstress', 'mostly', 'costume', 'reenactment', 'garb', 'empath', 'line', 'work', 'incredibly', 'helpful', 'people', 'know', 'want', 'zero', 'idea', 'put', 'word', 'vision', 'head', 'conversation', 'person', 'course', 'get', 'solid', 'feel', 'clients', 'costume', 'desire', 'seem', 'think', 'magic', 'get', 'oddly', 'enough', 'im', 'certain', 'empath', 'abilities', 'also', 'translate', 'ability', 'make', 'things', 'fit', 'perfectly', 'first', 'time', 'try', 'on\\n\\njust', 'note', 'go', 'school', 'fashion', 'design', 'tailor', 'im', 'selftaught\\n\\nanyway', 'point', 'coworker', 'doesnt', 'necessarily', 'need', 'go', 'typical', 'healer', 'type', 'work', 'enjoy', 'work', 'hand', 'creative', 'job', 'involve', 'work', 'public', 'creative', 'activities', 'interior', 'design', 'home', 'remodel', 'custom', 'sew', 'commission', 'art', 'heck', 'even', 'designer', 'cake', 'parent', 'deadset', 'go', 'college', 'things', 'get', 'associate', 'bachelor', 'degree', 'in\\n\\nre', 'student', 'debt', 'tell', 'start', 'higher', 'education', 'community', 'college', 'basic', 'course', 'transfer', '4year', 'school', 'save', 'ton', 'money', 'wish', 'option', 'twenty-five', 'years', 'ago'])\n",
      "original document: \n",
      "['Oh', 'sorry', 'eh', 'but', 'it', 'does', 'get', 'a', 'tad', 'annoying', 'ya', 'know?', '', 'I', 'know', \"we're\", 'buds', 'and', 'all', 'eh', 'but', 'sometimes', \"it's\", 'really', 'nice', 'when', 'someone', 'asks', 'if', \"we're\", 'Canadian', 'eh!', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sorry', 'eh', 'get', 'tad', 'annoy', 'ya', 'know', 'know', 'bud', 'eh', 'sometim', 'real', 'nic', 'someon', 'ask', 'canad', 'eh'], ['oh', 'sorry', 'eh', 'get', 'tad', 'annoy', 'ya', 'know', 'know', 'bud', 'eh', 'sometimes', 'really', 'nice', 'someone', 'ask', 'canadian', 'eh'])\n",
      "original document: \n",
      "['Indigenous', 'rights,', 'unless', 'the', 'indigenous', 'people', 'are', 'white.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indig', 'right', 'unless', 'indig', 'peopl', 'whit'], ['indigenous', 'right', 'unless', 'indigenous', 'people', 'white'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'anyone', 'can', 'rock', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'anyon', 'rock'], ['dont', 'think', 'anyone', 'rock'])\n",
      "original document: \n",
      "['143414041|', '&gt;', 'None', 'Anonymous', '(ID:', 'pkH2Dye0)\\n\\n&gt;&gt;143412250', '(OP)\\nWhat', 'OP', 'was', 'really', 'asking', 'is:\\n\\nHow', 'far', 'up', 'my', 'asshole', 'do', 'you', 'think', 'the', 'government', 'can', 'reach?\\n\\n&gt;2008:', 'Deep\\n&gt;2012:', 'Really', 'Deep\\n&gt;2016:', 'Deepest', '(lungs', 'and', 'heart', 'deep)\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand and forty-on', 'gt', 'non', 'anonym', 'id', 'pkh2dye0\\n\\ngtgt143412250', 'op\\nwhat', 'op', 'real', 'ask', 'is\\n\\nhow', 'far', 'asshol', 'think', 'govern', 'reach\\n\\ngt2008', 'deep\\ngt2012', 'real', 'deep\\ngt2016', 'deepest', 'lung', 'heart', 'deep\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand and forty-one', 'gt', 'none', 'anonymous', 'id', 'pkh2dye0\\n\\ngtgt143412250', 'op\\nwhat', 'op', 'really', 'ask', 'is\\n\\nhow', 'far', 'asshole', 'think', 'government', 'reach\\n\\ngt2008', 'deep\\ngt2012', 'really', 'deep\\ngt2016', 'deepest', 'lungs', 'heart', 'deep\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['How', 'about', 'not', 'have', 'the', 'car', 'instantly', 'spin', 'out', 'whenmy', 'tires', 'touch', 'the', 'apron', 'at', 'like', 'New', 'Hampshire.', 'Its', 'cool', 'for', 'the', '1.5', 'and', 'platw', 'tracks', 'but', \"it's\", 'game', 'breaking', 'for', 'me', 'at', 'the', 'smaller,', 'flatter', 'tracks', 'when', \"I'm\", 'trying', 'to', 'gain', 'spots']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['car', 'inst', 'spin', 'whenmy', 'tir', 'touch', 'apron', 'lik', 'new', 'hampshir', 'cool', 'fifteen', 'platw', 'track', 'gam', 'break', 'smal', 'flat', 'track', 'im', 'try', 'gain', 'spot'], ['car', 'instantly', 'spin', 'whenmy', 'tire', 'touch', 'apron', 'like', 'new', 'hampshire', 'cool', 'fifteen', 'platw', 'track', 'game', 'break', 'smaller', 'flatter', 'track', 'im', 'try', 'gain', 'spot'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'gonna', 'give', 'out', 'details', 'of', 'where', 'this', 'happened,', 'but', 'a', 'friend', 'of', 'mine', 'would', 'go', 'see', 'a', 'movie', 'and', 'he', 'would', 'leave', 'the', 'outside', 'door', 'cracked', 'ever', 'so', 'slightly.', 'He', 'smoked,', 'so', 'he', 'was', 'always', 'taking', 'a', 'step', 'out,', 'propping', 'it', 'with', 'something,', 'smoking,', 'and', 'returning.', 'He', 'was', 'caught', 'once', 'and', 'they', 'asked', 'that', 'he', 'go', 'out', 'the', 'front', 'to', 'smoke', 'but', 'he', 'kept', 'doing', \"it.\\n\\nHe's\", 'in', 'jail,', 'but', \"that's\", 'unrelated', 'to', 'my', 'point.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'gonn', 'giv', 'detail', 'hap', 'friend', 'min', 'would', 'go', 'see', 'movy', 'would', 'leav', 'outsid', 'door', 'crack', 'ev', 'slight', 'smok', 'alway', 'tak', 'step', 'prop', 'someth', 'smok', 'return', 'caught', 'ask', 'go', 'front', 'smok', 'kept', 'it\\n\\nhes', 'jail', 'that', 'unrel', 'point'], ['im', 'gonna', 'give', 'detail', 'happen', 'friend', 'mine', 'would', 'go', 'see', 'movie', 'would', 'leave', 'outside', 'door', 'crack', 'ever', 'slightly', 'smoke', 'always', 'take', 'step', 'prop', 'something', 'smoke', 'return', 'catch', 'ask', 'go', 'front', 'smoke', 'keep', 'it\\n\\nhes', 'jail', 'thats', 'unrelated', 'point'])\n",
      "original document: \n",
      "['Baaaahahahaha.', '', '$0-$100k....really', 'nailing', 'down', 'that', 'sugar', 'daddy', 'status.', '', '5\\'2\"', 'and', 'an', 'angry', 'internet', 'troll...someone', 'will', 'be', 'lucky.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['baaaahahahah', '0100kreally', 'nail', 'sug', 'daddy', 'stat', 'fifty-two', 'angry', 'internet', 'trollsomeon', 'lucky'], ['baaaahahahaha', '0100kreally', 'nail', 'sugar', 'daddy', 'status', 'fifty-two', 'angry', 'internet', 'trollsomeone', 'lucky'])\n",
      "original document: \n",
      "['Many', 'of', 'the', 'local', 'tool', 'libraries', 'have', 'presses', 'available', 'for', 'loan.', 'I', 'like', 'the', 'one', 'at', 'the', 'Phinney', 'Neighborhood', 'Assoc.', 'best,', \"it's\", 'got', 'an', 'electric', 'scratter', 'and', 'two', 'baskets.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'loc', 'tool', 'libr', 'press', 'avail', 'loan', 'lik', 'on', 'phinney', 'neighb', 'assoc', 'best', 'got', 'elect', 'scratter', 'two', 'basket'], ['many', 'local', 'tool', 'libraries', 'press', 'available', 'loan', 'like', 'one', 'phinney', 'neighborhood', 'assoc', 'best', 'get', 'electric', 'scratter', 'two', 'baskets'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['*a', 'new', 'cracked', 'game']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'crack', 'gam'], ['new', 'crack', 'game'])\n",
      "original document: \n",
      "['Before', 'and', 'after', 'of', 'your', 'ass', 'from', 'being', 'spanked', 'would', 'be', 'hot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ass', 'spank', 'would', 'hot'], ['ass', 'spank', 'would', 'hot'])\n",
      "original document: \n",
      "['Y', 'siendo', 'monotributista.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['siendo', 'monotributist'], ['siendo', 'monotributista'])\n",
      "original document: \n",
      "['how', 'much', 'was', 'your', 'guitar?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'guit'], ['much', 'guitar'])\n",
      "original document: \n",
      "['cant', 'get', 'off', 'the', 'plane...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'get', 'plan'], ['cant', 'get', 'plane'])\n",
      "original document: \n",
      "['Its', 'the', '6th', 'pillar', 'or', 'Islam.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['6th', 'pill', 'islam'], ['6th', 'pillar', 'islam'])\n",
      "original document: \n",
      "['S!!!!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['The', 'demand', 'for', 'the', '70', 'mats', 'is', 'still', 'there', 'because', 'of', 'how', 'good', 'they', 'are', 'for', 'spiritbonding.', 'Molybdenum', 'Ore', 'is', 'one', 'of', 'those', 'items.\\n\\nEven', 'some', 'of', 'the', 'lower', 'SB', 'stuff', 'like', 'Kopranickel', 'Sand', 'is', 'good', 'because', 'people', 'leveling', 'their', 'crafters', 'need', 'so', 'much', 'of', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['demand', 'seventy', 'mat', 'stil', 'good', 'spiritbond', 'molybden', 'or', 'on', 'items\\n\\neven', 'low', 'sb', 'stuff', 'lik', 'kopranickel', 'sand', 'good', 'peopl', 'level', 'craft', 'nee', 'much'], ['demand', 'seventy', 'mat', 'still', 'good', 'spiritbonding', 'molybdenum', 'ore', 'one', 'items\\n\\neven', 'lower', 'sb', 'stuff', 'like', 'kopranickel', 'sand', 'good', 'people', 'level', 'crafters', 'need', 'much'])\n",
      "original document: \n",
      "['Here', 'is', 'the', 'post', 'for', 'archival', 'purposes:', '', '\\n\\n**Author**:', '', '_uacdeepfield_', '\\n\\n', '**Content**:', '', '\\n\\n', '&gt;Why', 'can', 'we', 'see', 'this?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'arch', 'purpos', '\\n\\nauthor', '_uacdeepfield_', '\\n\\n', 'cont', '\\n\\n', 'gtwhy', 'see'], ['post', 'archival', 'purpose', '\\n\\nauthor', '_uacdeepfield_', '\\n\\n', 'content', '\\n\\n', 'gtwhy', 'see'])\n",
      "original document: \n",
      "['2)', 'Mechanics', 'is', 'basically', 'reaction', 'time', 'similar', 'to', 'gun', 'skill.', '', 'All', 'that', 'you', 'listed', 'is', 'relevant', 'with', 'one', 'additional', 'important', 'note.', '', 'In', 'Dota', 'you', 'acquire', 'two', 'resources:', 'gold', 'and', 'experience.', '', 'You', 'get', 'these', 'resources', 'in', 'three', 'ways,', 'killing', 'creeps,', 'killing', 'enemy', 'heroes,', 'and', 'killing', 'towers.', '', 'In', 'the', 'early', 'game', 'killing', 'creeps', 'is', 'the', 'most', 'important', 'way', 'to', 'get', 'these', 'resources.', 'The', 'mid', 'lane', 'is', 'as', 'close', 'at', 'it', 'gets', 'to', 'a', 'fair', 'fight', '(its', 'a', 'neutral', 'lane', 'not', 'closer', 'to', 'either', 'teams', 'base', 'and', 'historically', 'was', 'played', 'as', 'a', '1v1', 'contest)', 'so', 'there', 'is', 'a', 'lot', 'of', 'contesting', 'over', 'last', 'hits', '(also', 'known', 'as', 'CS).', '', 'So', 'people', 'that', 'find', 'success', 'in', 'the', 'mid', 'lane', 'tend', 'to', 'have', 'really', 'good', 'reactions', 'to', 'win', 'these', '1v1s.', '', 'Also', 'a', 'lot', 'of', 'mid', 'heroes', 'are', '\"flashy\"', 'with', 'fast', 'movement', 'based', 'abilities.', '', 'So', 'you', 'usually', 'want', 'your', 'most', 'mechanically', 'talented', 'player', 'located', 'there.\\n\\n3)', 'The', 'offlaner', 'is', 'probably', 'the', 'most', 'vulnerable', '(its', 'sometimes', 'called', 'the', 'hard', 'lane)', 'since', 'you', 'are', 'usually', 'playing', '1', 'vs', '2', 'or', '3', 'heroes.', '', 'But', 'mid', 'and', 'the', 'mid', 'tower', 'are', 'often', 'a', 'very', 'important', 'early', 'advantage', 'and', 'the', 'focus', 'of', 'a', 'lot', 'of', 'action', 'early', 'on', 'in', 'the', 'game.', '', 'Since', 'you', 'can', 'teleport', 'to', 'towers', 'and', 'they', 'give', 'vision', 'the', 'mid', 'tower', '(with', 'its', 'neutral', 'central', 'location)', 'is', 'a', 'very', 'important', 'objective', 'to', 'open', 'up', 'space', 'on', 'the', 'opponents', 'side', 'of', 'the', 'map.\\n\\nRecently', 'the', '\"new', 'meta\"', 'has', 'been', 'the', 'position', '4', 'player', 'sitting', 'mid,', 'turning', 'mid', 'into', 'a', 'duel', 'lane', 'vs', 'its', 'historical', '1v1.', '', 'The', 'nice', 'thing', 'about', 'Dota', 'is', 'that', 'there', \"aren't\", 'really', 'fixed', 'positions', 'for', 'how', 'you', 'set', 'up', 'your', 'lanes.', '', 'Every', 'team', 'will', 'send', '1', 'hero', 'to', 'each', 'lane,', 'but', 'what', 'you', 'do', 'with', 'your', 'two', 'supports', 'depends', 'on', 'draft', 'and', 'strategy.', '', 'You', 'can', 'go', 'aggro', 'and', 'send', 'them', 'all', 'to', 'the', 'offlane,', 'you', 'can', 'have', '1', 'in', 'the', 'safe', 'lane', 'and', '1', 'roam,', 'etc.', '', '\\n\\n4)', 'Depends', 'on', 'the', 'team,', 'a', 'lot', 'of', 'times', 'coaches', 'in', 'Dota', 'are', 'picked', 'up', 'right', 'before', 'a', 'LAN', 'since', 'they', 'are', 'often', 'pro', 'players', 'that', \"didn't\", 'qualify', 'for', 'the', 'LAN.', '', 'For', 'instance', 'Pajkatt', 'was', 'the', 'coach', 'of', 'DC', 'during', 'their', 'TI6', '2nd', 'place', 'run', \"(Misery's\", 'team)', 'since', 'his', 'team', \"wasn't\", 'able', 'to', 'qualify.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'mech', 'bas', 'react', 'tim', 'simil', 'gun', 'skil', 'list', 'relev', 'on', 'addit', 'import', 'not', 'dot', 'acquir', 'two', 'resourc', 'gold', 'expery', 'get', 'resourc', 'three', 'way', 'kil', 'creep', 'kil', 'enemy', 'hero', 'kil', 'tow', 'ear', 'gam', 'kil', 'creep', 'import', 'way', 'get', 'resourc', 'mid', 'lan', 'clos', 'get', 'fair', 'fight', 'neut', 'lan', 'clos', 'eith', 'team', 'bas', 'hist', 'play', '1v1', 'contest', 'lot', 'contest', 'last', 'hit', 'also', 'known', 'cs', 'peopl', 'find', 'success', 'mid', 'lan', 'tend', 'real', 'good', 'react', 'win', '1v1s', 'also', 'lot', 'mid', 'hero', 'flashy', 'fast', 'mov', 'bas', 'abl', 'us', 'want', 'mech', 'tal', 'play', 'loc', 'there\\n\\n3', 'offl', 'prob', 'vuln', 'sometim', 'cal', 'hard', 'lan', 'sint', 'us', 'play', 'on', 'vs', 'two', 'three', 'hero', 'mid', 'mid', 'tow', 'oft', 'import', 'ear', 'adv', 'foc', 'lot', 'act', 'ear', 'gam', 'sint', 'teleport', 'tow', 'giv', 'vis', 'mid', 'tow', 'neut', 'cent', 'loc', 'import', 'object', 'op', 'spac', 'oppon', 'sid', 'map\\n\\nrecently', 'new', 'met', 'posit', 'four', 'play', 'sit', 'mid', 'turn', 'mid', 'duel', 'lan', 'vs', 'hist', '1v1', 'nic', 'thing', 'dot', 'ar', 'real', 'fix', 'posit', 'set', 'lan', 'every', 'team', 'send', 'on', 'hero', 'lan', 'two', 'support', 'depend', 'draft', 'strategy', 'go', 'aggro', 'send', 'offl', 'on', 'saf', 'lan', 'on', 'roam', 'etc', '\\n\\n4', 'depend', 'team', 'lot', 'tim', 'coach', 'dot', 'pick', 'right', 'lan', 'sint', 'oft', 'pro', 'play', 'didnt', 'qual', 'lan', 'inst', 'pajkat', 'coach', 'dc', 'ti6', '2nd', 'plac', 'run', 'misery', 'team', 'sint', 'team', 'wasnt', 'abl', 'qual'], ['two', 'mechanics', 'basically', 'reaction', 'time', 'similar', 'gun', 'skill', 'list', 'relevant', 'one', 'additional', 'important', 'note', 'dota', 'acquire', 'two', 'resources', 'gold', 'experience', 'get', 'resources', 'three', 'ways', 'kill', 'creep', 'kill', 'enemy', 'heroes', 'kill', 'tower', 'early', 'game', 'kill', 'creep', 'important', 'way', 'get', 'resources', 'mid', 'lane', 'close', 'get', 'fair', 'fight', 'neutral', 'lane', 'closer', 'either', 'team', 'base', 'historically', 'play', '1v1', 'contest', 'lot', 'contest', 'last', 'hit', 'also', 'know', 'cs', 'people', 'find', 'success', 'mid', 'lane', 'tend', 'really', 'good', 'reactions', 'win', '1v1s', 'also', 'lot', 'mid', 'heroes', 'flashy', 'fast', 'movement', 'base', 'abilities', 'usually', 'want', 'mechanically', 'talented', 'player', 'locate', 'there\\n\\n3', 'offlaner', 'probably', 'vulnerable', 'sometimes', 'call', 'hard', 'lane', 'since', 'usually', 'play', 'one', 'vs', 'two', 'three', 'heroes', 'mid', 'mid', 'tower', 'often', 'important', 'early', 'advantage', 'focus', 'lot', 'action', 'early', 'game', 'since', 'teleport', 'tower', 'give', 'vision', 'mid', 'tower', 'neutral', 'central', 'location', 'important', 'objective', 'open', 'space', 'opponents', 'side', 'map\\n\\nrecently', 'new', 'meta', 'position', 'four', 'player', 'sit', 'mid', 'turn', 'mid', 'duel', 'lane', 'vs', 'historical', '1v1', 'nice', 'thing', 'dota', 'arent', 'really', 'fix', 'position', 'set', 'lanes', 'every', 'team', 'send', 'one', 'hero', 'lane', 'two', 'support', 'depend', 'draft', 'strategy', 'go', 'aggro', 'send', 'offlane', 'one', 'safe', 'lane', 'one', 'roam', 'etc', '\\n\\n4', 'depend', 'team', 'lot', 'time', 'coach', 'dota', 'pick', 'right', 'lan', 'since', 'often', 'pro', 'players', 'didnt', 'qualify', 'lan', 'instance', 'pajkatt', 'coach', 'dc', 'ti6', '2nd', 'place', 'run', 'miserys', 'team', 'since', 'team', 'wasnt', 'able', 'qualify'])\n",
      "original document: \n",
      "['i', 'think', 'this', 'is', 'a', 'solid', 'coin\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'solid', 'coin\\n'], ['think', 'solid', 'coin\\n'])\n",
      "original document: \n",
      "['I', 'saw', 'Wazzu', 'go', 'victory', 'from', 'the', 'gun', 'last', 'night,', 'we', 'may', 'need', 'to', 'look', 'into', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'wazzu', 'go', 'vict', 'gun', 'last', 'night', 'may', 'nee', 'look'], ['saw', 'wazzu', 'go', 'victory', 'gun', 'last', 'night', 'may', 'need', 'look'])\n",
      "original document: \n",
      "['Now', 'we', 'just', 'need', 'to', 'find', 'someone', 'who', \"isn't\", 'banned', 'from', 'that', 'subreddit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'find', 'someon', 'isnt', 'ban', 'subreddit'], ['need', 'find', 'someone', 'isnt', 'ban', 'subreddit'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['No', 'offense', 'need', 'meant', 'but', 'that', 'sounds', 'like', \"they're\", 'really', 'not', 'interested,', 'hence', 'the', 'one', 'word', 'answers', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['offens', 'nee', 'meant', 'sound', 'lik', 'theyr', 'real', 'interest', 'hent', 'on', 'word', 'answ'], ['offense', 'need', 'mean', 'sound', 'like', 'theyre', 'really', 'interest', 'hence', 'one', 'word', 'answer'])\n",
      "original document: \n",
      "['王力宏']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['You', 'mean', 'your', 'monthly', 'is', '70.', 'Anyways', '110', 'at', 'union', 'station', 'is', 'probably', 'the', 'cheapest.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'month', 'seventy', 'anyway', 'one hundred and ten', 'un', 'stat', 'prob', 'cheapest'], ['mean', 'monthly', 'seventy', 'anyways', 'one hundred and ten', 'union', 'station', 'probably', 'cheapest'])\n",
      "original document: \n",
      "['⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆', '❗', '**NOTICE**', '❗', '⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆\\n\\n*', 'Please', 'consult', 'the', '[List', 'of', 'Scammers](https://www.reddit.com/r/SmiteTrades/wiki/scammer_list)', 'before', 'trading.\\n\\n*', 'If', 'someone', 'contacts', 'you', 'via', 'PMs', 'and', 'they', 'have', 'not', 'responded', 'to', 'your', 'thread', 'be', 'aware', 'that', 'the', 'user', 'may', 'already', 'be', 'banned', 'from', 'the', 'subreddit', 'for', 'being', 'a', 'scammer.\\n\\n*', 'Consider', 'taking', 'screenshots', 'of', 'all', 'parts', 'of', 'your', 'trade', 'to', 'use', 'as', 'evidence', 'if', 'the', 'need', 'arises', 'and', '[report](https://www.reddit.com/message/compose?to=%2Fr%2FSmiteTrades)', 'anything', 'untoward', 'to', 'the', 'moderators.\\n\\n*', '*Never*', 'share', 'your', 'Smite', 'or', 'Reddit', 'password', 'with', 'anyone.\\n\\n----------------------------------------------------\\n^^^^^^^Post', '^^^^^^^made', '^^^^^^^by', '^^^^^^^/u/Ronnenn\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/SmiteTrades)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['not', '\\n\\n', 'pleas', 'consult', 'list', 'scammershttpswwwredditcomrsmitetradeswikiscammer_list', 'trading\\n\\n', 'someon', 'contact', 'via', 'pms', 'respond', 'thread', 'aw', 'us', 'may', 'already', 'ban', 'subreddit', 'scammer\\n\\n', 'consid', 'tak', 'screenshots', 'part', 'trad', 'us', 'evid', 'nee', 'ar', 'reporthttpswwwredditcommessagecomposeto2fr2fsmitetrades', 'anyth', 'untoward', 'moderators\\n\\n', 'nev', 'shar', 'smit', 'reddit', 'password', 'anyone\\n\\n\\npost', 'mad', 'uronnenn\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorsmitetrad', 'quest', 'concern'], ['notice', '\\n\\n', 'please', 'consult', 'list', 'scammershttpswwwredditcomrsmitetradeswikiscammer_list', 'trading\\n\\n', 'someone', 'contact', 'via', 'pms', 'respond', 'thread', 'aware', 'user', 'may', 'already', 'ban', 'subreddit', 'scammer\\n\\n', 'consider', 'take', 'screenshots', 'part', 'trade', 'use', 'evidence', 'need', 'arise', 'reporthttpswwwredditcommessagecomposeto2fr2fsmitetrades', 'anything', 'untoward', 'moderators\\n\\n', 'never', 'share', 'smite', 'reddit', 'password', 'anyone\\n\\n\\npost', 'make', 'uronnenn\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorsmitetrades', 'question', 'concern'])\n",
      "original document: \n",
      "['E']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['e'], ['e'])\n",
      "original document: \n",
      "['hahahahaha', 'oh', 'yeah?', '\\nhttps://en.wikipedia.org/wiki/Demographic_history_of_New_York_City\\nas', 'recently', 'as', '1960', 'more', 'than', '90%', 'nohhispanic', 'white.', \"you're\", 'being', 'overrun']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahahahah', 'oh', 'yeah', '\\nhttpsenwikipediaorgwikidemographic_history_of_new_york_city\\nas', 'rec', 'one thousand, nine hundred and sixty', 'nin', 'nohhisp', 'whit', 'yo', 'overrun'], ['hahahahaha', 'oh', 'yeah', '\\nhttpsenwikipediaorgwikidemographic_history_of_new_york_city\\nas', 'recently', 'one thousand, nine hundred and sixty', 'ninety', 'nohhispanic', 'white', 'youre', 'overrun'])\n",
      "original document: \n",
      "['I', 'picked', 'up', '\"three', 'green,', 'everything', 'else', 'is', 'optional,\"', 'look', 'at', 'the', 'indicators', 'and', 'touch', 'the', 'gear', 'handle', 'when', 'turning', 'final', 'and', 'again', 'on', 'short', 'final.', '', 'If', \"I'm\", 'in', 'a', 'fixed', 'gear', 'airplane', \"I'll\", 'still', 'say', 'it', 'and', 'look', 'out', 'the', 'window', 'at', 'the', 'gear.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pick', 'three', 'green', 'everyth', 'els', 'opt', 'look', 'ind', 'touch', 'gear', 'handl', 'turn', 'fin', 'short', 'fin', 'im', 'fix', 'gear', 'airpl', 'il', 'stil', 'say', 'look', 'window', 'gear'], ['pick', 'three', 'green', 'everything', 'else', 'optional', 'look', 'indicators', 'touch', 'gear', 'handle', 'turn', 'final', 'short', 'final', 'im', 'fix', 'gear', 'airplane', 'ill', 'still', 'say', 'look', 'window', 'gear'])\n",
      "original document: \n",
      "['I', 'dont', 'know', 'about', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know'], ['dont', 'know'])\n",
      "original document: \n",
      "['[+MarkoWolf](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnona1q/):\\n\\nHow', 'much', 'have', 'you', 'spent,', 'out', 'of', 'pocket,', 'from', 'start', 'to', 'finish?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['markowolfhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnona1q\\n\\nhow', 'much', 'spent', 'pocket', 'start', 'fin'], ['markowolfhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnona1q\\n\\nhow', 'much', 'spend', 'pocket', 'start', 'finish'])\n",
      "original document: \n",
      "['Not', 'upvoted', 'for', 'ending', 'a', 'sentence', 'with', 'a', 'preposition.', 'FeelsBadMan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['upvot', 'end', 'sent', 'preposit', 'feelsbadm'], ['upvoted', 'end', 'sentence', 'preposition', 'feelsbadman'])\n",
      "original document: \n",
      "['Nice', 'place', 'for', 'an', 'e-sport']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'plac', 'esport'], ['nice', 'place', 'esport'])\n",
      "original document: \n",
      "['Also,', 'internet', 'tubes', 'should', 'be', 'shut', 'down', 'to', 'defeat', 'ISIS', 'recruitment.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'internet', 'tub', 'shut', 'def', 'is', 'recruit'], ['also', 'internet', 'tube', 'shut', 'defeat', 'isis', 'recruitment'])\n",
      "original document: \n",
      "['Not', 'really.', 'The', 'only', 'difference', 'is', 'the', 'battery', 'gets', 'slightly', 'worn', 'down', 'over', 'time.', 'If', 'you', 'have', 'a', 'used', 'and', 'new', 'watch', 'side', 'by', 'side,', 'you', 'couldn’t', 'tell', 'the', 'difference.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'diff', 'battery', 'get', 'slight', 'worn', 'tim', 'us', 'new', 'watch', 'sid', 'sid', 'couldnt', 'tel', 'diff'], ['really', 'difference', 'battery', 'get', 'slightly', 'wear', 'time', 'use', 'new', 'watch', 'side', 'side', 'couldnt', 'tell', 'difference'])\n",
      "original document: \n",
      "['good', 'deal,', \"it's\", 'gone.', 'there', 'was', 'probably', 'only', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'deal', 'gon', 'prob', 'on'], ['good', 'deal', 'go', 'probably', 'one'])\n",
      "original document: \n",
      "['Would', 'you', 'be', 'willing', 'to', 'share', 'your', 'decklist?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'wil', 'shar', 'deckl'], ['would', 'will', 'share', 'decklist'])\n",
      "original document: \n",
      "[\"It's\", 'only', 'a', 'coincidence', 'that', 'all', 'these', 'things', 'exist', 'in', 'this', 'one', 'person,', 'and', 'similar', 'weirdness', 'is', 'in', 'nearly', 'all', 'SJWs.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['coincid', 'thing', 'ex', 'on', 'person', 'simil', 'weird', 'near', 'sjws'], ['coincidence', 'things', 'exist', 'one', 'person', 'similar', 'weirdness', 'nearly', 'sjws'])\n",
      "original document: \n",
      "['He', 'has', 'said', 'a', 'lot', 'of', 'completely', 'hateful', 'shit,', 'the', 'people', 'who', 'support', 'him', 'are', 'more', 'concerned', 'about', '\"the', 'blacks\"', 'kneeling.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'lot', 'complet', 'hat', 'shit', 'peopl', 'support', 'concern', 'black', 'kneel'], ['say', 'lot', 'completely', 'hateful', 'shit', 'people', 'support', 'concern', 'black', 'kneel'])\n",
      "original document: \n",
      "['Leaked', 'halloween', 'skin']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['leak', 'halloween', 'skin'], ['leak', 'halloween', 'skin'])\n",
      "original document: \n",
      "['\\n\\n\\n[***Harry', 'Potter', 'and', 'the', 'Chained', 'Souls***](http://www\\\\.fanfiction\\\\.net/s/3490702/1/)', 'by', '[*Theowyn', 'of', 'HPG*](https://www\\\\.fanfiction\\\\.net/u/633246/Theowyn\\\\-of\\\\-HPG)\\n\\n\\n\\n\\n\\n\\n&gt;', 'Harry', 'must', 'discover', 'how', 'Voldemort', 'cheated', 'death\\\\.', 'He', 'faces', 'Death', 'Eaters,', 'shadowy', 'Ministry', 'officials', '&amp;', 'suspicions', 'that', 'threaten', 'to', 'tear', 'his', 'own', 'allies', 'apart\\\\.', 'But', 'the', 'answers', 'lie', 'in', 'the', 'mind', 'where', 'victory', 'can', 'only', 'be', 'won', 'by', 'freeing', 'the', 'chained', 'souls\\\\.', 'SEQUEL\\n\\n^(*Site*:', '[fanfiction.net][140280130791632:site]', '**|**', '*Category*:', 'Harry', 'Potter', '**|**', '*Rated*:', 'Fiction', '', 'T', '**|**', '*Chapters*:', '31', '**|**', '*Words*:', '231,287', '**|**', '*Reviews*:', '760', '**|**', '*Favs*:', '929', '**|**', '*Follows*:', '221', '**|**', '*Updated*:', '7/12/2007', '**|**', '*Published*:', '4/16/2007', '**|**', '*Status*:', 'Complete', '**|**', '*id*:', '3490702', '', '**|**', '*Language*:', 'English', '**|**', '*Genre*:', 'Angst', '**|**', '*Characters*:', 'Harry', 'P.,', 'Severus', 'S.', '**|**', '*Download*:', '[EPUB][140280130791632:epub]', 'or', '[MOBI][140280130791632:mobi])\\n[140280130791632:site]:', 'http://www.fanfiction.net/\\n[140280130791632:epub]:', 'http://www.ff2ebook.com/old/ffn-bot/index.php?id=3490702&amp;source=ff&amp;filetype=epub\\n[140280130791632:mobi]:', 'http://www.ff2ebook.com/old/ffn-bot/index.php?id=3490702&amp;source=ff&amp;filetype=mobi\\n\\n\\n---\\n\\n\\n\\n\\n[***Harry', 'Potter', 'and', 'the', 'Enemy', 'Within***](http://www\\\\.fanfiction\\\\.net/s/3417954/1/)', 'by', '[*Theowyn', 'of', 'HPG*](https://www\\\\.fanfiction\\\\.net/u/633246/Theowyn\\\\-of\\\\-HPG)\\n\\n\\n\\n\\n\\n\\n&gt;', 'In', 'his', 'sixth', 'year', 'at', 'Hogwarts,', \"Harry's\", 'mental', 'link', 'to', 'Voldemort', 'is', 'stronger', 'than', 'ever\\\\.', '', 'Can', 'Snape', 'teach', 'him', 'to', 'control', 'the', 'nightmarish', 'visions?', 'And', 'is', 'their', 'connection', 'the', 'key', 'to', 'ending', \"Voldemort's\", 'reign?\\n\\n^(*Site*:', '[fanfiction.net][140280130674248:site]', '**|**', '*Category*:', 'Harry', 'Potter', '**|**', '*Rated*:', 'Fiction', '', 'T', '**|**', '*Chapters*:', '19', '**|**', '*Words*:', '173,220', '**|**', '*Reviews*:', '442', '**|**', '*Favs*:', '1,197', '**|**', '*Follows*:', '238', '**|**', '*Updated*:', '3/27/2007', '**|**', '*Published*:', '2/28/2007', '**|**', '*Status*:', 'Complete', '**|**', '*id*:', '3417954', '', '**|**', '*Language*:', 'English', '**|**', '*Genre*:', 'Angst', '**|**', '*Characters*:', 'Harry', 'P.,', 'Severus', 'S.', '**|**', '*Download*:', '[EPUB][140280130674248:epub]', 'or', '[MOBI][140280130674248:mobi])\\n[140280130674248:site]:', 'http://www.fanfiction.net/\\n[140280130674248:epub]:', 'http://www.ff2ebook.com/old/ffn-bot/index.php?id=3417954&amp;source=ff&amp;filetype=epub\\n[140280130674248:mobi]:', 'http://www.ff2ebook.com/old/ffn-bot/index.php?id=3417954&amp;source=ff&amp;filetype=mobi\\n\\n\\n---\\n\\n**FanfictionBot**^(1.4.0)', '**|**', '\\\\[[Usage][1]\\\\]', '|', '\\\\[[Changelog][2]\\\\]', '|', '\\\\[[Issues][3]\\\\]', '|', '\\\\[[GitHub][4]\\\\]', '|', '\\\\[[Contact][5]\\\\]\\n[1]:', 'https://github.com/tusing/reddit-ffn-bot/wiki/Usage', '', '', '', '', '', '', '\"How', 'to', 'use', 'the', 'bot\"\\n[2]:', 'https://github.com/tusing/reddit-ffn-bot/wiki/Changelog', '', '', '\"What', 'changed', 'until', 'now\"\\n[3]:', 'https://github.com/tusing/reddit-ffn-bot/issues/', '', '', '', '', '', '', '', '', '', '\"Bugs?', 'Suggestions?', 'Enter', 'them', 'here!\"\\n[4]:', 'https://github.com/tusing/reddit-ffn-bot/', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '\"Fork', 'me', 'on', 'GitHub\"\\n[5]:', 'https://www.reddit.com/message/compose?to=tusing', '', '', '', '', '', '', '', '', '', '\"The', 'maintainer\"\\n\\n^^^^^^^^^^^^^^^^^ffnbot!ignore\\n\\n^(*New', 'in', 'this', 'version:', 'Slim', 'recommendations', 'using*', 'ffnbot!slim!', '*Thread', 'recommendations', 'using*', 'linksub(thread_id)^)!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\n\\n\\nharry', 'pot', 'chain', 'soulshttpwwwfanfictionnets34907021', 'theowyn', 'hpghttpswwwfanfictionnetu633246theowynofhpg\\n\\n\\n\\n\\n\\n\\ngt', 'harry', 'must', 'discov', 'voldemort', 'che', 'dea', 'fac', 'dea', 'eat', 'shadowy', 'min', 'off', 'amp', 'susp', 'threaten', 'tear', 'al', 'apart', 'answ', 'lie', 'mind', 'vict', 'fre', 'chain', 'soul', 'sequel\\n\\nsite', 'fanfictionnet140280130791632site', 'categ', 'harry', 'pot', 'rat', 'fict', 'chapt', 'thirty-one', 'word', 'two hundred and thirty-one thousand, two hundred and eighty-seven', 'review', 'seven hundred and sixty', 'fav', 'nine hundred and twenty-nin', 'follow', 'two hundred and twenty-one', 'upd', 'seven million, one hundred and twenty-two thousand and sev', 'publ', 'four million, one hundred and sixty-two thousand and seven', 'stat', 'complet', 'id', 'three million, four hundred and ninety thousand, seven hundred and two', 'langu', 'engl', 'genr', 'angst', 'charact', 'harry', 'p', 'sever', 'download', 'epub140280130791632epub', 'mobi140280130791632mobi\\n140280130791632site', 'httpwwwfanfictionnet\\n140280130791632epub', 'httpwwwff2ebookcomoldffnbotindexphpid3490702ampsourceffampfiletypeepub\\n140280130791632mobi', 'httpwwwff2ebookcomoldffnbotindexphpid3490702ampsourceffampfiletypemobi\\n\\n\\n\\n\\n\\n\\n\\nharry', 'pot', 'enemy', 'withinhttpwwwfanfictionnets34179541', 'theowyn', 'hpghttpswwwfanfictionnetu633246theowynofhpg\\n\\n\\n\\n\\n\\n\\ngt', 'six', 'year', 'hogwart', 'harry', 'ment', 'link', 'voldemort', 'stronger', 'ev', 'snap', 'teach', 'control', 'nightm', 'vis', 'connect', 'key', 'end', 'voldemort', 'reign\\n\\nsite', 'fanfictionnet140280130674248site', 'categ', 'harry', 'pot', 'rat', 'fict', 'chapt', 'nineteen', 'word', 'one hundred and seventy-three thousand, two hundred and twenty', 'review', 'four hundred and forty-two', 'fav', 'one thousand, one hundred and ninety-seven', 'follow', 'two hundred and thirty-eight', 'upd', 'three million, two hundred and seventy-two thousand and seven', 'publ', 'two million, two hundred and eighty-two thousand and seven', 'stat', 'complet', 'id', 'three million, four hundred and seventeen thousand, nine hundred and fifty-four', 'langu', 'engl', 'genr', 'angst', 'charact', 'harry', 'p', 'sever', 'download', 'epub140280130674248epub', 'mobi140280130674248mobi\\n140280130674248site', 'httpwwwfanfictionnet\\n140280130674248epub', 'httpwwwff2ebookcomoldffnbotindexphpid3417954ampsourceffampfiletypeepub\\n140280130674248mobi', 'httpwwwff2ebookcomoldffnbotindexphpid3417954ampsourceffampfiletypemobi\\n\\n\\n\\n\\nfanfictionbot140', 'usage1', 'changelog2', 'issues3', 'github4', 'contact5\\n1', 'httpsgithubcomtusingredditffnbotwikiusage', 'us', 'bot\\n2', 'httpsgithubcomtusingredditffnbotwikichangelog', 'chang', 'now\\n3', 'httpsgithubcomtusingredditffnbotissues', 'bug', 'suggest', 'ent', 'here\\n4', 'httpsgithubcomtusingredditffnbot', 'fork', 'github\\n5', 'httpswwwredditcommessagecomposetotusing', 'maintainer\\n\\nffnbotignore\\n\\nnew', 'vert', 'slim', 'recommend', 'us', 'ffnbotslim', 'thread', 'recommend', 'us', 'linksubthread_id'], ['\\n\\n\\nharry', 'potter', 'chain', 'soulshttpwwwfanfictionnets34907021', 'theowyn', 'hpghttpswwwfanfictionnetu633246theowynofhpg\\n\\n\\n\\n\\n\\n\\ngt', 'harry', 'must', 'discover', 'voldemort', 'cheat', 'death', 'face', 'death', 'eaters', 'shadowy', 'ministry', 'officials', 'amp', 'suspicions', 'threaten', 'tear', 'ally', 'apart', 'answer', 'lie', 'mind', 'victory', 'free', 'chain', 'souls', 'sequel\\n\\nsite', 'fanfictionnet140280130791632site', 'category', 'harry', 'potter', 'rat', 'fiction', 'chapters', 'thirty-one', 'word', 'two hundred and thirty-one thousand, two hundred and eighty-seven', 'review', 'seven hundred and sixty', 'favs', 'nine hundred and twenty-nine', 'follow', 'two hundred and twenty-one', 'update', 'seven million, one hundred and twenty-two thousand and seven', 'publish', 'four million, one hundred and sixty-two thousand and seven', 'status', 'complete', 'id', 'three million, four hundred and ninety thousand, seven hundred and two', 'language', 'english', 'genre', 'angst', 'character', 'harry', 'p', 'severus', 'download', 'epub140280130791632epub', 'mobi140280130791632mobi\\n140280130791632site', 'httpwwwfanfictionnet\\n140280130791632epub', 'httpwwwff2ebookcomoldffnbotindexphpid3490702ampsourceffampfiletypeepub\\n140280130791632mobi', 'httpwwwff2ebookcomoldffnbotindexphpid3490702ampsourceffampfiletypemobi\\n\\n\\n\\n\\n\\n\\n\\nharry', 'potter', 'enemy', 'withinhttpwwwfanfictionnets34179541', 'theowyn', 'hpghttpswwwfanfictionnetu633246theowynofhpg\\n\\n\\n\\n\\n\\n\\ngt', 'sixth', 'year', 'hogwarts', 'harry', 'mental', 'link', 'voldemort', 'stronger', 'ever', 'snape', 'teach', 'control', 'nightmarish', 'visions', 'connection', 'key', 'end', 'voldemorts', 'reign\\n\\nsite', 'fanfictionnet140280130674248site', 'category', 'harry', 'potter', 'rat', 'fiction', 'chapters', 'nineteen', 'word', 'one hundred and seventy-three thousand, two hundred and twenty', 'review', 'four hundred and forty-two', 'favs', 'one thousand, one hundred and ninety-seven', 'follow', 'two hundred and thirty-eight', 'update', 'three million, two hundred and seventy-two thousand and seven', 'publish', 'two million, two hundred and eighty-two thousand and seven', 'status', 'complete', 'id', 'three million, four hundred and seventeen thousand, nine hundred and fifty-four', 'language', 'english', 'genre', 'angst', 'character', 'harry', 'p', 'severus', 'download', 'epub140280130674248epub', 'mobi140280130674248mobi\\n140280130674248site', 'httpwwwfanfictionnet\\n140280130674248epub', 'httpwwwff2ebookcomoldffnbotindexphpid3417954ampsourceffampfiletypeepub\\n140280130674248mobi', 'httpwwwff2ebookcomoldffnbotindexphpid3417954ampsourceffampfiletypemobi\\n\\n\\n\\n\\nfanfictionbot140', 'usage1', 'changelog2', 'issues3', 'github4', 'contact5\\n1', 'httpsgithubcomtusingredditffnbotwikiusage', 'use', 'bot\\n2', 'httpsgithubcomtusingredditffnbotwikichangelog', 'change', 'now\\n3', 'httpsgithubcomtusingredditffnbotissues', 'bug', 'suggestions', 'enter', 'here\\n4', 'httpsgithubcomtusingredditffnbot', 'fork', 'github\\n5', 'httpswwwredditcommessagecomposetotusing', 'maintainer\\n\\nffnbotignore\\n\\nnew', 'version', 'slim', 'recommendations', 'use', 'ffnbotslim', 'thread', 'recommendations', 'use', 'linksubthread_id'])\n",
      "original document: \n",
      "[\"He's\", 'pretty', 'good', 'on', 'the', 'Late', 'Show.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'pretty', 'good', 'lat', 'show'], ['hes', 'pretty', 'good', 'late', 'show'])\n",
      "original document: \n",
      "['Floor', '117', 'is', 'to', 'test', 'how', 'many', 'confusion', 'immunities', 'you', 'have..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flo', 'one hundred and seventeen', 'test', 'many', 'confus', 'immun'], ['floor', 'one hundred and seventeen', 'test', 'many', 'confusion', 'immunities'])\n",
      "original document: \n",
      "['I', 'was', 'just', 'telling', 'him', 'that', 'it', 'exists.', \"I'm\", 'not', 'telling', 'him', 'to', 'post', 'it', 'here.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tel', 'ex', 'im', 'tel', 'post'], ['tell', 'exist', 'im', 'tell', 'post'])\n",
      "original document: \n",
      "['Anything', '[here](https://www.reddit.com/r/indiegameswap/comments/73fbxx/h_keys_from_the_telltales_and_capcom_hb_paypal_w/)', 'for', 'infested', 'planet', '+', 'DLC?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyth', 'herehttpswwwredditcomrindiegameswapcomments73fbxxh_keys_from_the_telltales_and_capcom_hb_paypal_w', 'infest', 'planet', 'dlc'], ['anything', 'herehttpswwwredditcomrindiegameswapcomments73fbxxh_keys_from_the_telltales_and_capcom_hb_paypal_w', 'infest', 'planet', 'dlc'])\n",
      "original document: \n",
      "['That’s', 'true,', 'but', 'unfortunately', 'State', 'interactions', 'are', 'only', 'about', 'half/maybe', 'slightly', 'under', 'half', 'of', 'what', 'the', 'president', 'does.', 'A', 'president', 'guided', 'the', 'nation', 'in', 'making', 'new', 'laws', 'even', 'if', 'he', 'can’t', 'create', 'him.', 'But', 'what', 'he', 'focuses', 'on', 'is', 'sully', 'the', 'focus', 'of', 'Congress']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'tru', 'unfortun', 'stat', 'interact', 'halfmayb', 'slight', 'half', 'presid', 'presid', 'guid', 'nat', 'mak', 'new', 'law', 'ev', 'cant', 'cre', 'focus', 'sul', 'foc', 'congress'], ['thats', 'true', 'unfortunately', 'state', 'interactions', 'halfmaybe', 'slightly', 'half', 'president', 'president', 'guide', 'nation', 'make', 'new', 'laws', 'even', 'cant', 'create', 'focus', 'sully', 'focus', 'congress'])\n",
      "original document: \n",
      "['so', 'much', 'throws\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'throws\\n'], ['much', 'throws\\n'])\n",
      "original document: \n",
      "['The', 'Master', 'Race', 'is', 'forgiving,', 'but', 'only', 'if', 'you', 'submit', 'to', 'true', 'glory.', '\\n\\nThose', 'who', 'fail', 'to', 'submit', 'will', 'be', 'branded', 'as', 'peasants,', 'and', 'appropriately', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mast', 'rac', 'forg', 'submit', 'tru', 'glory', '\\n\\nthose', 'fail', 'submit', 'brand', 'peas', 'appropry'], ['master', 'race', 'forgive', 'submit', 'true', 'glory', '\\n\\nthose', 'fail', 'submit', 'brand', 'peasants', 'appropriately'])\n",
      "original document: \n",
      "[\"He's\", 'Mason', 'Verger', 'in', 'the', 'Hannibal', 'TV', 'show?\\n\\nOr', 'is', 'this', 'the', 'one', 'with', 'Anthony', 'Hopkins?', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'mason', 'verg', 'hannib', 'tv', 'show\\n\\nor', 'on', 'anthony', 'hopkin'], ['hes', 'mason', 'verger', 'hannibal', 'tv', 'show\\n\\nor', 'one', 'anthony', 'hopkins'])\n",
      "original document: \n",
      "[\"I'm\", 'currently', 'recovering', 'from', 'a', 'limb', 'salvage', 'at', 'the', '[Center', 'for', 'the', 'Intrepid](https://www.bamc.amedd.army.mil/departments/rehabilitation-medicine/cfi/)', '--', \"it's\", 'the', 'US', \"Army's\", 'biggest', 'research,', 'development,', 'and', 'rehabilitation', 'center', 'for', 'injured', 'veterans,', 'serving', 'all', 'branches', 'of', 'the', 'military.', '\\n\\nI', \"don't\", 'know', 'if', 'they', 'have', 'a', 'specific', 'point', 'of', 'contact', 'set', 'up,', 'but', 'I', 'do', 'know', 'that', 'they', 'occasionally', 'post', 'surveys', 'and', 'stuff', 'for', 'people', 'who', 'are', 'going', 'through', 'rehabilitation', 'there.', 'It', 'might', 'be', 'worth', 'it', 'to', 'find', 'a', 'contact', 'link', 'on', 'the', 'site', 'I', 'linked', 'to,', 'and', 'seeing', 'if', 'anyone', 'there', 'can', 'help', 'answer', 'questions,', 'or', 'is', 'willing', 'to', 'spread', 'your', 'survey', 'around.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'cur', 'recov', 'limb', 'salv', 'cent', 'intrepidhttpswwwbamcameddarmymildepartmentsrehabilitationmedicinecf', 'us', 'army', 'biggest', 'research', 'develop', 'rehabilit', 'cent', 'ind', 'vet', 'serv', 'branch', 'milit', '\\n\\ni', 'dont', 'know', 'spec', 'point', 'contact', 'set', 'know', 'occas', 'post', 'survey', 'stuff', 'peopl', 'going', 'rehabilit', 'might', 'wor', 'find', 'contact', 'link', 'sit', 'link', 'see', 'anyon', 'help', 'answ', 'quest', 'wil', 'spread', 'survey', 'around'], ['im', 'currently', 'recover', 'limb', 'salvage', 'center', 'intrepidhttpswwwbamcameddarmymildepartmentsrehabilitationmedicinecfi', 'us', 'armys', 'biggest', 'research', 'development', 'rehabilitation', 'center', 'injure', 'veterans', 'serve', 'branch', 'military', '\\n\\ni', 'dont', 'know', 'specific', 'point', 'contact', 'set', 'know', 'occasionally', 'post', 'survey', 'stuff', 'people', 'go', 'rehabilitation', 'might', 'worth', 'find', 'contact', 'link', 'site', 'link', 'see', 'anyone', 'help', 'answer', 'question', 'will', 'spread', 'survey', 'around'])\n",
      "original document: \n",
      "['No', 'you', 'definitely', 'missed', 'the', 'point.', 'The', 'frontflip', 'is', 'not', 'dumb,', 'doing', 'it', 'on', 'wet', 'ground', 'is.', '\\n\\nI', \"don't\", 'know', 'how', 'many', 'times', 'you', 'need', 'this', 'to', 'be', 'said']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'miss', 'point', 'frontflip', 'dumb', 'wet', 'ground', '\\n\\ni', 'dont', 'know', 'many', 'tim', 'nee', 'said'], ['definitely', 'miss', 'point', 'frontflip', 'dumb', 'wet', 'grind', '\\n\\ni', 'dont', 'know', 'many', 'time', 'need', 'say'])\n",
      "original document: \n",
      "[\"It's\", 'preseason', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['preseason'], ['preseason'])\n",
      "original document: \n",
      "['Rule', '#1', 'ladies', 'and', 'gentlemen.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rul', 'on', 'lady', 'gentlem'], ['rule', 'one', 'ladies', 'gentlemen'])\n",
      "original document: \n",
      "['Ian', 'and', 'then', 'Tycho', 'and', 'Dogmeat', 'in', 'Junktown.', 'Katja', 'in', 'the', 'boneyard', 'too', 'but', \"you're\", 'usually', 'pretty', 'tough', 'by', 'the', 'time', 'you', 'get', 'her.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ian', 'tycho', 'dogm', 'junktown', 'katj', 'boneyard', 'yo', 'us', 'pretty', 'tough', 'tim', 'get'], ['ian', 'tycho', 'dogmeat', 'junktown', 'katja', 'boneyard', 'youre', 'usually', 'pretty', 'tough', 'time', 'get'])\n",
      "original document: \n",
      "['Literally', 'all', 'of', 'that', 'is', 'one', 'big', 'load', 'of', 'crap.', 'Can', 'you', 'dispel', 'my', 'suspicion', 'that', \"you're\", 'just', 'trolling?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'on', 'big', 'load', 'crap', 'dispel', 'susp', 'yo', 'trol'], ['literally', 'one', 'big', 'load', 'crap', 'dispel', 'suspicion', 'youre', 'troll'])\n",
      "original document: \n",
      "['I', 'mean', 'this', 'with', 'NO', 'offense', 'to', 'McCann,', \"he's\", 'a', 'far', 'different', 'player', 'with', 'a', 'different', 'skillset...but', 'this', 'game', 'is', 'showing', 'how', 'important', 'Yamil', 'is', 'to', 'our', 'team', 'as', 'well.', '\\n\\nHis', 'pace', 'and', 'workrate', 'are', 'essential', 'for', 'Josef', 'and', 'Tito', 'to', 'get', 'going', 'up', 'top.', 'I', 'hope', 'we', 'sign', 'him', 'permanently!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'offens', 'mccann', 'hes', 'far', 'diff', 'play', 'diff', 'skillsetbut', 'gam', 'show', 'import', 'yamil', 'team', 'wel', '\\n\\nhis', 'pac', 'workr', 'ess', 'josef', 'tito', 'get', 'going', 'top', 'hop', 'sign', 'perm'], ['mean', 'offense', 'mccann', 'hes', 'far', 'different', 'player', 'different', 'skillsetbut', 'game', 'show', 'important', 'yamil', 'team', 'well', '\\n\\nhis', 'pace', 'workrate', 'essential', 'josef', 'tito', 'get', 'go', 'top', 'hope', 'sign', 'permanently'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'm\", 'actually', 'an', 'associate', 'myself!', 'I', 'might', 'be', 'the', 'bossman', 'in', 'a', 'few', 'years.', 'If', 'I', 'ever', 'pay', 'off', 'my', 'student', 'loans.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'act', 'assocy', 'might', 'bossm', 'year', 'ev', 'pay', 'stud', 'loan'], ['im', 'actually', 'associate', 'might', 'bossman', 'years', 'ever', 'pay', 'student', 'loan'])\n",
      "original document: \n",
      "['This', 'is', 'a', 'bit', 'inconsistent', 'with', 'Thanos', 'Imperative', 'considering', 'Robbie', 'made', 'it', 'safe', 'to', 'Hala', 'and', 'was', 'in', \"Rich's\", 'and', \"Star-Lord's\", 'funeral.', '', '\\n\\nThis', 'preview', 'implies', 'he', 'disappeared', 'sometime', 'before', 'they', 'launched', 'their', 'assault', 'against', 'the', 'Cancerverse', 'creatures', 'and', 'that', 'Rich', 'taking', 'the', 'full', 'Nova', 'force', 'partially', 'fucked', 'him', 'up.\\n\\nBut', 'seeing', 'how', 'much', 'of', 'a', 'minor', 'player', 'Robbie', 'was', 'in', 'Thanos', 'Imperative', 'I', 'really', \"don't\", 'mind', 'that', 'much', 'if', 'all', 'of', 'that', 'was', 'changed,', 'especially', 'considering', 'where', 'I', 'think', 'Duggan', 'is', 'going', 'with', 'Robbie.\\n\\nThat', 'said', 'Duggan', 'mentioned', 'this', 'issue', 'is', 'going', 'to', 'be', 'heartbreaking,', 'and', 'I', 'think', \"we're\", 'also', 'suppose', 'to', 'find', 'out', 'who', 'Talonar', 'is', 'in', 'this', 'issue', '(if', 'not', 'we', 'will', 'know', 'before', 'October', 'ends),', 'coupled', 'with', 'the', 'fact', 'that', 'ot', 'might', 'be', 'the', 'reunion', 'between', 'Rich', 'and', 'Peter,', 'yeah', \"I'm\", 'preparing', 'for', 'a', 'tearjerker', '(and', 'going', 'by', 'his', 'Deadpool', 'run', 'Duggan', 'can', 'do', 'depressing', 'scenes).\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bit', 'inconsist', 'thano', 'imp', 'consid', 'robby', 'mad', 'saf', 'hal', 'rich', 'starlord', 'fun', '\\n\\nthis', 'preview', 'imply', 'disappear', 'sometim', 'launch', 'assault', 'cancervers', 'cre', 'rich', 'tak', 'ful', 'nov', 'forc', 'part', 'fuck', 'up\\n\\nbut', 'see', 'much', 'min', 'play', 'robby', 'thano', 'imp', 'real', 'dont', 'mind', 'much', 'chang', 'espec', 'consid', 'think', 'dug', 'going', 'robbie\\n\\nthat', 'said', 'dug', 'ment', 'issu', 'going', 'heartbreak', 'think', 'also', 'suppos', 'find', 'talon', 'issu', 'know', 'octob', 'end', 'coupl', 'fact', 'ot', 'might', 'reun', 'rich', 'pet', 'yeah', 'im', 'prep', 'tearjerk', 'going', 'deadpool', 'run', 'dug', 'depress', 'scenes\\n\\n'], ['bite', 'inconsistent', 'thanos', 'imperative', 'consider', 'robbie', 'make', 'safe', 'hala', 'richs', 'starlords', 'funeral', '\\n\\nthis', 'preview', 'imply', 'disappear', 'sometime', 'launch', 'assault', 'cancerverse', 'creatures', 'rich', 'take', 'full', 'nova', 'force', 'partially', 'fuck', 'up\\n\\nbut', 'see', 'much', 'minor', 'player', 'robbie', 'thanos', 'imperative', 'really', 'dont', 'mind', 'much', 'change', 'especially', 'consider', 'think', 'duggan', 'go', 'robbie\\n\\nthat', 'say', 'duggan', 'mention', 'issue', 'go', 'heartbreaking', 'think', 'also', 'suppose', 'find', 'talonar', 'issue', 'know', 'october', 'end', 'couple', 'fact', 'ot', 'might', 'reunion', 'rich', 'peter', 'yeah', 'im', 'prepare', 'tearjerker', 'go', 'deadpool', 'run', 'duggan', 'depress', 'scenes\\n\\n'])\n",
      "original document: \n",
      "['(2,', '5)?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'fiv'], ['two', 'five'])\n",
      "original document: \n",
      "['More', 'please!', 'Where', 'is', 'part', '3,', '4', '&amp;', '5!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'part', 'three', 'four', 'amp', 'fiv'], ['please', 'part', 'three', 'four', 'amp', 'five'])\n",
      "original document: \n",
      "['Best', 'championship', 'ever', 'man.', '', 'Congrats', 'either', 'way.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'champ', 'ev', 'man', 'congr', 'eith', 'way'], ['best', 'championship', 'ever', 'man', 'congrats', 'either', 'way'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoptrj/):\\n\\nSo', 'far', 'about', 'less', 'than', '1,000.', 'The', 'editor', 'was', 'paid', 'for', 'by', 'my', 'parents', 'who', 'are', 'awaiting', 'me', 'to', 'pay', 'them', 'back', 'every', 'penny.', 'So', 'including', 'what', 'I', 'owe', 'my', 'parents', \"I'll\", 'be', 'at', 'about', '5,000.', 'I', \"could've\", 'paid', 'them', 'back', 'before,', 'but', 'they', \"don't\", 'want', 'me', 'to', 'deplete', 'my', 'savings.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoptrj\\n\\nso', 'far', 'less', 'one thousand', 'edit', 'paid', 'par', 'await', 'pay', 'back', 'every', 'penny', 'includ', 'ow', 'par', 'il', 'five thousand', 'couldv', 'paid', 'back', 'dont', 'want', 'deplet', 'sav'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoptrj\\n\\nso', 'far', 'less', 'one thousand', 'editor', 'pay', 'parent', 'await', 'pay', 'back', 'every', 'penny', 'include', 'owe', 'parent', 'ill', 'five thousand', 'couldve', 'pay', 'back', 'dont', 'want', 'deplete', 'save'])\n",
      "original document: \n",
      "['Awww.', '🙂', 'People', 'united', 'by', 'hate', 'are', 'sweet.', '😙', 'Just', 'like', 'when', 'George', 'Lincoln', 'Rockwell', 'met', 'the', 'Nation', 'of', 'Islam,', 'this', 'moment', 'warms', 'my', 'heart', '❤']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['awww', 'peopl', 'unit', 'hat', 'sweet', 'lik', 'georg', 'lincoln', 'rockwel', 'met', 'nat', 'islam', 'mom', 'warm', 'heart'], ['awww', 'people', 'unite', 'hate', 'sweet', 'like', 'george', 'lincoln', 'rockwell', 'meet', 'nation', 'islam', 'moment', 'warm', 'heart'])\n",
      "original document: \n",
      "['apex?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['apex'], ['apex'])\n",
      "original document: \n",
      "['It’s', 'honestly', 'my', 'favourite', 'part', 'of', 'the', 'song']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'favourit', 'part', 'song'], ['honestly', 'favourite', 'part', 'song'])\n",
      "original document: \n",
      "['Have', 'you', 'spoken', 'to', 'a', 'social', 'worker?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spok', 'soc', 'work'], ['speak', 'social', 'worker'])\n",
      "original document: \n",
      "['I', 'doubt', 'whether', 'it', 'would', 'be', 'powerful', 'enough', 'to', 'emulate', 'anything', 'after', 'the', 'SNES', '/', 'Sega', 'Megadrive', 'era.', 'Raspberry', 'pi', 'can', 'struggle', 'slightly', 'at', 'times', 'with', 'N64', 'and', 'most', 'playstation', 'roms', 'will', 'be', 'too', 'big', 'to', 'fit.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doubt', 'wheth', 'would', 'pow', 'enough', 'em', 'anyth', 'sne', 'seg', 'megadr', 'er', 'raspberry', 'pi', 'struggle', 'slight', 'tim', 'n64', 'playst', 'rom', 'big', 'fit'], ['doubt', 'whether', 'would', 'powerful', 'enough', 'emulate', 'anything', 'snes', 'sega', 'megadrive', 'era', 'raspberry', 'pi', 'struggle', 'slightly', 'time', 'n64', 'playstation', 'roms', 'big', 'fit'])\n",
      "original document: \n",
      "['thanks', 'for', 'the', 'help', 'everyone', '6mb', 'speeds', 'not', 'as', 'bad', 'as', 'the', 'tmobile', 'subreddit', 'had', 'me', 'thinking', 'and', 'for', 'such', 'a', 'low', 'price!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'help', 'everyon', '6mb', 'spee', 'bad', 'tmobl', 'subreddit', 'think', 'low', 'pric'], ['thank', 'help', 'everyone', '6mb', 'speed', 'bad', 'tmobile', 'subreddit', 'think', 'low', 'price'])\n",
      "original document: \n",
      "['Well', 'Kelly', 'and', 'Ed', '*did*', 'live', 'in', 'NYC', 'during', 'the', 'time', 'period', 'of', 'the', 'show...', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'kel', 'ed', 'liv', 'nyc', 'tim', 'period', 'show'], ['well', 'kelly', 'ed', 'live', 'nyc', 'time', 'period', 'show'])\n",
      "original document: \n",
      "['Probably', 'Rea', 'Lemon', 'bc', 'the', 'capitalizes', 'L', 'indicates', 'the', 'start', 'of', 'a', 'new', 'word.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'rea', 'lemon', 'bc', 'capit', 'l', 'ind', 'start', 'new', 'word'], ['probably', 'rea', 'lemon', 'bc', 'capitalize', 'l', 'indicate', 'start', 'new', 'word'])\n",
      "original document: \n",
      "['Restricted', '18+']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['restrict', 'eighteen'], ['restrict', 'eighteen'])\n",
      "original document: \n",
      "['Hit', 'me', 'up', 'if', 'you', 'see', 'me,', 'khakis', 'and', 'baseball', 'style', 'young', 'the', 'giant', 'tee.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hit', 'see', 'khak', 'basebal', 'styl', 'young', 'giant', 'tee'], ['hit', 'see', 'khakis', 'baseball', 'style', 'young', 'giant', 'tee'])\n",
      "original document: \n",
      "['Aside', 'from', 'the', 'included', 'ones:', 'Turtles', 'in', 'Time,', 'DKC', '2,', 'and', 'one', 'of', 'the', 'Super', 'Star', 'Wars', 'series', 'off', 'the', 'top', 'of', 'my', 'head...\\n\\nEdit:', 'and', 'since', 'there', 'are', 'a', 'few', 'FX', 'chip', 'games', 'with', 'the', 'Star', 'Fox', 'series', 'and', \"Yoshi's\", 'Island,', 'why', 'not', 'Stunt', 'Race', 'FX?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['asid', 'includ', 'on', 'turtl', 'tim', 'dkc', 'two', 'on', 'sup', 'star', 'war', 'sery', 'top', 'head\\n\\nedit', 'sint', 'fx', 'chip', 'gam', 'star', 'fox', 'sery', 'yosh', 'island', 'stunt', 'rac', 'fx'], ['aside', 'include', 'ones', 'turtle', 'time', 'dkc', 'two', 'one', 'super', 'star', 'war', 'series', 'top', 'head\\n\\nedit', 'since', 'fx', 'chip', 'game', 'star', 'fox', 'series', 'yoshis', 'island', 'stunt', 'race', 'fx'])\n",
      "original document: \n",
      "['This', 'was', 'from', 'WAY', 'back', 'in', 'the', 'day.', 'During', 'Christmas', 'time', 'our', 'mall', 'had', 'a', 'Santa/christmas', 'elf', 'that', 'was', 'making', 'balloons.', 'Boys', 'were', 'getting', 'swords,', 'girls', 'flowers.', 'Well', '7', 'year', 'old', 'me', 'wanted', 'a', 'sword', 'instead', 'and', 'he', \"wouldn't\", 'make', 'me', 'one.', 'So', 'i', 'started', 'playing', 'with', 'it', 'took', 'the', 'flower', 'apart', 'and', 'made', 'a', 'crappy', 'sword.', 'My', 'parents', 'got', 'me', 'a', 'balloon', 'book/kit', 'for', 'Christmas', 'and', 'the', 'rest', 'was', 'history.', 'Also', 'I', 'ride', 'a', 'unicycle.', 'Added', 'am', 'album', 'below\\n\\nhttps://imgur.com/gallery/21Djb']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'back', 'day', 'christmas', 'tim', 'mal', 'santachristma', 'elf', 'mak', 'balloon', 'boy', 'get', 'sword', 'girl', 'flow', 'wel', 'sev', 'year', 'old', 'want', 'sword', 'instead', 'wouldnt', 'mak', 'on', 'start', 'play', 'took', 'flow', 'apart', 'mad', 'crappy', 'sword', 'par', 'got', 'balloon', 'bookkit', 'christmas', 'rest', 'hist', 'also', 'rid', 'unicyc', 'ad', 'alb', 'below\\n\\nhttpsimgurcomgallery21djb'], ['way', 'back', 'day', 'christmas', 'time', 'mall', 'santachristmas', 'elf', 'make', 'balloon', 'boys', 'get', 'swords', 'girls', 'flower', 'well', 'seven', 'year', 'old', 'want', 'sword', 'instead', 'wouldnt', 'make', 'one', 'start', 'play', 'take', 'flower', 'apart', 'make', 'crappy', 'sword', 'parent', 'get', 'balloon', 'bookkit', 'christmas', 'rest', 'history', 'also', 'ride', 'unicycle', 'add', 'album', 'below\\n\\nhttpsimgurcomgallery21djb'])\n",
      "original document: \n",
      "['The', 'only', 'games', \"I'm\", 'really', 'worried', 'about', 'there', 'are', 'Arizona', 'and', 'Minnesota.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gam', 'im', 'real', 'worry', 'arizon', 'minnesot'], ['game', 'im', 'really', 'worry', 'arizona', 'minnesota'])\n",
      "original document: \n",
      "['Depends.', 'They', 'are', 'probably', 'still', 'paying', 'for', 'Miles.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'prob', 'stil', 'pay', 'mil'], ['depend', 'probably', 'still', 'pay', 'miles'])\n",
      "original document: \n",
      "['No', 'problems', 'bud.', 'Some', 'other', 'good', 'places', 'for', 'this', 'stuff', 'is', 'r/depression', 'and', 'to', 'some', 'extent', 'r/anxiety', \"\\nThere's\", 'a', 'couple', 'others', 'that', 'I', \"can't\", 'remember', 'the', 'name', 'of.', 'There', 'are', 'people', 'to', 'talk', 'to,', 'and', \"you're\", 'not', 'alone.', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'bud', 'good', 'plac', 'stuff', 'rdepress', 'ext', 'ranxy', '\\ntheres', 'coupl', 'oth', 'cant', 'rememb', 'nam', 'peopl', 'talk', 'yo', 'alon', 'lt3'], ['problems', 'bud', 'good', 'place', 'stuff', 'rdepression', 'extent', 'ranxiety', '\\ntheres', 'couple', 'others', 'cant', 'remember', 'name', 'people', 'talk', 'youre', 'alone', 'lt3'])\n",
      "original document: \n",
      "['agl', 'hits', 'between', '400', 'and', '500', 'on', 'a', 'double', 'vegeta', 'lead.', 'teq', 'only', 'hits', 'just', 'below', '300', 'but', \"that's\", 'on', 'a', 'godtenks/goku', 'lead.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agl', 'hit', 'four hundred', 'five hundred', 'doubl', 'veget', 'lead', 'teq', 'hit', 'three hundred', 'that', 'godtenksgoku', 'lead'], ['agl', 'hit', 'four hundred', 'five hundred', 'double', 'vegeta', 'lead', 'teq', 'hit', 'three hundred', 'thats', 'godtenksgoku', 'lead'])\n",
      "original document: \n",
      "['I', 'actually', 'think', 'that', 'I', 'could', 'probably', 'drive.', 'I', 'can', 'put', 'pressure', 'directly', 'on', 'it', 'but', 'any', 'sideways', 'pressure', 'makes', 'me', 'feel', 'like', 'passing', 'out.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'think', 'could', 'prob', 'driv', 'put', 'press', 'direct', 'sideway', 'press', 'mak', 'feel', 'lik', 'pass'], ['actually', 'think', 'could', 'probably', 'drive', 'put', 'pressure', 'directly', 'sideways', 'pressure', 'make', 'feel', 'like', 'pass'])\n",
      "original document: \n",
      "['And', \"she's\", 'flattered', 'because', 'OP', 'obviously', 'has', 'a', 'crush', 'on', 'her.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['she', 'flat', 'op', 'obvy', 'crush'], ['shes', 'flatter', 'op', 'obviously', 'crush'])\n",
      "original document: \n",
      "[\"He's\", 'reading', '2', 'pages', 'of', 'bulletpoints?', '', 'I', 'think', 'you', 'are', 'too', 'generous.', '', 'It', \"wouldn't\", 'surprise', 'me', 'if', \"he's\", 'being', 'briefed', 'in', 'the', 'form', 'of', 'daily', 'puppet', 'shows.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'read', 'two', 'pag', 'bulletpoint', 'think', 'gen', 'wouldnt', 'surpr', 'hes', 'brief', 'form', 'dai', 'puppet', 'show'], ['hes', 'read', 'two', 'page', 'bulletpoints', 'think', 'generous', 'wouldnt', 'surprise', 'hes', 'brief', 'form', 'daily', 'puppet', 'show'])\n",
      "original document: \n",
      "['Bald', 'cunts', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bald', 'cunt'], ['bald', 'cunts'])\n",
      "original document: \n",
      "['&gt;', 'So', \"you're\", 'cool', 'with', 'harassment', 'as', 'long', 'as', \"it's\", 'by', 'your', 'tribe?', '\\n\\nWhat', 'specifically', 'in', \"Dragon's\", 'previous', 'proceeding', 'comment', 'lead', 'you', 'to', 'believe', 'that', 'was', 'their', 'stance?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'yo', 'cool', 'harass', 'long', 'trib', '\\n\\nwhat', 'spec', 'dragon', 'prevy', 'process', 'com', 'lead', 'believ', 'stant'], ['gt', 'youre', 'cool', 'harassment', 'long', 'tribe', '\\n\\nwhat', 'specifically', 'dragons', 'previous', 'proceed', 'comment', 'lead', 'believe', 'stance'])\n",
      "original document: \n",
      "['Same', 'here.', 'In', 'fact,', 'any', 'Murakami', 'novel', \"I've\", 'attempted', '-', 'or,', 'er', '-', 'dragged', 'myself', 'through', '(the', 'first', 'couple', 'of', 'chapters', 'of).', 'He', 'tries', 'too', 'hard', 'to', 'find', 'something', \"'deep'\", 'for', 'his', 'characters', 'to', 'say.', 'And', 'this', 'whole', \"'ooOOOooOH\", 'm', 'y', 's', 't', 'e', 'r', 'i', 'o', 'u', 's', '', '', 'm', 'a', 'g', \"ic'\", 'thing', 'really', 'falls', 'flat', 'because', 'of', 'it,', 'imo.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['fact', 'murakam', 'novel', 'iv', 'attempt', 'er', 'drag', 'first', 'coupl', 'chapt', 'tri', 'hard', 'find', 'someth', 'deep', 'charact', 'say', 'whol', 'ooooooooh', 'e', 'r', 'u', 'g', 'ic', 'thing', 'real', 'fal', 'flat', 'imo'], ['fact', 'murakami', 'novel', 'ive', 'attempt', 'er', 'drag', 'first', 'couple', 'chapters', 'try', 'hard', 'find', 'something', 'deep', 'character', 'say', 'whole', 'ooooooooh', 'e', 'r', 'u', 'g', 'ic', 'thing', 'really', 'fall', 'flat', 'imo'])\n",
      "original document: \n",
      "['hazzymancer,', '305', 'warlock.', '', 'bad', 'enough', 'dude']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hazzym', 'three hundred and five', 'warlock', 'bad', 'enough', 'dud'], ['hazzymancer', 'three hundred and five', 'warlock', 'bad', 'enough', 'dude'])\n",
      "original document: \n",
      "['Well', 'that', 'one', 'looks', 'interesting', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'on', 'look', 'interest'], ['well', 'one', 'look', 'interest'])\n",
      "original document: \n",
      "['Checking', 'out', 'those', 'lines.', 'Just', 'a', 'correction--Aimee', \"isn't\", 'from', 'Dixie.', \"She's\", 'from', 'a', 'fantasy', 'world.', 'Even', 'then,', \"she's\", 'has', 'a', 'Brooklyn', 'accent.', 'It', \"doesn't\", 'really', 'come', 'out', 'that', 'much,', 'cause', 'her', 'subtitles', 'normally', 'just', 'drop', '\"g\"s', 'from', '\"ing\"', 'and', 'turns', 'you/your', 'to', 'ya/yer.', 'She', 'also', 'uses', 'a', 'lot', 'of', 'Yiddish,', 'which', 'is', 'much', 'more', 'common', 'in', 'New', 'York.', '\\n\\nGranted,', \"I've\", 'never', 'been', 'to', 'New', 'York,', 'so', 'I', 'was', 'mostly', 'just', 'listening', 'to', 'and', 'miming', 'speakers', 'and', 'stereotypical', 'New', 'Yorker', 'characters.', 'If', \"anybody's\", 'got', 'some', 'Brooklynisms,', 'slam', 'them', 'in', 'my', 'inbox.', '\\n\\nI', 'guess', 'I', 'could', 'have', 'her', 'target', 'lines', 'back', 'saying', 'that', 'she', \"doesn't\", 'know', 'where', 'this', '\"America\"', 'city', 'is,', 'but', 'later', 'Marionette', 'calls', 'her', 'a', 'Westerner,', 'which', 'means', 'Aimee', \"can't\", 'have', 'already', 'corrected', 'her.', 'I', \"don't\", 'mind', 'finding', 'a', 'way', 'to', 'make', 'this', 'dialogue', 'work,', 'but', 'I', 'think', \"we'll\", 'need', 'to', 'collaborate', 'a', 'bit.', '\\n\\nThank', 'you', \"though--that's\", 'a', 'heavenly', 'host', 'of', 'lines.', 'I', 'need', 'to', 'learn', 'more', 'about', 'Miraculous', 'Ladybug', 'so', 'I', 'can', 'return', 'the', 'favor.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['check', 'lin', 'correctionaim', 'isnt', 'dixy', 'she', 'fantasy', 'world', 'ev', 'she', 'brooklyn', 'acc', 'doesnt', 'real', 'com', 'much', 'caus', 'subtitl', 'norm', 'drop', 'gs', 'ing', 'turn', 'youyo', 'yay', 'also', 'us', 'lot', 'yid', 'much', 'common', 'new', 'york', '\\n\\ngranted', 'iv', 'nev', 'new', 'york', 'most', 'list', 'mim', 'speak', 'stereotyp', 'new', 'york', 'charact', 'anybody', 'got', 'brooklyn', 'slam', 'inbox', '\\n\\ni', 'guess', 'could', 'target', 'lin', 'back', 'say', 'doesnt', 'know', 'americ', 'city', 'lat', 'marionet', 'cal', 'western', 'mean', 'aim', 'cant', 'already', 'correct', 'dont', 'mind', 'find', 'way', 'mak', 'dialog', 'work', 'think', 'wel', 'nee', 'collab', 'bit', '\\n\\nthank', 'thoughth', 'heav', 'host', 'lin', 'nee', 'learn', 'mirac', 'ladybug', 'return', 'fav'], ['check', 'line', 'correctionaimee', 'isnt', 'dixie', 'shes', 'fantasy', 'world', 'even', 'shes', 'brooklyn', 'accent', 'doesnt', 'really', 'come', 'much', 'cause', 'subtitle', 'normally', 'drop', 'gs', 'ing', 'turn', 'youyour', 'yayer', 'also', 'use', 'lot', 'yiddish', 'much', 'common', 'new', 'york', '\\n\\ngranted', 'ive', 'never', 'new', 'york', 'mostly', 'listen', 'mime', 'speakers', 'stereotypical', 'new', 'yorker', 'character', 'anybodys', 'get', 'brooklynisms', 'slam', 'inbox', '\\n\\ni', 'guess', 'could', 'target', 'line', 'back', 'say', 'doesnt', 'know', 'america', 'city', 'later', 'marionette', 'call', 'westerner', 'mean', 'aimee', 'cant', 'already', 'correct', 'dont', 'mind', 'find', 'way', 'make', 'dialogue', 'work', 'think', 'well', 'need', 'collaborate', 'bite', '\\n\\nthank', 'thoughthats', 'heavenly', 'host', 'line', 'need', 'learn', 'miraculous', 'ladybug', 'return', 'favor'])\n",
      "original document: \n",
      "['143417870|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', \"Cr/oee8K)\\n\\n&gt;2012\\n&gt;Obama\\n&gt;2016\\n&gt;Trump\\nI'll\", 'leave', 'to', 'your', 'imagination', 'what', 'happened', 'in', 'between.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and seventy', 'gt', 'unit', 'stat', 'anonym', 'id', 'croee8k\\n\\ngt2012\\ngtobama\\ngt2016\\ngttrump\\nill', 'leav', 'imagin', 'hap', 'between\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and seventy', 'gt', 'unite', 'state', 'anonymous', 'id', 'croee8k\\n\\ngt2012\\ngtobama\\ngt2016\\ngttrump\\nill', 'leave', 'imagination', 'happen', 'between\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Me', 'too', '--', 'sad', 'to', 'one', 'of', 'the', 'little', 'people.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'on', 'littl', 'peopl'], ['sad', 'one', 'little', 'people'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Thank', 'you', 'for', 'the', 'response.', 'It’s', 'a', 'warehouse', 'supervisor', 'for', 'a', 'certain', 'department', 'if', 'that', 'makes', 'any', 'difference', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'respons', 'wareh', 'superv', 'certain', 'depart', 'mak', 'diff'], ['thank', 'response', 'warehouse', 'supervisor', 'certain', 'department', 'make', 'difference'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'nice', 'heavy', 'cylindrical', 'transformer.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'nic', 'heavy', 'cylindr', 'transform'], ['thats', 'nice', 'heavy', 'cylindrical', 'transformer'])\n",
      "original document: \n",
      "['9', 'PCC,', '2', 'ODC,', '1', 'AC']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nin', 'pcc', 'two', 'odc', 'on', 'ac'], ['nine', 'pcc', 'two', 'odc', 'one', 'ac'])\n",
      "original document: \n",
      "['just', 'your', 'typical', 'r/all', 'user']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['typ', 'ral', 'us'], ['typical', 'rall', 'user'])\n",
      "original document: \n",
      "['The', 'job', 'market', 'actually', 'is', 'that', 'awful', 'outside', 'of', 'hot', 'beds.', 'Sure,', 'the', 'pay', \"doesn't\", 'suck', 'if', 'you', 'get', 'a', 'job,', 'but', 'good', 'luck', 'getting', 'a', 'job.', 'Especially', 'if', 'your', 'school', \"doesn't\", 'have', 'a', 'significant', 'relationship', 'with', 'chemical', 'companies.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['job', 'market', 'act', 'aw', 'outsid', 'hot', 'bed', 'sur', 'pay', 'doesnt', 'suck', 'get', 'job', 'good', 'luck', 'get', 'job', 'espec', 'school', 'doesnt', 'sign', 'rel', 'chem', 'company'], ['job', 'market', 'actually', 'awful', 'outside', 'hot', 'bed', 'sure', 'pay', 'doesnt', 'suck', 'get', 'job', 'good', 'luck', 'get', 'job', 'especially', 'school', 'doesnt', 'significant', 'relationship', 'chemical', 'company'])\n",
      "original document: \n",
      "['**/r/CrackWatch**\\n\\nIf', \"you're\", 'looking', 'for', 'the', 'latest', 'news', 'on', 'cracks', 'for', 'your', 'favorite', 'games,', 'want', 'to', 'post', 'game', 'related', 'topics', 'or', 'discuss', 'your', 'issues', 'with', 'certain', 'games?', 'Well', 'if', 'so,', 'this', 'is', 'the', 'place', 'to', 'be!\\n\\n*****\\n^(Bot', 'created', 'by', '/u\\u200b', '/el_loke', '-', ')^[Feedback](http://www.reddit.com/message/compose/?to=el_loke&amp;subject=TrendingCommenterBot%20Feedback)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rcrackwatch\\n\\nif', 'yo', 'look', 'latest', 'new', 'crack', 'favorit', 'gam', 'want', 'post', 'gam', 'rel', 'top', 'discuss', 'issu', 'certain', 'gam', 'wel', 'plac', 'be\\n\\n\\nbot', 'cre', 'u', 'el_loke', 'feedbackhttpwwwredditcommessagecomposetoel_lokeampsubjecttrendingcommenterbot20feedback'], ['rcrackwatch\\n\\nif', 'youre', 'look', 'latest', 'news', 'crack', 'favorite', 'game', 'want', 'post', 'game', 'relate', 'topics', 'discuss', 'issue', 'certain', 'game', 'well', 'place', 'be\\n\\n\\nbot', 'create', 'u', 'el_loke', 'feedbackhttpwwwredditcommessagecomposetoel_lokeampsubjecttrendingcommenterbot20feedback'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'always', 'preferred', 'Rocky', 'Flop', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'prefer', 'rocky', 'flop'], ['always', 'prefer', 'rocky', 'flop'])\n",
      "original document: \n",
      "['Two', 'episodes.\\n\\n**Two**.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'episodes\\n\\ntwo'], ['two', 'episodes\\n\\ntwo'])\n",
      "original document: \n",
      "['Oh', 'they', 'do,', 'curious.', 'I', 'read', 'that', 'Terminator', 'was', 'also', 'basically', 'gnome-terminal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'cury', 'read', 'termin', 'also', 'bas', 'gnometermin'], ['oh', 'curious', 'read', 'terminator', 'also', 'basically', 'gnometerminal'])\n",
      "original document: \n",
      "[\"pm'd\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pmd'], ['pmd'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['thanks', 'now', 'i', \"don't\", 'need', 'to', 'write', 'this', 'myself.', 'Exactly', 'my', 'point', 'also.', 'Fuck', 'jacob', 'manipulative']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'dont', 'nee', 'writ', 'exact', 'point', 'also', 'fuck', 'jacob', 'manip'], ['thank', 'dont', 'need', 'write', 'exactly', 'point', 'also', 'fuck', 'jacob', 'manipulative'])\n",
      "original document: \n",
      "['you', 'get', 'credits.', 'woah', \"didn't\", 'see', 'that', 'coming.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'credit', 'woah', 'didnt', 'see', 'com'], ['get', 'credit', 'woah', 'didnt', 'see', 'come'])\n",
      "original document: \n",
      "['Yes,', 'but', 'does', 'it', 'hang', 'like', 'sleeve', 'of', 'wizard?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'hang', 'lik', 'sleev', 'wizard'], ['yes', 'hang', 'like', 'sleeve', 'wizard'])\n",
      "original document: \n",
      "['give', 'you', 'a', 'target', 'in', 'a', 'bh', 'world,', 'which', 'it', 'does.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'target', 'bh', 'world'], ['give', 'target', 'bh', 'world'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', \"don't\", 'see', \"what's\", 'wrong', 'with', 'making', 'mythic', 'flex', 'too.', '\\n\\nBeing', 'locked', 'to', 'one', 'raid', 'ID', 'is', 'pointless', 'and', 'serves', 'no', 'purpose', 'other', 'than', 'to', 'stop', 'people', 'pugging', 'it.\\n\\nBeing', 'realm', 'specific', 'is', 'the', 'same.', \"There's\", 'no', 'such', 'thing', 'as', 'realm', 'competition', 'any', 'more,', 'so', \"there's\", 'no', 'point', 'using', 'that', 'as', 'an', 'excuse', 'anymore.\\n\\nThe', 'only', 'thing', 'mythic', 'only', 'raid', 'rules', 'do', 'is', 'limit', 'the', 'number', 'of', 'people', 'who', 'can', 'do', 'it', 'and', 'the', 'only', 'reason', 'people', 'defend', 'it', 'is', 'because', 'they', \"don't\", 'want', 'to', 'lose', 'their', 'epeen', 'when', 'puggers', 'start', 'clearing', 'it', 'faster', 'than', 'guilds', 'do.', 'Just', 'like', 'when', 'heroics', 'where', 'meant', 'to', 'be', 'in', 'pugable', 'or', 'even', 'raids', 'in', 'general', 'if', 'you', 'go', 'back', 'far', 'enough.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'see', 'what', 'wrong', 'mak', 'myth', 'flex', '\\n\\nbeing', 'lock', 'on', 'raid', 'id', 'pointless', 'serv', 'purpos', 'stop', 'peopl', 'pug', 'it\\n\\nbeing', 'realm', 'spec', 'ther', 'thing', 'realm', 'competit', 'ther', 'point', 'us', 'excus', 'anymore\\n\\nth', 'thing', 'myth', 'raid', 'rul', 'limit', 'numb', 'peopl', 'reason', 'peopl', 'defend', 'dont', 'want', 'los', 'epeen', 'pug', 'start', 'clear', 'fast', 'guild', 'lik', 'hero', 'meant', 'pug', 'ev', 'raid', 'gen', 'go', 'back', 'far', 'enough'], ['dont', 'see', 'whats', 'wrong', 'make', 'mythic', 'flex', '\\n\\nbeing', 'lock', 'one', 'raid', 'id', 'pointless', 'serve', 'purpose', 'stop', 'people', 'pugging', 'it\\n\\nbeing', 'realm', 'specific', 'theres', 'thing', 'realm', 'competition', 'theres', 'point', 'use', 'excuse', 'anymore\\n\\nthe', 'thing', 'mythic', 'raid', 'rule', 'limit', 'number', 'people', 'reason', 'people', 'defend', 'dont', 'want', 'lose', 'epeen', 'puggers', 'start', 'clear', 'faster', 'guilds', 'like', 'heroics', 'mean', 'pugable', 'even', 'raid', 'general', 'go', 'back', 'far', 'enough'])\n",
      "original document: \n",
      "['Your', 'yellow', 'marker', 'light', 'bulb', \"isn't\", 'blinking', 'at', 'the', 'correct', 'speed,', 'tire', 'pressure', 'too', 'low,', 'and', 'no', 'cups', 'in', 'cup', 'holders.', 'That', 'will', 'need', 'impounded', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yellow', 'mark', 'light', 'bulb', 'isnt', 'blink', 'correct', 'spee', 'tir', 'press', 'low', 'cup', 'cup', 'hold', 'nee', 'impound'], ['yellow', 'marker', 'light', 'bulb', 'isnt', 'blink', 'correct', 'speed', 'tire', 'pressure', 'low', 'cup', 'cup', 'holders', 'need', 'impound'])\n",
      "original document: \n",
      "['At', 'least', 'this', 'is', 'something', 'you', 'can', 'change.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'someth', 'chang'], ['least', 'something', 'change'])\n",
      "original document: \n",
      "['On', 'one', 'hand,', \"we've\", 'got', 'SKSE', 'and', 'CBBE', 'for', 'SSE.\\n\\nOn', 'the', 'other', 'hand,', \"there's\", 'the', 'Creation', 'Club.\\n\\nThis', 'month', 'has', 'been', 'full', 'of', 'ups', 'and', 'downs.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'hand', 'wev', 'got', 'skse', 'cbbe', 'sse\\n\\non', 'hand', 'ther', 'cre', 'club\\n\\nthis', 'mon', 'ful', 'up', 'down'], ['one', 'hand', 'weve', 'get', 'skse', 'cbbe', 'sse\\n\\non', 'hand', 'theres', 'creation', 'club\\n\\nthis', 'month', 'full', 'up', 'down'])\n",
      "original document: \n",
      "['godammit,', 'if', 'only', 'we', 'had', 'Mark', 'Craig,', 'a', 'battle', 'of', 'gods.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['godammit', 'mark', 'craig', 'battl', 'god'], ['godammit', 'mark', 'craig', 'battle', 'gods'])\n",
      "original document: \n",
      "['Pineapples', 'are', 'good.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pineappl', 'good'], ['pineapples', 'good'])\n",
      "original document: \n",
      "['It', 'copies', 'the', 'stats', 'of', 'a', '6/6', 'trickster/kitchen', 'sink', 'for', '2', 'brains']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cop', 'stat', 'sixty-six', 'tricksterkitch', 'sink', 'two', 'brain'], ['copy', 'stats', 'sixty-six', 'tricksterkitchen', 'sink', 'two', 'brain'])\n",
      "original document: \n",
      "['Primus', 'is', 'eternal', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prim', 'etern'], ['primus', 'eternal'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Good', 'point', \"it's\", 'late', \"I'm\", 'not', 'paying', 'enough', 'attention', 'apparently.', 'OP', 'can', 'you', 'confirm', 'where', 'your', 'getting', 'the', 'clock', 'jnformation?', 'If', 'it', 'from', 'the', 'sticks', 'themselves', 'or', 'bios/software?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'point', 'lat', 'im', 'pay', 'enough', 'at', 'app', 'op', 'confirm', 'get', 'clock', 'jnformation', 'stick', 'biossoftw'], ['good', 'point', 'late', 'im', 'pay', 'enough', 'attention', 'apparently', 'op', 'confirm', 'get', 'clock', 'jnformation', 'stick', 'biossoftware'])\n",
      "original document: \n",
      "['Ugh', 'my', 'dad', 'flew', 'bombers', 'in', 'the', 'Gulf', 'war', 'and', 'is', 'in', 'disbelief', 'that', \"I'm\", 'on', 'the', 'side', 'of', 'the', 'kneelers', 'after', 'even', 'saying', \"it's\", 'not', 'even', 'their', 'cause', 'that', 'I', 'necessarily', 'support', 'but', 'their', 'freedom', 'to', 'do', 'so.', 'He', 'literally', \"doesn't\", 'understand', 'the', 'true', 'cause', 'that', 'he', 'fought', 'for']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ugh', 'dad', 'flew', 'bomb', 'gulf', 'war', 'disbeliev', 'im', 'sid', 'kneel', 'ev', 'say', 'ev', 'caus', 'necess', 'support', 'freedom', 'lit', 'doesnt', 'understand', 'tru', 'caus', 'fought'], ['ugh', 'dad', 'fly', 'bombers', 'gulf', 'war', 'disbelief', 'im', 'side', 'kneelers', 'even', 'say', 'even', 'cause', 'necessarily', 'support', 'freedom', 'literally', 'doesnt', 'understand', 'true', 'cause', 'fight'])\n",
      "original document: \n",
      "['Been', 'banned', 'for', 'years']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ban', 'year'], ['ban', 'years'])\n",
      "original document: \n",
      "['Hmmmm.', 'If', 'you', 'think', 'the', 'pension', 'clause', 'is', 'so', 'destructive,', 'why', 'do', 'you', 'need', 'to', 'ask?', \"It's\", 'almost', 'like', 'you', 'have', 'only', 'a', 'very', 'cursory', 'understanding', 'of', 'the', 'topic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmmmm', 'think', 'pend', 'claus', 'destruct', 'nee', 'ask', 'almost', 'lik', 'curs', 'understand', 'top'], ['hmmmm', 'think', 'pension', 'clause', 'destructive', 'need', 'ask', 'almost', 'like', 'cursory', 'understand', 'topic'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['But', 'then', 'gives', 'one', 'up!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'on'], ['give', 'one'])\n",
      "original document: \n",
      "['Sorry,', 'the', 'bot', 'told', 'me', 'it', 'deleted', 'the', 'post', 'already.', 'I', 'sent', 'it', 'to', 'the', 'other', 'subreddit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'bot', 'told', 'delet', 'post', 'already', 'sent', 'subreddit'], ['sorry', 'bot', 'tell', 'delete', 'post', 'already', 'send', 'subreddit'])\n",
      "original document: \n",
      "[\"Let's\", 'see,', \"I'm\", 'going', 'to', 'guess', '25?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'im', 'going', 'guess', 'twenty-five'], ['let', 'see', 'im', 'go', 'guess', 'twenty-five'])\n",
      "original document: \n",
      "['They', 'hired', 'Qualcomm', 'engineers', 'for', 'their', 'OS', 'and', 'camera', 'stack...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hir', 'qualcom', 'engin', 'os', 'camer', 'stack'], ['hire', 'qualcomm', 'engineer', 'os', 'camera', 'stack'])\n",
      "original document: \n",
      "['All', 'fire', 'but', 'why', \"didn't\", 'Nitty', 'say\\n\\n\"This', \"baby'll\", 'come', 'out', 'slow', 'if', 'I', 'heard', 'he', 'gettin', 'bread', 'like', 'a', 'incest', 'birth\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fir', 'didnt', 'nitty', 'say\\n\\nthis', 'babyl', 'com', 'slow', 'heard', 'gettin', 'bread', 'lik', 'incest', 'bir'], ['fire', 'didnt', 'nitty', 'say\\n\\nthis', 'babyll', 'come', 'slow', 'hear', 'gettin', 'bread', 'like', 'incest', 'birth'])\n",
      "original document: \n",
      "['Unrelated.', 'You', 'should', 'go', 'post', 'in', 'redacted,', 'there', 'always', 'looking', 'for', 'retards.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unrel', 'go', 'post', 'redact', 'alway', 'look', 'retard'], ['unrelated', 'go', 'post', 'redact', 'always', 'look', 'retard'])\n",
      "original document: \n",
      "['I', 'like', 'how', 'the', 'person', 'filming', 'her', 'goes', 'just', 'a', 'little', 'slower', 'at', 'her', 'butt']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'person', 'film', 'goe', 'littl', 'slow', 'but'], ['like', 'person', 'film', 'go', 'little', 'slower', 'butt'])\n",
      "original document: \n",
      "['Ser', 'Emmon', 'Costayne', '+4\\n\\nLord', 'Addam', 'Costayne\\n\\nSer', 'Ryswin', 'Costayne']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ser', 'emmon', 'costayn', '4\\n\\nlord', 'addam', 'costayne\\n\\nser', 'ryswin', 'costayn'], ['ser', 'emmon', 'costayne', '4\\n\\nlord', 'addam', 'costayne\\n\\nser', 'ryswin', 'costayne'])\n",
      "original document: \n",
      "['http://lmgtfy.com/?q=donk']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httplmgtfycomqdonk'], ['httplmgtfycomqdonk'])\n",
      "original document: \n",
      "['There', 'is', 'no', 'place.', 'If', 'your', 'wife', 'is', 'allowed', 'so', 'is', 'the', 'dog.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plac', 'wif', 'allow', 'dog'], ['place', 'wife', 'allow', 'dog'])\n",
      "original document: \n",
      "['owenwilsonwow.wav']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['owenwilsonwowwav'], ['owenwilsonwowwav'])\n",
      "original document: \n",
      "['[+entropizer](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnor3j8/):\\n\\nThank', 'you', 'for', 'answering', 'this.', 'I', \"didn't\", 'expect', 'it,', 'but', 'I', 'really', 'appreciate', 'the', 'honesty.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['entropizerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnor3j8\\n\\nthank', 'answ', 'didnt', 'expect', 'real', 'apprecy', 'honesty'], ['entropizerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnor3j8\\n\\nthank', 'answer', 'didnt', 'expect', 'really', 'appreciate', 'honesty'])\n",
      "original document: \n",
      "['Brexit.', '\\n\\nNeed', 'to', 'tap', 'into', 'jingoist', 'nostalgia', 'to', 'soothe', 'the', 'current', 'anxiety.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brexit', '\\n\\nneed', 'tap', 'jingo', 'nostalg', 'sooth', 'cur', 'anxiety\\n'], ['brexit', '\\n\\nneed', 'tap', 'jingoist', 'nostalgia', 'soothe', 'current', 'anxiety\\n'])\n",
      "original document: \n",
      "['OOOOOOH', 'SHIT!', 'HERE', 'WE', 'GO!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ooooooh', 'shit', 'go'], ['ooooooh', 'shit', 'go'])\n",
      "original document: \n",
      "['Oh', 'sorry', 'man,', 'thought', 'you', 'were', 'saying', 'bike', 'was', 'titanium', 'or', 'something.', 'I’ll', 'stop', 'joking', 'around', 'around', 'now.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sorry', 'man', 'thought', 'say', 'bik', 'titan', 'someth', 'il', 'stop', 'jok', 'around', 'around'], ['oh', 'sorry', 'man', 'think', 'say', 'bike', 'titanium', 'something', 'ill', 'stop', 'joke', 'around', 'around'])\n",
      "original document: \n",
      "['I', 'was', 'also', 'overweight', 'going', 'into', 'pregnancy,', 'and', 'I', 'lost', 'weight', 'both', 'pregnancies', 'during', 'the', 'first', 'half.', 'I', 'finally', 'stabilized', 'at', 'the', 'same', 'weight', 'as', 'my', 'last', 'visit', 'at', 'my', '20', 'week', 'appointment.', 'I', 'think', 'I', 'followed', 'the', 'same', 'trend', 'last', 'time,', 'but', 'I', 'still', 'only', 'gained', 'a', 'net', '7', 'pounds', 'when', 'my', '7', 'pound', 'baby', 'was', 'delivered.', 'But', 'she', 'was', '7', 'pounds', '5', 'ounces', 'at', '38', 'weeks', 'and', 'perfectly', 'healthy.', 'She', 'always', 'measured', 'on', 'track,', 'and', 'my', 'doctor', 'never', 'said', 'anything', 'negative.', 'I', 'also', 'went', 'on', 'to', 'gain', 'all', 'the', 'baby', 'weight', 'from', 'stress', 'and', 'comfort', 'eating', 'postpartum.', ':/', 'Really,', 'if', 'your', 'baby', 'is', 'growing', 'well', 'and', 'your', 'doctor', \"isn't\", 'worried,', 'especially', 'if', 'you', 'have', 'fat', 'stored', 'up,', 'weight', 'gain', \"isn't\", 'strictly', 'necessary.', 'You', 'and', 'baby', 'both', 'will', 'be', 'fueled', 'off', 'your', 'stored', 'fat', '(which', 'is', 'the', 'purpose', 'of', 'it).', 'You', 'just', 'really', 'want', 'to', 'make', 'sure', \"you're\", 'getting', 'enough', 'calcium', '(baby', 'will', 'leach', 'it', 'straight', 'out', 'of', 'your', 'bones', 'if', \"you're\", 'not', 'getting', 'enough', 'from', 'your', 'diet).', 'And', 'keep', 'up', 'on', 'your', 'prenatal', 'vitamin.\\n\\nSo', 'my', 'appetite', 'is', 'pretty', 'small.', 'I', 'just', \"can't\", 'eat', 'much', 'at', 'a', 'sitting', 'and', 'loading', 'up', 'at', 'meals', \"isn't\", 'an', 'option', 'for', 'me.', 'Figure', 'out', 'healthy,', 'reasonably', 'energy', 'dense', 'foods', 'you', 'can', 'snack', 'on', 'through', 'the', 'day', 'like', 'nuts', 'and', 'cheese,', 'fruits', 'and', 'veggies.', 'I', 'like', 'to', 'nibble', 'on', 'beef', 'jerky', 'sometimes', 'or', 'peanut', 'butter.', 'I', 'also', 'started', 'drinking', 'milk', 'sometimes,', 'mostly', 'because', 'I', 'think', \"it's\", 'tasty', 'but', 'I', 'only', 'seem', 'to', 'when', \"I'm\", 'pregnant.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'overweight', 'going', 'pregn', 'lost', 'weight', 'pregn', 'first', 'half', 'fin', 'stabl', 'weight', 'last', 'visit', 'twenty', 'week', 'appoint', 'think', 'follow', 'trend', 'last', 'tim', 'stil', 'gain', 'net', 'sev', 'pound', 'sev', 'pound', 'baby', 'del', 'sev', 'pound', 'fiv', 'ount', 'thirty-eight', 'week', 'perfect', 'healthy', 'alway', 'meas', 'track', 'doct', 'nev', 'said', 'anyth', 'neg', 'also', 'went', 'gain', 'baby', 'weight', 'stress', 'comfort', 'eat', 'postpart', 'real', 'baby', 'grow', 'wel', 'doct', 'isnt', 'worry', 'espec', 'fat', 'stor', 'weight', 'gain', 'isnt', 'strictly', 'necess', 'baby', 'fuel', 'stor', 'fat', 'purpos', 'real', 'want', 'mak', 'sur', 'yo', 'get', 'enough', 'calc', 'baby', 'leach', 'straight', 'bon', 'yo', 'get', 'enough', 'diet', 'keep', 'pren', 'vitamin\\n\\nso', 'appetit', 'pretty', 'smal', 'cant', 'eat', 'much', 'sit', 'load', 'meal', 'isnt', 'opt', 'fig', 'healthy', 'reason', 'energy', 'dens', 'food', 'snack', 'day', 'lik', 'nut', 'chees', 'fruit', 'veggy', 'lik', 'nibbl', 'beef', 'jerky', 'sometim', 'peanut', 'but', 'also', 'start', 'drink', 'milk', 'sometim', 'most', 'think', 'tasty', 'seem', 'im', 'pregn'], ['also', 'overweight', 'go', 'pregnancy', 'lose', 'weight', 'pregnancies', 'first', 'half', 'finally', 'stabilize', 'weight', 'last', 'visit', 'twenty', 'week', 'appointment', 'think', 'follow', 'trend', 'last', 'time', 'still', 'gain', 'net', 'seven', 'pound', 'seven', 'pound', 'baby', 'deliver', 'seven', 'pound', 'five', 'ounces', 'thirty-eight', 'weeks', 'perfectly', 'healthy', 'always', 'measure', 'track', 'doctor', 'never', 'say', 'anything', 'negative', 'also', 'go', 'gain', 'baby', 'weight', 'stress', 'comfort', 'eat', 'postpartum', 'really', 'baby', 'grow', 'well', 'doctor', 'isnt', 'worry', 'especially', 'fat', 'store', 'weight', 'gain', 'isnt', 'strictly', 'necessary', 'baby', 'fuel', 'store', 'fat', 'purpose', 'really', 'want', 'make', 'sure', 'youre', 'get', 'enough', 'calcium', 'baby', 'leach', 'straight', 'bone', 'youre', 'get', 'enough', 'diet', 'keep', 'prenatal', 'vitamin\\n\\nso', 'appetite', 'pretty', 'small', 'cant', 'eat', 'much', 'sit', 'load', 'meals', 'isnt', 'option', 'figure', 'healthy', 'reasonably', 'energy', 'dense', 'foods', 'snack', 'day', 'like', 'nut', 'cheese', 'fruit', 'veggies', 'like', 'nibble', 'beef', 'jerky', 'sometimes', 'peanut', 'butter', 'also', 'start', 'drink', 'milk', 'sometimes', 'mostly', 'think', 'tasty', 'seem', 'im', 'pregnant'])\n",
      "original document: \n",
      "['I', 'can', 'speculate,', 'but', 'still', \"can't\", 'be', 'sure,', 'nonetheless', 'though', \"I'm\", 'thinking', '*real*.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spec', 'stil', 'cant', 'sur', 'nonetheless', 'though', 'im', 'think', 'real'], ['speculate', 'still', 'cant', 'sure', 'nonetheless', 'though', 'im', 'think', 'real'])\n",
      "original document: \n",
      "['City’s', 'sub', 'actually', 'has', 'more', 'subscribers', 'than', 'their', 'average', 'attendance.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['city', 'sub', 'act', 'subscrib', 'av', 'attend'], ['citys', 'sub', 'actually', 'subscribers', 'average', 'attendance'])\n",
      "original document: \n",
      "['Last', '10-11', 'games', 'of', 'that', '2009', 'season', 'were', 'legendary.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['last', 'one thousand and eleven', 'gam', 'two thousand and nine', 'season', 'legend'], ['last', 'one thousand and eleven', 'game', 'two thousand and nine', 'season', 'legendary'])\n",
      "original document: \n",
      "['Geico.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['geico'], ['geico'])\n",
      "original document: \n",
      "['All', 'of', 'us', 'gets', 'drained', 'by', 'negative', 'social', 'interaction', 'and', 'energized', 'by', 'positive', 'social', 'interaction', 'regardless', 'if', \"you're\", 'an', 'introvert', 'or', 'extrovert.', \"It's\", 'not', 'about', 'social', 'stimulation,', \"it's\", 'about', 'whether', 'you', 'thrive', 'off', 'your', 'energy', 'more', 'from', 'any', 'form', 'of', 'external', 'stimuli', 'whether', \"it's\", 'going', 'to', 'interesting', 'places', 'or', 'being', 'in', 'charge', 'of', 'people', '(extrovert)', 'or', 'if', 'you', 'thrive', 'off', 'your', 'energy', 'from', 'any', 'form', 'of', 'internal', 'stimuli', 'such', 'as', 'being', 'inside', 'your', 'head', 'all', 'the', 'time', '(introvert).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'get', 'drain', 'neg', 'soc', 'interact', 'energ', 'posit', 'soc', 'interact', 'regardless', 'yo', 'introvert', 'extrovert', 'soc', 'stim', 'wheth', 'thrive', 'energy', 'form', 'extern', 'stimul', 'wheth', 'going', 'interest', 'plac', 'charg', 'peopl', 'extrovert', 'thrive', 'energy', 'form', 'intern', 'stimul', 'insid', 'head', 'tim', 'introvert'], ['us', 'get', 'drain', 'negative', 'social', 'interaction', 'energize', 'positive', 'social', 'interaction', 'regardless', 'youre', 'introvert', 'extrovert', 'social', 'stimulation', 'whether', 'thrive', 'energy', 'form', 'external', 'stimuli', 'whether', 'go', 'interest', 'place', 'charge', 'people', 'extrovert', 'thrive', 'energy', 'form', 'internal', 'stimuli', 'inside', 'head', 'time', 'introvert'])\n",
      "original document: \n",
      "['as', 'does', 'yours']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "[\"It's\", 'amazing', 'how', 'TA3', 'gets', 'so', 'much', 'subjective', 'praise.\\n\\nBut', 'on', 'topic', \"RobinF's\", 'defensive', 'stats', 'are', 'better', 'than', 'Indigo', 'due', 'to', 'her', 'physical', 'bulk,', 'Indigo', 'requires', 'lots', 'of', 'work', 'to', 'make', 'work', 'in', 'dealing', 'with', 'the', 'more', 'advanced', 'players', 'with', 'heavy', 'merged', \"BLyn's\", 'and', 'Reinhardts,', 'which', 'are', 'really', 'common', 'personally.', 'Compared', 'to', 'RobinF', 'who', 'with', 'a', 'few', 'merges', 'can', 'counter', '+10', 'opposition', 'running', 'double', 'goads', 'and', 'hone.\\n\\nIndigo', 'requires', 'bowbreaker', '(or', 'lots', 'of', 'spurs)', 'to', 'kill', 'a', 'BLyn', 'that', 'runs', 'Mulagir.', 'Ignoring', 'defensive', 'battles,', 'at', 'neutral', 'atk,', 'a', 'simple', '+6', 'atk', 'from', 'any', 'source', '(Ex.', 'Hone/Ally', 'Support)', 'is', 'enough', 'for', 'Indigo', 'to', 'kill', 'a', 'BLyn', 'running', 'Brave', 'Bow,', 'this', 'of', 'course', 'changes', 'dramatically', 'if', 'they', 'run', 'fortify', 'as', 'well.', '\\n\\nThe', 'simplest', 'way', 'for', 'Indigo', 'to', 'deal', 'with', 'Blyn', 'is', 'bow', 'breaker,', 'which', 'comes', 'at', 'an', 'expense', 'for', 'some', 'in', 'the', 'form', 'of', 'WoM', 'or', 'GTB.\\n\\nMeta', \"Reinhardt's\", 'running', 'QP/DB/DB3/Moonbow', 'with', '1', 'hone', 'and', 'goad', 'can', 'still', 'kill', 'a', 'unmerged', 'RES', 'Indigo', 'not', 'running', 'buffs,', 'requiring', '+3~5', 'res', 'to', 'handle', 'hone', 'x1goad/x2', 'goads,', 'which', 'can', 'be', 'simply', 'Fort', 'Res1', 'Seal+Ally', 'Support+HP/Res', 'Seal', 'on', 'Indigo.\\n\\ntl;dr', 'unmerged', 'Indigo', 'will', 'help', 'against', 'unmerged', 'opposition', 'like', 'a', 'prince', 'charming.', 'But', 'against', 'more', 'invested', 'teams,', 'not', 'so', 'much', 'without', 'severe', 'buff', 'work,', \"you'll\", 'probably', 'sacrifice', 'something', 'to', 'fit', 'him', 'to', 'do', 'his', 'job', 'when', 'he', 'needs', 'to', 'pull', 'the', 'weight', 'of', 'his', 'intended', 'roll.', '\\n\\nDef', 'not', 'S+-Rank', 'unless', 'your', 'just', 'saying', 'that', 'because', 'he', 'has', 'dance,', 'then', 'at', 'that', 'point', 'you', 'can', 'argue', 'another', 'color+dance.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amaz', 'ta3', 'get', 'much', 'subject', 'praise\\n\\nbut', 'top', 'robinf', 'defend', 'stat', 'bet', 'indigo', 'due', 'phys', 'bulk', 'indigo', 'requir', 'lot', 'work', 'mak', 'work', 'deal', 'adv', 'play', 'heavy', 'merg', 'blyn', 'reinhardt', 'real', 'common', 'person', 'comp', 'robinf', 'merg', 'count', 'ten', 'opposit', 'run', 'doubl', 'goad', 'hone\\n\\nindigo', 'requir', 'bowbreak', 'lot', 'spur', 'kil', 'blyn', 'run', 'mulagir', 'ign', 'defend', 'battl', 'neut', 'atk', 'simpl', 'six', 'atk', 'sourc', 'ex', 'hon', 'support', 'enough', 'indigo', 'kil', 'blyn', 'run', 'brav', 'bow', 'cours', 'chang', 'dram', 'run', 'fort', 'wel', '\\n\\nthe', 'simplest', 'way', 'indigo', 'deal', 'blyn', 'bow', 'break', 'com', 'expens', 'form', 'wom', 'gtb\\n\\nmeta', 'reinhardt', 'run', 'qpdbdb3moonbow', 'on', 'hon', 'goad', 'stil', 'kil', 'unmerg', 'res', 'indigo', 'run', 'buff', 'requir', 'thirty-five', 'res', 'handl', 'hon', 'x1goadx2', 'goad', 'simply', 'fort', 'res1', 'seal', 'supporthpr', 'seal', 'indigo\\n\\ntldr', 'unmerg', 'indigo', 'help', 'unmerg', 'opposit', 'lik', 'print', 'charm', 'invest', 'team', 'much', 'without', 'sev', 'buff', 'work', 'youl', 'prob', 'sacr', 'someth', 'fit', 'job', 'nee', 'pul', 'weight', 'intend', 'rol', '\\n\\ndef', 'srank', 'unless', 'say', 'dant', 'point', 'argu', 'anoth', 'colord'], ['amaze', 'ta3', 'get', 'much', 'subjective', 'praise\\n\\nbut', 'topic', 'robinfs', 'defensive', 'stats', 'better', 'indigo', 'due', 'physical', 'bulk', 'indigo', 'require', 'lot', 'work', 'make', 'work', 'deal', 'advance', 'players', 'heavy', 'merge', 'blyns', 'reinhardts', 'really', 'common', 'personally', 'compare', 'robinf', 'merge', 'counter', 'ten', 'opposition', 'run', 'double', 'goad', 'hone\\n\\nindigo', 'require', 'bowbreaker', 'lot', 'spur', 'kill', 'blyn', 'run', 'mulagir', 'ignore', 'defensive', 'battle', 'neutral', 'atk', 'simple', 'six', 'atk', 'source', 'ex', 'honeally', 'support', 'enough', 'indigo', 'kill', 'blyn', 'run', 'brave', 'bow', 'course', 'change', 'dramatically', 'run', 'fortify', 'well', '\\n\\nthe', 'simplest', 'way', 'indigo', 'deal', 'blyn', 'bow', 'breaker', 'come', 'expense', 'form', 'wom', 'gtb\\n\\nmeta', 'reinhardts', 'run', 'qpdbdb3moonbow', 'one', 'hone', 'goad', 'still', 'kill', 'unmerged', 'res', 'indigo', 'run', 'buff', 'require', 'thirty-five', 'res', 'handle', 'hone', 'x1goadx2', 'goad', 'simply', 'fort', 'res1', 'sealally', 'supporthpres', 'seal', 'indigo\\n\\ntldr', 'unmerged', 'indigo', 'help', 'unmerged', 'opposition', 'like', 'prince', 'charm', 'invest', 'team', 'much', 'without', 'severe', 'buff', 'work', 'youll', 'probably', 'sacrifice', 'something', 'fit', 'job', 'need', 'pull', 'weight', 'intend', 'roll', '\\n\\ndef', 'srank', 'unless', 'say', 'dance', 'point', 'argue', 'another', 'colordance'])\n",
      "original document: \n",
      "['I', 'know', 'you', 'what', \"you're\", 'saying,', 'But', 'you', 'create', 'a', 'false', 'dichotomy', 'that', 'we', 'either', 'gotta', 'be', 'in', 'a', 'lawless', 'land', 'or', 'a', 'police', 'state.', \"You'd\", 'make', 'a', 'better', 'argument', 'if', 'you', 'just', 'paraphrased', 'Milton', 'Friedman', 'critique', 'on', 'the', 'war', 'of', 'drugs', 'and', 'link', 'using', 'drugs', 'to', 'over', 'eating', 'since', 'a', 'significant', 'portion', 'of', 'Americans', 'do', 'actually', 'overeat', 'while', 'we', \"don't\", 'actively', 'drink', 'HCl.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'yo', 'say', 'cre', 'fals', 'dichotom', 'eith', 'gott', 'lawless', 'land', 'pol', 'stat', 'youd', 'mak', 'bet', 'argu', 'paraphras', 'milton', 'friedm', 'crit', 'war', 'drug', 'link', 'us', 'drug', 'eat', 'sint', 'sign', 'port', 'am', 'act', 'ov', 'dont', 'act', 'drink', 'hcl'], ['know', 'youre', 'say', 'create', 'false', 'dichotomy', 'either', 'gotta', 'lawless', 'land', 'police', 'state', 'youd', 'make', 'better', 'argument', 'paraphrase', 'milton', 'friedman', 'critique', 'war', 'drug', 'link', 'use', 'drug', 'eat', 'since', 'significant', 'portion', 'americans', 'actually', 'overeat', 'dont', 'actively', 'drink', 'hcl'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['The', 'above', 'submission', 'has', 'been', '**removed**', 'because:\\n\\n*', 'It', 'does', 'not', 'appear', 'to', 'contain', 'a', 'tl;dr', 'or', 'any', 'sort', 'of', 'short', 'summary.', 'Please', 'edit', 'your', 'post', 'to', 'add', 'a', '**\\\\*\\\\*bolded****', 'tl;dr.', 'Refer', 'to', 'FAQ:', '[What', 'is', 'TL;DR?', 'Why', 'do', 'I', 'need', 'it?](http://www.reddit.com/r/relationships/wiki/index#wiki_what_is_tl.3Bdr.3F_why_do_i_need_it.3F)', 'for', 'some', 'pointers.', '', 'If', 'you', 'feel', 'you', 'are', 'receiving', 'this', 'message', 'in', 'error,', 'please', '[contact', 'the', 'moderators](http://www.reddit.com/message/compose?to=%2Fr%2Frelationships&amp;subject=Submission+removed+for+no+TLDR&amp;message=My+post+can+be+found+at:+https://www.reddit.com/r/relationships/comments/73hehs/me_20_m_and_tinder_girl_20_f_after_first_date_is/)', 'and', 'include', 'your', 'problem.', '', '\\n\\nYou', 'must', 'make', 'suitable', 'edits', 'to', 'ensure', 'that', 'the', 'submission', 'complies', 'to', '**all', 'the', 'rules**', 'listed', 'in', 'the', 'sidebar,', 'and', 'in', 'the', '[wiki](/r/relationships/wiki/index#wiki_about_.2Fr.2Frelationships)', '(relevant', 'for', 'mobile', 'users).', '', 'When', 'you', 'are', 'done,', 'please', 'repost', 'your', 'submission.', '', '\\n\\n---\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/relationships)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'remov', 'because\\n\\n', 'appear', 'contain', 'tldr', 'sort', 'short', 'sum', 'pleas', 'edit', 'post', 'ad', 'bold', 'tldr', 'ref', 'faq', 'tldr', 'nee', 'ithttpwwwredditcomrrelationshipswikiindexwiki_what_is_tl3bdr3f_why_do_i_need_it3f', 'point', 'feel', 'receiv', 'mess', 'er', 'pleas', 'contact', 'moderatorshttpwwwredditcommessagecomposeto2fr2frelationshipsampsubjectsubmissionremovedfornotldrampmessagemypostcanbefoundathttpswwwredditcomrrelationshipscomments73hehsme_20_m_and_tinder_girl_20_f_after_first_date_is', 'includ', 'problem', '\\n\\nyou', 'must', 'mak', 'suit', 'edit', 'ens', 'submit', 'comply', 'rul', 'list', 'sideb', 'wikirrelationshipswikiindexwiki_about_2fr2frelationships', 'relev', 'mobl', 'us', 'don', 'pleas', 'repost', 'submit', '\\n\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorrel', 'quest', 'concern'], ['submission', 'remove', 'because\\n\\n', 'appear', 'contain', 'tldr', 'sort', 'short', 'summary', 'please', 'edit', 'post', 'add', 'bolded', 'tldr', 'refer', 'faq', 'tldr', 'need', 'ithttpwwwredditcomrrelationshipswikiindexwiki_what_is_tl3bdr3f_why_do_i_need_it3f', 'pointers', 'feel', 'receive', 'message', 'error', 'please', 'contact', 'moderatorshttpwwwredditcommessagecomposeto2fr2frelationshipsampsubjectsubmissionremovedfornotldrampmessagemypostcanbefoundathttpswwwredditcomrrelationshipscomments73hehsme_20_m_and_tinder_girl_20_f_after_first_date_is', 'include', 'problem', '\\n\\nyou', 'must', 'make', 'suitable', 'edit', 'ensure', 'submission', 'comply', 'rule', 'list', 'sidebar', 'wikirrelationshipswikiindexwiki_about_2fr2frelationships', 'relevant', 'mobile', 'users', 'do', 'please', 'repost', 'submission', '\\n\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorrelationships', 'question', 'concern'])\n",
      "original document: \n",
      "['143416975|', '&gt;', 'Canada', 'Anonymous', '(ID:', 'rW+pZFeX)\\n\\n&gt;&gt;143412250', '(OP)\\n&gt;rura\\nThe', 'only', 'people', 'who', 'voted', 'for', 'Trump', 'are', 'rural', 'and', 'suburban', 'retards.\\n\\nCity', 'people', 'all', 'voted', 'for', 'Hillary.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and sixteen thousand, nine hundred and seventy-fiv', 'gt', 'canad', 'anonym', 'id', 'rwpzfex\\n\\ngtgt143412250', 'op\\ngtrura\\nthe', 'peopl', 'vot', 'trump', 'rur', 'suburb', 'retards\\n\\ncity', 'peopl', 'vot', 'hillary\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and sixteen thousand, nine hundred and seventy-five', 'gt', 'canada', 'anonymous', 'id', 'rwpzfex\\n\\ngtgt143412250', 'op\\ngtrura\\nthe', 'people', 'vote', 'trump', 'rural', 'suburban', 'retards\\n\\ncity', 'people', 'vote', 'hillary\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Could', 'be', 'just', 'friends', 'and', 'see', 'how', \"you're\", 'doing.', '', 'Could', 'be', 'testing', 'for', 'spark.', '', 'Could', 'be', 'that', \"she's\", 'got', 'a', 'room', 'booked', 'next', 'to', 'the', 'coffee', 'shop.', '', 'Give', 'us', 'an', 'update', 'afterwards']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'friend', 'see', 'yo', 'could', 'test', 'spark', 'could', 'she', 'got', 'room', 'book', 'next', 'coff', 'shop', 'giv', 'us', 'upd', 'afterward'], ['could', 'friends', 'see', 'youre', 'could', 'test', 'spark', 'could', 'shes', 'get', 'room', 'book', 'next', 'coffee', 'shop', 'give', 'us', 'update', 'afterwards'])\n",
      "original document: \n",
      "['What', 'an', 'epic', 'show.', '[This](https://i.imgur.com/ZvqT4iS.png)', 'was', 'just', 'too', 'epic.', 'Also,', 'it', 'was', 'awesome', 'to', 'have', 'an', 'extended', 'episode', 'for', 'the', 'season', 'finale.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ep', 'show', 'thishttpsiimgurcomzvqt4ispng', 'ep', 'also', 'awesom', 'extend', 'episod', 'season', 'fin'], ['epic', 'show', 'thishttpsiimgurcomzvqt4ispng', 'epic', 'also', 'awesome', 'extend', 'episode', 'season', 'finale'])\n",
      "original document: \n",
      "['Sorry,', 'was', 'out', 'later', 'than', 'expected!', 'If', \"you're\", 'still', 'up', 'let', 'me', 'know,', 'otherwise', 'we', 'can', 'always', 'try', 'tomorrow!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'lat', 'expect', 'yo', 'stil', 'let', 'know', 'otherw', 'alway', 'try', 'tomorrow'], ['sorry', 'later', 'expect', 'youre', 'still', 'let', 'know', 'otherwise', 'always', 'try', 'tomorrow'])\n",
      "original document: \n",
      "['Being', 'all', 'about', 'Jesus.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jes'], ['jesus'])\n",
      "original document: \n",
      "['First', 'of', 'all,', 'Phoinex', 'o', 'hara', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'phoinex', 'har'], ['first', 'phoinex', 'hara'])\n",
      "original document: \n",
      "['It', 'sounds', 'a', 'lot', 'more', 'like', 'the', 'October', '3rd', 'FNN', 'will', 'be', 'announcing', 'no', 'more', 'major', 'content', 'updates', 'from', \"Jay's\", 'wording', 'in', 'the', '/r/Titanfall', 'post.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'lot', 'lik', 'octob', '3rd', 'fnn', 'annount', 'maj', 'cont', 'upd', 'jay', 'word', 'rtitanfal', 'post'], ['sound', 'lot', 'like', 'october', '3rd', 'fnn', 'announce', 'major', 'content', 'update', 'jays', 'word', 'rtitanfall', 'post'])\n",
      "original document: \n",
      "['*The', 'shelving', 'contorts', 'and', 'begins', 'bringing', 'a', 'specific', 'shelf', 'to', 'the', 'top.', 'Once', 'it', 'arrives', 'at', 'your', 'eye', 'level,', 'the', 'shelf', 'slides', 'to', 'you,', 'stopping', 'inches', 'from', 'the', 'top', 'of', 'the', 'podium.*\\n\\n*The', 'shelf', 'contains', 'a', 'number', 'of', 'small', 'paphlets,', 'and', 'two', 'larger', 'sized', 'hardback', 'volumes.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shelv', 'contort', 'begin', 'bring', 'spec', 'shelf', 'top', 'ar', 'ey', 'level', 'shelf', 'slid', 'stop', 'inch', 'top', 'podium\\n\\nthe', 'shelf', 'contain', 'numb', 'smal', 'paphlet', 'two', 'larg', 'siz', 'hardback', 'volum'], ['shelve', 'contort', 'begin', 'bring', 'specific', 'shelf', 'top', 'arrive', 'eye', 'level', 'shelf', 'slide', 'stop', 'inch', 'top', 'podium\\n\\nthe', 'shelf', 'contain', 'number', 'small', 'paphlets', 'two', 'larger', 'size', 'hardback', 'volumes'])\n",
      "original document: \n",
      "['Also,', \"don't\", 'blame', 'yourself', 'so', 'much', 'for', 'letting', 'it', '\"get\"', 'this', 'way.', '', 'Ultimately', 'the', 'majority', 'of', 'the', 'responsibility', '(it', 'sounds', 'like)', 'should', 'fall', 'on', 'him', 'for', 'not', 'being', 'willing', 'to', 'take', 'a', 'fair', 'share', 'of', 'household', 'labor.', '\\n\\nBut', 'yes,', 'let', 'us', 'both', 'be', 'a', 'warning', 'to', 'have', 'these', 'conversations', 'early', 'in', 'the', 'relationship!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'dont', 'blam', 'much', 'let', 'get', 'way', 'ultim', 'maj', 'respons', 'sound', 'lik', 'fal', 'wil', 'tak', 'fair', 'shar', 'household', 'lab', '\\n\\nbut', 'ye', 'let', 'us', 'warn', 'convers', 'ear', 'rel'], ['also', 'dont', 'blame', 'much', 'let', 'get', 'way', 'ultimately', 'majority', 'responsibility', 'sound', 'like', 'fall', 'will', 'take', 'fair', 'share', 'household', 'labor', '\\n\\nbut', 'yes', 'let', 'us', 'warn', 'conversations', 'early', 'relationship'])\n",
      "original document: \n",
      "[\"We're\", \"gonna'\", 'need', 'some', 'of', 'that', 'tomorrow.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gonn', 'nee', 'tomorrow'], ['gonna', 'need', 'tomorrow'])\n",
      "original document: \n",
      "['Get', 'yer', 'ass', 'to', \"upvotin'\", 'in', 'here.', \"That's\", 'what', 'you', 'do']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'yer', 'ass', 'upvotin', 'that'], ['get', 'yer', 'ass', 'upvotin', 'thats'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Maybe', 'firefox', 'is', 'just', 'a', 'turd.', '', 'I', 'installed', 'that', 'modify', 'headers', 'add', 'on', 'into', 'chrome', 'and', 'now', \"it's\", 'playing.', '', 'woot!\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'firefox', 'turd', 'instal', 'mod', 'head', 'ad', 'chrome', 'play', 'woot\\n'], ['maybe', 'firefox', 'turd', 'instal', 'modify', 'headers', 'add', 'chrome', 'play', 'woot\\n'])\n",
      "original document: \n",
      "['&gt;', 'on', 'mobile.\\n\\nSo', 'why', 'would', 'it', 'be', 'a', 'risky', 'click?', 'Is', 'your', 'personal', 'cell', 'phone', 'screen', 'broadcast', 'to', 'the', 'entire', 'company', 'or', 'something?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'mobile\\n\\nso', 'would', 'risky', 'click', 'person', 'cel', 'phon', 'screen', 'broadcast', 'entir', 'company', 'someth'], ['gt', 'mobile\\n\\nso', 'would', 'risky', 'click', 'personal', 'cell', 'phone', 'screen', 'broadcast', 'entire', 'company', 'something'])\n",
      "original document: \n",
      "['It', 'says', 'in', 'the', 'article', 'that', 'there', 'is', 'a', 'good', '4', 'weeks', 'to', 'complete.', 'I', 'ended', 'up', 'isolating', 'myself', 'from', 'all', 'of', 'their', 'services,', 'it', 'would', 'be', 'an', 'on', 'off', 'of', 'you', 'and', 'your', 'side', 'of', 'the', 'room.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'artic', 'good', 'four', 'week', 'complet', 'end', 'isol', 'serv', 'would', 'sid', 'room'], ['say', 'article', 'good', 'four', 'weeks', 'complete', 'end', 'isolate', 'service', 'would', 'side', 'room'])\n",
      "original document: \n",
      "['Anyone', 'know', 'anything', 'about', 'the', \"90's\", 'starters?', \"I've\", 'love', 'to', 'learn', 'more.\\n\\nAlso,', 'was', 'this', 'an', 'alternate', 'back', 'in', 'the', 'day?', 'I', 'know', 'the', 'pooh', 'patches', 'date', 'it', 'but', 'I', 'am', 'not', 'sure', 'what', 'I', 'actually', 'have.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'know', 'anyth', '90s', 'start', 'iv', 'lov', 'learn', 'more\\n\\nalso', 'altern', 'back', 'day', 'know', 'pooh', 'patch', 'dat', 'sur', 'act'], ['anyone', 'know', 'anything', '90s', 'starters', 'ive', 'love', 'learn', 'more\\n\\nalso', 'alternate', 'back', 'day', 'know', 'pooh', 'patch', 'date', 'sure', 'actually'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnor9aa/):\\n\\nI', 'always', 'try', 'to', 'be', 'honest.', 'Unfortunately', 'some', 'people', 'on', 'here', 'think', \"I'm\", 'a', 'lying', 'trust', 'fund', 'baby', ':(', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnor9aa\\n\\ni', 'alway', 'try', 'honest', 'unfortun', 'peopl', 'think', 'im', 'lying', 'trust', 'fund', 'baby'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnor9aa\\n\\ni', 'always', 'try', 'honest', 'unfortunately', 'people', 'think', 'im', 'lie', 'trust', 'fund', 'baby'])\n",
      "original document: \n",
      "['The', 'EDA', \"isn't\", 'broken,', 'just', 'not', 'designed', 'well', 'so', 'it', \"doesn't\", 'handle', 'the', 'adjustments', 'as', 'gracefully', 'as', 'it', 'should.', 'The', 'next', 'update', '(which', 'will', 'be', 'a', 'hardfork)', 'will', 'adjust', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ed', 'isnt', 'brok', 'design', 'wel', 'doesnt', 'handl', 'adjust', 'grac', 'next', 'upd', 'hardfork', 'adjust'], ['eda', 'isnt', 'break', 'design', 'well', 'doesnt', 'handle', 'adjustments', 'gracefully', 'next', 'update', 'hardfork', 'adjust'])\n",
      "original document: \n",
      "['Ok', 'Captain', 'Critic,', '\\n\\nIn', 'the', 'meantime,', 'Elon', 'Musk', 'has', 'already', 'started', 'fixing', 'the', 'problem.', '\\n\\nhttp://fortune.com/2017/09/28/tesla-battery-puerto-rico-power/\\n\\nMaybe', 'we', 'should', 'all', 'invest', 'in', 'Tesla', 'stock,', 'in', 'addition', 'to', 'donating', 'to', 'charity.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'captain', 'crit', '\\n\\nin', 'meantim', 'elon', 'musk', 'already', 'start', 'fix', 'problem', '\\n\\nhttpfortunecom20170928teslabatterypuertoricopower\\n\\nmaybe', 'invest', 'tesl', 'stock', 'addit', 'don', 'char'], ['ok', 'captain', 'critic', '\\n\\nin', 'meantime', 'elon', 'musk', 'already', 'start', 'fix', 'problem', '\\n\\nhttpfortunecom20170928teslabatterypuertoricopower\\n\\nmaybe', 'invest', 'tesla', 'stock', 'addition', 'donate', 'charity'])\n",
      "original document: \n",
      "['you', \"can't\", 'spot', 'a', 'speed', 'ling', 'from', 'a', 'regular', 'one??', \"it's\", 'got', 'wings.', 'only', 'issue', 'with', 'speedlings', \"i've\", 'seen', 'complained', 'about', 'is', 'counting', 'them.\\n\\nthey', 'like', 'start', 'to', 'shimmer', 'with', 'wings.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'spot', 'spee', 'ling', 'regul', 'on', 'got', 'wing', 'issu', 'speedl', 'iv', 'seen', 'complain', 'count', 'them\\n\\nthey', 'lik', 'start', 'shim', 'wing'], ['cant', 'spot', 'speed', 'ling', 'regular', 'one', 'get', 'wing', 'issue', 'speedlings', 'ive', 'see', 'complain', 'count', 'them\\n\\nthey', 'like', 'start', 'shimmer', 'wing'])\n",
      "original document: \n",
      "['Yeah,', \"didn't\", 'plan', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'didnt', 'plan', 'wel'], ['yeah', 'didnt', 'plan', 'well'])\n",
      "original document: \n",
      "['its', 'like', 'using', 'a', 'condom', 'vs.', 'no', 'condom.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'us', 'condom', 'vs', 'condom'], ['like', 'use', 'condom', 'vs', 'condom'])\n",
      "original document: \n",
      "['Could', 'be', '-', 'I', \"can't\", 'remember.', 'When', 'I', 'thought', 'of', 'thunder', 'and', 'lightning', 'I', 'thought', 'of', 'Jacobs', 'and', 'Bradshaw', 'though.', 'I', 'watched', 'football', 'during', 'the', 'Tiki', 'years,', 'but', 'not', 'nearly', 'as', 'heavily', 'as', 'I', 'watched', 'during', 'the', 'Bradshaw', 'years.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'cant', 'rememb', 'thought', 'thund', 'lightn', 'thought', 'jacob', 'bradshaw', 'though', 'watch', 'footbal', 'tik', 'year', 'near', 'heavy', 'watch', 'bradshaw', 'year'], ['could', 'cant', 'remember', 'think', 'thunder', 'lightning', 'think', 'jacobs', 'bradshaw', 'though', 'watch', 'football', 'tiki', 'years', 'nearly', 'heavily', 'watch', 'bradshaw', 'years'])\n",
      "original document: \n",
      "['Okay', 'lets', 'upvote', 'this', 'to', 'share', 'this', 'info!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'let', 'upvot', 'shar', 'info'], ['okay', 'let', 'upvote', 'share', 'info'])\n",
      "original document: \n",
      "['mxfushi', 'fam']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mxfushi', 'fam'], ['mxfushi', 'fam'])\n",
      "original document: \n",
      "['I', 'always', 'assumed', 'he', 'would', 'crash', 'the', 'dogfight', 'over', \"D'Qar,\", 'have', 'an', 'emotional', 'encounter', 'with', 'Leia,', 'and', 'make', 'off', 'with', 'the', 'map', 'to', 'Ahch-To.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'assum', 'would', 'crash', 'dogfight', 'dqar', 'emot', 'encount', 'lei', 'mak', 'map', 'ahchto'], ['always', 'assume', 'would', 'crash', 'dogfight', 'dqar', 'emotional', 'encounter', 'leia', 'make', 'map', 'ahchto'])\n",
      "original document: \n",
      "['Deepest', 'lore']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['deepest', 'lor'], ['deepest', 'lore'])\n",
      "original document: \n",
      "['Not', 'really,', 'if', 'he', \"doesn't\", 'take', 'care', 'of', 'it', 'then,', 'you', 'just', 'have', 'a', 'crappy', 'school', 'system.', 'No', 'offence.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'doesnt', 'tak', 'car', 'crappy', 'school', 'system', 'off'], ['really', 'doesnt', 'take', 'care', 'crappy', 'school', 'system', 'offence'])\n",
      "original document: \n",
      "['Wtf.', '', '', 'we', 'are', 'getting', 'pushed', 'around', 'by', 'a', 'sun', 'belt', 'team.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wtf', 'get', 'push', 'around', 'sun', 'belt', 'team'], ['wtf', 'get', 'push', 'around', 'sun', 'belt', 'team'])\n",
      "original document: \n",
      "['Google', 'is', 'too', 'difficult', 'for', 'you,', 'there', 'is', 'homepage,', 'there', 'is', 'GitHub', 'repo', '-', 'and', 'your', 'parents', 'are', 'related.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['googl', 'difficult', 'homep', 'github', 'repo', 'par', 'rel'], ['google', 'difficult', 'homepage', 'github', 'repo', 'parent', 'relate'])\n",
      "original document: \n",
      "['I', 'know', 'it', 'started', 'but', 'maybe', 'not', 'the', 'main', 'event.', 'Usually', 'this', 'goes', 'on', 'for', 'a', 'little', 'so', 'I', 'would', 'keep', 'checking', 'ESPN', '3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'start', 'mayb', 'main', 'ev', 'us', 'goe', 'littl', 'would', 'keep', 'check', 'espn', 'three'], ['know', 'start', 'maybe', 'main', 'event', 'usually', 'go', 'little', 'would', 'keep', 'check', 'espn', 'three'])\n",
      "original document: \n",
      "['Funny', 'thing', 'is', 'the', 'guy', 'next', 'to', 'me', 'is', 'a', 'co-worker', 'who', 'actually', 'was', 'racing', 'me.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['funny', 'thing', 'guy', 'next', 'cowork', 'act', 'rac'], ['funny', 'thing', 'guy', 'next', 'coworker', 'actually', 'race'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'after', 'you', 'watch', 'Wonder', 'Woman,', 'that', 'dim', 'light', 'of', 'hope', \"you're\", 'holding', 'onto', 'will', 'grow.', 'Great', 'film.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'watch', 'wond', 'wom', 'dim', 'light', 'hop', 'yo', 'hold', 'onto', 'grow', 'gre', 'film'], ['feel', 'like', 'watch', 'wonder', 'woman', 'dim', 'light', 'hope', 'youre', 'hold', 'onto', 'grow', 'great', 'film'])\n",
      "original document: \n",
      "['196pts', 'by', 'late', 'June', 'too']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['196pts', 'lat', 'jun'], ['196pts', 'late', 'june'])\n",
      "original document: \n",
      "['Meh.', 'Fuck', 'em']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meh', 'fuck', 'em'], ['meh', 'fuck', 'em'])\n",
      "original document: \n",
      "['**/r/UnethicalLifeProTips**\\n\\nAn', 'Unethical', 'Life', 'Pro', 'Tip', '(or', 'ULPT)', 'is', 'a', 'tip', 'that', 'improves', 'your', 'life', 'in', 'a', 'meaningful', 'way,', 'perhaps', 'at', 'the', 'expense', 'of', 'others', 'and/or', 'with', 'questionable', 'legality.', 'Due', 'to', 'their', 'nature,', 'do', 'not', 'actually', 'follow', 'any', 'of', 'these', \"tips–they're\", 'just', 'for', 'fun.', 'Share', 'your', 'best', 'tips', \"you've\", 'picked', 'up', 'throughout', 'your', 'life,', 'and', 'learn', 'from', 'others!\\n\\n*****\\n^(Bot', 'created', 'by', '/u\\u200b', '/el_loke', '-', ')^[Feedback](http://www.reddit.com/message/compose/?to=el_loke&amp;subject=TrendingCommenterBot%20Feedback)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['runethicallifeprotips\\n\\nan', 'uneth', 'lif', 'pro', 'tip', 'ulpt', 'tip', 'improv', 'lif', 'mean', 'way', 'perhap', 'expens', 'oth', 'and', 'quest', 'leg', 'due', 'nat', 'act', 'follow', 'tipstheyr', 'fun', 'shar', 'best', 'tip', 'youv', 'pick', 'throughout', 'lif', 'learn', 'others\\n\\n\\nbot', 'cre', 'u', 'el_loke', 'feedbackhttpwwwredditcommessagecomposetoel_lokeampsubjecttrendingcommenterbot20feedback'], ['runethicallifeprotips\\n\\nan', 'unethical', 'life', 'pro', 'tip', 'ulpt', 'tip', 'improve', 'life', 'meaningful', 'way', 'perhaps', 'expense', 'others', 'andor', 'questionable', 'legality', 'due', 'nature', 'actually', 'follow', 'tipstheyre', 'fun', 'share', 'best', 'tip', 'youve', 'pick', 'throughout', 'life', 'learn', 'others\\n\\n\\nbot', 'create', 'u', 'el_loke', 'feedbackhttpwwwredditcommessagecomposetoel_lokeampsubjecttrendingcommenterbot20feedback'])\n",
      "original document: \n",
      "['ahh', 'if', 'only', 'your', 'presence', 'would', 'have', 'a', 'permanent', 'effect', 'lol!', 'Sadly', 'it', 'started', 'happening', 'again', 'tonight,', 'especially', 'during', 'fractals', '(with', 'only', 'a', 'temperature', 'of', '82)', 'so', 'i', 'have', 'no', 'idea', \"what's\", 'going', 'on', 'now', 'anymore.', 'So', 'weird', 'that', 'it', 'worked', 'great', 'the', 'entire', 'day,', 'until', 'tonight...', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahh', 'pres', 'would', 'perm', 'effect', 'lol', 'sad', 'start', 'hap', 'tonight', 'espec', 'fract', 'temp', 'eighty-two', 'ide', 'what', 'going', 'anym', 'weird', 'work', 'gre', 'entir', 'day', 'tonight'], ['ahh', 'presence', 'would', 'permanent', 'effect', 'lol', 'sadly', 'start', 'happen', 'tonight', 'especially', 'fractals', 'temperature', 'eighty-two', 'idea', 'whats', 'go', 'anymore', 'weird', 'work', 'great', 'entire', 'day', 'tonight'])\n",
      "original document: \n",
      "['John', 'the', 'Baptist?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['john', 'bapt'], ['john', 'baptist'])\n",
      "original document: \n",
      "['I', 'never', 'discouraged', 'the', 'US', 'helping.', 'Although,', 'administering', 'aid', 'is', 'a', 'two', 'way', 'street.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'disco', 'us', 'help', 'although', 'admin', 'aid', 'two', 'way', 'street'], ['never', 'discourage', 'us', 'help', 'although', 'administer', 'aid', 'two', 'way', 'street'])\n",
      "original document: \n",
      "['Awesome', 'thanks', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['awesom', 'thank'], ['awesome', 'thank'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Two', 'separate', 'people', 'have', 'emailed', 'me', 'this', 'article', 'and', 'asked', 'about', 'Brock...', 'people', 'who', 'I', \"wouldn't\", 'expect', 'to', 'even', 'care.', '', 'I', 'feel', 'like', \"I'm\", 'missing', 'something.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two', 'sep', 'peopl', 'email', 'artic', 'ask', 'brock', 'peopl', 'wouldnt', 'expect', 'ev', 'car', 'feel', 'lik', 'im', 'miss', 'someth'], ['two', 'separate', 'people', 'email', 'article', 'ask', 'brock', 'people', 'wouldnt', 'expect', 'even', 'care', 'feel', 'like', 'im', 'miss', 'something'])\n",
      "original document: \n",
      "['Dont', '@', 'him']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont'], ['dont'])\n",
      "original document: \n",
      "['Like', 'fuck', 'would', 'I', 'give', 'my', 'phone', 'to', 'a', '\"friend\"', 'like', 'OP!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'fuck', 'would', 'giv', 'phon', 'friend', 'lik', 'op'], ['like', 'fuck', 'would', 'give', 'phone', 'friend', 'like', 'op'])\n",
      "original document: \n",
      "['I', 'mean,', 'I', 'firmly', 'believe', 'if', 'you', 'expect', 'me', 'to', 'put', 'my', 'mouth', 'near', 'your', 'genitals,', 'common', 'courtesy', 'dictates', \"you'll\", 'reciprocate.', '\\n\\nBut', \"it's\", 'up', 'to', 'you.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'firm', 'believ', 'expect', 'put', 'mou', 'near', 'genit', 'common', 'courtesy', 'dict', 'youl', 'reciproc', '\\n\\nbut'], ['mean', 'firmly', 'believe', 'expect', 'put', 'mouth', 'near', 'genitals', 'common', 'courtesy', 'dictate', 'youll', 'reciprocate', '\\n\\nbut'])\n",
      "original document: \n",
      "['It', 'was', 'incidental', 'hand', 'contact.', 'If', 'you', 'look', 'at', 'the', 'replay,', 'Ricketts', 'hands', 'never', 'moved', 'at', 'all', 'or', 'intentionally', 'did', 'anything']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['incid', 'hand', 'contact', 'look', 'replay', 'ricket', 'hand', 'nev', 'mov', 'int', 'anyth'], ['incidental', 'hand', 'contact', 'look', 'replay', 'ricketts', 'hand', 'never', 'move', 'intentionally', 'anything'])\n",
      "original document: \n",
      "['i', 'have', 'both.', 'i', 'give', 'the', 'edge', 'to', 'the', 'pg1', 'as', 'i', 'need', 'cushion', 'in', 'the', 'forefoot', 'area.', \"They're\", 'also', 'a', 'bit', 'cheaper']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'edg', 'pg1', 'nee', 'cush', 'forefoot', 'are', 'theyr', 'also', 'bit', 'cheap'], ['give', 'edge', 'pg1', 'need', 'cushion', 'forefoot', 'area', 'theyre', 'also', 'bite', 'cheaper'])\n",
      "original document: \n",
      "[\"I'll\", 'give', 'you', 'the', 'face', 'is', 'cute', 'but', 'her', 'jorts', 'are', 'scraping', 'her', 'nipples.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'giv', 'fac', 'cut', 'jort', 'scraping', 'nippl'], ['ill', 'give', 'face', 'cute', 'jorts', 'scrap', 'nipples'])\n",
      "original document: \n",
      "['Love', 'my', 'OD-2r.', 'I', 'use', 'it', 'more', 'as', 'low', 'gain', 'filter', 'for', 'nailing', 'late', '80s', 'U2', 'tones', 'or', 'for', 'stacking', 'with', 'other', 'overdrives.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'od2r', 'us', 'low', 'gain', 'filt', 'nail', 'lat', '80s', 'u2', 'ton', 'stack', 'overdr'], ['love', 'od2r', 'use', 'low', 'gain', 'filter', 'nail', 'late', '80s', 'u2', 'tone', 'stack', 'overdrive'])\n",
      "original document: \n",
      "['You’ll', 'almost', 'always', 'see', 'a', 'reverse', 'variation', 'on', 'a', 'gender/age/job', 'based', 'question.\\n\\n“Men', 'of', 'Reddit...”\\n\\nFollowed', 'by\\n\\n“Women', 'of', 'Reddit...”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youl', 'almost', 'alway', 'see', 'revers', 'vary', 'genderagejob', 'bas', 'question\\n\\nm', 'reddit\\n\\nfollowed', 'by\\n\\nwomen', 'reddit'], ['youll', 'almost', 'always', 'see', 'reverse', 'variation', 'genderagejob', 'base', 'question\\n\\nmen', 'reddit\\n\\nfollowed', 'by\\n\\nwomen', 'reddit'])\n",
      "original document: \n",
      "['See', 'it', 'like', 'this,', 'if', 'you', 'touch', 'the', 'ground', 'outside', 'the', 'boundary', 'you', 'get', 'tagged,', 'and', 'you', \"can't\", 'take', 'catches', 'if', 'you', 'are', 'tagged,', 'only', 'way', 'to', 'untag', 'is', 'to', 'touch', 'the', 'ground', 'inside', 'the', 'boundary', 'again.', 'As', 'long', 'as', 'your', 'last', 'touch', 'with', 'the', 'ground', '(before', 'taking', 'the', 'catch)', 'is', 'inside', 'meaning', 'you', 'are', 'untagged,', 'your', 'catch', 'is', 'legal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'lik', 'touch', 'ground', 'outsid', 'bound', 'get', 'tag', 'cant', 'tak', 'catch', 'tag', 'way', 'unt', 'touch', 'ground', 'insid', 'bound', 'long', 'last', 'touch', 'ground', 'tak', 'catch', 'insid', 'mean', 'untag', 'catch', 'leg'], ['see', 'like', 'touch', 'grind', 'outside', 'boundary', 'get', 'tag', 'cant', 'take', 'catch', 'tag', 'way', 'untag', 'touch', 'grind', 'inside', 'boundary', 'long', 'last', 'touch', 'grind', 'take', 'catch', 'inside', 'mean', 'untagged', 'catch', 'legal'])\n",
      "original document: \n",
      "['Yeah', 'and', 'then', 'when', 'she', 'runs', 'up', 'on', 'Armin', 'it’s', 'like', 'oh', 'shit', 'this', 'mission', 'is', 'not', 'going', 'to', 'go', 'well']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'run', 'armin', 'lik', 'oh', 'shit', 'miss', 'going', 'go', 'wel'], ['yeah', 'run', 'armin', 'like', 'oh', 'shit', 'mission', 'go', 'go', 'well'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"Didn't\", 'realise', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'real'], ['didnt', 'realise'])\n",
      "original document: \n",
      "['Ball', 'boys', 'are', 'not', 'getting', 'as', 'much', 'of', 'a', 'workout', 'as', 'they', 'did', 'when', 'Auburn', \"didn't\", 'have', 'nets', 'behind', 'their', 'goal', 'posts.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bal', 'boy', 'get', 'much', 'workout', 'auburn', 'didnt', 'net', 'behind', 'goal', 'post'], ['ball', 'boys', 'get', 'much', 'workout', 'auburn', 'didnt', 'net', 'behind', 'goal', 'post'])\n",
      "original document: \n",
      "['I', 'have', 'no', 'idea', 'what', \"I'm\", 'feeling', 'from', 'it..', 'but', 'I', 'do', 'feel', 'more', 'calm', 'yet', 'more', 'energetic', 'at', 'the', 'same', 'time', 'so', 'I', 'guess', 'I', 'can', 'attribute', 'it', 'to', 'the', 'kratom', 'since', \"I'm\", 'just', 'taking', 'it', 'alongside', 'my', 'normal', 'daily', 'stack.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ide', 'im', 'feel', 'feel', 'calm', 'yet', 'energet', 'tim', 'guess', 'attribut', 'kratom', 'sint', 'im', 'tak', 'alongsid', 'norm', 'dai', 'stack'], ['idea', 'im', 'feel', 'feel', 'calm', 'yet', 'energetic', 'time', 'guess', 'attribute', 'kratom', 'since', 'im', 'take', 'alongside', 'normal', 'daily', 'stack'])\n",
      "original document: \n",
      "['Wait', 'a', 'minute,', 'this', 'seems', 'familiar', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'minut', 'seem', 'famili'], ['wait', 'minute', 'seem', 'familiar'])\n",
      "original document: \n",
      "['Finally,', 'a', 'quality', 'submission', 'that', 'I', 'feel', 'like', 'I', 'as', 'a', 'normal', 'person', 'should', 'be', 'able', 'to', 'do.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fin', 'qual', 'submit', 'feel', 'lik', 'norm', 'person', 'abl'], ['finally', 'quality', 'submission', 'feel', 'like', 'normal', 'person', 'able'])\n",
      "original document: \n",
      "['Sure', 'but', \"you're\", 'forgetting', 'Nigel', 'is', 'a', 'single', 'person', 'working', 'on', 'his', 'free', 'time', 'and', 'without', 'a', 'budget.\\n\\nI', 'would', 'love', 'to', 'see', 'what', 'he', 'could', 'come', 'up', 'with', 'if', 'he', 'was', 'to', 'redo', 'the', 'whole', 'official', 'app', 'with', 'a', 'proper', 'budget', 'and', 'a', 'small', 'team', 'to', 'help.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'yo', 'forget', 'nigel', 'singl', 'person', 'work', 'fre', 'tim', 'without', 'budget\\n\\ni', 'would', 'lov', 'see', 'could', 'com', 'redo', 'whol', 'off', 'ap', 'prop', 'budget', 'smal', 'team', 'help'], ['sure', 'youre', 'forget', 'nigel', 'single', 'person', 'work', 'free', 'time', 'without', 'budget\\n\\ni', 'would', 'love', 'see', 'could', 'come', 'redo', 'whole', 'official', 'app', 'proper', 'budget', 'small', 'team', 'help'])\n",
      "original document: \n",
      "['Mine', 'just', 'said', 'complete', 'for', 'a', 'couple', 'days', 'and', 'it', 'arrived.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['min', 'said', 'complet', 'coupl', 'day', 'ar'], ['mine', 'say', 'complete', 'couple', 'days', 'arrive'])\n",
      "original document: \n",
      "['Flash', 'pan', 'camera,', 'but', 'close', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flash', 'pan', 'camer', 'clos'], ['flash', 'pan', 'camera', 'close'])\n",
      "original document: \n",
      "['143418575|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', '7vmrTfB3)\\n\\n&gt;&gt;143418344\\nFucking', 'boomers...\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, five hundred and seventy-fiv', 'gt', 'unit', 'stat', 'anonym', 'id', '7vmrtfb3\\n\\ngtgt143418344\\nfucking', 'boomers\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, five hundred and seventy-five', 'gt', 'unite', 'state', 'anonymous', 'id', '7vmrtfb3\\n\\ngtgt143418344\\nfucking', 'boomers\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"That's\", 'fair', 'enough']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fair', 'enough'], ['thats', 'fair', 'enough'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Uh', 'they', 'went', 'to', 'the', 'doctor', 'to', 'get', 'the', 'baby', 'extracted']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uh', 'went', 'doct', 'get', 'baby', 'extract'], ['uh', 'go', 'doctor', 'get', 'baby', 'extract'])\n",
      "original document: \n",
      "['Something', 'something', '*\"all', 'feel', 'gay', 'whe', 'Johnny', 'comes', 'marching', 'home.\"*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someth', 'someth', 'feel', 'gay', 'whe', 'johnny', 'com', 'march', 'hom'], ['something', 'something', 'feel', 'gay', 'whe', 'johnny', 'come', 'march', 'home'])\n",
      "original document: \n",
      "['I', 'remember', 'those.', 'I', 'was', 'like', '7', 'though', 'when', 'i', 'used', 'them.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'lik', 'sev', 'though', 'us'], ['remember', 'like', 'seven', 'though', 'use'])\n",
      "original document: \n",
      "['Mobil', 'is', 'an', 'oil', 'company,', \"it's\", 'most', 'likely', 'a', 'subtle', 'commentary', 'on', 'the', 'Tyranny', 'of', 'America', 'outlined', 'in', 'Drag', 'Queen.', '\\n\\nBasically', 'saying', 'the', 'government', 'is', 'profiting', 'from', 'war', '(with', 'oil)', 'if', 'anything', 'he', 'is', 'wearing', 'it', 'ironically.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mobl', 'oil', 'company', 'lik', 'subtl', 'com', 'tyranny', 'americ', 'outlin', 'drag', 'queen', '\\n\\nbasically', 'say', 'govern', 'profit', 'war', 'oil', 'anyth', 'wear', 'iron'], ['mobil', 'oil', 'company', 'likely', 'subtle', 'commentary', 'tyranny', 'america', 'outline', 'drag', 'queen', '\\n\\nbasically', 'say', 'government', 'profit', 'war', 'oil', 'anything', 'wear', 'ironically'])\n",
      "original document: \n",
      "['Cosquillas', 'en', 'el', 'ano.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cosquilla', 'en', 'el', 'ano'], ['cosquillas', 'en', 'el', 'ano'])\n",
      "original document: \n",
      "['\"No', 'need', 'for', 'formalities', 'when', \"you're\", 'in', 'the', 'wrong', 'place', 'at', 'the', 'wrong', 'time.\"\\n\\n*Joyce', 'has', 'her', 'pistol', 'out.', 'Since', 'Sturge', 'is', 'behind', 'cover,', \"she's\", 'trying', 'to', 'move', 'to', 'where', 'he', 'is.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'form', 'yo', 'wrong', 'plac', 'wrong', 'time\\n\\njoyc', 'pistol', 'sint', 'sturg', 'behind', 'cov', 'she', 'try', 'mov'], ['need', 'formalities', 'youre', 'wrong', 'place', 'wrong', 'time\\n\\njoyce', 'pistol', 'since', 'sturge', 'behind', 'cover', 'shes', 'try', 'move'])\n",
      "original document: \n",
      "['You', 'should', 'start', '/r/JunkieProTips.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'rjunkieprotip'], ['start', 'rjunkieprotips'])\n",
      "original document: \n",
      "[\"That's\", 'great', 'to', 'hear.', 'I', 'also', 'miss', 'it', 'quite', 'a', 'lot.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'gre', 'hear', 'also', 'miss', 'quit', 'lot'], ['thats', 'great', 'hear', 'also', 'miss', 'quite', 'lot'])\n",
      "original document: \n",
      "['This', 'game', 'is', 'shit,', '', '\\nThe', 'servers', 'need', 'improvement,', '', '\\n\"A', 'system', 'error', 'occurred', '', '\\nduring', 'event', 'movement.\"\\n\\n\\\\-', 'Anonymous']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gam', 'shit', '\\nthe', 'serv', 'nee', 'improv', '\\na', 'system', 'er', 'occur', '\\nduring', 'ev', 'movement\\n\\n', 'anonym'], ['game', 'shit', '\\nthe', 'servers', 'need', 'improvement', '\\na', 'system', 'error', 'occur', '\\nduring', 'event', 'movement\\n\\n', 'anonymous'])\n",
      "original document: \n",
      "['This.', '', 'I', 'got', 'it', 'on', 'sale', 'with', 'the', 'throttle', 'for', '30$.', '', 'Amazing', 'value', 'for', 'that', 'money.', '', 'Got', 'it', 'to', 'play', 'Elite:', 'Dangerous.', '', 'Funny', 'story,', 'my', 'friend', 'dropped', '300$', 'on', 'a', 'high', 'end', 'HOTAS', 'n', 'throttle', 'but', 'in', 'the', 'end', 'it', 'was', 'so', 'heavy', 'there', 'was', 'no', 'way', 'he', 'could', 'whip', 'the', 'stick', 'around', 'fast', 'enough', 'or', 'pull/push', 'the', 'throttle', 'fast', 'enough', 'to', 'be', 'effective', 'in', 'combat.', '', 'At', '30$', 'I', 'could', 'swing', 'the', 'thing', 'all', 'over', 'the', 'place', 'without', 'worrying', 'about', 'breaking', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'sal', 'throttle', 'thirty', 'amaz', 'valu', 'money', 'got', 'play', 'elit', 'dang', 'funny', 'story', 'friend', 'drop', 'three hundred', 'high', 'end', 'hota', 'n', 'throttle', 'end', 'heavy', 'way', 'could', 'whip', 'stick', 'around', 'fast', 'enough', 'pullpush', 'throttle', 'fast', 'enough', 'effect', 'comb', 'thirty', 'could', 'swing', 'thing', 'plac', 'without', 'worry', 'break'], ['get', 'sale', 'throttle', 'thirty', 'amaze', 'value', 'money', 'get', 'play', 'elite', 'dangerous', 'funny', 'story', 'friend', 'drop', 'three hundred', 'high', 'end', 'hotas', 'n', 'throttle', 'end', 'heavy', 'way', 'could', 'whip', 'stick', 'around', 'fast', 'enough', 'pullpush', 'throttle', 'fast', 'enough', 'effective', 'combat', 'thirty', 'could', 'swing', 'thing', 'place', 'without', 'worry', 'break'])\n",
      "original document: \n",
      "['[+Fetus-P](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnot0o8/):\\n\\nI', 'think', 'people', 'are', 'overusing', 'trust', 'fund,', 'but', 'you', 'seem', 'to', 'be', 'blessed', 'with', 'a', 'nicer', 'upraising', 'than', 'a', 'good', 'chunk', 'of', 'people.', 'That', 'is', 'why', 'they', 'are', 'being', 'like', 'that.', '\\n\\nNot', 'to', 'many', 'people', 'can', 'get', 'their', 'folks', 'to', 'give', 'them', '4k', 'without', 'wanting', 'to', 'be', 'paid', 'back.\\n\\nI', \"wouldn't\", 'be', 'offended', 'by', 'it,', \"I'd\", 'be', 'proud', 'that', 'my', 'family', 'did', 'well', 'and', 'are', 'supportive']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fetusphttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnot0o8\\n\\ni', 'think', 'peopl', 'overus', 'trust', 'fund', 'seem', 'bless', 'nic', 'upra', 'good', 'chunk', 'peopl', 'lik', '\\n\\nnot', 'many', 'peopl', 'get', 'folk', 'giv', '4k', 'without', 'want', 'paid', 'back\\n\\ni', 'wouldnt', 'offend', 'id', 'proud', 'famy', 'wel', 'support'], ['fetusphttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnot0o8\\n\\ni', 'think', 'people', 'overuse', 'trust', 'fund', 'seem', 'bless', 'nicer', 'upraise', 'good', 'chunk', 'people', 'like', '\\n\\nnot', 'many', 'people', 'get', 'folks', 'give', '4k', 'without', 'want', 'pay', 'back\\n\\ni', 'wouldnt', 'offend', 'id', 'proud', 'family', 'well', 'supportive'])\n",
      "original document: \n",
      "['Navy', 'Federal:', '\"Good', 'morning', 'Lance', 'Corporal!\"\\n\\nWeirds', 'me', 'out.', '', 'No', 'one', 'ever', 'addresses', 'me', 'by', 'rank.', '', \"It's\", 'always', 'just', '\"Hey', 'radio!', '', 'Get', 'over', 'here!\"\\n\\nEdit:', 'drunken', 'typo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['navy', 'fed', 'good', 'morn', 'lant', 'corporal\\n\\nweirds', 'on', 'ev', 'address', 'rank', 'alway', 'hey', 'radio', 'get', 'here\\n\\nedit', 'drunk', 'typo'], ['navy', 'federal', 'good', 'morning', 'lance', 'corporal\\n\\nweirds', 'one', 'ever', 'address', 'rank', 'always', 'hey', 'radio', 'get', 'here\\n\\nedit', 'drunken', 'typo'])\n",
      "original document: \n",
      "['I', 'reinstalled', 'the', 'os', 'and', 'set', 'a', 'static', 'ip', 'but', 'how', 'do', 'i', 'restart', 'the', 'service?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reinstal', 'os', 'set', 'stat', 'ip', 'restart', 'serv'], ['reinstall', 'os', 'set', 'static', 'ip', 'restart', 'service'])\n",
      "original document: \n",
      "['I', 'am', 'suspicious', 'too.', 'Lay', 'low', 'until', 'she', 'presents', 'you', 'with', 'an', 'actual', 'baby', 'and', 'then', 'ask', 'for', 'a', 'paternity', 'test.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suspicy', 'lay', 'low', 'pres', 'act', 'baby', 'ask', 'patern', 'test'], ['suspicious', 'lay', 'low', 'present', 'actual', 'baby', 'ask', 'paternity', 'test'])\n",
      "original document: \n",
      "['I', 'just', 'beat', 'him.', 'I', 'ended', 'up', 'using', 'the', 'blue', 'bouncy', 'ball,', 'the', 'pink', 'shotgun,', 'the', 'smoke', 'grenade', '(for', 'the', 'dash)', 'and', 'the', 'super', '1.', '\\nThe', 'blue', 'bouncer', 'murders', 'his', 'first', 'form', 'and', 'with', 'the', 'smoke', 'dash,', 'you', 'can', 'dash', 'through', 'him', 'as', 'he', 'charges', 'so', 'you', \"don't\", 'need', 'to', 'worry', 'about', 'the', 'ducks.', '\\n\\nSwitch', 'to', 'the', 'shotgun', 'for', 'the', 'second', 'part.', 'That', 'murders', 'the', 'dogs.', 'The', 'key', 'hear', 'is', 'to', 'kill', 'this', 'part', 'right', 'AFTER', 'the', 'second', 'train', 'so', 'you', \"don't\", 'have', 'to', 'worry', 'about', 'that', 'when', 'his', 'third', 'form', 'appears.', '\\n\\nFor', 'the', 'third', 'you', 'get', 'under', 'him', 'and', 'use', 'the', 'blue', 'lob', 'shot.', 'You', 'can', 'dash', 'through', 'his', 'green', 'horse', 'shoe', 'attacks', 'if', 'you', 'happen', 'to', 'be', 'in', 'front', 'of', 'him.', '\\n\\nFor', 'the', 'last', 'form', 'use', 'the', 'blue', 'lobber.', 'Also', 'use', 'your', 'super', 'attack', 'right', 'as', 'the', 'teacups', 'are', 'descending.', 'Stay', 'on', 'the', 'teacup', 'and', 'shoot.', 'Once', 'he', 'spawns', 'those', 'green', 'guys', 'drop', 'to', 'the', 'middle', 'and', 'kill', 'a', 'couple', 'and', 'hop', 'back', 'up', 'to', 'the', 'teacups', 'and', 'let', 'the', 'train', 'kill', 'the', 'other', 'two.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['beat', 'end', 'us', 'blu', 'bount', 'bal', 'pink', 'shotgun', 'smok', 'grenad', 'dash', 'sup', 'on', '\\nthe', 'blu', 'bount', 'murd', 'first', 'form', 'smok', 'dash', 'dash', 'charg', 'dont', 'nee', 'worry', 'duck', '\\n\\nswitch', 'shotgun', 'second', 'part', 'murd', 'dog', 'key', 'hear', 'kil', 'part', 'right', 'second', 'train', 'dont', 'worry', 'third', 'form', 'appear', '\\n\\nfor', 'third', 'get', 'us', 'blu', 'lob', 'shot', 'dash', 'green', 'hors', 'sho', 'attack', 'hap', 'front', '\\n\\nfor', 'last', 'form', 'us', 'blu', 'lob', 'also', 'us', 'sup', 'attack', 'right', 'teacup', 'descend', 'stay', 'teacup', 'shoot', 'spawn', 'green', 'guy', 'drop', 'middl', 'kil', 'coupl', 'hop', 'back', 'teacup', 'let', 'train', 'kil', 'two'], ['beat', 'end', 'use', 'blue', 'bouncy', 'ball', 'pink', 'shotgun', 'smoke', 'grenade', 'dash', 'super', 'one', '\\nthe', 'blue', 'bouncer', 'murder', 'first', 'form', 'smoke', 'dash', 'dash', 'charge', 'dont', 'need', 'worry', 'duck', '\\n\\nswitch', 'shotgun', 'second', 'part', 'murder', 'dog', 'key', 'hear', 'kill', 'part', 'right', 'second', 'train', 'dont', 'worry', 'third', 'form', 'appear', '\\n\\nfor', 'third', 'get', 'use', 'blue', 'lob', 'shoot', 'dash', 'green', 'horse', 'shoe', 'attack', 'happen', 'front', '\\n\\nfor', 'last', 'form', 'use', 'blue', 'lobber', 'also', 'use', 'super', 'attack', 'right', 'teacups', 'descend', 'stay', 'teacup', 'shoot', 'spawn', 'green', 'guy', 'drop', 'middle', 'kill', 'couple', 'hop', 'back', 'teacups', 'let', 'train', 'kill', 'two'])\n",
      "original document: \n",
      "['For', 'some', 'reason', 'I', 'read', 'the', 'heading', 'as', 'Ferris', 'Buellers', 'Day', 'Off.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'read', 'head', 'fer', 'buel', 'day'], ['reason', 'read', 'head', 'ferris', 'buellers', 'day'])\n",
      "original document: \n",
      "['This', 'is', 'really', 'pretty.', 'You', 'did', 'a', 'wonderful', 'job.', 'Keep', 'posting!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'pretty', 'wond', 'job', 'keep', 'post'], ['really', 'pretty', 'wonderful', 'job', 'keep', 'post'])\n",
      "original document: \n",
      "['I’ve', 'seen', 'Corelle', 'plates', 'in', 'different', 'patterns,', 'sizes', 'and', 'quantities', 'at', 'stores', 'like', 'Walmart,', 'Target', 'and', 'even', 'our', 'local', 'grocery', 'store', '(Publix).', '\\nI’ve', 'also', 'had', 'great', 'luck', 'with', 'stores', 'like', 'CB2', 'and', 'Crate', '&amp;', 'Barrel', '(they', 'sell', 'them', 'individually).', 'Not', 'sure', 'where', 'you’re', 'located,', 'but', 'I', 'also', 'liked', 'Fishes', 'Eddy', '&amp;', 'Anthropologie', '', 'for', 'individual', 'plates', 'as', 'well.', 'If', 'you', 'like', 'turquoise,', 'now', 'should', 'be', 'a', 'good', 'time', 'to', 'get', 'them', 'since', 'it’s', 'more', 'of', 'a', 'summer', 'color', 'and', 'will', 'probably', 'be', 'on', 'sale.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'seen', 'corel', 'plat', 'diff', 'pattern', 'siz', 'quant', 'stor', 'lik', 'walmart', 'target', 'ev', 'loc', 'grocery', 'stor', 'publix', '\\nive', 'also', 'gre', 'luck', 'stor', 'lik', 'cb2', 'crat', 'amp', 'barrel', 'sel', 'individ', 'sur', 'yo', 'loc', 'also', 'lik', 'fish', 'eddy', 'amp', 'anthropolog', 'individ', 'plat', 'wel', 'lik', 'turquo', 'good', 'tim', 'get', 'sint', 'sum', 'col', 'prob', 'sal'], ['ive', 'see', 'corelle', 'plat', 'different', 'pattern', 'size', 'quantities', 'store', 'like', 'walmart', 'target', 'even', 'local', 'grocery', 'store', 'publix', '\\nive', 'also', 'great', 'luck', 'store', 'like', 'cb2', 'crate', 'amp', 'barrel', 'sell', 'individually', 'sure', 'youre', 'locate', 'also', 'like', 'fish', 'eddy', 'amp', 'anthropologie', 'individual', 'plat', 'well', 'like', 'turquoise', 'good', 'time', 'get', 'since', 'summer', 'color', 'probably', 'sale'])\n",
      "original document: \n",
      "['I', 'know', 'someone', 'who', 'has', 'had', 'an', 'iPhone', '7', 'for', 'less', 'than', 'a', 'year.', 'The', 'glass', 'was', 'extremely', 'scratched', 'and', 'even', 'slightly', 'cracked,', 'and', 'the', 'matte', 'black', 'coating', 'was', 'rubbing', 'off.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'someon', 'iphon', 'sev', 'less', 'year', 'glass', 'extrem', 'scratched', 'ev', 'slight', 'crack', 'mat', 'black', 'coat', 'rub'], ['know', 'someone', 'iphone', 'seven', 'less', 'year', 'glass', 'extremely', 'scratch', 'even', 'slightly', 'crack', 'matte', 'black', 'coat', 'rub'])\n",
      "original document: \n",
      "['I', 'am', 'sure', 'there', 'is', 'a', 'conspiracy', 'here.', 'I', 'just', \"can't\", 'locate', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'conspir', 'cant', 'loc'], ['sure', 'conspiracy', 'cant', 'locate'])\n",
      "original document: \n",
      "['You', 'are', 'assuming', 'a', 'lot.', '', 'The', 'fact', 'is', 'I', 'have', 'played', 'organized', 'football,', 'as', 'well', 'as', 'other', 'sports.', '', \"I've\", 'also', 'been', 'a', 'coach', 'for', 'the', 'past', 'few', 'decades.', '', 'These', 'boys', \"didn't\", 'go', 'out', 'drinking', 'the', 'night', 'before', 'the', 'game,', 'they', \"didn't\", 'celebrate', 'in', 'the', 'endzone,', 'they', \"didn't\", 'get', 'into', 'a', 'fight...', '', 'They', 'wanted', 'to', 'bring', 'attention', 'to', 'an', 'issue', 'even', 'though', 'they', 'were', 'warned.', '', 'To', 'me,', 'they', 'believed', 'in', 'what', 'they', 'did', 'and', 'were', 'willing', 'to', 'suffer', 'the', 'consequences', 'even', 'if', 'those', 'consequences', 'were', 'because', 'their', 'coach', 'felt', 'his', 'beliefs', 'were', 'more', 'important', 'than', 'theirs.', '', 'As', 'a', 'coach,', \"I'm\", 'not', 'a', 'huge', 'advocate', 'of', 'those', 'kneeling', 'during', 'the', 'anthem,', 'but', 'I', 'also', 'respect', 'their', 'right', 'to', 'do', 'it.', '', 'The', 'right', 'to', 'a', 'peaceful', 'protest', 'is', 'what', 'makes', 'this', 'country', 'great.', '', \"It's\", 'small', 'and', 'petty', 'to', 'abuse', 'your', 'power', 'as', 'a', 'coach', 'to', 'punish', 'players', 'for', 'something', 'like', 'this.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['assum', 'lot', 'fact', 'play', 'org', 'footbal', 'wel', 'sport', 'iv', 'also', 'coach', 'past', 'decad', 'boy', 'didnt', 'go', 'drink', 'night', 'gam', 'didnt', 'celebr', 'endzon', 'didnt', 'get', 'fight', 'want', 'bring', 'at', 'issu', 'ev', 'though', 'warn', 'believ', 'wil', 'suff', 'consequ', 'ev', 'consequ', 'coach', 'felt', 'believ', 'import', 'coach', 'im', 'hug', 'advoc', 'kneel', 'anthem', 'also', 'respect', 'right', 'right', 'peac', 'protest', 'mak', 'country', 'gre', 'smal', 'petty', 'abus', 'pow', 'coach', 'pun', 'play', 'someth', 'lik'], ['assume', 'lot', 'fact', 'play', 'organize', 'football', 'well', 'sport', 'ive', 'also', 'coach', 'past', 'decades', 'boys', 'didnt', 'go', 'drink', 'night', 'game', 'didnt', 'celebrate', 'endzone', 'didnt', 'get', 'fight', 'want', 'bring', 'attention', 'issue', 'even', 'though', 'warn', 'believe', 'will', 'suffer', 'consequences', 'even', 'consequences', 'coach', 'felt', 'beliefs', 'important', 'coach', 'im', 'huge', 'advocate', 'kneel', 'anthem', 'also', 'respect', 'right', 'right', 'peaceful', 'protest', 'make', 'country', 'great', 'small', 'petty', 'abuse', 'power', 'coach', 'punish', 'players', 'something', 'like'])\n",
      "original document: \n",
      "['i', 'just', 'want', 'to', 'play', 'lag-free,', 'crash-free......']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'play', 'lagfr', 'crashfr'], ['want', 'play', 'lagfree', 'crashfree'])\n",
      "original document: \n",
      "['Wouldn’t', 'it', 'be', 'better', 'to', 'transfer', 'SPG', 'to', 'Marriott', 'for', '1:3', 'rate?', '\\n\\nBut', 'thank', 'you!', 'I’ll', 'just', 'go', 'with', 'these', 'two', 'then', ':)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'bet', 'transf', 'spg', 'marriot', 'thirteen', 'rat', '\\n\\nbut', 'thank', 'il', 'go', 'two'], ['wouldnt', 'better', 'transfer', 'spg', 'marriott', 'thirteen', 'rate', '\\n\\nbut', 'thank', 'ill', 'go', 'two'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Definitely', 'should', 'have', 'shown', 'more', 'of', 'Houli', 'when', 'I', 'was', 'trying', 'to', 'explain', 'Aussie', 'Rules', 'to', 'locals', 'in', 'the', 'Middle', 'East', 'haha.', '\\n\\nBut', \"it's\", 'great', 'to', 'see.', 'This', 'game', 'knows', 'no', 'bounds', 'for', 'who', 'they', 'want', 'to', 'participate,', 'and', 'that', 'is', 'fantastic.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'shown', 'houl', 'try', 'explain', 'aussy', 'rul', 'loc', 'middl', 'east', 'hah', '\\n\\nbut', 'gre', 'see', 'gam', 'know', 'bound', 'want', 'particip', 'fantast'], ['definitely', 'show', 'houli', 'try', 'explain', 'aussie', 'rule', 'locals', 'middle', 'east', 'haha', '\\n\\nbut', 'great', 'see', 'game', 'know', 'bound', 'want', 'participate', 'fantastic'])\n",
      "original document: \n",
      "['it’s', 'too', 'cold\\n\\nwhy', 'was', 'i', 'downvoted']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cold\\n\\nwhy', 'downvot'], ['cold\\n\\nwhy', 'downvoted'])\n",
      "original document: \n",
      "['Yep.', 'You', 'guys', 'get', 'to', 'play', 'us', 'in', 'a', 'tiny', 'arena', 'that', \"isn't\", 'on', 'our', 'campus!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'guy', 'get', 'play', 'us', 'tiny', 'aren', 'isnt', 'camp'], ['yep', 'guy', 'get', 'play', 'us', 'tiny', 'arena', 'isnt', 'campus'])\n",
      "original document: \n",
      "['Uncontrolled', 'Substance', 'is', 'dope', 'too,', 'Inspectah', 'Deck', 'tends', 'to', 'fly', 'under', 'the', 'radar', 'but', 'he’s', 'got', 'some', 'of', 'the', 'dopest', 'rhymes', 'out', 'of', 'the', 'group.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uncontrol', 'subst', 'dop', 'inspectah', 'deck', 'tend', 'fly', 'rad', 'hes', 'got', 'dopest', 'rhym', 'group'], ['uncontrolled', 'substance', 'dope', 'inspectah', 'deck', 'tend', 'fly', 'radar', 'hes', 'get', 'dopest', 'rhyme', 'group'])\n",
      "original document: \n",
      "['Je', 'mate', 'du', 'College', 'Football', \"jusqu'au\", 'bout', 'de', 'la', 'nuit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['je', 'mat', 'du', 'colleg', 'footbal', 'jusquau', 'bout', 'de', 'la', 'nuit'], ['je', 'mate', 'du', 'college', 'football', 'jusquau', 'bout', 'de', 'la', 'nuit'])\n",
      "original document: \n",
      "['Thanks!', 'I', 'briefly', 'looked', 'into', 'accordion', 'tables', 'earlier,', 'and', 'also', 'noticed', 'that', \"it's\", 'possible', 'to', 'insert', 'them', 'in', 'Dreamweaver.', 'Is', 'it', 'a', 'good', 'idea', 'to', 'do', 'that,', 'then', 'modify', 'the', 'code', 'or', 'should', 'I', 'start', 'with', 'some', 'other', 'sample', 'code?\\n\\nAlso', '-', 'an', 'issue', 'I', 'had', 'when', 'playing', 'around', 'with', 'accordion', 'tables', 'was', 'that', 'I', \"couldn't\", 'collapse', 'all', 'panels', 'simultaneously,', 'and', 'was', 'forced', 'to', 'have', 'exactly', 'one', 'expanded', 'at', 'all', 'times.', 'Can', 'this', 'behaviour', 'be', 'changed', 'so', 'that', 'they', 'all', 'start', 'collapsed', 'and', 'only', 'expand/collapse', 'when', 'clicked?\\n\\nWould', 'it', 'be', 'reasonable', 'to', 'put', 'an', 'accordion', 'table', 'inside', 'another?', 'Cause', 'it', 'seems', 'to', 'me', 'like', \"I'd\", 'have', 'to', 'to', 'achieve', 'two', 'levels', 'as', 'intended.', 'While', \"we're\", 'at', 'it,', 'would', 'it', 'be', 'possible', 'to', 'dynamically', 'add', 'panels', 'to', 'both', 'levels', 'based', 'on', 'the', 'imported', 'data', '(mentioned', 'in', 'point', '3', 'in', 'the', 'OP)?\\n\\nLastly;', 'how', 'flexible', 'are', 'accordion', 'tables', 'when', 'it', 'comes', 'to', 'styling?', \"I'm\", 'a', 'little', 'picky', 'when', 'it', 'comes', 'to', 'design.\\n\\nDamn,', 'that', 'turned', 'out', 'a', 'wall', 'of', 'questions.', \"Don't\", 'feel', 'obligated', 'to', 'answer', 'them', 'all!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'brief', 'look', 'accord', 'tabl', 'ear', 'also', 'not', 'poss', 'insert', 'dreamweav', 'good', 'ide', 'mod', 'cod', 'start', 'sampl', 'code\\n\\nalso', 'issu', 'play', 'around', 'accord', 'tabl', 'couldnt', 'collaps', 'panel', 'simult', 'forc', 'exact', 'on', 'expand', 'tim', 'behavio', 'chang', 'start', 'collaps', 'expandcollaps', 'clicked\\n\\nwould', 'reason', 'put', 'accord', 'tabl', 'insid', 'anoth', 'caus', 'seem', 'lik', 'id', 'achiev', 'two', 'level', 'intend', 'would', 'poss', 'dynam', 'ad', 'panel', 'level', 'bas', 'import', 'dat', 'ment', 'point', 'three', 'op\\n\\nlastly', 'flex', 'accord', 'tabl', 'com', 'styl', 'im', 'littl', 'picky', 'com', 'design\\n\\ndamn', 'turn', 'wal', 'quest', 'dont', 'feel', 'oblig', 'answ'], ['thank', 'briefly', 'look', 'accordion', 'table', 'earlier', 'also', 'notice', 'possible', 'insert', 'dreamweaver', 'good', 'idea', 'modify', 'code', 'start', 'sample', 'code\\n\\nalso', 'issue', 'play', 'around', 'accordion', 'table', 'couldnt', 'collapse', 'panel', 'simultaneously', 'force', 'exactly', 'one', 'expand', 'time', 'behaviour', 'change', 'start', 'collapse', 'expandcollapse', 'clicked\\n\\nwould', 'reasonable', 'put', 'accordion', 'table', 'inside', 'another', 'cause', 'seem', 'like', 'id', 'achieve', 'two', 'level', 'intend', 'would', 'possible', 'dynamically', 'add', 'panel', 'level', 'base', 'import', 'data', 'mention', 'point', 'three', 'op\\n\\nlastly', 'flexible', 'accordion', 'table', 'come', 'style', 'im', 'little', 'picky', 'come', 'design\\n\\ndamn', 'turn', 'wall', 'question', 'dont', 'feel', 'obligate', 'answer'])\n",
      "original document: \n",
      "['From', 'the', 'same', 'guy', 'too', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guy', 'lol'], ['guy', 'lol'])\n",
      "original document: \n",
      "['So...', '[John', 'Cena,', 'Paladin', 'of', 'the', 'Cenation?](https://www.youtube.com/watch?v=ui9egS_0PQ8)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['john', 'cen', 'paladin', 'cenationhttpswwwyoutubecomwatchvui9egs_0pq8'], ['john', 'cena', 'paladin', 'cenationhttpswwwyoutubecomwatchvui9egs_0pq8'])\n",
      "original document: \n",
      "['It', 'is,', 'yes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['It', 'weighs', '2', 'pounds', 'is', 'made', 'of', 'electronic', 'grade', 'aluminum.', 'It', 'screws', 'apart', 'and', 'is', 'a', 'tubular', 'cylinder.', 'It', 'was', 'in', 'a', '\"mixed', 'lot\"', 'box', 'of', 'items', 'from', 'an', 'industrial', 'auction.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weigh', 'two', 'pound', 'mad', 'electron', 'grad', 'alumin', 'screws', 'apart', 'tubul', 'cylind', 'mix', 'lot', 'box', 'item', 'indust', 'auct'], ['weigh', 'two', 'pound', 'make', 'electronic', 'grade', 'aluminum', 'screw', 'apart', 'tubular', 'cylinder', 'mix', 'lot', 'box', 'items', 'industrial', 'auction'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'everything', 'in', 'this', 'photo', 'was', 'planned', 'out.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'everyth', 'photo', 'plan'], ['feel', 'like', 'everything', 'photo', 'plan'])\n",
      "original document: \n",
      "['That’s', 'fine.', 'You’re', 'not', 'president,', 'though', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fin', 'yo', 'presid', 'though'], ['thats', 'fine', 'youre', 'president', 'though'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['it', 'needs', 'to', 'be', 'someone', 'with', 'a', 'KWS']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'someon', 'kws'], ['need', 'someone', 'kws'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnow70m/):\\n\\nNot', 'trust', 'fund,', 'but', 'I', 'was', 'blessed', 'with', 'a', 'nice', 'upraising.', \"I'm\", 'glad', \"they're\", 'supportive', 'of', 'me', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnow70m\\n\\nnot', 'trust', 'fund', 'bless', 'nic', 'upra', 'im', 'glad', 'theyr', 'support'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnow70m\\n\\nnot', 'trust', 'fund', 'bless', 'nice', 'upraise', 'im', 'glad', 'theyre', 'supportive'])\n",
      "original document: \n",
      "[\"He's\", 'A', 'POS', 'apparently', 'according', 'to', 'his', 'shoulder', 'patch.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'pos', 'app', 'accord', 'should', 'patch'], ['hes', 'pos', 'apparently', 'accord', 'shoulder', 'patch'])\n",
      "original document: \n",
      "['I', \"wouldn't\", 'hold', 'out', 'much', 'hope', 'this', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'hold', 'much', 'hop', 'gam'], ['wouldnt', 'hold', 'much', 'hope', 'game'])\n",
      "original document: \n",
      "['huh.', '', \"didn't\", 'know', 'that.', '', 'thanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['huh', 'didnt', 'know', 'thank'], ['huh', 'didnt', 'know', 'thank'])\n",
      "original document: \n",
      "['Now', \"that's\", 'a', 'sexy', 'pic']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'sexy', 'pic'], ['thats', 'sexy', 'pic'])\n",
      "original document: \n",
      "['Santa']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sant'], ['santa'])\n",
      "original document: \n",
      "['I', 'could', 'see', 'this', 'being', 'useful', 'in', 'the', 'city,', 'or', 'at', 'the', 'very', 'least', 'a', 'mall.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'see', 'us', 'city', 'least', 'mal'], ['could', 'see', 'useful', 'city', 'least', 'mall'])\n",
      "original document: \n",
      "['She', 'might', '‘be', 'the', 'night!’,', 'but', 'she', 'is', 'also', 'an', 'elaborate', 'bin', 'bag.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['might', 'night', 'also', 'elab', 'bin', 'bag'], ['might', 'night', 'also', 'elaborate', 'bin', 'bag'])\n",
      "original document: \n",
      "['Right', 'I', 'find', 'trafficking', 'children', 'to', 'be', 'the', 'most', 'heinous', 'crime', 'to', 'be', 'committed', '', 'and', 'yet', 'the', 'CIA', 'is', 'involved.', 'WOO', 'HOO!!!!!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'find', 'traffick', 'childr', 'hein', 'crim', 'commit', 'yet', 'cia', 'involv', 'woo', 'hoo'], ['right', 'find', 'traffic', 'children', 'heinous', 'crime', 'commit', 'yet', 'cia', 'involve', 'woo', 'hoo'])\n",
      "original document: \n",
      "['Eh', 'I', \"wouldn't,\", 'I', \"don't\", 'think', 'either', 'Big', 'Ben', 'or', 'Dak', 'are', 'matchup-proof', 'and', 'would', 'keep', 'both', 'and', 'play', 'the', 'best', 'matchup.', \"I'd\", 'only', 'do', 'it', 'if', \"you're\", 'really', 'weak', 'in', 'one', 'position', 'and', 'are', 'looking', 'for', 'a', 'lotto', 'ticket', 'in', 'that', 'position']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eh', 'wouldnt', 'dont', 'think', 'eith', 'big', 'ben', 'dak', 'matchupproof', 'would', 'keep', 'play', 'best', 'matchup', 'id', 'yo', 'real', 'weak', 'on', 'posit', 'look', 'lotto', 'ticket', 'posit'], ['eh', 'wouldnt', 'dont', 'think', 'either', 'big', 'ben', 'dak', 'matchupproof', 'would', 'keep', 'play', 'best', 'matchup', 'id', 'youre', 'really', 'weak', 'one', 'position', 'look', 'lotto', 'ticket', 'position'])\n",
      "original document: \n",
      "['Classic', 'Halo', '3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['class', 'halo', 'three'], ['classic', 'halo', 'three'])\n",
      "original document: \n",
      "['While', 'I', 'agree,', \"we've\", 'gotta', 'start', 'somewhere', 'lol.', \"It's\", 'clear', 'that', 'Vince', \"isn't\", 'going', 'to', 'change', 'the', 'main', 'product,', 'so', 'until', 'he', 'goes,', 'what', 'can', 'we', 'hope', 'for?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'wev', 'gott', 'start', 'somewh', 'lol', 'clear', 'vint', 'isnt', 'going', 'chang', 'main', 'produc', 'goe', 'hop'], ['agree', 'weve', 'gotta', 'start', 'somewhere', 'lol', 'clear', 'vince', 'isnt', 'go', 'change', 'main', 'product', 'go', 'hope'])\n",
      "original document: \n",
      "['Are', 'you', 'internet', 'explorer?\\nCause', \"you're\", 'kinda', 'slow.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['internet', 'explorer\\ncause', 'yo', 'kind', 'slow'], ['internet', 'explorer\\ncause', 'youre', 'kinda', 'slow'])\n",
      "original document: \n",
      "['To', 'play', 'baseball', 'also']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'basebal', 'also'], ['play', 'baseball', 'also'])\n",
      "original document: \n",
      "['You', 'are', 'asking', 'the', 'wrong', 'question.', '\\n\\nYou', 'should', 'ask', 'what', 'places', 'to', 'avoid.', 'It’s', 'a', 'MUCH', 'shorter', 'list.\\n\\nBut', 'in', 'all', 'honesty', 'it', 'depends', 'on', 'what', 'type', 'of', 'Asian', 'cuisine', 'you’re', 'in', 'the', 'mood', 'for.', 'It’s', 'all', 'different', 'and', 'it’s', 'all', 'DELICIOUS!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ask', 'wrong', 'quest', '\\n\\nyou', 'ask', 'plac', 'avoid', 'much', 'short', 'list\\n\\nbut', 'honesty', 'depend', 'typ', 'as', 'cuisin', 'yo', 'mood', 'diff', 'delicy'], ['ask', 'wrong', 'question', '\\n\\nyou', 'ask', 'place', 'avoid', 'much', 'shorter', 'list\\n\\nbut', 'honesty', 'depend', 'type', 'asian', 'cuisine', 'youre', 'mood', 'different', 'delicious'])\n",
      "original document: \n",
      "['Tens', 'of', 'thousands', 'of', 'dollars', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'thousand', 'doll'], ['tens', 'thousands', 'dollars'])\n",
      "original document: \n",
      "['&gt;', 'when', 'the', 'section', 'WO', 'says', \"they're\", 'not', 'leaving', 'because', 'the', 'sky', 'is', 'blue,', \"it's\", 'a', 'valid', 'order.\\n\\nNo', 'argument', 'from', 'me', 'at', 'all,', 'but', 'those', 'dinosaurs', 'are', '(hopefully)', 'quickly', 'being', 'extinguished', 'as', 'new', 'blood', '(slowly)', 'comes', 'into', 'the', 'CF.', 'There', 'are', 'some', '&lt;', '27', 'year', 'old', \"WO's\", 'starting', 'to', 'show', 'up', 'and', 'maybe', \"they'll\", 'remember', 'where', 'they', 'came', 'from', 'because', 'the', 'old', 'fucks', 'certainly', \"don't.\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'sect', 'wo', 'say', 'theyr', 'leav', 'sky', 'blu', 'valid', 'order\\n\\nno', 'argu', 'dinosa', 'hop', 'quick', 'extinct', 'new', 'blood', 'slow', 'com', 'cf', 'lt', 'twenty-seven', 'year', 'old', 'wos', 'start', 'show', 'mayb', 'theyl', 'rememb', 'cam', 'old', 'fuck', 'certain', 'dont'], ['gt', 'section', 'wo', 'say', 'theyre', 'leave', 'sky', 'blue', 'valid', 'order\\n\\nno', 'argument', 'dinosaurs', 'hopefully', 'quickly', 'extinguish', 'new', 'blood', 'slowly', 'come', 'cf', 'lt', 'twenty-seven', 'year', 'old', 'wos', 'start', 'show', 'maybe', 'theyll', 'remember', 'come', 'old', 'fuck', 'certainly', 'dont'])\n",
      "original document: \n",
      "[\"I'll\", 'admit', 'I', 'had', 'to', 'Google', \"'netorar'\", 'and', 'yes', 'indeed', \"you're\", 'right', 'and', \"you're\", 'welcome.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'admit', 'googl', 'netor', 'ye', 'indee', 'yo', 'right', 'yo', 'welcom', '\\n'], ['ill', 'admit', 'google', 'netorar', 'yes', 'indeed', 'youre', 'right', 'youre', 'welcome', '\\n'])\n",
      "original document: \n",
      "['Do', 'you', 'have', 'a', 'horse', 'emblem,', 'with', 'emblem', 'buffs', 'Lyn', '', 'outclasses', 'Cordelia.\\n\\nHone', 'can', 'give', '+6', 'attack(I', 'think', '+Attack', 'Cordelia', 'will', 'have', 'the', 'same', 'attack', 'as', 'a', 'neutral', 'buffed', 'Lyn)', 'and', 'speed,', 'Goad', 'Cavalry', 'which', 'is', 'situational', 'but', 'it', 'has', 'saved', 'me', 'in', 'a', 'lot', 'of', 'places', 'like', 'TT', 'and', 'Arena', 'gives', '+4', 'to', 'attack', 'and', 'Speed.\\n\\nSo', 'essentially,', 'she', 'is', 'Cordelia', 'with', 'more', 'movement', 'and', 'speed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hors', 'emblem', 'emblem', 'buff', 'lyn', 'outclass', 'cordelia\\n\\nhone', 'giv', 'six', 'attack', 'think', 'attack', 'cordel', 'attack', 'neut', 'buff', 'lyn', 'spee', 'goad', 'cavalry', 'situ', 'sav', 'lot', 'plac', 'lik', 'tt', 'aren', 'giv', 'four', 'attack', 'speed\\n\\nso', 'ess', 'cordel', 'mov', 'spee'], ['horse', 'emblem', 'emblem', 'buff', 'lyn', 'outclass', 'cordelia\\n\\nhone', 'give', 'six', 'attacki', 'think', 'attack', 'cordelia', 'attack', 'neutral', 'buff', 'lyn', 'speed', 'goad', 'cavalry', 'situational', 'save', 'lot', 'place', 'like', 'tt', 'arena', 'give', 'four', 'attack', 'speed\\n\\nso', 'essentially', 'cordelia', 'movement', 'speed'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'googled', 'because', 'your', 'question', 'made', 'me', 'wonder.', 'It', 'looks', 'like', 'Michigan', \"doesn't\", 'allow', 'early', 'voting.', 'It', 'is', 'the', 'same', 'here', 'in', 'Alabama.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['googl', 'quest', 'mad', 'wond', 'look', 'lik', 'michig', 'doesnt', 'allow', 'ear', 'vot', 'alabam'], ['google', 'question', 'make', 'wonder', 'look', 'like', 'michigan', 'doesnt', 'allow', 'early', 'vote', 'alabama'])\n",
      "original document: \n",
      "['Vietnam', 'and', 'Korea', 'were', 'to', 'stop', 'the', 'spread', 'of', 'communism.', \"That's\", 'preemptive', 'war.', 'You', \"can't\", 'get', 'much', 'more', 'trigger', 'happy', 'than', 'that.', 'And', 'the', 'bombing', 'campaigns', 'in', 'Vietnam,', 'Cambodia', 'and', 'Laos', 'could', 'definitely', 'be', 'described', 'as', 'carpet', 'bombing.', 'The', 'amount', 'of', 'UXO', 'there', 'is', 'incredible,', 'as', 'well', 'as', 'ongoing', 'effects', 'from', 'agent', 'orange', 'and', 'the', 'near', 'indiscriminate', 'use', 'of', 'napalm.', 'That', 'war', 'was', 'trigger', 'happy', 'and', 'far', 'more', 'unjustified', 'than', 'anything', 'Obama', 'or', 'Clinton', 'did.', 'Same', 'with', 'BUsh', 'Jr', 'and', 'both', 'of', 'his', 'major', 'wars', 'that', 'are', 'still', 'ongoing.\\n\\nObama', 'showed', 'restraint', 'against', 'Syria,', 'refusing', 'to', 'bomb', 'without', 'congressional', 'approval', 'which', 'he', 'did', 'not', 'get,', 'whereas', 'Trump', 'did', 'exactly', 'that', 'seemingly', 'on', 'a', 'whim.', 'Obamas', 'use', 'of', 'drones,', 'however,', 'is', 'unconscionable.', 'I', 'just', \"don't\", 'see', 'where', 'you', 'are', 'drawing', 'this', 'distinction', 'apart', 'from', 'the', 'fact', 'that', 'the', 'two', 'administrations', 'you', 'mention', 'are', 'Democrats,', 'and', 'the', 'others', 'are', 'Republican.', \"I'm\", 'still', 'calling', 'partisanship.', \"I'm\", 'not', 'American,', \"I'm\", 'not', 'a', 'democrat', 'or', 'a', 'republican,', 'but', 'I', 'can', 'spot', 'hypocrisy', 'when', 'I', 'see', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vietnam', 'kore', 'stop', 'spread', 'commun', 'that', 'preempt', 'war', 'cant', 'get', 'much', 'trig', 'happy', 'bomb', 'campaign', 'vietnam', 'cambod', 'lao', 'could', 'definit', 'describ', 'carpet', 'bomb', 'amount', 'uxo', 'incred', 'wel', 'ongo', 'effect', 'ag', 'orang', 'near', 'indiscrimin', 'us', 'napalm', 'war', 'trig', 'happy', 'far', 'unjust', 'anyth', 'obam', 'clinton', 'bush', 'jr', 'maj', 'war', 'stil', 'ongoing\\n\\nobama', 'show', 'restraint', 'syr', 'refus', 'bomb', 'without', 'congress', 'approv', 'get', 'wherea', 'trump', 'exact', 'seem', 'whim', 'obama', 'us', 'dron', 'howev', 'unconsc', 'dont', 'see', 'draw', 'distinct', 'apart', 'fact', 'two', 'admin', 'ment', 'democr', 'oth', 'republ', 'im', 'stil', 'cal', 'part', 'im', 'am', 'im', 'democr', 'republ', 'spot', 'hypocrisy', 'see'], ['vietnam', 'korea', 'stop', 'spread', 'communism', 'thats', 'preemptive', 'war', 'cant', 'get', 'much', 'trigger', 'happy', 'bomb', 'campaign', 'vietnam', 'cambodia', 'laos', 'could', 'definitely', 'describe', 'carpet', 'bomb', 'amount', 'uxo', 'incredible', 'well', 'ongoing', 'effect', 'agent', 'orange', 'near', 'indiscriminate', 'use', 'napalm', 'war', 'trigger', 'happy', 'far', 'unjustified', 'anything', 'obama', 'clinton', 'bush', 'jr', 'major', 'war', 'still', 'ongoing\\n\\nobama', 'show', 'restraint', 'syria', 'refuse', 'bomb', 'without', 'congressional', 'approval', 'get', 'whereas', 'trump', 'exactly', 'seemingly', 'whim', 'obamas', 'use', 'drone', 'however', 'unconscionable', 'dont', 'see', 'draw', 'distinction', 'apart', 'fact', 'two', 'administrations', 'mention', 'democrats', 'others', 'republican', 'im', 'still', 'call', 'partisanship', 'im', 'american', 'im', 'democrat', 'republican', 'spot', 'hypocrisy', 'see'])\n",
      "original document: \n",
      "['I', 'love', 'roasting', 'kids', 'with', 'my', 'katana.', \"Here's\", 'my', 'setup\\n\\nLoadout', 'Name:', 'Ninjitsu\\n\\nKatana', '(Black', 'Sky),', 'Ghost', '+', 'Recon,', 'Tracker', '+', 'Momentum', '(if', 'I', 'had', 'epic', 'katana', 'I', 'would', 'trade', 'momentum', 'for', 'Tac', 'Resist),', 'Hardwired', '+', 'Dead', 'Silence.\\n\\nKillstreaks:', 'UAV', 'Purple,', 'Care', 'Package', 'Purple,', 'CUAV', 'Purple\\n\\nRig:', 'Warfighter', 'Overdrive', 'Ping', '(Oni),', 'or', 'Skinnyboob', 'Rushdown', 'Rewind', '(Black', 'Sky)\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'roast', 'kid', 'katan', 'her', 'setup\\n\\nloadout', 'nam', 'ninjitsu\\n\\nkatana', 'black', 'sky', 'ghost', 'recon', 'track', 'moment', 'ep', 'katan', 'would', 'trad', 'moment', 'tac', 'resist', 'hardwir', 'dead', 'silence\\n\\nkillstreaks', 'uav', 'purpl', 'car', 'pack', 'purpl', 'cuav', 'purple\\n\\nrig', 'warfight', 'overdr', 'ping', 'on', 'skinnyboob', 'rushdown', 'rewind', 'black', 'sky\\n\\n'], ['love', 'roast', 'kid', 'katana', 'heres', 'setup\\n\\nloadout', 'name', 'ninjitsu\\n\\nkatana', 'black', 'sky', 'ghost', 'recon', 'tracker', 'momentum', 'epic', 'katana', 'would', 'trade', 'momentum', 'tac', 'resist', 'hardwired', 'dead', 'silence\\n\\nkillstreaks', 'uav', 'purple', 'care', 'package', 'purple', 'cuav', 'purple\\n\\nrig', 'warfighter', 'overdrive', 'ping', 'oni', 'skinnyboob', 'rushdown', 'rewind', 'black', 'sky\\n\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['143413280|', '&gt;', 'United', 'Kingdom', 'Anonymous', '(ID:', 'a9Bg/T2P)\\n\\n&gt;&gt;143412250', '(OP)\\nOf', 'course', 'the', 'shill', 'voted', 'for', 'obongo', 'and', 'killary,', 'they', 'pay', 'your', 'wages\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, two hundred and eighty', 'gt', 'unit', 'kingdom', 'anonym', 'id', 'a9bgt2p\\n\\ngtgt143412250', 'op\\nof', 'cours', 'shil', 'vot', 'obongo', 'kil', 'pay', 'wages\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, two hundred and eighty', 'gt', 'unite', 'kingdom', 'anonymous', 'id', 'a9bgt2p\\n\\ngtgt143412250', 'op\\nof', 'course', 'shill', 'vote', 'obongo', 'killary', 'pay', 'wages\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['By', 'refusing', 'to', 'send', 'more', 'help,', 'people', 'will', 'die.', 'Which', 'is', 'akin', 'to', 'killing', 'them.', '\\n\\nThe', 'mayor', 'of', 'San', \"Juan's\", 'word,', 'not', 'mine.\\n\\nNo', 'need', 'to', 'yell,', 'douchebag.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['refus', 'send', 'help', 'peopl', 'die', 'akin', 'kil', '\\n\\nthe', 'may', 'san', 'juan', 'word', 'mine\\n\\nno', 'nee', 'yel', 'doucheb'], ['refuse', 'send', 'help', 'people', 'die', 'akin', 'kill', '\\n\\nthe', 'mayor', 'san', 'juans', 'word', 'mine\\n\\nno', 'need', 'yell', 'douchebag'])\n",
      "original document: \n",
      "[\"It'sa\", 'mee', '@dankfiber!', 'That’s', 'so', 'sweet', 'of', 'you,', 'thank', 'you', 'so', 'much!', 'It', 'was', 'really', 'cool', 'to', 'see', 'everyone’s', 'different', 'variations', 'and', 'how', 'the', 'colours', 'evolve', 'in', 'the', 'brioche', 'and', 'garter', 'fade', 'sections.', 'Cant', 'wait', 'to', 'see', 'your', 'Hedgehog', 'shawls', 'here', '&amp;', 'on', 'Insta', 'when', 'you’re', 'finished!', '😍']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['its', 'mee', 'dankfib', 'that', 'sweet', 'thank', 'much', 'real', 'cool', 'see', 'everyon', 'diff', 'vary', 'colo', 'evolv', 'brioch', 'gart', 'fad', 'sect', 'cant', 'wait', 'see', 'hedgehog', 'shawl', 'amp', 'inst', 'yo', 'fin'], ['itsa', 'mee', 'dankfiber', 'thats', 'sweet', 'thank', 'much', 'really', 'cool', 'see', 'everyones', 'different', 'variations', 'colour', 'evolve', 'brioche', 'garter', 'fade', 'section', 'cant', 'wait', 'see', 'hedgehog', 'shawls', 'amp', 'insta', 'youre', 'finish'])\n",
      "original document: \n",
      "['kinda', 'reminds', 'me', 'of', 'the', 'basement', 'from', 'that', \"70's\", 'show']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'remind', 'bas', '70s', 'show'], ['kinda', 'remind', 'basement', '70s', 'show'])\n",
      "original document: \n",
      "['Nah', 'I', 'mean', 'I', 'learn', 'better', 'when', 'I', 'have', 'something', 'in', 'my', 'hands.', 'Once', 'something', 'breaks', 'and', 'I', 'fix', 'it', 'once,', \"I'll\", 'always', 'remember', 'how', 'to', 'fix', 'it', 'again.', 'I', \"didn't\", 'know', 'much', 'about', 'computer', 'parts', 'until', 'a', 'few', 'weeks', 'ago', 'I', 'upgraded', 'my', 'PC,', 'something', 'stopped', 'working,', 'and', 'now', 'I', 'know', 'everything', 'xD', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah', 'mean', 'learn', 'bet', 'someth', 'hand', 'someth', 'break', 'fix', 'il', 'alway', 'rememb', 'fix', 'didnt', 'know', 'much', 'comput', 'part', 'week', 'ago', 'upgrad', 'pc', 'someth', 'stop', 'work', 'know', 'everyth', 'xd'], ['nah', 'mean', 'learn', 'better', 'something', 'hand', 'something', 'break', 'fix', 'ill', 'always', 'remember', 'fix', 'didnt', 'know', 'much', 'computer', 'part', 'weeks', 'ago', 'upgrade', 'pc', 'something', 'stop', 'work', 'know', 'everything', 'xd'])\n",
      "original document: \n",
      "['Are', 'you', 'the', 'most', 'downvoted', 'person', 'on', 'this', 'sub', 'lmao']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['downvot', 'person', 'sub', 'lmao'], ['downvoted', 'person', 'sub', 'lmao'])\n",
      "original document: \n",
      "['Just', 'wanted', 'to', 'point', 'out', 'that', 'his', 'first', 'name', 'is', 'Jong', 'Un,', 'most', 'Korean', 'first', 'names', 'have', 'two', 'parts.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'point', 'first', 'nam', 'jong', 'un', 'kor', 'first', 'nam', 'two', 'part'], ['want', 'point', 'first', 'name', 'jong', 'un', 'korean', 'first', 'name', 'two', 'part'])\n",
      "original document: \n",
      "['Leeeeeroy', 'Jeeeeenkins...', '\\n\\nJeeenkins...', '\\n\\nJenkins...', '\\n\\nJen...\\n\\nJ...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['leeeeeroy', 'jeeeeenkin', '\\n\\njeeenkins', '\\n\\njenkins', '\\n\\njen\\n\\nj'], ['leeeeeroy', 'jeeeeenkins', '\\n\\njeeenkins', '\\n\\njenkins', '\\n\\njen\\n\\nj'])\n",
      "original document: \n",
      "['The', 'purpose', 'of', 'birth', 'control', 'is', 'to', 'be', 'able', 'to', 'cum', 'inside', 'the', 'pussy.', 'If', 'she', 'is', 'on', 'the', 'pill', 'and', 'she', 'is', 'regular', 'with', 'taking', 'it,', 'you', 'can', 'cum', 'when', 'you', 'want.', 'My', 'wife', 'was', 'on', 'the', 'pill', 'during', 'our', 'dating', 'years', 'and', 'after', 'before', 'we', 'started', 'trying', 'to', 'have', 'kids', 'and', 'we', 'had', 'no', 'issues', 'whatsoever.', 'I', \"can't\", 'believe', 'how', 'uninformed', 'people', 'are', 'these', 'days.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['purpos', 'bir', 'control', 'abl', 'cum', 'insid', 'pussy', 'pil', 'regul', 'tak', 'cum', 'want', 'wif', 'pil', 'dat', 'year', 'start', 'try', 'kid', 'issu', 'whatsoev', 'cant', 'believ', 'uninform', 'peopl', 'day'], ['purpose', 'birth', 'control', 'able', 'cum', 'inside', 'pussy', 'pill', 'regular', 'take', 'cum', 'want', 'wife', 'pill', 'date', 'years', 'start', 'try', 'kid', 'issue', 'whatsoever', 'cant', 'believe', 'uninformed', 'people', 'days'])\n",
      "original document: \n",
      "['I’m', 'really', 'interested', 'if', 'this', 'is', 'a', 'viable', 'option', 'for', 'adult', 'video', 'websites', 'as', 'a', 'supplement', 'or', 'alternative', 'to', 'ads', 'https://tubeace.com/will-mining-cryptocurrency-porn-tube-sites-way-future/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'real', 'interest', 'viabl', 'opt', 'adult', 'video', 'websit', 'suppl', 'altern', 'ad', 'httpstubeacecomwillminingcryptocurrencyporntubesiteswayfuture'], ['im', 'really', 'interest', 'viable', 'option', 'adult', 'video', 'websites', 'supplement', 'alternative', 'ads', 'httpstubeacecomwillminingcryptocurrencyporntubesiteswayfuture'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'm\", 'sure', 'it', 'is.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur'], ['im', 'sure'])\n",
      "original document: \n",
      "['Just', 'when', 'I', 'think', \"I've\", 'heard', 'it', 'all', 'some', 'new', 'piece', 'of', 'shit', 'emerges', 'to', 'surprise', 'me', 'again', 'with', 'a', 'new', 'low.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'iv', 'heard', 'new', 'piec', 'shit', 'emerg', 'surpr', 'new', 'low'], ['think', 'ive', 'hear', 'new', 'piece', 'shit', 'emerge', 'surprise', 'new', 'low'])\n",
      "original document: \n",
      "['I', 'worked', 'at', 'a', 'churrasco', 'place', 'for', '6', 'years!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'churrasco', 'plac', 'six', 'year'], ['work', 'churrasco', 'place', 'six', 'years'])\n",
      "original document: \n",
      "['My', 'parents', 'took', 'me', '', 'to', 'my', 'first', 'real', 'restaurant.', 'I', 'thought', 'I', 'was', 'living', 'the', 'high', 'life', 'when', 'I', 'got', 'a', 'Shirley', 'temple', 'drink.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['par', 'took', 'first', 'real', 'resta', 'thought', 'liv', 'high', 'lif', 'got', 'shirley', 'templ', 'drink'], ['parent', 'take', 'first', 'real', 'restaurant', 'think', 'live', 'high', 'life', 'get', 'shirley', 'temple', 'drink'])\n",
      "original document: \n",
      "['ye', 'bro']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'bro'], ['ye', 'bro'])\n",
      "original document: \n",
      "['Dunno', 'what', 'are', 'those,', 'but', 'nofap', 'generally', 'not', 'about', 'hate', 'against', 'females.', 'It', 'is', 'protest', 'as', 'saying', '\"no\"', 'to', 'porn.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dunno', 'nofap', 'gen', 'hat', 'fem', 'protest', 'say', 'porn'], ['dunno', 'nofap', 'generally', 'hate', 'females', 'protest', 'say', 'porn'])\n",
      "original document: \n",
      "['No,', 'but', 'I', 'have', 'a', 'Rowdy', 'Tellez', 'prospect', 'base.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rowdy', 'tellez', 'prospect', 'bas'], ['rowdy', 'tellez', 'prospect', 'base'])\n",
      "original document: \n",
      "['It', 'looks', 'good,', 'but', 'I', 'see', 'what', 'you', 'mean', 'by', 'not', 'quite', 'right.', 'It', 'could', 'be', 'that', 'the', 'tip', 'of', 'the', 'nail', 'doesn’t', 'extend', 'far', 'enough.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'good', 'see', 'mean', 'quit', 'right', 'could', 'tip', 'nail', 'doesnt', 'extend', 'far', 'enough'], ['look', 'good', 'see', 'mean', 'quite', 'right', 'could', 'tip', 'nail', 'doesnt', 'extend', 'far', 'enough'])\n",
      "original document: \n",
      "['Aww', 'thanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aww', 'thank'], ['aww', 'thank'])\n",
      "original document: \n",
      "['I', 'would', 'have', 'forced', 'a', 'return.', '', 'People', 'like', 'this', 'need', 'to', 'try', 'a', 'little', 'bit.', '', 'Sometimes', 'when', 'you', 'force', 'the', 'return,', 'and', 'the', 'person', 'realizes', 'they', 'need', 'to', 'spend', 'their', 'inept', 'life', 'packaging', 'it', 'and', 'driving', 'it', 'to', 'a', 'dropoff', 'point,', 'it', 'suddenly', 'starts', 'working', 'correctly', 'again.\\n\\nNow', 'they', 'get', 'a', 'free', 'machine', 'that', 'you', 'even', 'paid', 'to', 'give', 'to', 'them.', 'So', 'what', 'was', 'the', 'alleged', '\"problem\"?', '', '(Needs', 'new', 'belt?', '', 'New', 'light', 'bulb?', '', 'Bent', 'needle?', '', 'wTF', 'else', 'could', 'possibly', 'go', 'wrong', 'with', 'one?)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'forc', 'return', 'peopl', 'lik', 'nee', 'try', 'littl', 'bit', 'sometim', 'forc', 'return', 'person', 'real', 'nee', 'spend', 'inept', 'lif', 'pack', 'driv', 'dropoff', 'point', 'sud', 'start', 'work', 'correct', 'again\\n\\nnow', 'get', 'fre', 'machin', 'ev', 'paid', 'giv', 'alleg', 'problem', 'nee', 'new', 'belt', 'new', 'light', 'bulb', 'bent', 'needl', 'wtf', 'els', 'could', 'poss', 'go', 'wrong', 'on'], ['would', 'force', 'return', 'people', 'like', 'need', 'try', 'little', 'bite', 'sometimes', 'force', 'return', 'person', 'realize', 'need', 'spend', 'inept', 'life', 'package', 'drive', 'dropoff', 'point', 'suddenly', 'start', 'work', 'correctly', 'again\\n\\nnow', 'get', 'free', 'machine', 'even', 'pay', 'give', 'allege', 'problem', 'need', 'new', 'belt', 'new', 'light', 'bulb', 'bend', 'needle', 'wtf', 'else', 'could', 'possibly', 'go', 'wrong', 'one'])\n",
      "original document: \n",
      "['What’s', 'the', 'budget?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'budget'], ['whats', 'budget'])\n",
      "original document: \n",
      "['If', '90%', 'sold', 'the', 'calls', 'that', 'they', 'owned', 'at', 'the', 'same', 'time,', 'it', 'does', 'nothing', 'to', 'the', 'price', 'of', 'the', 'stock', 'but', 'it', 'does', 'have', 'an', 'impact', 'on', 'the', 'price', 'of', 'that', 'call', 'option', 'itself.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nin', 'sold', 'cal', 'own', 'tim', 'noth', 'pric', 'stock', 'impact', 'pric', 'cal', 'opt'], ['ninety', 'sell', 'call', 'own', 'time', 'nothing', 'price', 'stock', 'impact', 'price', 'call', 'option'])\n",
      "original document: \n",
      "['For', 'certain', \"you're\", 'going', 'to', 'want', 'to', 'buy', 'Intel', 'then,', 'assuming', \"you're\", 'playing', 'at', '1080p', 'with', 'at', 'least', 'a', 'GTX', '1080.', 'But', 'wait', 'to', 'see', 'how', 'to', '8600K', 'fares,', 'might', 'not', 'be', 'enough', 'to', 'merit', 'spending', '$100', 'more.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['certain', 'yo', 'going', 'want', 'buy', 'intel', 'assum', 'yo', 'play', '1080p', 'least', 'gtx', 'one thousand and eighty', 'wait', 'see', '8600k', 'far', 'might', 'enough', 'merit', 'spend', 'one hundred'], ['certain', 'youre', 'go', 'want', 'buy', 'intel', 'assume', 'youre', 'play', '1080p', 'least', 'gtx', 'one thousand and eighty', 'wait', 'see', '8600k', 'fare', 'might', 'enough', 'merit', 'spend', 'one hundred'])\n",
      "original document: \n",
      "['It', 'would', 'be', 'nice', 'to', 'have', 'a', 'male', 'manakete.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'nic', 'mal', 'manaket'], ['would', 'nice', 'male', 'manakete'])\n",
      "original document: \n",
      "['Presumably', 'it', \"doesn't\", 'go', 'bad,', 'hang', 'onto', 'it', 'until', 'you', 'want', 'to', 'buy', 'something', 'from', 'there?', \"There's\", 'no', 'reason', 'to', 'rush', 'to', 'spend', 'your', 'money,', 'even', 'if', \"it's\", 'only', 'valid', 'at', 'target.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['presum', 'doesnt', 'go', 'bad', 'hang', 'onto', 'want', 'buy', 'someth', 'ther', 'reason', 'rush', 'spend', 'money', 'ev', 'valid', 'target'], ['presumably', 'doesnt', 'go', 'bad', 'hang', 'onto', 'want', 'buy', 'something', 'theres', 'reason', 'rush', 'spend', 'money', 'even', 'valid', 'target'])\n",
      "original document: \n",
      "['Translation:', 'my', 'wife', \"won't\", 'stop', 'complaining', 'about', 'this,', 'so', 'I', 'promised', 'that', \"I'd\", '\"do', 'something\"', 'to', 'shut', 'her', 'up.\\n\\nWhat', 'a', 'hypocritical', 'loser.', '', 'If', 'he', 'feels', 'sooo', 'strongly', 'about', 'this,', 'and', 'Jesus', 'is', 'telling', 'him', 'to', 'step', 'up,', 'then', 'let', 'him', 'change', '*his*', 'name.', '', 'I', 'would', 'pretend', 'that', 'I', 'never', 'got', 'the', 'letter', 'if', 'it', 'were', 'me.', '', 'And', 'if', 'confronted,', 'say', 'that', \"it's\", 'your', 'policy', 'to', 'recycle', 'junk', 'mail', 'without', 'reading', 'it.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['transl', 'wif', 'wont', 'stop', 'complain', 'prom', 'id', 'someth', 'shut', 'up\\n\\nwhat', 'hypocrit', 'los', 'feel', 'sooo', 'strongly', 'jes', 'tel', 'step', 'let', 'chang', 'nam', 'would', 'pretend', 'nev', 'got', 'let', 'confront', 'say', 'policy', 'recyc', 'junk', 'mail', 'without', 'read'], ['translation', 'wife', 'wont', 'stop', 'complain', 'promise', 'id', 'something', 'shut', 'up\\n\\nwhat', 'hypocritical', 'loser', 'feel', 'sooo', 'strongly', 'jesus', 'tell', 'step', 'let', 'change', 'name', 'would', 'pretend', 'never', 'get', 'letter', 'confront', 'say', 'policy', 'recycle', 'junk', 'mail', 'without', 'read'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'honestly', 'believe', 'Guy', 'tweaked', 'the', 'system', 'for', 'this', 'season', 'and', \"it's\", 'not', 'working', 'out', 'too', 'well.', 'Seems', 'like', 'they', 'are', 'trying', 'to', 'be', 'a', 'little', 'more', 'offense', 'minded', 'and', \"it's\", 'burning', 'them', 'on', 'the', 'defensive.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'believ', 'guy', 'tweak', 'system', 'season', 'work', 'wel', 'seem', 'lik', 'try', 'littl', 'offens', 'mind', 'burn', 'defend'], ['honestly', 'believe', 'guy', 'tweak', 'system', 'season', 'work', 'well', 'seem', 'like', 'try', 'little', 'offense', 'mind', 'burn', 'defensive'])\n",
      "original document: \n",
      "['Depends', 'on', 'what', 'you', 'want...', 'The', 'most', 'common', 'is', 'an', 'M4', 'model', 'since', 'parts', 'for', 'it', 'are', 'readily', 'available.', 'You', 'might', 'want', 'to', 'check', 'the', 'sidebar.', 'It', 'will', 'help', 'you', 'decide', 'on', 'what', 'brands', 'you', 'might', 'want', 'to', 'go', 'with.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'want', 'common', 'm4', 'model', 'sint', 'part', 'ready', 'avail', 'might', 'want', 'check', 'sideb', 'help', 'decid', 'brand', 'might', 'want', 'go'], ['depend', 'want', 'common', 'm4', 'model', 'since', 'part', 'readily', 'available', 'might', 'want', 'check', 'sidebar', 'help', 'decide', 'brand', 'might', 'want', 'go'])\n",
      "original document: \n",
      "['No,', \"it's\", 'all', 'local', 'to', 'the', 'phone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['loc', 'phon'], ['local', 'phone'])\n",
      "original document: \n",
      "['Thank', 'you', 'very', 'much!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'much'], ['thank', 'much'])\n",
      "original document: \n",
      "['As', 'I', 'said', 'before', 'I', 'am', 'not', 'hating', 'the', 'players,', \"it's\", 'their', 'money', 'they', 'can', 'do', 'whatever', 'they', 'want,', 'I', 'dislike', 'the', 'fact', 'that', 'Niantic', 'has', 'decreased', 'the', 'value', 'of', 'reaching', 'Level', '40', 'because', \"it's\", 'easy', 'to', 'farm', 'XP', 'by', 'buying', 'raid', 'passes.\\nHardcore', 'Free2players', 'do', 'not', 'use', 'money', 'on', 'the', 'game', 'so', \"it's\", 'not', 'an', 'option', 'to', 'start', 'using', 'money,', 'that', 'would', 'go', 'against', 'their', 'terms.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'hat', 'play', 'money', 'whatev', 'want', 'dislik', 'fact', 'niant', 'decreas', 'valu', 'reach', 'level', 'forty', 'easy', 'farm', 'xp', 'buy', 'raid', 'passes\\nhardcore', 'free2players', 'us', 'money', 'gam', 'opt', 'start', 'us', 'money', 'would', 'go', 'term'], ['say', 'hat', 'players', 'money', 'whatever', 'want', 'dislike', 'fact', 'niantic', 'decrease', 'value', 'reach', 'level', 'forty', 'easy', 'farm', 'xp', 'buy', 'raid', 'passes\\nhardcore', 'free2players', 'use', 'money', 'game', 'option', 'start', 'use', 'money', 'would', 'go', 'term'])\n",
      "original document: \n",
      "['Same!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Oh', 'you', 'motherfucker,', 'well', 'played!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'motherfuck', 'wel', 'play'], ['oh', 'motherfucker', 'well', 'play'])\n",
      "original document: \n",
      "['everyone', 'knows', 'that.', 'even', 'you', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['everyon', 'know', 'ev'], ['everyone', 'know', 'even'])\n",
      "original document: \n",
      "['I', 'think', 'what', \"he's\", 'saying', 'is', 'that', 'people', 'tend', 'to', 'weigh', 'offense', 'more', 'than', 'defense', 'for', 'MVP.', 'This', 'is', 'historically', 'what', 'the', 'voters', 'do', '(usually).', '\\n\\nAlthough', 'with', 'Trout', 'it', 'seems', \"they're\", 'looking', 'more', 'at', 'sabermetrics', 'so', 'who', 'knows?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'hes', 'say', 'peopl', 'tend', 'weigh', 'offens', 'defens', 'mvp', 'hist', 'vot', 'us', '\\n\\nalthough', 'trout', 'seem', 'theyr', 'look', 'sabermet', 'know'], ['think', 'hes', 'say', 'people', 'tend', 'weigh', 'offense', 'defense', 'mvp', 'historically', 'voters', 'usually', '\\n\\nalthough', 'trout', 'seem', 'theyre', 'look', 'sabermetrics', 'know'])\n",
      "original document: \n",
      "['Looks', 'like', 'GMK', 'Sky', 'Dolch,', 'or', 'something', 'super', 'similar']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'gmk', 'sky', 'dolch', 'someth', 'sup', 'simil'], ['look', 'like', 'gmk', 'sky', 'dolch', 'something', 'super', 'similar'])\n",
      "original document: \n",
      "['I,', 'too,', 'spent', 'a', 'great', 'deal', 'of', 'time', 'trying', 'to', 'figure', 'out', 'what', 'the', 'Roadhouse', 'characters', 'had', 'to', 'do', 'with', 'the', 'main', 'story.', 'I', 'thought', 'they', 'had', 'to', 'tie', 'into', 'Audrey', 'because', 'of', 'the', '\"Billy\"', 'and', '\"Tina\"', 'connections.', 'Alas,', 'we', 'are', 'left', 'with', 'no', 'resolution', 'on', 'this,', 'just', 'like', 'we', 'are', 'left', 'with', 'no', 'resolution', 'on', 'Audrey', 'whatsoever.\\n\\nI', 'think', 'your', 'instincts', 'are', 'good,', 'but', 'that', 'you', 'are', 'being', 'too', 'specific', 'and', 'literal', 'with', 'these', 'scenes.', 'Someone', 'already', 'posted', 'the', 'Lynch', 'quote', 'saying', 'that', 'the', 'Roadhouse', 'vignettes', 'were', 'never', 'meant', 'to', 'be', 'anything', 'other', 'than', '\"slice-of-life\"', 'moments', 'in', 'the', 'town', 'of', 'Twin', 'Peaks,', 'with', 'no', 'connection', 'to', 'the', 'larger', 'story.', 'What', 'I', 'do', 'think', 'they', 'connect', 'to', 'are', 'the', 'themes', 'of', 'awakening', 'and', 'duality.', 'The', 'show', 'gives', 'us', 'many', 'clues', 'that', 'we', 'are', 'watching', 'two', 'worlds,', 'and', 'that', 'awakening', 'is', 'the', 'key', 'to', 'crossing', 'from', 'one', 'to', 'the', 'other.\\n\\nIn', 'my', 'opinion,', 'the', 'Roadhouse', 'is', 'a', 'junction', 'point', 'between', 'worlds.', 'Anyone', 'seen', 'in', 'the', 'Roadhouse', 'is', 'not', 'awakened', 'in', 'the', 'beginning', 'of', 'TP,', 'and', 'awakens', 'by', 'the', 'end.\\n\\n*', 'Audrey', 'is', 'the', 'most', 'obvious\\n*', 'James', 'was', 'in', 'an', 'accident', 'and', 'is', '\"quiet\"', 'now\\n*', 'Shelly', 'is', 'trapped', 'in', 'the', 'same', 'self-destructive', 'cycle', 'of', 'being', 'with', 'terrible', 'men\\n\\nThe', 'random', 'bar', 'conversations', 'are', 'part', 'of', 'this', 'same', 'routine.', 'Whether', \"it's\", 'drugs', 'or', 'insanity', 'or', 'infidelity,', 'the', 'people', 'are', 'all', 'trapped', 'in', 'some', 'kind', 'of', 'cage.', 'When', 'they', 'awaken,', 'they', 'will', 'be', 'gone', 'from', 'the', 'Roadhouse', 'forever,', 'just', 'like', 'Audrey.\\n\\nRichard', 'was', 'probably', 'trapped', 'by', 'his', 'own', 'evil', 'heart,', 'hence', 'his', 'appearance', 'at', 'the', 'Roadhouse.', 'But', 'unlike', 'the', 'others,', 'he', \"didn't\", 'get', 'a', 'chance', 'to', 'awaken.', 'He', 'just', 'got', 'killed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spent', 'gre', 'deal', 'tim', 'try', 'fig', 'roadh', 'charact', 'main', 'story', 'thought', 'tie', 'audrey', 'bil', 'tin', 'connect', 'ala', 'left', 'resolv', 'lik', 'left', 'resolv', 'audrey', 'whatsoever\\n\\ni', 'think', 'instinct', 'good', 'spec', 'lit', 'scen', 'someon', 'already', 'post', 'lynch', 'quot', 'say', 'roadh', 'vignet', 'nev', 'meant', 'anyth', 'sliceofl', 'mom', 'town', 'twin', 'peak', 'connect', 'larg', 'story', 'think', 'connect', 'them', 'awak', 'dual', 'show', 'giv', 'us', 'many', 'clu', 'watch', 'two', 'world', 'awak', 'key', 'cross', 'on', 'other\\n\\nin', 'opin', 'roadh', 'junct', 'point', 'world', 'anyon', 'seen', 'roadh', 'awak', 'begin', 'tp', 'awak', 'end\\n\\n', 'audrey', 'obvious\\n', 'jam', 'accid', 'quiet', 'now\\n', 'shel', 'trap', 'selfdestruct', 'cyc', 'terr', 'men\\n\\nthe', 'random', 'bar', 'convers', 'part', 'routin', 'wheth', 'drug', 'ins', 'infidel', 'peopl', 'trap', 'kind', 'cag', 'awak', 'gon', 'roadh', 'forev', 'lik', 'audrey\\n\\nrichard', 'prob', 'trap', 'evil', 'heart', 'hent', 'appear', 'roadh', 'unlik', 'oth', 'didnt', 'get', 'chant', 'awak', 'got', 'kil'], ['spend', 'great', 'deal', 'time', 'try', 'figure', 'roadhouse', 'character', 'main', 'story', 'think', 'tie', 'audrey', 'billy', 'tina', 'connections', 'alas', 'leave', 'resolution', 'like', 'leave', 'resolution', 'audrey', 'whatsoever\\n\\ni', 'think', 'instincts', 'good', 'specific', 'literal', 'scenes', 'someone', 'already', 'post', 'lynch', 'quote', 'say', 'roadhouse', 'vignettes', 'never', 'mean', 'anything', 'sliceoflife', 'moments', 'town', 'twin', 'peak', 'connection', 'larger', 'story', 'think', 'connect', 'theme', 'awaken', 'duality', 'show', 'give', 'us', 'many', 'clue', 'watch', 'two', 'worlds', 'awaken', 'key', 'cross', 'one', 'other\\n\\nin', 'opinion', 'roadhouse', 'junction', 'point', 'worlds', 'anyone', 'see', 'roadhouse', 'awaken', 'begin', 'tp', 'awaken', 'end\\n\\n', 'audrey', 'obvious\\n', 'jam', 'accident', 'quiet', 'now\\n', 'shelly', 'trap', 'selfdestructive', 'cycle', 'terrible', 'men\\n\\nthe', 'random', 'bar', 'conversations', 'part', 'routine', 'whether', 'drug', 'insanity', 'infidelity', 'people', 'trap', 'kind', 'cage', 'awaken', 'go', 'roadhouse', 'forever', 'like', 'audrey\\n\\nrichard', 'probably', 'trap', 'evil', 'heart', 'hence', 'appearance', 'roadhouse', 'unlike', 'others', 'didnt', 'get', 'chance', 'awaken', 'get', 'kill'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnodjog/):\\n\\nI've\", 'often', 'thought', 'that', 'SciFi', 'is', 'at', 'its', 'core', 'a', 'sociological', 'genre', '-', 'put', 'normal', 'human', 'beings', 'in', 'outlandish', 'situations', 'and', 'examine', 'what', 'the', 'outcome', 'of', 'that', 'may', 'be.\\n\\nDo', 'you', 'agree', 'with', 'that', 'and', 'what', 'are', 'some', 'of', 'the', 'main', 'themes', 'you', 'adress', 'in', 'this', 'book?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnodjog\\n\\nive', 'oft', 'thought', 'scif', 'cor', 'sociolog', 'genr', 'put', 'norm', 'hum', 'being', 'outland', 'situ', 'examin', 'outcom', 'may', 'be\\n\\ndo', 'agr', 'main', 'them', 'adress', 'book'], ['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnodjog\\n\\nive', 'often', 'think', 'scifi', 'core', 'sociological', 'genre', 'put', 'normal', 'human', 'be', 'outlandish', 'situations', 'examine', 'outcome', 'may', 'be\\n\\ndo', 'agree', 'main', 'theme', 'adress', 'book'])\n",
      "original document: \n",
      "['They', 'won', 'the', 'restaurant', 'wars!', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['resta', 'war'], ['restaurant', 'war'])\n",
      "original document: \n",
      "['Le', 'problème', \"c'est\", 'le', 'nombre', 'de', 'gens', 'qui', 'se', 'cassent', 'justement', 'parce', 'que', \"d'une\", 'il', 'y', 'a', 'relativement', 'très', 'peu', 'de', 'postes', 'de', 'permanents,', 'et', 'de', 'deux', 'parce', 'que', 'le', 'salaire', 'est', 'minable', 'comparé', 'à', 'ce', 'qui', 'ce', 'fait', 'à', \"l'étranger\", 'ou', 'dans', 'le', 'privé.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['le', 'problem', 'cest', 'le', 'nombr', 'de', 'gen', 'qui', 'se', 'cass', 'just', 'parc', 'que', 'dun', 'il', 'rel', 'tre', 'peu', 'de', 'post', 'de', 'perm', 'et', 'de', 'deux', 'parc', 'que', 'le', 'salair', 'est', 'min', 'comp', 'ce', 'qui', 'ce', 'fait', 'letrang', 'ou', 'dan', 'le', 'priv'], ['le', 'probleme', 'cest', 'le', 'nombre', 'de', 'gens', 'qui', 'se', 'cassent', 'justement', 'parce', 'que', 'dune', 'il', 'relativement', 'tres', 'peu', 'de', 'post', 'de', 'permanents', 'et', 'de', 'deux', 'parce', 'que', 'le', 'salaire', 'est', 'minable', 'compare', 'ce', 'qui', 'ce', 'fait', 'letranger', 'ou', 'dans', 'le', 'prive'])\n",
      "original document: \n",
      "['Sick', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sick', 'lol'], ['sick', 'lol'])\n",
      "original document: \n",
      "[\"Who's\", 'the', 'last', 'hab', 'to', 'have', 'a', '6', 'point', 'game', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['who', 'last', 'hab', 'six', 'point', 'gam', 'lol'], ['whos', 'last', 'hab', 'six', 'point', 'game', 'lol'])\n",
      "original document: \n",
      "['Yeah', 'this', 'was', 'the', 'biggest', 'plot', 'hole', 'to', 'me.', 'Instead', 'of', 'just', 'giving', 'up', 'when', 'they', 'figure', 'it', 'out', 'he', 'needs', 'to', 'plan', 'Phase', '2', 'where', 'he', 'either', 'A)', 'convinces', 'them', \"they're\", 'wrong,', 'or', 'B)', 'tortures', 'them', 'in', 'other', 'ways', 'once', 'they', 'figure', 'it', 'out.', 'I', 'feel', 'like', 'Vicki', 'will', 'have', 'a', 'better', 'grasp', 'of', 'this.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'biggest', 'plot', 'hol', 'instead', 'giv', 'fig', 'nee', 'plan', 'phas', 'two', 'eith', 'convint', 'theyr', 'wrong', 'b', 'tort', 'way', 'fig', 'feel', 'lik', 'vick', 'bet', 'grasp'], ['yeah', 'biggest', 'plot', 'hole', 'instead', 'give', 'figure', 'need', 'plan', 'phase', 'two', 'either', 'convince', 'theyre', 'wrong', 'b', 'torture', 'ways', 'figure', 'feel', 'like', 'vicki', 'better', 'grasp'])\n",
      "original document: \n",
      "['garfield_irl']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['garfield_irl'], ['garfield_irl'])\n",
      "original document: \n",
      "['Not', 'really', 'no,', 'my', 'guys', 'are', 'union', 'and', 'just', 'got', 'their', 'pension', 'slashed', 'by', '70%,', 'although', 'the', 'pension', 'is', 'through', 'the', 'union', 'so', 'perhaps', \"it's\", 'different', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'guy', 'un', 'got', 'pend', 'slash', 'seventy', 'although', 'pend', 'un', 'perhap', 'diff'], ['really', 'guy', 'union', 'get', 'pension', 'slash', 'seventy', 'although', 'pension', 'union', 'perhaps', 'different'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Question', 'from', 'a', 'noob,', 'Im', 'having', 'trouble', 'getting', 'some', 'hard', 'stuck', 'on', 'bird', 'poop', 'that', \"didn't\", 'come', 'off', 'after', '2', 'soap', 'sprays', 'and', 'rinses', 'with', 'a', 'power', 'washer', 'and', 'hand', 'wash', 'with', 'microfiber', 'mit.', '', 'I', 'used', 'the', 'mit', 'and', 'got', 'most', 'of', 'it', 'but', 'there', 'is', 'a', 'white', 'haze', 'area', 'around', 'it', 'still', 'that', 'i', 'cannot', 'for', 'the', 'life', 'of', 'me', 'get', 'off.', 'I', 'dont', 'want', 'to', 'scratch', 'the', 'paint', 'trying', 'to', 'get', 'it', 'off.\\n\\nAny', 'ideas', 'would', 'be', 'greatly', 'appreciated.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quest', 'noob', 'im', 'troubl', 'get', 'hard', 'stuck', 'bird', 'poop', 'didnt', 'com', 'two', 'soap', 'sprays', 'rins', 'pow', 'wash', 'hand', 'wash', 'microfib', 'mit', 'us', 'mit', 'got', 'whit', 'haz', 'are', 'around', 'stil', 'cannot', 'lif', 'get', 'dont', 'want', 'scratch', 'paint', 'try', 'get', 'off\\n\\nany', 'idea', 'would', 'gre', 'apprecy'], ['question', 'noob', 'im', 'trouble', 'get', 'hard', 'stick', 'bird', 'poop', 'didnt', 'come', 'two', 'soap', 'spray', 'rinse', 'power', 'washer', 'hand', 'wash', 'microfiber', 'mit', 'use', 'mit', 'get', 'white', 'haze', 'area', 'around', 'still', 'cannot', 'life', 'get', 'dont', 'want', 'scratch', 'paint', 'try', 'get', 'off\\n\\nany', 'ideas', 'would', 'greatly', 'appreciate'])\n",
      "original document: \n",
      "['&gt;he', 'became', 'a', 'multiversal', 'abstract', '\\n\\nThis', 'is', 'not', 'true.', 'He', 'spread', 'across', 'the', 'universe', 'but', \"that's\", 'not', 'multiversal', 'at', 'all.', 'Source?\\n\\n&gt;Here', 'is', 'a', 'time', 'ring.\\n\\nJust', 'as', 'you', 'said', 'in', 'this', 'same', 'post,', 'a', 'wiki', 'link', \"isn't\", 'proof', 'of', 'anything.', 'Go', 'get', 'me', 'actual', 'proof.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gthe', 'becam', 'multivers', 'abstract', '\\n\\nthis', 'tru', 'spread', 'across', 'univers', 'that', 'multivers', 'source\\n\\ngther', 'tim', 'ring\\n\\njust', 'said', 'post', 'wik', 'link', 'isnt', 'proof', 'anyth', 'go', 'get', 'act', 'proof'], ['gthe', 'become', 'multiversal', 'abstract', '\\n\\nthis', 'true', 'spread', 'across', 'universe', 'thats', 'multiversal', 'source\\n\\ngthere', 'time', 'ring\\n\\njust', 'say', 'post', 'wiki', 'link', 'isnt', 'proof', 'anything', 'go', 'get', 'actual', 'proof'])\n",
      "original document: \n",
      "['Plot', 'twist:', 'there', 'was', 'a', 'Mini', 'P.E.K.K.A.', 'with', '239', 'HP', 'underneath.', 'Playing', 'The', 'Log', 'allowed', 'me', 'to', 'kill', 'the', 'Mpekka', 'and', 'cycle', 'to', 'my', 'Arrows,', 'saving', 'us', 'the', \"game!\\n\\nDon't\", 'quit,', 'kids!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plot', 'twist', 'min', 'pekk', 'two hundred and thirty-nine', 'hp', 'undernea', 'play', 'log', 'allow', 'kil', 'mpekk', 'cyc', 'arrow', 'sav', 'us', 'game\\n\\ndont', 'quit', 'kid'], ['plot', 'twist', 'mini', 'pekka', 'two hundred and thirty-nine', 'hp', 'underneath', 'play', 'log', 'allow', 'kill', 'mpekka', 'cycle', 'arrows', 'save', 'us', 'game\\n\\ndont', 'quit', 'kid'])\n",
      "original document: \n",
      "['Take', 'it', 'to', 'a', 'vape', 'shop.', 'Garuntee', \"they'll\", 'have', 'something', 'that', 'can', 'fit', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'vap', 'shop', 'garunt', 'theyl', 'someth', 'fit'], ['take', 'vape', 'shop', 'garuntee', 'theyll', 'something', 'fit'])\n",
      "original document: \n",
      "['Goku', \"wasn't\", 'really', 'brought', 'back', 'either', 'yet', \"he's\", 'in', 'the', 'category.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goku', 'wasnt', 'real', 'brought', 'back', 'eith', 'yet', 'hes', 'categ'], ['goku', 'wasnt', 'really', 'bring', 'back', 'either', 'yet', 'hes', 'category'])\n",
      "original document: \n",
      "['Clean!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cle'], ['clean'])\n",
      "original document: \n",
      "['Fuck', 'off.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck'], ['fuck'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['The', 'NLCS', 'perhaps?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nlcs', 'perhap'], ['nlcs', 'perhaps'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['People', 'are', 'looking', 'at', 'me', 'funny', 'cause', 'I', 'was', 'laughing', 'so', 'hard😂', 'that', 'is', 'one', 'of', 'the', 'greatest', 'things', 'I’ve', 'ever', 'seen']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'look', 'funny', 'caus', 'laugh', 'hard', 'on', 'greatest', 'thing', 'iv', 'ev', 'seen'], ['people', 'look', 'funny', 'cause', 'laugh', 'hard', 'one', 'greatest', 'things', 'ive', 'ever', 'see'])\n",
      "original document: \n",
      "['23']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty-three'], ['twenty-three'])\n",
      "original document: \n",
      "['America', 'looking', 'like', 'Syria.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['americ', 'look', 'lik', 'syr'], ['america', 'look', 'like', 'syria'])\n",
      "original document: \n",
      "['lolwut?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lolwut'], ['lolwut'])\n",
      "original document: \n",
      "['Cue', 'the', 'patented', 'Auburn', 'third', 'quarter', 'slump.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cue', 'pat', 'auburn', 'third', 'quart', 'slump'], ['cue', 'patent', 'auburn', 'third', 'quarter', 'slump'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['*Slow', 'Clap*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['slow', 'clap'], ['slow', 'clap'])\n",
      "original document: \n",
      "['As', 'the', 'name', 'suggests,', 'should', 'be', 'a', 'place', 'to', 'farm', 'something...', 'some', 'sort', 'of', 'special', 'token?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nam', 'suggest', 'plac', 'farm', 'someth', 'sort', 'spec', 'tok'], ['name', 'suggest', 'place', 'farm', 'something', 'sort', 'special', 'token'])\n",
      "original document: \n",
      "['Garbage', 'bags', 'on', 'the', 'seats', 'and', 'steering', 'column.', 'Carpet', 'comes', 'out', 'and', 'dries', 'much', 'easier.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['garb', 'bag', 'seat', 'ste', 'column', 'carpet', 'com', 'dri', 'much', 'easy'], ['garbage', 'bag', 'seat', 'steer', 'column', 'carpet', 'come', 'dry', 'much', 'easier'])\n",
      "original document: \n",
      "['Cool!', 'Sent', 'a', 'pm.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cool', 'sent', 'pm'], ['cool', 'send', 'pm'])\n",
      "original document: \n",
      "['If', 'your', 'party', 'has', 'a', 'healer', '(especially', 'a', 'healer', 'who', 'casts', '*aid*', 'every', 'morning),', 'drop', '*armor', 'of', 'agathys*.', 'If', 'your', 'party', 'has', 'a', 'character', 'with', 'good', 'Charisma', 'skill', 'modifiers,', 'drop', '*disguise', 'self*.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['party', 'heal', 'espec', 'heal', 'cast', 'aid', 'every', 'morn', 'drop', 'arm', 'agathy', 'party', 'charact', 'good', 'charism', 'skil', 'mod', 'drop', 'disgu', 'self'], ['party', 'healer', 'especially', 'healer', 'cast', 'aid', 'every', 'morning', 'drop', 'armor', 'agathys', 'party', 'character', 'good', 'charisma', 'skill', 'modifiers', 'drop', 'disguise', 'self'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'sweet', 'deal', 'right', 'there.', 'I', 'almost', 'bought', 'one', 'in', 'way', 'worse', 'condition', 'for', '$100', 'a', 'few', 'months', 'back,', 'ungraded', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'sweet', 'deal', 'right', 'almost', 'bought', 'on', 'way', 'wors', 'condit', 'one hundred', 'month', 'back', 'ungrad'], ['thats', 'sweet', 'deal', 'right', 'almost', 'buy', 'one', 'way', 'worse', 'condition', 'one hundred', 'months', 'back', 'ungraded'])\n",
      "original document: \n",
      "['Thats', 'exactly', 'what', 'someone', 'who', 'hates', 'people', 'like', 'them', 'would', 'say.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'exact', 'someon', 'hat', 'peopl', 'lik', 'would', 'say'], ['thats', 'exactly', 'someone', 'hat', 'people', 'like', 'would', 'say'])\n",
      "original document: \n",
      "[\"That's\", 'unlucky.', 'Cults', 'suck,', 'huh?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'unlucky', 'cult', 'suck', 'huh'], ['thats', 'unlucky', 'cults', 'suck', 'huh'])\n",
      "original document: \n",
      "['Good', 'Lord', 'they', 'just', 'shifted', 'the', 'entire', 'Old', 'World', 'down...\\n\\n[Risk', 'world', 'map](http://static4.businessinsider.com/image/51e6d49b6bb3f7a42d000008-926-563/screen%20shot%202013-07-17%20at%201.29.36%20pm.png)\\n\\n[Real', 'world', 'map](http://geology.com/world/world-map.gif)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'lord', 'shift', 'entir', 'old', 'world', 'down\\n\\nrisk', 'world', 'maphttpstatic4businessinsidercomimage51e6d49b6bb3f7a42d000008926563screen20shot202013071720at201293620pmpng\\n\\nreal', 'world', 'maphttpgeologycomworldworldmapg'], ['good', 'lord', 'shift', 'entire', 'old', 'world', 'down\\n\\nrisk', 'world', 'maphttpstatic4businessinsidercomimage51e6d49b6bb3f7a42d000008926563screen20shot202013071720at201293620pmpng\\n\\nreal', 'world', 'maphttpgeologycomworldworldmapgif'])\n",
      "original document: \n",
      "['You', 'have', 'thoughtfully', 'tried', 'to', 'explain', 'another', 'perspective,', 'shit', 'like', 'that', 'will', 'get', 'you', 'downvoted', 'and', 'kicked', 'out', 'of', 'the', 'circle', 'jerk.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'tri', 'explain', 'anoth', 'perspect', 'shit', 'lik', 'get', 'downvot', 'kick', 'circ', 'jerk\\n\\n'], ['thoughtfully', 'try', 'explain', 'another', 'perspective', 'shit', 'like', 'get', 'downvoted', 'kick', 'circle', 'jerk\\n\\n'])\n",
      "original document: \n",
      "['aaaaaaaaaaaaaahhh', 'freaking', 'out!!', 'I', 'made', 'him', 'get', 'me', 'tampons', 'while', 'he', 'was', 'out!', 'He', 'was', 'texting', 'asking', 'what', 'kind', 'the', 'same', 'time', 'I', 'was', 'trying', 'to', 'take', 'a', 'pic', 'trying', 'to', 'process', 'what', 'I', 'was', 'seeing!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aaaaaaaaaaaaaahhh', 'freak', 'mad', 'get', 'tampon', 'text', 'ask', 'kind', 'tim', 'try', 'tak', 'pic', 'try', 'process', 'see'], ['aaaaaaaaaaaaaahhh', 'freak', 'make', 'get', 'tampon', 'texting', 'ask', 'kind', 'time', 'try', 'take', 'pic', 'try', 'process', 'see'])\n",
      "original document: \n",
      "['Finally', 'defeated', 'Lyrith', 'for', 'the', 'first', 'time.', '', 'All', 'that', 'work...', 'for', 'a', 'water', 'samurai.', '', 'At', 'least', 'I', \"don't\", 'need', 'to', 'fuse', 'one', 'for', 'the', 'fusion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fin', 'def', 'lyri', 'first', 'tim', 'work', 'wat', 'samura', 'least', 'dont', 'nee', 'fus', 'on', 'fus'], ['finally', 'defeat', 'lyrith', 'first', 'time', 'work', 'water', 'samurai', 'least', 'dont', 'need', 'fuse', 'one', 'fusion'])\n",
      "original document: \n",
      "['143418691|', '&gt;', 'Japan', 'Anonymous', '(ID:', 'bwkKKbpn)\\n\\n&gt;&gt;143412250', '(OP)\\n2005', 'Meme', 'Tadoshi\\n2009', 'Dikku', 'Ratino\\n2013', 'Keku', 'Potato\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, six hundred and ninety-on', 'gt', 'jap', 'anonym', 'id', 'bwkkkbpn\\n\\ngtgt143412250', 'op\\n2005', 'mem', 'tadoshi\\n2009', 'dikku', 'ratino\\n2013', 'keku', 'potato\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, six hundred and ninety-one', 'gt', 'japan', 'anonymous', 'id', 'bwkkkbpn\\n\\ngtgt143412250', 'op\\n2005', 'meme', 'tadoshi\\n2009', 'dikku', 'ratino\\n2013', 'keku', 'potato\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnodwfm/):\\n\\nOhhhh,', 'I', 'like', 'your', 'view', 'on', 'that.', 'I', 'think', 'that', 'it', 'could', 'be,', 'for', 'certain', 'novels,', 'but', 'I', \"wouldn't\", 'put', 'the', 'whole', 'genre', 'in', 'there.', 'This', 'is', 'because', 'there', 'are', 'books,', 'like', 'mine,', 'where', 'the', 'main', 'characters', \"aren't\", 'human', 'and', 'therefore', 'it', 'creates', 'a', 'different', 'balance', 'of', 'things.', 'However,', 'I', 'could', 'see', 'the', 'validity', 'of', 'your', 'point', 'in', 'novels', 'that', 'do', 'feature', 'humans', 'in', 'outlandish', 'situations.', '\\nMy', 'main', 'themes', 'that', 'are', 'addressed', 'within', 'this', 'book', 'are', 'ambition,', 'coming', 'of', 'age,', 'courage,', 'discovery,', 'and', 'loss.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnodwfm\\n\\nohhhh', 'lik', 'view', 'think', 'could', 'certain', 'novel', 'wouldnt', 'put', 'whol', 'genr', 'book', 'lik', 'min', 'main', 'charact', 'ar', 'hum', 'theref', 'cre', 'diff', 'bal', 'thing', 'howev', 'could', 'see', 'valid', 'point', 'novel', 'feat', 'hum', 'outland', 'situ', '\\nmy', 'main', 'them', 'address', 'within', 'book', 'ambit', 'com', 'ag', 'cour', 'discovery', 'loss'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnodwfm\\n\\nohhhh', 'like', 'view', 'think', 'could', 'certain', 'novels', 'wouldnt', 'put', 'whole', 'genre', 'book', 'like', 'mine', 'main', 'character', 'arent', 'human', 'therefore', 'create', 'different', 'balance', 'things', 'however', 'could', 'see', 'validity', 'point', 'novels', 'feature', 'humans', 'outlandish', 'situations', '\\nmy', 'main', 'theme', 'address', 'within', 'book', 'ambition', 'come', 'age', 'courage', 'discovery', 'loss'])\n",
      "original document: \n",
      "['this,', 'but', 'unironically']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uniron'], ['unironically'])\n",
      "original document: \n",
      "['Thanks!', 'I', 'guess', 'we', 'would', 'call', 'that', 'a', 'camp', 'bed.', \"I've\", 'always', 'wondered', 'what', 'you', 'meant', 'by', 'cot.', 'I', 'know', 'I', 'could', 'have', 'googled', 'it', 'so', 'thanks', 'for', 'the', 'reply.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'guess', 'would', 'cal', 'camp', 'bed', 'iv', 'alway', 'wond', 'meant', 'cot', 'know', 'could', 'googl', 'thank', 'reply'], ['thank', 'guess', 'would', 'call', 'camp', 'bed', 'ive', 'always', 'wonder', 'mean', 'cot', 'know', 'could', 'google', 'thank', 'reply'])\n",
      "original document: \n",
      "['You', 'are', 'correct.', 'You', 'are', 'biased', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['correct', 'bias'], ['correct', 'bias'])\n",
      "original document: \n",
      "['The', 'Dark', 'Tower', 'series', 'has', 'been', 'fantastic', 'so', 'far', 'for', 'me.', 'The', 'Gunslinger', '(first', 'one', 'in', 'the', 'series)', 'was', 'a', 'great', 'intro', 'to', \"King's\", 'writing', 'I', 'thought.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dark', 'tow', 'sery', 'fantast', 'far', 'gunsl', 'first', 'on', 'sery', 'gre', 'intro', 'king', 'writ', 'thought'], ['dark', 'tower', 'series', 'fantastic', 'far', 'gunslinger', 'first', 'one', 'series', 'great', 'intro', 'kings', 'write', 'think'])\n",
      "original document: \n",
      "['Fuck', 'that,', 'I', 'make', 'semi-near', 'that', 'in', 'my', 'job', 'and', 'I', 'love', 'what', 'I', 'do.', 'No', 'fucking', 'way', 'I', 'could', 'lay', 'down', 'that', 'long', 'for', 'that.', \"You'd\", 'need', 'to', 'comp', 'me', 'like', '200k', 'for', 'that.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'mak', 'seminear', 'job', 'lov', 'fuck', 'way', 'could', 'lay', 'long', 'youd', 'nee', 'comp', 'lik', '200k'], ['fuck', 'make', 'seminear', 'job', 'love', 'fuck', 'way', 'could', 'lay', 'long', 'youd', 'need', 'comp', 'like', '200k'])\n",
      "original document: \n",
      "['Someone', 'get', 'a', 'bucket', 'of', 'water', 'for', \"Marlo's\", 'thristy', 'derrière.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someon', 'get', 'bucket', 'wat', 'marlo', 'thristy', 'derry'], ['someone', 'get', 'bucket', 'water', 'marlos', 'thristy', 'derriere'])\n",
      "original document: \n",
      "['Is', 'your', 'letter', 'K', 'gone', 'as', 'well?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'k', 'gon', 'wel'], ['letter', 'k', 'go', 'well'])\n",
      "original document: \n",
      "['Roll', 'Tide,', 'fuck', 'Tennessee,', 'fuck', 'Auburn\\n\\nGeorgia,', 'you', 'are', 'alright', 'people']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rol', 'tid', 'fuck', 'ten', 'fuck', 'auburn\\n\\ngeorgia', 'alright', 'peopl'], ['roll', 'tide', 'fuck', 'tennessee', 'fuck', 'auburn\\n\\ngeorgia', 'alright', 'people'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Bogus', 'news.\\n\\n', '#fakenews']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bog', 'news\\n\\n', 'fakenew'], ['bogus', 'news\\n\\n', 'fakenews'])\n",
      "original document: \n",
      "['This', 'guy', 'jokes.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guy', 'jok'], ['guy', 'joke'])\n",
      "original document: \n",
      "['because', 'its', 'a', '4-5*', 'egg......pretty', 'sure', 'its', 'less', 'than', 'a', '5%', 'chance', 'for', 'a', 'nat', '5']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['forty-five', 'eggpretty', 'sur', 'less', 'fiv', 'chant', 'nat', 'fiv'], ['forty-five', 'eggpretty', 'sure', 'less', 'five', 'chance', 'nat', 'five'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"What's\", 'his', 'name!?!?!?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'nam'], ['whats', 'name'])\n",
      "original document: \n",
      "['This', 'was', 'unnecessary', 'bot.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unnecess', 'bot'], ['unnecessary', 'bot'])\n",
      "original document: \n",
      "['This', 'post', 'has', 'been', 'removed', 'by', 'AutoModerator.\\n\\nYour', 'account', 'must', 'be', 'at', 'least', 'one', 'month', 'old', '*and*', 'you', 'must', 'have', 'at', 'least', '50', 'comment', 'karma', 'in', 'order', 'to', 'create', 'a', 'self', 'post.\\n\\nIf', \"you're\", 'new', 'to', 'the', 'sport', 'and', 'want', 'to', 'ask', 'a', 'question', 'then', 'check', 'out', '/r/SoccerNoobs.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/soccer)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'remov', 'automoderator\\n\\nyo', 'account', 'must', 'least', 'on', 'mon', 'old', 'must', 'least', 'fifty', 'com', 'karm', 'ord', 'cre', 'self', 'post\\n\\nif', 'yo', 'new', 'sport', 'want', 'ask', 'quest', 'check', 'rsoccernoobs\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorsocc', 'quest', 'concern'], ['post', 'remove', 'automoderator\\n\\nyour', 'account', 'must', 'least', 'one', 'month', 'old', 'must', 'least', 'fifty', 'comment', 'karma', 'order', 'create', 'self', 'post\\n\\nif', 'youre', 'new', 'sport', 'want', 'ask', 'question', 'check', 'rsoccernoobs\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorsoccer', 'question', 'concern'])\n",
      "original document: \n",
      "['Sorry', 'for', 'your', 'loss,', 'happy', 'for', 'your', 'memories.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'loss', 'happy', 'mem'], ['sorry', 'loss', 'happy', 'memories'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Praise', 'be.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pra'], ['praise'])\n",
      "original document: \n",
      "[\"What's\", 'the', 'plane', 'behind', 'the', '737?', 'Is', 'that', 'a', 'Dreamliner?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'plan', 'behind', 'seven hundred and thirty-sev', 'dreamlin'], ['whats', 'plane', 'behind', 'seven hundred and thirty-seven', 'dreamliner'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['This', 'is', 'a', 'weird', 'one', 'for', 'sure']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weird', 'on', 'sur'], ['weird', 'one', 'sure'])\n",
      "original document: \n",
      "['Can', 'confirm.', 'Am', 'fan.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['confirm', 'fan'], ['confirm', 'fan'])\n",
      "original document: \n",
      "['Wow', 'beautiful', 'wig,', 'what', 'a', 'fag']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'beauty', 'wig', 'fag'], ['wow', 'beautiful', 'wig', 'fag'])\n",
      "original document: \n",
      "['Mularkey!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mularkey'], ['mularkey'])\n",
      "original document: \n",
      "['Looks', 'normal', 'to', 'me,', 'cheers.', '🍻', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'norm', 'che'], ['look', 'normal', 'cheer'])\n",
      "original document: \n",
      "['Thanks', 'for', 'posting', '.', \"I've\", '', 'sent', 'the', 'link', 'to', 'others.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'post', 'iv', 'sent', 'link', 'oth'], ['thank', 'post', 'ive', 'send', 'link', 'others'])\n",
      "original document: \n",
      "['Everyone', 'wearing', 'pajamas', 'outside', 'of', 'the', 'house!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['everyon', 'wear', 'pajama', 'outsid', 'hous'], ['everyone', 'wear', 'pajamas', 'outside', 'house'])\n",
      "original document: \n",
      "['I', 'expected', 'a', 'Photoshop,', 'but', 'I', 'think', 'this', 'one', 'is', 'legit,', 'guys!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['expect', 'photoshop', 'think', 'on', 'legit', 'guy'], ['expect', 'photoshop', 'think', 'one', 'legit', 'guy'])\n",
      "original document: \n",
      "['Yeah,', 'sorry', 'about', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'sorry'], ['yeah', 'sorry'])\n",
      "original document: \n",
      "['22,', 'about', '4-6', 'months.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty-two', 'forty-six', 'month'], ['twenty-two', 'forty-six', 'months'])\n",
      "original document: \n",
      "['Try', 'using', '[[Liberty]]', 'or', 'JailProtect', 'from', \"Julio's\", 'repo.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'us', 'liberty', 'jailprotect', 'julio', 'repo'], ['try', 'use', 'liberty', 'jailprotect', 'julios', 'repo'])\n",
      "original document: \n",
      "['I', 'had', 'a', 'similar', 'problem', 'where', 'I', 'would', 'turn', 'it', 'on', 'and', 'audio', \"wouldn't\", 'come', 'out', 'or', 'it', 'would', 'be', 'flashing', 'the', 'menu', 'screen', 'while', 'ear', 'deafening', 'audio', 'would', 'play.', 'It', 'turned', 'out', 'that', 'it', 'was', 'because', 'of', 'the', 'HDMI', 'switch', 'that', 'I', 'was', 'using,', 'even', 'though', 'every', 'other', 'device', 'I', 'had', 'connected', 'to', 'it', 'worked', 'fine', 'the', 'snes', 'mini', 'just', \"doesn't\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['simil', 'problem', 'would', 'turn', 'audio', 'wouldnt', 'com', 'would', 'flash', 'menu', 'screen', 'ear', 'deaf', 'audio', 'would', 'play', 'turn', 'hdmi', 'switch', 'us', 'ev', 'though', 'every', 'dev', 'connect', 'work', 'fin', 'sne', 'min', 'doesnt'], ['similar', 'problem', 'would', 'turn', 'audio', 'wouldnt', 'come', 'would', 'flash', 'menu', 'screen', 'ear', 'deafen', 'audio', 'would', 'play', 'turn', 'hdmi', 'switch', 'use', 'even', 'though', 'every', 'device', 'connect', 'work', 'fine', 'snes', 'mini', 'doesnt'])\n",
      "original document: \n",
      "['Yep,', 'Just', 'got', 'my', 'copy', 'refunded.', 'Definitely', 'disappointing', 'that', 'it', \"doesn't\", 'live', 'up', 'to', 'the', 'old', 'forza', 'games.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'got', 'cop', 'refund', 'definit', 'disappoint', 'doesnt', 'liv', 'old', 'forz', 'gam'], ['yep', 'get', 'copy', 'refund', 'definitely', 'disappoint', 'doesnt', 'live', 'old', 'forza', 'game'])\n",
      "original document: \n",
      "['Dude,', 'CoD1', 'and', 'UO', 'are', 'still', 'active.', ':P']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dud', 'cod1', 'uo', 'stil', 'act', 'p'], ['dude', 'cod1', 'uo', 'still', 'active', 'p'])\n",
      "original document: \n",
      "['Noice.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noic'], ['noice'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'seeing', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'see'], ['im', 'see'])\n",
      "original document: \n",
      "['D-Daddy...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ddaddy'], ['ddaddy'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/traps/comments/57fwi6/i_go_on_dates_with_guys_and_dont_tell_them_im/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrtrapscomments57fwi6i_go_on_dates_with_guys_and_dont_tell_them_im'], ['httpswwwredditcomrtrapscomments57fwi6i_go_on_dates_with_guys_and_dont_tell_them_im'])\n",
      "original document: \n",
      "['Okay', 'np']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'np'], ['okay', 'np'])\n",
      "original document: \n",
      "['HEY', 'SIR', \"THAT'S\", 'MY', 'VOICE']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'sir', 'that', 'voic'], ['hey', 'sir', 'thats', 'voice'])\n",
      "original document: \n",
      "['Aghs', 'BS', 'is', 'pretty', 'common', 'in', 'pro', 'games', 'tho.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agh', 'bs', 'pretty', 'common', 'pro', 'gam', 'tho'], ['aghs', 'bs', 'pretty', 'common', 'pro', 'game', 'tho'])\n",
      "original document: \n",
      "['yeah', 'it', 'was', 'about', '[Q2', '2015](http://darrellx.com/buildapc/in-depth-seagate-western-digitals-harddrive-price-fixing/)', 'when', 'on', 'a', 'deal', 'you', 'could', 'snag', 'a', 'TB', 'at', 'less', 'per', 'TB', 'than', '2011.', '', 'Still', 'pisses', 'me', 'off.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'q2', '2015httpdarrellxcombuildapcindepthseagatewesterndigitalsharddrivepricefixing', 'deal', 'could', 'snag', 'tb', 'less', 'per', 'tb', 'two thousand and eleven', 'stil', 'piss'], ['yeah', 'q2', '2015httpdarrellxcombuildapcindepthseagatewesterndigitalsharddrivepricefixing', 'deal', 'could', 'snag', 'tb', 'less', 'per', 'tb', 'two thousand and eleven', 'still', 'piss'])\n",
      "original document: \n",
      "['One', 'random', 'spot', 'please!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'spot', 'pleas'], ['one', 'random', 'spot', 'please'])\n",
      "original document: \n",
      "['or', 'german,', 'or', 'portuguese,', 'or', 'dutch,', 'or', '~~romans~~', 'italian,', 'or', 'russian,', 'or', 'from', 'the', 'Balkans,', 'or', '...\\n\\nYou', 'know', 'what,', 'just', 'tell', 'us', 'where', 'you', 'come', 'from,', \"that'd\", 'make', 'it', 'easier.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['germ', 'portugues', 'dutch', 'rom', 'it', 'russ', 'balk', '\\n\\nyou', 'know', 'tel', 'us', 'com', 'thatd', 'mak', 'easy'], ['german', 'portuguese', 'dutch', 'romans', 'italian', 'russian', 'balkans', '\\n\\nyou', 'know', 'tell', 'us', 'come', 'thatd', 'make', 'easier'])\n",
      "original document: \n",
      "['yes', 'exactly!', 'Most', 'fish', 'once', 'you', 'get', 'them', 'in', 'a', 'good', 'tank', 'will', 'change', 'color,', 'typically', 'become', 'more', 'vibrant\\n\\nBetas', 'are', 'one', 'of', 'the', 'few', 'where', 'the', 'color', 'change', 'can', 'be', 'so', 'drastic', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'exact', 'fish', 'get', 'good', 'tank', 'chang', 'col', 'typ', 'becom', 'vibrant\\n\\nbetas', 'on', 'col', 'chang', 'drast'], ['yes', 'exactly', 'fish', 'get', 'good', 'tank', 'change', 'color', 'typically', 'become', 'vibrant\\n\\nbetas', 'one', 'color', 'change', 'drastic'])\n",
      "original document: \n",
      "[\"I've\", 'read', 'that', 'part', 'too.', 'And', 'it', \"doesn't\", 'change', 'my', 'mind', 'about', 'corruption', 'that', \"I've\", 'stated', 'before.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'read', 'part', 'doesnt', 'chang', 'mind', 'corrupt', 'iv', 'stat'], ['ive', 'read', 'part', 'doesnt', 'change', 'mind', 'corruption', 'ive', 'state'])\n",
      "original document: \n",
      "['Won', 'Super', 'Bowls', 'as', 'a', 'coach,', 'AND', 'a', 'player.', 'Still', 'no', 'love.', 'Also,', 'first', 'Hispanic', 'head', 'coach', 'to', 'win', 'a', 'Super', 'Bowl.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sup', 'bowl', 'coach', 'play', 'stil', 'lov', 'also', 'first', 'hisp', 'head', 'coach', 'win', 'sup', 'bowl'], ['super', 'bowl', 'coach', 'player', 'still', 'love', 'also', 'first', 'hispanic', 'head', 'coach', 'win', 'super', 'bowl'])\n",
      "original document: \n",
      "['kinosaurus6']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kinosaurus6'], ['kinosaurus6'])\n",
      "original document: \n",
      "['Seems', 'to', 'be', 'spurious:', 'https://en.wikiquote.org/wiki/Aristotle#Disputed\\n\\nI', 'agree', 'with', 'the', 'sentiment,', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seem', 'spury', 'httpsenwikiquoteorgwikiaristotledisputed\\n\\ni', 'agr', 'senty', 'though'], ['seem', 'spurious', 'httpsenwikiquoteorgwikiaristotledisputed\\n\\ni', 'agree', 'sentiment', 'though'])\n",
      "original document: \n",
      "['To', 'me', \"it's\", 'kind', 'of', 'a', 'costume', 'I', 'put', 'on', '5', 'or', '6', 'times', 'a', 'year.', \"It's\", 'fun', 'and', 'women', 'love', 'it.', 'If', 'I', 'had', 'to', 'wear', 'every', 'day', \"I'd\", 'be', 'in', 'the', 'wrong', 'career.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'costum', 'put', 'fiv', 'six', 'tim', 'year', 'fun', 'wom', 'lov', 'wear', 'every', 'day', 'id', 'wrong', 'car'], ['kind', 'costume', 'put', 'five', 'six', 'time', 'year', 'fun', 'women', 'love', 'wear', 'every', 'day', 'id', 'wrong', 'career'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['If', 'your', 'bench', 'is', 'big', 'enough', 'to', 'stash', 'him', 'and', 'you', 'have', 'someone', 'worth', 'dropping,', 'why', \"not?\\n\\nWho's\", 'available', 'on', 'the', 'WW/FA', 'that', \"you're\", 'thinking', 'about?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bench', 'big', 'enough', 'stash', 'someon', 'wor', 'drop', 'not\\n\\nwhos', 'avail', 'wwfa', 'yo', 'think'], ['bench', 'big', 'enough', 'stash', 'someone', 'worth', 'drop', 'not\\n\\nwhos', 'available', 'wwfa', 'youre', 'think'])\n",
      "original document: \n",
      "['John', 'Malkovich', 'seems', 'to', 'know', 'what', 'he’s', 'talking', 'about.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['john', 'malkovich', 'seem', 'know', 'hes', 'talk'], ['john', 'malkovich', 'seem', 'know', 'hes', 'talk'])\n",
      "original document: \n",
      "['they', 'added', 'favele', 'back', 'years', 'ago']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad', 'favel', 'back', 'year', 'ago'], ['add', 'favele', 'back', 'years', 'ago'])\n",
      "original document: \n",
      "[\"I'm\", 'a', \"6'8\", 'SF', 'Playmaker/Shot', 'Creator', 'and', \"I'm\", 'shooting', 'close', 'to', '47%', 'at', '81', 'ovr.\\n\\nI', 'basically', 'just', 'do', 'pick', 'n', 'roll', 'and', 'can', 'usually', 'beat', 'my', 'guy', 'to', 'rim', '1-on-1']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sixty-eight', 'sf', 'playmakershot', 'cre', 'im', 'shoot', 'clos', 'forty-seven', 'eighty-one', 'ovr\\n\\ni', 'bas', 'pick', 'n', 'rol', 'us', 'beat', 'guy', 'rim', '1on1'], ['im', 'sixty-eight', 'sf', 'playmakershot', 'creator', 'im', 'shoot', 'close', 'forty-seven', 'eighty-one', 'ovr\\n\\ni', 'basically', 'pick', 'n', 'roll', 'usually', 'beat', 'guy', 'rim', '1on1'])\n",
      "original document: \n",
      "['Yes!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['It', \"doesn't\", 'start', 'shooting', 'until', 'January', 'next', 'year.....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'start', 'shoot', 'janu', 'next', 'year'], ['doesnt', 'start', 'shoot', 'january', 'next', 'year'])\n",
      "original document: \n",
      "['It', 'depends', 'on', 'where', 'you', 'go.', '', 'Go', 'somewhere', 'that', 'makes', 'the', 'most', 'economical', 'sense', 'such', 'as', 'An', 'in-state', 'school', '(if', 'your', 'not', 'in', 'like', 'California),', 'or', 'any', 'non-for-profit', 'schools.\\n\\nAs', 'a', 'musician', 'or', 'producer,', 'you', 'will', 'naturally', 'not', 'be', 'making', 'a', 'whole', 'lot', 'to', 'pay', 'off', 'a', 'huge', 'amount', 'of', 'debt', 'at', 'a', 'reasonable', 'rate', 'unless', 'you', 'would', 'make', 'it', 'big.', '', 'And', 'let’s', 'face', 'it,', 'few', 'make', 'it', 'big.', '', 'So', 'if', 'you', 'college', 'would', 'cost', 'you', '$200,000', 'for', '4', 'Year’s,', 'not', 'worth', 'it.', '', 'If', 'college', 'would', 'cost', 'you', '$50,000', 'potentially!\\n\\nA', 'degree', 'would', 'provide', 'a', 'good', 'structured', 'environment', 'and', 'fellow', 'peers', 'that', 'would', 'allow', 'you', 'to', 'collaborate', 'and', 'build', 'connections', 'with.', '', 'At', 'the', 'end', 'of', 'the', 'program,', 'you', 'would', 'know', 'your', 'stuff,', 'and', 'proof', 'to', 'back', 'it', 'up,', 'while', 'without', 'a', 'degree', 'you', 'may', 'have', 'not', 'had', 'experience', 'with', 'one', 'thing,', 'but', 'a', 'lot', 'in', 'another.\\n\\nFor', 'me,', 'I’m', 'going', 'to', 'school', 'for', 'a', 'job', 'that', 'pays', 'alright', 'that', 'I', 'enjoy,', 'but', 'not', 'my', 'favorite', 'hobby,', 'while', 'also', 'getting', 'a', 'music', 'minor.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'go', 'go', 'somewh', 'mak', 'econom', 'sens', 'inst', 'school', 'lik', 'californ', 'nonforprofit', 'schools\\n\\nas', 'mus', 'produc', 'nat', 'mak', 'whol', 'lot', 'pay', 'hug', 'amount', 'debt', 'reason', 'rat', 'unless', 'would', 'mak', 'big', 'let', 'fac', 'mak', 'big', 'colleg', 'would', 'cost', 'two hundred thousand', 'four', 'year', 'wor', 'colleg', 'would', 'cost', 'fifty thousand', 'potentially\\n\\na', 'degr', 'would', 'provid', 'good', 'structured', 'environ', 'fellow', 'peer', 'would', 'allow', 'collab', 'build', 'connect', 'end', 'program', 'would', 'know', 'stuff', 'proof', 'back', 'without', 'degr', 'may', 'expery', 'on', 'thing', 'lot', 'another\\n\\nf', 'im', 'going', 'school', 'job', 'pay', 'alright', 'enjoy', 'favorit', 'hobby', 'also', 'get', 'mus', 'minor\\n\\n'], ['depend', 'go', 'go', 'somewhere', 'make', 'economical', 'sense', 'instate', 'school', 'like', 'california', 'nonforprofit', 'schools\\n\\nas', 'musician', 'producer', 'naturally', 'make', 'whole', 'lot', 'pay', 'huge', 'amount', 'debt', 'reasonable', 'rate', 'unless', 'would', 'make', 'big', 'let', 'face', 'make', 'big', 'college', 'would', 'cost', 'two hundred thousand', 'four', 'years', 'worth', 'college', 'would', 'cost', 'fifty thousand', 'potentially\\n\\na', 'degree', 'would', 'provide', 'good', 'structure', 'environment', 'fellow', 'peer', 'would', 'allow', 'collaborate', 'build', 'connections', 'end', 'program', 'would', 'know', 'stuff', 'proof', 'back', 'without', 'degree', 'may', 'experience', 'one', 'thing', 'lot', 'another\\n\\nfor', 'im', 'go', 'school', 'job', 'pay', 'alright', 'enjoy', 'favorite', 'hobby', 'also', 'get', 'music', 'minor\\n\\n'])\n",
      "original document: \n",
      "['i', 'use', 'both', 'the', 'greens', 'and', 'the', 'whites.', 'if', 'you', 'cook', 'them', 'for', 'a', 'couple', 'of', 'hours,', \"they'll\", 'be', 'soft', 'enough.', 'i', 'know', \"that's\", 'anathema', 'to', 'some,', 'so', 'maybe', 'one', 'of', 'these', 'days', 'you', 'should', 'try', 'both', 'with', 'and', 'without', 'the', 'greens', 'and', 'see', 'if', 'it', 'makes', 'a', 'difference', 'to', 'you.', 'lots', 'of', 'less-informed', 'people', 'use', 'leek', 'greens', 'all', 'the', 'time,', 'and', 'leek', 'greens', '(which', 'i', 'realize', 'are', 'not', 'the', 'same', 'thing', 'as', 'green', 'onions)', 'are', 'commonly', 'used', 'in', 'certain', 'Chinese', 'dishes.\\n\\nand', 'like', 'others', 'have', 'said,', 'rinse', 'rinse', 'rinse', 'all', 'the', 'dirt', 'out!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'green', 'whit', 'cook', 'coupl', 'hour', 'theyl', 'soft', 'enough', 'know', 'that', 'anathem', 'mayb', 'on', 'day', 'try', 'without', 'green', 'see', 'mak', 'diff', 'lot', 'lessinform', 'peopl', 'us', 'leek', 'green', 'tim', 'leek', 'green', 'real', 'thing', 'green', 'on', 'common', 'us', 'certain', 'chines', 'dishes\\n\\nand', 'lik', 'oth', 'said', 'rins', 'rins', 'rins', 'dirt'], ['use', 'green', 'white', 'cook', 'couple', 'hours', 'theyll', 'soft', 'enough', 'know', 'thats', 'anathema', 'maybe', 'one', 'days', 'try', 'without', 'green', 'see', 'make', 'difference', 'lot', 'lessinformed', 'people', 'use', 'leek', 'green', 'time', 'leek', 'green', 'realize', 'thing', 'green', 'onions', 'commonly', 'use', 'certain', 'chinese', 'dishes\\n\\nand', 'like', 'others', 'say', 'rinse', 'rinse', 'rinse', 'dirt'])\n",
      "original document: \n",
      "['That', 'back', 'seam', 'is', 'fucking', 'great', 'man.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['back', 'seam', 'fuck', 'gre', 'man'], ['back', 'seam', 'fuck', 'great', 'man'])\n",
      "original document: \n",
      "['Traps', 'are', 'gay', 'and', \"that's\", 'okay.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trap', 'gay', 'that', 'okay'], ['trap', 'gay', 'thats', 'okay'])\n",
      "original document: \n",
      "['\"I', 'do', 'not', 'admit', 'that', 'a', 'great', 'wrong', 'has', 'been', 'done', 'to', 'the', 'Red', 'Indians', 'of', 'America,', 'or', 'the', 'black', 'people', 'of', 'Australia', 'by', 'the', 'fact', 'that', 'a', 'stronger', 'race,', 'a', 'higher', 'grade', 'race', 'has', 'come', 'in', 'and', 'taken', 'its', 'place.\"\\n\\nHe', 'was', 'a', 'fucko', 'by', 'nearly', 'any', 'modern', 'standard.', '', 'That', 'said,', 'he', 'did', 'have', 'some', 'redeeming', 'qualities.', 'I', 'think', 'the', 'main', 'problem', 'with', 'most', 'Churchill', 'films', 'is', 'that', 'they', 'fail', 'to', 'show', 'both', 'sides', 'of', 'the', 'man.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['admit', 'gre', 'wrong', 'don', 'red', 'ind', 'americ', 'black', 'peopl', 'austral', 'fact', 'stronger', 'rac', 'high', 'grad', 'rac', 'com', 'tak', 'place\\n\\nh', 'fucko', 'near', 'modern', 'standard', 'said', 'redeem', 'qual', 'think', 'main', 'problem', 'churchil', 'film', 'fail', 'show', 'sid', 'man'], ['admit', 'great', 'wrong', 'do', 'red', 'indians', 'america', 'black', 'people', 'australia', 'fact', 'stronger', 'race', 'higher', 'grade', 'race', 'come', 'take', 'place\\n\\nhe', 'fucko', 'nearly', 'modern', 'standard', 'say', 'redeem', 'qualities', 'think', 'main', 'problem', 'churchill', 'film', 'fail', 'show', 'side', 'man'])\n",
      "original document: \n",
      "['The', 'only', 'reason', 'I', 'have', 'two', 'landing', 'pads', 'instead', 'of', 'one', 'is', 'to', 'have', 'ships', 'visit', 'and', 'make', 'the', 'base', 'seem', 'more', 'alive.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'two', 'land', 'pad', 'instead', 'on', 'ship', 'visit', 'mak', 'bas', 'seem', 'al'], ['reason', 'two', 'land', 'pad', 'instead', 'one', 'ship', 'visit', 'make', 'base', 'seem', 'alive'])\n",
      "original document: \n",
      "['I', 'think', 'the', 'only', 'need', 'he', 'needs', 'is', 'his', 'turrets.', 'Not', 'their', 'damage', 'or', 'anything', 'but', 'only', 'their', 'fire', 'rate.', 'If', 'you', 'lower', 'that', 'you', 'could', 'make', 'his', 'turrets', 'much', 'more', 'balanced.', 'It’s', 'just', 'the', 'torvald', 'Barik', 'combination', 'that', 'has', 'become', 'the', 'meta,', 'it’s', 'hard', 'to', 'balance', 'wrecker,', 'caut', 'and', 'bulldozer', 'in', 'a', 'team.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'nee', 'nee', 'turret', 'dam', 'anyth', 'fir', 'rat', 'low', 'could', 'mak', 'turret', 'much', 'bal', 'torvald', 'barik', 'combin', 'becom', 'met', 'hard', 'bal', 'wreck', 'caut', 'bulldoz', 'team'], ['think', 'need', 'need', 'turrets', 'damage', 'anything', 'fire', 'rate', 'lower', 'could', 'make', 'turrets', 'much', 'balance', 'torvald', 'barik', 'combination', 'become', 'meta', 'hard', 'balance', 'wrecker', 'caut', 'bulldozer', 'team'])\n",
      "original document: \n",
      "['It', 'honestly', 'never', 'occurred', 'to', 'me', 'that', 'people', 'saw', 'the', 'two', 'entities', 'as', 'separate.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'nev', 'occur', 'peopl', 'saw', 'two', 'ent', 'sep'], ['honestly', 'never', 'occur', 'people', 'saw', 'two', 'entities', 'separate'])\n",
      "original document: \n",
      "['I', 'am', 'very', 'sorry', 'for', 'your', 'pain.', '', '\\nWas', 'your', 'husband', '17', 'when', 'you', 'married??']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'pain', '\\nwas', 'husband', 'seventeen', 'marry'], ['sorry', 'pain', '\\nwas', 'husband', 'seventeen', 'marry'])\n",
      "original document: \n",
      "['[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoeidp/):\\n\\nThanks', 'for', 'the', 'reply!', '\\n\\nI', 'think', 'that', 'even', 'in', 'books', 'that', 'feature', 'primarily', 'or', 'solely', 'non-human', 'characters', 'the', 'main', 'scope', 'would', 'still', 'fall', 'under', 'sociology,', 'since', 'it', 'is', 'impossible', 'to', 'be', 'completely', 'divorced', 'from', 'our', 'own', 'humanity.', 'It', 'would', 'then', 'be', 'a', 'view', 'of', 'humanity', 'through', 'an', 'alien', 'lens,', 'but', 'still', 'a', 'human', 'view', 'by', 'necessity.', 'Because', 'if', 'it', 'were', 'to', 'be', 'fully', 'alien', 'then', 'there', 'would', 'be', 'nothing', 'for', 'a', 'human', 'reader', 'to', 'identify', 'themselves', 'with.', '\\n\\nIn', 'the', 'seminal', '\"The', 'Dance', 'of', 'the', 'Changer', 'and', 'Three\"', 'by', 'Terry', 'Carr', 'the', 'only', 'human', 'character', 'is', 'the', 'narrator', 'who', 'tries', 'to', 'explain', 'this', 'fully', 'alien', 'story', 'to', 'a', 'human', 'audience', 'and', 'coming', 'to', 'the', 'conclusion', 'that', 'the', 'story', 'must', 'be', 'taken', 'as', 'is,', 'because', 'they', 'are', 'alien', 'any', 'human', 'interpretation', 'of', 'this', 'story', 'that', 'is', 'so', 'important', 'to', 'this', 'alien', 'culture', 'must', 'always', 'fall', 'flat', 'because', 'we', 'cannot', 'ever', 'completely', 'understand', 'their', 'psychology.', '\\n\\nThe', 'Dance', 'of', 'the', 'Changer', 'and', 'Three', 'teaches', 'us', 'that', 'whatever', 'we', 'do,', 'we', 'will', 'always', 'be', 'locked', 'inside', 'our', 'own', 'skulls,', 'viewing', 'the', 'universe', 'through', 'human', 'eyes.', '\\n\\nI', '', 'wish', 'you', 'the', 'best', 'and', 'hope', 'your', 'book', 'is', 'a', 'great', 'success.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoeidp\\n\\nthanks', 'reply', '\\n\\ni', 'think', 'ev', 'book', 'feat', 'prim', 'sol', 'nonhum', 'charact', 'main', 'scop', 'would', 'stil', 'fal', 'sociolog', 'sint', 'imposs', 'complet', 'divorc', 'hum', 'would', 'view', 'hum', 'aly', 'len', 'stil', 'hum', 'view', 'necess', 'ful', 'aly', 'would', 'noth', 'hum', 'read', 'ident', '\\n\\nin', 'semin', 'dant', 'chang', 'three', 'terry', 'car', 'hum', 'charact', 'nar', 'tri', 'explain', 'ful', 'aly', 'story', 'hum', 'audy', 'com', 'conclud', 'story', 'must', 'tak', 'aly', 'hum', 'interpret', 'story', 'import', 'aly', 'cult', 'must', 'alway', 'fal', 'flat', 'cannot', 'ev', 'complet', 'understand', 'psycholog', '\\n\\nthe', 'dant', 'chang', 'three', 'teach', 'us', 'whatev', 'alway', 'lock', 'insid', 'skul', 'view', 'univers', 'hum', 'ey', '\\n\\ni', 'wish', 'best', 'hop', 'book', 'gre', 'success'], ['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoeidp\\n\\nthanks', 'reply', '\\n\\ni', 'think', 'even', 'book', 'feature', 'primarily', 'solely', 'nonhuman', 'character', 'main', 'scope', 'would', 'still', 'fall', 'sociology', 'since', 'impossible', 'completely', 'divorce', 'humanity', 'would', 'view', 'humanity', 'alien', 'lens', 'still', 'human', 'view', 'necessity', 'fully', 'alien', 'would', 'nothing', 'human', 'reader', 'identify', '\\n\\nin', 'seminal', 'dance', 'changer', 'three', 'terry', 'carr', 'human', 'character', 'narrator', 'try', 'explain', 'fully', 'alien', 'story', 'human', 'audience', 'come', 'conclusion', 'story', 'must', 'take', 'alien', 'human', 'interpretation', 'story', 'important', 'alien', 'culture', 'must', 'always', 'fall', 'flat', 'cannot', 'ever', 'completely', 'understand', 'psychology', '\\n\\nthe', 'dance', 'changer', 'three', 'teach', 'us', 'whatever', 'always', 'lock', 'inside', 'skulls', 'view', 'universe', 'human', 'eye', '\\n\\ni', 'wish', 'best', 'hope', 'book', 'great', 'success'])\n",
      "original document: \n",
      "['Jemmye', 'claims', 'to', 'be', 'smart', 'because', \"she's\", 'not', 'a', 'good', 'athlete', 'and', 'out', 'of', 'shape.', \"She's\", 'a', 'good', 'social', 'player,', 'but', 'is', 'also', 'weak', 'mentally.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jemmy', 'claim', 'smart', 'she', 'good', 'athlet', 'shap', 'she', 'good', 'soc', 'play', 'also', 'weak', 'ment'], ['jemmye', 'claim', 'smart', 'shes', 'good', 'athlete', 'shape', 'shes', 'good', 'social', 'player', 'also', 'weak', 'mentally'])\n",
      "original document: \n",
      "[\"I'd\", 'drive', 'it..maybe', 'change', 'out', 'those', 'headlights..', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'driv', 'itmayb', 'chang', 'headlight'], ['id', 'drive', 'itmaybe', 'change', 'headlights'])\n",
      "original document: \n",
      "['Honestly', 'as', 'much', 'as', 'I', 'wish', 'he', \"wasn't\", 'the', 'qb', 'I', 'do', 'respect', 'him', 'for', 'trying.', 'Chicago', 'is', 'by', 'no', 'means', 'a', 'dream', 'team', 'to', 'play', 'on', 'and', 'he', 'knew', 'that', 'coming', 'in', 'here.', 'It', 'was', 'a', 'shot', 'in', 'the', 'dark', 'and', 'I', 'thank', 'him', 'for', 'giving', 'me', 'something', 'to', 'cheer', 'for', 'I', 'guess.', 'Would', 'love', 'to', 'keep', 'him', 'as', 'a', 'back', 'up,', 'I', 'think', 'he', 'makes', 'a', 'decent', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'much', 'wish', 'wasnt', 'qb', 'respect', 'try', 'chicago', 'mean', 'dream', 'team', 'play', 'knew', 'com', 'shot', 'dark', 'thank', 'giv', 'someth', 'che', 'guess', 'would', 'lov', 'keep', 'back', 'think', 'mak', 'dec', 'on'], ['honestly', 'much', 'wish', 'wasnt', 'qb', 'respect', 'try', 'chicago', 'mean', 'dream', 'team', 'play', 'know', 'come', 'shoot', 'dark', 'thank', 'give', 'something', 'cheer', 'guess', 'would', 'love', 'keep', 'back', 'think', 'make', 'decent', 'one'])\n",
      "original document: \n",
      "['143412306|', '&gt;', 'None', 'Anonymous', '(ID:', '0SeYs2Tv)\\n\\n&gt;&gt;143412250', '(OP)\\nneck', 'yourself\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, three hundred and six', 'gt', 'non', 'anonym', 'id', '0seys2tv\\n\\ngtgt143412250', 'op\\nneck', 'yourself\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, three hundred and six', 'gt', 'none', 'anonymous', 'id', '0seys2tv\\n\\ngtgt143412250', 'op\\nneck', 'yourself\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['No,', 'I', 'am', 'not', 'vegan.', \"That's\", 'a', 'different', 'argument', 'in', 'my', 'opinion.', 'Rights', 'and', 'treatment', 'of', 'livestock', 'are,', 'of', 'course,', 'also', 'an', 'important', 'thing', 'to', 'be', 'critical', 'of.', 'However,', 'the', 'fact', 'that', 'PETA', 'runs', '\"adoption', 'centers\"', 'for', 'companion', 'pets', 'and', 'kills', 'almost', '90%', 'of', 'them', 'is', 'fucking', 'atrocious.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['veg', 'that', 'diff', 'argu', 'opin', 'right', 'tre', 'livestock', 'cours', 'also', 'import', 'thing', 'crit', 'howev', 'fact', 'pet', 'run', 'adopt', 'cent', 'comp', 'pet', 'kil', 'almost', 'nin', 'fuck', 'atrocy'], ['vegan', 'thats', 'different', 'argument', 'opinion', 'right', 'treatment', 'livestock', 'course', 'also', 'important', 'thing', 'critical', 'however', 'fact', 'peta', 'run', 'adoption', 'center', 'companion', 'pet', 'kill', 'almost', 'ninety', 'fuck', 'atrocious'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'm\", 'sure', 'this', '\"Skeptic', 'conference\"', 'was', 'very', 'informative', 'and', 'not', 'just', 'a', 'auditorium', 'full', 'of', 'people', 'who', 'continue', 'to', 'make', 'the', 'YouTube', 'comment', 'section', 'a', 'cesspool.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'skept', 'conf', 'inform', 'auditor', 'ful', 'peopl', 'continu', 'mak', 'youtub', 'com', 'sect', 'cesspool'], ['im', 'sure', 'skeptic', 'conference', 'informative', 'auditorium', 'full', 'people', 'continue', 'make', 'youtube', 'comment', 'section', 'cesspool'])\n",
      "original document: \n",
      "['There', 'are', 'some', 'pretty', 'good', 'LGA775', 'Motherboard', 'out', 'there', 'from', 'reputable', 'recyclers.', '', 'This', 'pc', 'has', 'a', 'Asus', 'p5g41t-m', 'lx', 'in', 'it.', '', 'It', 'is', 'the', 'second', 'one', 'I', 'have', 'bought', 'on', 'eBay.', '', 'The', 'other', 'is', 'in', 'a', 'PC', 'with', 'a', 'Q6600', 'I', 'made', 'for', 'the', 'kids', 'to', 'do', 'homework.', '', \"Can't\", 'beat', 'the', 'value', 'of', 'these', 'Core', '2.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'good', 'lga775', 'motherboard', 'reput', 'recyc', 'pc', 'as', 'p5g41tm', 'lx', 'second', 'on', 'bought', 'ebay', 'pc', 'q6600', 'mad', 'kid', 'homework', 'cant', 'beat', 'valu', 'cor', 'two'], ['pretty', 'good', 'lga775', 'motherboard', 'reputable', 'recyclers', 'pc', 'asus', 'p5g41tm', 'lx', 'second', 'one', 'buy', 'ebay', 'pc', 'q6600', 'make', 'kid', 'homework', 'cant', 'beat', 'value', 'core', 'two'])\n",
      "original document: \n",
      "['My', 'my', 'you', 'are', 'amazing', 'at', 'this...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amaz'], ['amaze'])\n",
      "original document: \n",
      "['Monroe', \"wasn't\", 'photographed', 'for', 'Playboy.', 'The', 'publishing', 'rights', 'to', 'the', 'photographs', \"weren't\", 'owned', 'by', 'Monroe.', 'They', 'had', 'been', 'taken', 'previous', 'to', \"Playboy's\", 'publication,', 'and', 'were', 'owned', 'by', 'a', 'third', 'party.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['monro', 'wasnt', 'photograph', 'playboy', 'publ', 'right', 'photograph', 'wer', 'own', 'monro', 'tak', 'prevy', 'playboy', 'publ', 'own', 'third', 'party'], ['monroe', 'wasnt', 'photograph', 'playboy', 'publish', 'right', 'photograph', 'werent', 'own', 'monroe', 'take', 'previous', 'playboys', 'publication', 'own', 'third', 'party'])\n",
      "original document: \n",
      "['Lowes', 'works', 'fine', 'for', 'me.', '', 'I', 'even', 'had', 'to', 'do', 'a', 'return', 'at', 'Lowes', 'and', 'had', 'to', '\"swipe\"', 'my', 'phone', 'for', 'that', 'and', 'the', 'return', 'processed', 'successfully.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['low', 'work', 'fin', 'ev', 'return', 'low', 'swip', 'phon', 'return', 'process', 'success'], ['low', 'work', 'fine', 'even', 'return', 'low', 'swipe', 'phone', 'return', 'process', 'successfully'])\n",
      "original document: \n",
      "['You', \"wouldn't\", 'jam', 'a', 'king', 'in', 'there?', 'It', 'takes', '2', 'seconds', 'and', 'you', \"don't\", 'have', 'to', 'stop', 'compressions.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'jam', 'king', 'tak', 'two', 'second', 'dont', 'stop', 'compress'], ['wouldnt', 'jam', 'king', 'take', 'two', 'second', 'dont', 'stop', 'compressions'])\n",
      "original document: \n",
      "['Best', 'moment', 'of', 'the', 'episode.', '\\n\\nI', 'was', 'never', 'really', 'a', 'fan', 'of', 'the', 'comedic', 'styling', 'in', 'some', 'of', 'the', 'older/no-longer-around', 'videos,', 'and', 'for', 'a', 'while', 'after', 'Greg', 'left', 'it', 'seemed', 'pretty', 'evident', 'that', 'Jirard', 'may', 'have', 'been', 'struggling', 'to', 'find', 'his', 'new', 'comedic', 'voice.', 'But', 'with', 'this', 'scene,', 'this', 'one', 'side-skit,', 'this', 'could', 'not', 'have', 'been', 'done', 'better.', 'It', 'may', 'not', 'be', 'to', 'some', 'peoples', 'taste,', 'but', 'to', 'me,', 'the', 'immediate', 'sight', 'gag', 'of', 'Brett', 'and', 'the', 'long', 'drawn', 'out', '\"where', 'is', 'this', 'going\"', 'Tarantino-esque', 'vibe,', 'followed', 'by', 'the', 'perfect', 'timing', 'of', 'the', 'punchline.', 'Goddamn.', 'I', \"wasn't\", 'laughing,', 'but', 'this', 'mother', 'fucker', 'right', 'here', 'could', 'have', 'given', 'the', 'Cheshire', 'Cat', 'a', \"fuckin'\", 'inferiority', 'complex.', 'Grinning', 'ear', 'to', 'ear.\\n\\nThis', 'shit', 'is', 'my', 'jam.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'mom', 'episod', '\\n\\ni', 'nev', 'real', 'fan', 'com', 'styl', 'oldernolongeraround', 'video', 'greg', 'left', 'seem', 'pretty', 'evid', 'jirard', 'may', 'struggling', 'find', 'new', 'com', 'voic', 'scen', 'on', 'sideskit', 'could', 'don', 'bet', 'may', 'peopl', 'tast', 'immedy', 'sight', 'gag', 'bret', 'long', 'drawn', 'going', 'tarantinoesqu', 'vib', 'follow', 'perfect', 'tim', 'punchlin', 'goddamn', 'wasnt', 'laugh', 'moth', 'fuck', 'right', 'could', 'giv', 'cheshir', 'cat', 'fuckin', 'infery', 'complex', 'grin', 'ear', 'ear\\n\\nthis', 'shit', 'jam'], ['best', 'moment', 'episode', '\\n\\ni', 'never', 'really', 'fan', 'comedic', 'style', 'oldernolongeraround', 'videos', 'greg', 'leave', 'seem', 'pretty', 'evident', 'jirard', 'may', 'struggle', 'find', 'new', 'comedic', 'voice', 'scene', 'one', 'sideskit', 'could', 'do', 'better', 'may', 'people', 'taste', 'immediate', 'sight', 'gag', 'brett', 'long', 'draw', 'go', 'tarantinoesque', 'vibe', 'follow', 'perfect', 'time', 'punchline', 'goddamn', 'wasnt', 'laugh', 'mother', 'fucker', 'right', 'could', 'give', 'cheshire', 'cat', 'fuckin', 'inferiority', 'complex', 'grin', 'ear', 'ear\\n\\nthis', 'shit', 'jam'])\n",
      "original document: \n",
      "['YES!', 'More', 'Kuzma', 'playing', 'time!!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'kuzm', 'play', 'tim'], ['yes', 'kuzma', 'play', 'time'])\n",
      "original document: \n",
      "['You', 'could', 'argue', 'that', 'Nazi', 'ideology', 'poses', 'a', 'threat']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'argu', 'naz', 'ideolog', 'pos', 'threat'], ['could', 'argue', 'nazi', 'ideology', 'pose', 'threat'])\n",
      "original document: \n",
      "['Damn,', 'they', 'shit', 'should', 'be', 'better', 'monitored', 'and', 'controlled.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'shit', 'bet', 'monit', 'control'], ['damn', 'shit', 'better', 'monitor', 'control'])\n",
      "original document: \n",
      "['Walmart.com\\nThey', 'deliver', 'for', 'free', 'and', 'you', 'can', 'take', 'what', 'you', 'do', 'not', 'want', 'right', 'back', 'to', 'the', 'store!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['walmartcom\\nthey', 'del', 'fre', 'tak', 'want', 'right', 'back', 'stor'], ['walmartcom\\nthey', 'deliver', 'free', 'take', 'want', 'right', 'back', 'store'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Fuck', 'Bethany']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'bethany'], ['fuck', 'bethany'])\n",
      "original document: \n",
      "['Thank', 'you', 'both', 'for', 'sharing', 'your', 'experiences!', 'It’s', 'helping', 'me', 'think', 'of', 'questions', 'to', 'ask', 'my', 'doctor.', 'I', 'feel', 'like', 'I', 'can', 'never', 'think', 'of', 'any', 'until', 'after', 'all', 'is', 'said', 'and', 'done.', '\\n\\nI’m', 'sorry', 'to', 'hear', 'about', 'your', 'failed', 'induction', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'shar', 'expery', 'help', 'think', 'quest', 'ask', 'doct', 'feel', 'lik', 'nev', 'think', 'said', 'don', '\\n\\nim', 'sorry', 'hear', 'fail', 'induc'], ['thank', 'share', 'experience', 'help', 'think', 'question', 'ask', 'doctor', 'feel', 'like', 'never', 'think', 'say', 'do', '\\n\\nim', 'sorry', 'hear', 'fail', 'induction'])\n",
      "original document: \n",
      "['Yeah', 'that', 'was', 'a', 'completely', 'isolated', 'yellow', 'card', 'incident,', \"shouldn't\", 'have', 'anything', 'to', 'do', 'with', 'repeated', 'infringements.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'complet', 'isol', 'yellow', 'card', 'incid', 'shouldnt', 'anyth', 'rep', 'infr'], ['yeah', 'completely', 'isolate', 'yellow', 'card', 'incident', 'shouldnt', 'anything', 'repeat', 'infringements'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'low', 'key', 'believe', 'that', 'they', 'purposefully', 'lower', 'the', 'green', 'orb', 'rate', 'during', 'certain', 'banners.', '', 'I', 'sniped', 'exclusively', 'green', '(and', 'even', 'bought', '100+', 'orbs)', 'on', 'the', 'Hero', 'Fest', 'banner', 'a', 'while', 'back,', 'and', 'in', 'every', 'summon,', 'never', 'more', 'than', '2', 'green', 'orbs', 'appeared', 'at', 'once,', 'when', 'they', 'bothered', 'to', 'show', 'up', 'at', 'all.\\n\\n/tinfoil', 'hat']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['low', 'key', 'believ', 'purpos', 'low', 'green', 'orb', 'rat', 'certain', 'ban', 'snip', 'exclud', 'green', 'ev', 'bought', 'one hundred', 'orb', 'hero', 'fest', 'ban', 'back', 'every', 'summon', 'nev', 'two', 'green', 'orb', 'appear', 'both', 'show', 'all\\n\\ntinfoil', 'hat'], ['low', 'key', 'believe', 'purposefully', 'lower', 'green', 'orb', 'rate', 'certain', 'banners', 'snip', 'exclusively', 'green', 'even', 'buy', 'one hundred', 'orb', 'hero', 'fest', 'banner', 'back', 'every', 'summon', 'never', 'two', 'green', 'orb', 'appear', 'bother', 'show', 'all\\n\\ntinfoil', 'hat'])\n",
      "original document: \n",
      "['Veignn']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['veign'], ['veignn'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnof35w/):\\n\\nThat', 'sounds', 'interesting-', \"I'll\", 'have', 'to', 'give', 'it', 'a', 'read!', '\\n\\nThank', 'you', 'so', 'much', 'for', 'your', 'support!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnof35w\\n\\nthat', 'sound', 'interest', 'il', 'giv', 'read', '\\n\\nthank', 'much', 'support'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnof35w\\n\\nthat', 'sound', 'interest', 'ill', 'give', 'read', '\\n\\nthank', 'much', 'support'])\n",
      "original document: \n",
      "['Did', 'anything', 'change', 'with', 'your', 'settings?', 'Double', 'check', 'all', 'the', 'boxes', 'and', 'addresses.', 'I’ve', 'had', 'the', 'window', 'immediately', 'close', 'when', 'I', 'have', 'errors', 'in', 'my', 'command/batch', 'file']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyth', 'chang', 'set', 'doubl', 'check', 'box', 'address', 'iv', 'window', 'immedy', 'clos', 'er', 'commandbatch', 'fil'], ['anything', 'change', 'settings', 'double', 'check', 'box', 'address', 'ive', 'window', 'immediately', 'close', 'errors', 'commandbatch', 'file'])\n",
      "original document: \n",
      "['Damn,', 'Humans', 'of', 'New', 'York', 'is', 'really', 'getting', 'stretched', 'for', 'ideas.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'hum', 'new', 'york', 'real', 'get', 'stretched', 'idea'], ['damn', 'humans', 'new', 'york', 'really', 'get', 'stretch', 'ideas'])\n",
      "original document: \n",
      "['Mahalo', 'broseph', 'ranch', 'it', 'up', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mahalo', 'broseph', 'ranch'], ['mahalo', 'broseph', 'ranch'])\n",
      "original document: \n",
      "['So', 'what', \"you're\", 'saying', \"is...\\n\\n...we're\", 'gonna', 'need', 'a', 'bigger', 'bolt']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'say', 'is\\n\\nwere', 'gonn', 'nee', 'big', 'bolt'], ['youre', 'say', 'is\\n\\nwere', 'gonna', 'need', 'bigger', 'bolt'])\n",
      "original document: \n",
      "['love', 'you', 'bb']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'bb'], ['love', 'bb'])\n",
      "original document: \n",
      "['We', 'ran', 'out', 'a', 'couple', 'of', 'weeks', 'ago', '):']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ran', 'coupl', 'week', 'ago'], ['run', 'couple', 'weeks', 'ago'])\n",
      "original document: \n",
      "['I', 'wanted', 'to', 'join', 'the', 'military.', 'I', 'wanted', 'to', 'see', 'the', 'world,', 'and', 'to', 'prove', 'that', 'there', 'was', 'more', 'to', 'me', 'than', 'everybody', 'thought.\\n\\nOne', 'bout', 'of', 'self-harming', 'later,', 'that', 'dream', 'was', 'dead.', \"I'm\", 'now', 'a', 'engineer', 'who', 'spends', 'everyday', 'trying', 'to', 'prove', 'that', 'there', 'is', 'more', 'to', 'me', 'than', 'everybody', 'thinks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'join', 'milit', 'want', 'see', 'world', 'prov', 'everybody', 'thought\\n\\none', 'bout', 'selfharm', 'lat', 'dream', 'dead', 'im', 'engin', 'spend', 'everyday', 'try', 'prov', 'everybody', 'think'], ['want', 'join', 'military', 'want', 'see', 'world', 'prove', 'everybody', 'thought\\n\\none', 'bout', 'selfharming', 'later', 'dream', 'dead', 'im', 'engineer', 'spend', 'everyday', 'try', 'prove', 'everybody', 'think'])\n",
      "original document: \n",
      "['Ctrl-alt-NOPE']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ctrlaltnope'], ['ctrlaltnope'])\n",
      "original document: \n",
      "['I', 'work', 'for', 'Child', 'Services.', 'We', 'had', 'a', 'parent', 'whose', 'kids', 'were', 'detained', 'because', 'we', 'received', 'a', 'report', 'that', 'both', 'parents', 'were', 'dealing', 'drugs', 'from', 'the', 'home', 'shared', 'with', 'the', 'kids', 'and', 'using', 'regularly', 'without', 'anyone', 'else', 'there', 'to', 'watch', 'the', 'children.', 'When', 'investigated,', 'this', 'parent', 'admitted', 'to', 'all', 'of', 'the', 'allegations', 'as', 'well', 'as', 'to', 'shooting', 'up', 'in', 'front', 'of', 'the', 'kids.', 'After', 'the', 'intial', 'hearing,', 'this', 'parent', 'posted', 'on', 'Facebook', 'that', 'they', 'were', '\"lockin', 'n', 'loadin.', 'comin', 'for', 'that', 'fat', 'bitch', 'and', 'mofo', 'lawyer', 'who', 'think', 'they', 'can', 'take', 'my', 'kids.\"\\n\\nThere', 'have', 'been', 'other', 'vague', 'threats', 'and', 'anger', 'at', 'court,', 'but', 'usually', 'more', 'ambiguous', 'than', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'child', 'serv', 'par', 'whos', 'kid', 'detain', 'receiv', 'report', 'par', 'deal', 'drug', 'hom', 'shar', 'kid', 'us', 'regul', 'without', 'anyon', 'els', 'watch', 'childr', 'investig', 'par', 'admit', 'alleg', 'wel', 'shoot', 'front', 'kid', 'int', 'hear', 'par', 'post', 'facebook', 'lockin', 'n', 'loadin', 'comin', 'fat', 'bitch', 'mofo', 'lawy', 'think', 'tak', 'kids\\n\\nthere', 'vagu', 'threats', 'ang', 'court', 'us', 'ambigu'], ['work', 'child', 'service', 'parent', 'whose', 'kid', 'detain', 'receive', 'report', 'parent', 'deal', 'drug', 'home', 'share', 'kid', 'use', 'regularly', 'without', 'anyone', 'else', 'watch', 'children', 'investigate', 'parent', 'admit', 'allegations', 'well', 'shoot', 'front', 'kid', 'intial', 'hear', 'parent', 'post', 'facebook', 'lockin', 'n', 'loadin', 'comin', 'fat', 'bitch', 'mofo', 'lawyer', 'think', 'take', 'kids\\n\\nthere', 'vague', 'threats', 'anger', 'court', 'usually', 'ambiguous'])\n",
      "original document: \n",
      "['yes', 'he', 'has', 'had', 'my', 'money', 'for', '5', 'days', 'and', 'i', 'was', 'upset', 'that', 'he', 'never', 'sent', 'and', 'he', 'went', 'back', 'on', 'the', 'deal', 'we', 'made.', 'and', 'i', 'got', 'banned', 'for', 'being', 'upset', 'about', 'this']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'money', 'fiv', 'day', 'upset', 'nev', 'sent', 'went', 'back', 'deal', 'mad', 'got', 'ban', 'upset'], ['yes', 'money', 'five', 'days', 'upset', 'never', 'send', 'go', 'back', 'deal', 'make', 'get', 'ban', 'upset'])\n",
      "original document: \n",
      "['Priceless']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['priceless'], ['priceless'])\n",
      "original document: \n",
      "['Mod', 'the', 'colours', 'yourself,', \"it's\", 'pretty', 'simple.\\n\\nNow,', 'about', 'the', 'borders,', 'IIRC', 'the', 'old', 'border', 'was', 'the', 'modern', 'border,', 'while', 'the', 'current', 'one', 'is', 'more', 'accurate.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mod', 'colo', 'pretty', 'simple\\n\\nnow', 'bord', 'iirc', 'old', 'bord', 'modern', 'bord', 'cur', 'on', 'acc'], ['mod', 'colour', 'pretty', 'simple\\n\\nnow', 'border', 'iirc', 'old', 'border', 'modern', 'border', 'current', 'one', 'accurate'])\n",
      "original document: \n",
      "['Osc', '1', 'and', '2', 'are', 'turned', 'on', 'and', 'so', 'is', 'Noise.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['osc', 'on', 'two', 'turn', 'nois'], ['osc', 'one', 'two', 'turn', 'noise'])\n",
      "original document: \n",
      "['Yes,', 'when', \"it's\", 'time.', 'Not', 'before', \"it's\", 'ready.', 'Not', 'because', 'Reagan', 'or', 'Trump', 'or', 'whoever', 'the', 'president', 'is', 'wanted', 'to', 'make', 'US', 'corporations', 'more', 'profitable', 'for', 'political', 'reasons.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'tim', 'ready', 'reag', 'trump', 'whoev', 'presid', 'want', 'mak', 'us', 'corp', 'profit', 'polit', 'reason'], ['yes', 'time', 'ready', 'reagan', 'trump', 'whoever', 'president', 'want', 'make', 'us', 'corporations', 'profitable', 'political', 'reason'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'have', 'no', 'problem', 'with', 'people', 'using', 'it.', '', 'My', 'issue', 'is', \"I've\", 'never', 'played', 'with', 'it', 'a', 'n', 'any', 'game', 'and', 'now', \"it's\", 'messing', 'up', 'my', 'aim', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'peopl', 'us', 'issu', 'iv', 'nev', 'play', 'n', 'gam', 'mess', 'aim', 'lol'], ['problem', 'people', 'use', 'issue', 'ive', 'never', 'play', 'n', 'game', 'mess', 'aim', 'lol'])\n",
      "original document: \n",
      "['Gipsy', 'Proud.', 'e', 'misto', 'tare', 'ms', 'de', 'raspuns...', 'faza', 'e', 'ca', 'cel', 'mai', 'trol', 'nu', 'mai', 'am', 'unde.', 'Decy', 'mai', 'bine', 'ca', 'mama', 'sa', 'fie', 'mai', 'frumos', 'lucru', 'pe', 'care', 'o', 'sa', 'aiba', 's', 'classe.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gipsy', 'proud', 'e', 'misto', 'tar', 'ms', 'de', 'raspun', 'faz', 'e', 'ca', 'cel', 'mai', 'trol', 'nu', 'mai', 'und', 'decy', 'mai', 'bin', 'ca', 'mam', 'sa', 'fie', 'mai', 'frumo', 'lucru', 'pe', 'car', 'sa', 'aib', 'class'], ['gipsy', 'proud', 'e', 'misto', 'tare', 'ms', 'de', 'raspuns', 'faza', 'e', 'ca', 'cel', 'mai', 'trol', 'nu', 'mai', 'unde', 'decy', 'mai', 'bine', 'ca', 'mama', 'sa', 'fie', 'mai', 'frumos', 'lucru', 'pe', 'care', 'sa', 'aiba', 'classe'])\n",
      "original document: \n",
      "['Cat.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cat'], ['cat'])\n",
      "original document: \n",
      "['Gradle', 'tasks.', 'Or', 'Maven', 'if', 'you', 'happen', 'to', 'use', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gradl', 'task', 'mav', 'hap', 'us'], ['gradle', 'task', 'maven', 'happen', 'use'])\n",
      "original document: \n",
      "['I', 'still', \"don't\", 'understand', 'why', 'this', 'man', 'ever', 'reached', 'any', 'sense', 'of', 'fame', 'at', 'all', '-', 'seems', 'like', 'a', 'massive', 'douche,', 'terrible', 'journalist', 'and', 'just', 'straight', 'up', 'unlikeable.', 'The', 'only', 'thing', 'good', 'about', 'Piers', 'Morgan', 'is', 'his', 'Twitter', 'account', 'when', 'Arsenal', 'loses.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'dont', 'understand', 'man', 'ev', 'reach', 'sens', 'fam', 'seem', 'lik', 'mass', 'douch', 'terr', 'journ', 'straight', 'unlik', 'thing', 'good', 'pier', 'morg', 'twit', 'account', 'ars', 'los'], ['still', 'dont', 'understand', 'man', 'ever', 'reach', 'sense', 'fame', 'seem', 'like', 'massive', 'douche', 'terrible', 'journalist', 'straight', 'unlikeable', 'thing', 'good', 'piers', 'morgan', 'twitter', 'account', 'arsenal', 'lose'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['You', 'want', 'to', 'do', 'that', 'on', 'a', 'basketball', 'discussion', 'board?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'basketbal', 'discuss', 'board'], ['want', 'basketball', 'discussion', 'board'])\n",
      "original document: \n",
      "['Very', 'cool', 'guide.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cool', 'guid'], ['cool', 'guide'])\n",
      "original document: \n",
      "[\"That's\", 'so', 'strange!', 'On', 'my', 'mobile', 'it', 'says', '\"13', 'images\"', 'and', 'the', 'wheel', 'is', 'missing.', 'Thank', 'you', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'strange', 'mobl', 'say', 'thirteen', 'im', 'wheel', 'miss', 'thank'], ['thats', 'strange', 'mobile', 'say', 'thirteen', 'image', 'wheel', 'miss', 'thank'])\n",
      "original document: \n",
      "['Race', 'relations', 'are', 'fucked', 'in', 'this', 'country', 'and', 'it', 'only', 'got', 'worse', 'with', 'Trump', 'being', 'elected.', \"I'd\", 'rather', 'a', 'group', 'of', 'celebrities', 'kneel', 'for', '2', 'minutes', 'every', 'week', 'than', 'have', 'citizens', 'tear', 'up', 'their', 'city', 'with', 'riots.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rac', 'rel', 'fuck', 'country', 'got', 'wors', 'trump', 'elect', 'id', 'rath', 'group', 'celebr', 'kneel', 'two', 'minut', 'every', 'week', 'cit', 'tear', 'city', 'riot'], ['race', 'relations', 'fuck', 'country', 'get', 'worse', 'trump', 'elect', 'id', 'rather', 'group', 'celebrities', 'kneel', 'two', 'minutes', 'every', 'week', 'citizens', 'tear', 'city', 'riot'])\n",
      "original document: \n",
      "[\"You're\", 'right', 'there', \"isn't...no\", 'wonder', 'I', 'lost', 'the', 'link', 'at', 'the', 'home', 'page.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'right', 'isntno', 'wond', 'lost', 'link', 'hom', 'pag'], ['youre', 'right', 'isntno', 'wonder', 'lose', 'link', 'home', 'page'])\n",
      "original document: \n",
      "['I', 'like', 'to', 'diversify', 'my', 'investments', 'so', 'check', 'out', 'another', 'great', 'site', 'outside', 'of', 'the', 'bitconnect', 'zone.', 'FYI', 'I', 'also', 'invest', 'with', 'Bitconnect', 'both', 'of', 'these', 'are', 'great', 'platforms.\\nI', 'wanted', 'to', 'give', 'you', 'another', 'site', 'that', 'has', 'been', 'paying', 'me', 'and', 'I', 'was', 'able', 'to', 'withdraw', 'my', 'earnings', 'on', 'a', 'daily', 'basis', 'and', 'the', 'transactions', 'are', 'quick', 'too!', 'The', 'company', 'is', 'called', 'BitPetite', 'and', 'the', 'concept', 'is', 'great!', 'Below', 'are', 'few', 'details', 'of', 'what', 'they', 'have', 'going', 'on', 'and', 'I', 'also', 'received', 'a', 'newsletter', 'from', 'them', 'that', 'they', 'will', 'be', 'expanding', 'their', 'investment', 'options', 'so,', 'I', 'think', 'this', 'site', 'is', 'not', 'a', 'scam', 'but', 'like', 'all', \"HYIP's,\", 'withdraw', 'your', 'monies', 'on', 'a', 'daily', 'basis.', 'So', 'far,', 'I', 'have', 'earned', 'over', '$100', 'and', 'have', 'only', 'been', 'in', 'it', 'for', '2', 'weeks.', 'I', 'usually', 'test', 'the', 'waters', 'first', 'before', 'I', 'invest', 'a', 'bit', 'more', 'and', 'with', 'that', 'said,', 'I', 'will', 'be', 'investing', 'into', 'LiteCoin.', '2', 'investment', 'options:', '147%', 'after', '6', 'weeks', 'and', '180%', 'after', '9', 'weeks.', 'Interest:', '3.97%', 'on', 'working', 'days', 'and', '1.00%', 'on', 'weekends.', 'Minimum', 'deposit:', '0.005', 'BTC.', 'Refund', 'of', 'the', 'principal', 'along', 'with', 'interest', 'payments.', 'Withdrawal', 'minimum:', '0.005', 'BTC', 'or', '$10.', 'Payment', 'methods:', 'Bitcoin', 'or', 'USD', 'equivalent.', 'Affiliate', 'program:', '10%-5%-2%.', \"Here's\", 'the', 'link', 'to', 'join', 'this', 'opportunity', 'and', 'like', 'I', 'always', 'say,', 'I', 'am', 'not', 'a', 'financial', 'adviser', 'thus,', 'you', 'are', 'investing', 'into', 'these', 'things', 'at', 'your', 'own', 'risk.', 'But', 'as', 'for', 'me,', 'what', 'is', 'life', 'without', 'taking', 'some', 'small', 'risk?', 'Invest', 'small', 'and', 'see', 'what', 'you', 'think', 'but', 'I', 'honestly', 'can', 'tell', 'you', \"it's\", 'worked', 'for', 'me', 'thus', 'far.', \"Here's\", 'the', 'link:https://bitpetite.com/?aff=stonesour73']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'divers', 'invest', 'check', 'anoth', 'gre', 'sit', 'outsid', 'bitconnect', 'zon', 'fyi', 'also', 'invest', 'bitconnect', 'gre', 'platforms\\ni', 'want', 'giv', 'anoth', 'sit', 'pay', 'abl', 'withdraw', 'earn', 'dai', 'bas', 'transact', 'quick', 'company', 'cal', 'bitpetit', 'conceiv', 'gre', 'detail', 'going', 'also', 'receiv', 'newslet', 'expand', 'invest', 'opt', 'think', 'sit', 'scam', 'lik', 'hyip', 'withdraw', 'mony', 'dai', 'bas', 'far', 'earn', 'one hundred', 'two', 'week', 'us', 'test', 'wat', 'first', 'invest', 'bit', 'said', 'invest', 'litecoin', 'two', 'invest', 'opt', 'one hundred and forty-seven', 'six', 'week', 'one hundred and eighty', 'nin', 'week', 'interest', 'three hundred and ninety-seven', 'work', 'day', 'one hundred', 'weekend', 'minim', 'deposit', 'fiv', 'btc', 'refund', 'princip', 'along', 'interest', 'pay', 'withdraw', 'minim', 'fiv', 'btc', 'ten', 'pay', 'method', 'bitcoin', 'usd', 'equ', 'affy', 'program', 'one thousand and fifty-two', 'her', 'link', 'join', 'opportun', 'lik', 'alway', 'say', 'fin', 'adv', 'thu', 'invest', 'thing', 'risk', 'lif', 'without', 'tak', 'smal', 'risk', 'invest', 'smal', 'see', 'think', 'honest', 'tel', 'work', 'thu', 'far', 'her', 'linkhttpsbitpetitecomaffstonesour73'], ['like', 'diversify', 'investments', 'check', 'another', 'great', 'site', 'outside', 'bitconnect', 'zone', 'fyi', 'also', 'invest', 'bitconnect', 'great', 'platforms\\ni', 'want', 'give', 'another', 'site', 'pay', 'able', 'withdraw', 'earn', 'daily', 'basis', 'transactions', 'quick', 'company', 'call', 'bitpetite', 'concept', 'great', 'detail', 'go', 'also', 'receive', 'newsletter', 'expand', 'investment', 'options', 'think', 'site', 'scam', 'like', 'hyips', 'withdraw', 'monies', 'daily', 'basis', 'far', 'earn', 'one hundred', 'two', 'weeks', 'usually', 'test', 'water', 'first', 'invest', 'bite', 'say', 'invest', 'litecoin', 'two', 'investment', 'options', 'one hundred and forty-seven', 'six', 'weeks', 'one hundred and eighty', 'nine', 'weeks', 'interest', 'three hundred and ninety-seven', 'work', 'days', 'one hundred', 'weekend', 'minimum', 'deposit', 'five', 'btc', 'refund', 'principal', 'along', 'interest', 'payments', 'withdrawal', 'minimum', 'five', 'btc', 'ten', 'payment', 'methods', 'bitcoin', 'usd', 'equivalent', 'affiliate', 'program', 'one thousand and fifty-two', 'heres', 'link', 'join', 'opportunity', 'like', 'always', 'say', 'financial', 'adviser', 'thus', 'invest', 'things', 'risk', 'life', 'without', 'take', 'small', 'risk', 'invest', 'small', 'see', 'think', 'honestly', 'tell', 'work', 'thus', 'far', 'heres', 'linkhttpsbitpetitecomaffstonesour73'])\n",
      "original document: \n",
      "['Why', 'is', 'this', 'a', 'post']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post'], ['post'])\n",
      "original document: \n",
      "['If', 'this', 'has', 'always', 'been', 'a', 'thing,', \"I'm\", 'amazed', 'the', \"map's\", 'been', 'available', 'for', 'almost', 'a', 'year', 'and', 'this', 'was', 'just', 'now', 'noticed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'thing', 'im', 'amaz', 'map', 'avail', 'almost', 'year', 'not'], ['always', 'thing', 'im', 'amaze', 'map', 'available', 'almost', 'year', 'notice'])\n",
      "original document: \n",
      "[\"You'd\", 'think', 'an', 'old', 'world', 'symbol', 'at', 'a', 'sacred', 'site', 'would', 'be', 'a', 'bad', 'thing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youd', 'think', 'old', 'world', 'symbol', 'sacr', 'sit', 'would', 'bad', 'thing'], ['youd', 'think', 'old', 'world', 'symbol', 'sacred', 'site', 'would', 'bad', 'thing'])\n",
      "original document: \n",
      "['Keyword', 'is', '*might*', 'clearly', \"it's\", 'context', 'dependent.', 'Like', 'I', \"wouldn't\", 'buy', 'someone', 'a', 'beer', 'for', 'randomly', 'punching', 'someone', 'in', 'a', 'Che', 'T-Shirt.', 'But', 'if', 'one', 'was', 'being', 'sufficiently', 'dickish', 'and', 'got', 'punched', 'I', 'might', 'buy', 'them', 'a', 'beer', 'when', 'they', 'got', 'out', 'of', 'jail.', \"I've\", 'had', 'friends', 'go', 'to', 'jail', 'for', 'things', 'that,', 'while', 'illegal,', 'were', 'emotionally', 'understandable.', 'A', 'cold', 'beer', 'when', 'they', 'get', 'out', 'is', 'just', 'being', 'nice.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['keyword', 'might', 'clear', 'context', 'depend', 'lik', 'wouldnt', 'buy', 'someon', 'beer', 'random', 'punch', 'someon', 'che', 'tshirt', 'on', 'sufficy', 'dick', 'got', 'punch', 'might', 'buy', 'beer', 'got', 'jail', 'iv', 'friend', 'go', 'jail', 'thing', 'illeg', 'emot', 'understand', 'cold', 'beer', 'get', 'nic'], ['keyword', 'might', 'clearly', 'context', 'dependent', 'like', 'wouldnt', 'buy', 'someone', 'beer', 'randomly', 'punch', 'someone', 'che', 'tshirt', 'one', 'sufficiently', 'dickish', 'get', 'punch', 'might', 'buy', 'beer', 'get', 'jail', 'ive', 'friends', 'go', 'jail', 'things', 'illegal', 'emotionally', 'understandable', 'cold', 'beer', 'get', 'nice'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'so.', \"Doesn't\", 'the', 'game', 'have', 'black', 'cinema', 'bars', 'when', 'you', 'warp', 'in?', 'Then', 'they', 'disappear', 'when', 'you', 'get', 'control', 'of', 'your', 'ship', 'back?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'doesnt', 'gam', 'black', 'cinem', 'bar', 'warp', 'disappear', 'get', 'control', 'ship', 'back'], ['dont', 'think', 'doesnt', 'game', 'black', 'cinema', 'bar', 'warp', 'disappear', 'get', 'control', 'ship', 'back'])\n",
      "original document: \n",
      "['\"Cake.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cak'], ['cake'])\n",
      "original document: \n",
      "['Because,', 'as', 'of', 'right', 'now', 'while', 'religion', 'is', 'a', 'protected', 'class,', 'ideology', \"isn't.\", \"That's\", 'why', 'people', 'are', 'allowed', 'to', 'discriminate', 'against', 'neo-Nazis,', 'but', \"aren't\", 'allowed', 'to', 'discriminate', 'against', 'Muslims.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'relig', 'protect', 'class', 'ideolog', 'isnt', 'that', 'peopl', 'allow', 'discrimin', 'neonaz', 'ar', 'allow', 'discrimin', 'muslim'], ['right', 'religion', 'protect', 'class', 'ideology', 'isnt', 'thats', 'people', 'allow', 'discriminate', 'neonazis', 'arent', 'allow', 'discriminate', 'muslims'])\n",
      "original document: \n",
      "['How', 'about', 'a', 'Chinese', 'painting', 'instead?', '\\n\\nhttp://www.pheilcia.com/products/keycaps/35/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chines', 'paint', 'instead', '\\n\\nhttpwwwpheilciacomproductskeycaps35'], ['chinese', 'paint', 'instead', '\\n\\nhttpwwwpheilciacomproductskeycaps35'])\n",
      "original document: \n",
      "['When', 'he', 'rolled', 'for', \"neckbeards'\", 'charisma', 'stats,', 'he', 'rolled', 'a', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rol', 'neckbeard', 'charism', 'stat', 'rol', 'on'], ['roll', 'neckbeards', 'charisma', 'stats', 'roll', 'one'])\n",
      "original document: \n",
      "['That', 'sucks.', 'I', 'came', 'across', 'some', \"4'10\", 'woman', 'on', 'AIU', 'u/', 'pipcleaner.', \"You'd\", 'appear', 'very', 'tall', 'to', 'her']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suck', 'cam', 'across', 'four hundred and ten', 'wom', 'aiu', 'u', 'pipc', 'youd', 'appear', 'tal'], ['suck', 'come', 'across', 'four hundred and ten', 'woman', 'aiu', 'u', 'pipcleaner', 'youd', 'appear', 'tall'])\n",
      "original document: \n",
      "['143417872|', '&gt;', 'None', 'Anonymous', '(ID:', 'o0cqoZbW)\\n\\n&gt;&gt;143417408\\nbut', 'cities', 'are', 'centers', 'of', 'diversity', 'and', 'culture', 'you', 'racist', 'bigoted', 'misogynistic', 'trans', 'phobic', 'piece', 'of', 'shit\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and seventy-two', 'gt', 'non', 'anonym', 'id', 'o0cqozbw\\n\\ngtgt143417408\\nbut', 'city', 'cent', 'divers', 'cult', 'rac', 'bigot', 'misogyn', 'tran', 'phob', 'piec', 'shit\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, eight hundred and seventy-two', 'gt', 'none', 'anonymous', 'id', 'o0cqozbw\\n\\ngtgt143417408\\nbut', 'cities', 'center', 'diversity', 'culture', 'racist', 'bigoted', 'misogynistic', 'trans', 'phobic', 'piece', 'shit\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Mind', 'linking', 'the', 'cable', 'you’re', 'using?', 'It', 'might', 'just', 'be', 'the', 'inline', 'repeater', 'that’s', 'preventing', 'you', 'from', 'having', 'the', 'same', 'issue.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mind', 'link', 'cabl', 'yo', 'us', 'might', 'inlin', 'rep', 'that', 'prev', 'issu'], ['mind', 'link', 'cable', 'youre', 'use', 'might', 'inline', 'repeater', 'thats', 'prevent', 'issue'])\n",
      "original document: \n",
      "[\"It's\", 'just', 'logical', 'though,', 'if', 'dozens', 'of', 'devs', 'that', 'all', 'code', 'differently', 'have', 'come', 'and', 'gone,', 'of', 'course', 'the', 'code', 'is', 'going', 'to', 'be', 'all', 'over', 'the', 'place.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['log', 'though', 'doz', 'dev', 'cod', 'diff', 'com', 'gon', 'cours', 'cod', 'going', 'plac'], ['logical', 'though', 'dozens', 'devs', 'code', 'differently', 'come', 'go', 'course', 'code', 'go', 'place'])\n",
      "original document: \n",
      "['&gt;', 'a', '99%', 'white', 'town', 'in', 'america\\n\\nhahahahaha.', 'there', 'is', 'no', 'such', 'thing.', 'give', 'me', 'the', 'name.', '\\n\\nbtw', 'my', 'country', 'has', 'less', 'than', '5000', 'muslims', 'out', 'of', '5', 'million', 'people.', 'you', 'probably', 'have', 'more', 'than', 'that', 'in', 'your', 'supposedly', 'white', 'town', 'lmao.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'ninety-nine', 'whit', 'town', 'america\\n\\nhahahahah', 'thing', 'giv', 'nam', '\\n\\nbtw', 'country', 'less', 'five thousand', 'muslim', 'fiv', 'mil', 'peopl', 'prob', 'suppos', 'whit', 'town', 'lmao'], ['gt', 'ninety-nine', 'white', 'town', 'america\\n\\nhahahahaha', 'thing', 'give', 'name', '\\n\\nbtw', 'country', 'less', 'five thousand', 'muslims', 'five', 'million', 'people', 'probably', 'supposedly', 'white', 'town', 'lmao'])\n",
      "original document: \n",
      "['Solving', 'the', 'discrete', 'log', 'problem', 'would', 'break', 'RSA?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['solv', 'discret', 'log', 'problem', 'would', 'break', 'rsa'], ['solve', 'discrete', 'log', 'problem', 'would', 'break', 'rsa'])\n",
      "original document: \n",
      "['This', 'is', 'Solo', 'Q', 'man.', 'The', 'point', 'is', 'that', 'the', 'scaling', 'should', 'be', 'done', 'on', 'an', 'overall', 'team', 'basis.', 'And', 'the', 'difference', 'was', 'way', 'more', 'than', '10ip', 'overall', 'as', 'you', 'said', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['solo', 'q', 'man', 'point', 'scal', 'don', 'overal', 'team', 'bas', 'diff', 'way', '10ip', 'overal', 'said'], ['solo', 'q', 'man', 'point', 'scale', 'do', 'overall', 'team', 'basis', 'difference', 'way', '10ip', 'overall', 'say'])\n",
      "original document: \n",
      "['#[We', 'know', 'what', 'you', 'are', 'doing,', 'OP.](https://www.reddit.com/r/UnethicalLifeProTips/comments/73igbu/ulpt_persuade_some_redditors_to_keep_cash_between/)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'ophttpswwwredditcomrunethicallifeprotipscomments73igbuulpt_persuade_some_redditors_to_keep_cash_between'], ['know', 'ophttpswwwredditcomrunethicallifeprotipscomments73igbuulpt_persuade_some_redditors_to_keep_cash_between'])\n",
      "original document: \n",
      "['Interesting.\\n\\nStill,', 'unless', \"you're\", 'overclocking', 'or', 'run', 'the', 'highest', 'end', 'i7,', 'pk3', 'is', 'more', 'than', 'enough', 'thermal', 'conductivity', 'for', '15', 'watts.', 'The', 'hard', 'limit', 'is', 'really', 'the', 'metal', 'and', 'fan', 'involved.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interesting\\n\\nstill', 'unless', 'yo', 'overclock', 'run', 'highest', 'end', 'i7', 'pk3', 'enough', 'therm', 'conduc', 'fifteen', 'wat', 'hard', 'limit', 'real', 'met', 'fan', 'involv'], ['interesting\\n\\nstill', 'unless', 'youre', 'overclocking', 'run', 'highest', 'end', 'i7', 'pk3', 'enough', 'thermal', 'conductivity', 'fifteen', 'watts', 'hard', 'limit', 'really', 'metal', 'fan', 'involve'])\n",
      "original document: \n",
      "['lol', 'yes', 'actually.', 'He', 'was', 'gassed', 'from', 'carrying.', 'We', 'got', 'killed', 'whenever', 'he', 'was', 'on', 'the', 'bench', 'so', 'he', 'played', 'more', 'minutes', 'than', 'Harden', 'while', 'continuing', 'to', 'have', 'more', 'useage', 'than', 'him.', 'There', 'was', 'one', 'particularly', 'disastrous', '2', 'minute', 'spell', 'at', 'the', 'end', 'of', 'a', '3rd', 'quarter', 'with', 'a', '10', 'point', 'turnaround.', 'Beginning', 'of', 'the', '4th', 'quarter', 'he', 'was', 'straight', 'back', 'in', 'early.', 'It', 'was', 'probably', 'the', 'game', \"you've\", 'referenced', 'actually.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'ye', 'act', 'gass', 'carry', 'got', 'kil', 'whenev', 'bench', 'play', 'minut', 'hard', 'continu', 'us', 'on', 'particul', 'disast', 'two', 'minut', 'spel', 'end', '3rd', 'quart', 'ten', 'point', 'turnaround', 'begin', '4th', 'quart', 'straight', 'back', 'ear', 'prob', 'gam', 'youv', 'ref', 'act'], ['lol', 'yes', 'actually', 'gas', 'carry', 'get', 'kill', 'whenever', 'bench', 'play', 'minutes', 'harden', 'continue', 'useage', 'one', 'particularly', 'disastrous', 'two', 'minute', 'spell', 'end', '3rd', 'quarter', 'ten', 'point', 'turnaround', 'begin', '4th', 'quarter', 'straight', 'back', 'early', 'probably', 'game', 'youve', 'reference', 'actually'])\n",
      "original document: \n",
      "['I', 'was', 'the', 'whm,', 'he', 'was', 'holding', 'the', 'sign', 'for', 'me', 'haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whm', 'hold', 'sign', 'hah'], ['whm', 'hold', 'sign', 'haha'])\n",
      "original document: \n",
      "['[Then', 'why', \"can't\", 'I', 'use', 'all', 'of', 'the', 'warfare', 'abilities', 'with', 'a', 'wand', 'equipped?](https://i.imgur.com/nkJjZCi.png)\\n\\nedit:', 'Downvoted', 'for', 'posting', 'proof?', 'OK', 'then,', 'lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'us', 'warf', 'abl', 'wand', 'equippedhttpsiimgurcomnkjjzcipng\\n\\nedit', 'downvot', 'post', 'proof', 'ok', 'lol'], ['cant', 'use', 'warfare', 'abilities', 'wand', 'equippedhttpsiimgurcomnkjjzcipng\\n\\nedit', 'downvoted', 'post', 'proof', 'ok', 'lol'])\n",
      "original document: \n",
      "['It', \"doesn't,\", 'but', 'people', 'think', 'we', 'honor', 'our', 'nation', 'and', 'troops', 'by', 'playing', 'the', 'anthem', 'at', 'every', 'single', 'sporting', 'event.', 'I', 'think', \"it's\", 'odd.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'peopl', 'think', 'hon', 'nat', 'troop', 'play', 'anthem', 'every', 'singl', 'sport', 'ev', 'think', 'od'], ['doesnt', 'people', 'think', 'honor', 'nation', 'troop', 'play', 'anthem', 'every', 'single', 'sport', 'event', 'think', 'odd'])\n",
      "original document: \n",
      "['We', 'kinda', 'did', 'that', 'last', 'week.', 'There', 'were', 'a', 'lot', 'of', 'quick', 'passes,', 'some', 'with', 'a', 'moving', 'pocket,', 'and', 'a', 'lot', 'of', 'play', 'action', 'on', 'the', 'deeper', 'passes', 'to', 'help', 'slow', 'down', 'the', 'pass', 'rush.', 'We', 'made', 'them', 'respect', 'the', 'running', 'game', 'and', 'slow', 'down', 'the', 'pass', 'rush.', 'So', 'in', 'a', 'way', 'we', 'did', 'do', 'that', 'and', 'that’s', 'why', 'our', 'offense', 'was', 'more', 'successful']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'last', 'week', 'lot', 'quick', 'pass', 'mov', 'pocket', 'lot', 'play', 'act', 'deep', 'pass', 'help', 'slow', 'pass', 'rush', 'mad', 'respect', 'run', 'gam', 'slow', 'pass', 'rush', 'way', 'that', 'offens', 'success'], ['kinda', 'last', 'week', 'lot', 'quick', 'pass', 'move', 'pocket', 'lot', 'play', 'action', 'deeper', 'pass', 'help', 'slow', 'pass', 'rush', 'make', 'respect', 'run', 'game', 'slow', 'pass', 'rush', 'way', 'thats', 'offense', 'successful'])\n",
      "original document: \n",
      "['red', 'onions', 'and', 'red', 'peppers', 'are', 'a', 'must.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['red', 'on', 'red', 'pep', 'must'], ['red', 'onions', 'red', 'pepper', 'must'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'it', 'was', 'the', 'killing,', 'I', 'think', 'it', 'was', 'the', 'chase.', 'It', 'was', 'the', 'ferreting', 'out', 'of', 'anything', 'or', 'anyone', 'he', 'was', 'after.', 'He', 'was', 'still', 'a', 'detective', 'at', 'heart.', 'An', 'amoral,', 'vicious', 'detective,', 'but', 'still.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'kil', 'think', 'chas', 'ferret', 'anyth', 'anyon', 'stil', 'detect', 'heart', 'am', 'vicy', 'detect', 'stil'], ['dont', 'think', 'kill', 'think', 'chase', 'ferret', 'anything', 'anyone', 'still', 'detective', 'heart', 'amoral', 'vicious', 'detective', 'still'])\n",
      "original document: \n",
      "['4?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['four'], ['four'])\n",
      "original document: \n",
      "['Thanks', 'for', 'posting,', '**/u/Bestielingerie**!', 'Your', 'submission', 'has', 'been', 'automatically', 'removed', 'because', 'the', 'title', 'includes', 'tags', 'that', 'are', 'not', 'used', 'by', 'this', 'subreddit.', 'Please', 'see', 'the', 'tags', 'we', 'use', 'in', 'the', 'sidebar.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/FetishSelling)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'post', 'ubestielingery', 'submit', 'autom', 'remov', 'titl', 'includ', 'tag', 'us', 'subreddit', 'pleas', 'see', 'tag', 'us', 'sidebar\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorfetishsel', 'quest', 'concern'], ['thank', 'post', 'ubestielingerie', 'submission', 'automatically', 'remove', 'title', 'include', 'tag', 'use', 'subreddit', 'please', 'see', 'tag', 'use', 'sidebar\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorfetishselling', 'question', 'concern'])\n",
      "original document: \n",
      "[\"I'm\", '100%', 'sure', \"it's\", 'fake', 'but', 'with', 'some', 'decent', 'guesses.', 'But', 'guess', '=/=', 'insider', 'info.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'one hundred', 'sur', 'fak', 'dec', 'guess', 'guess', 'insid', 'info'], ['im', 'one hundred', 'sure', 'fake', 'decent', 'guess', 'guess', 'insider', 'info'])\n",
      "original document: \n",
      "['&gt;', 'oing', 'locums', 'work', 'for', 'a', 'few', 'years', 'in', 'between', 'jobs', 'too.', 'So', 'yes,', 'you', 'can', 'work', 'locums', 'in', 'surgery.', 'Sounded', 'like', 'it', 'paid', 'pretty', 'decently', 'but', 'hard', 't\\n\\nthanks.', 'I', 'would', 'really', 'like', 'to', 'work', 'locums', 'for', 'a', 'few', 'years']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'oing', 'locum', 'work', 'year', 'job', 'ye', 'work', 'locum', 'surgery', 'sound', 'lik', 'paid', 'pretty', 'dec', 'hard', 't\\n\\nthanks', 'would', 'real', 'lik', 'work', 'locum', 'year'], ['gt', 'oing', 'locums', 'work', 'years', 'job', 'yes', 'work', 'locums', 'surgery', 'sound', 'like', 'pay', 'pretty', 'decently', 'hard', 't\\n\\nthanks', 'would', 'really', 'like', 'work', 'locums', 'years'])\n",
      "original document: \n",
      "['He’s', 'a', 'great', 'vendor,', 'he', 'will', 'sort', 'it', 'out', 'for', 'you.', 'Very', 'reliable', 'and', 'it’s', 'a', 'great', 'ID.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'gre', 'vend', 'sort', 'rely', 'gre', 'id'], ['hes', 'great', 'vendor', 'sort', 'reliable', 'great', 'id'])\n",
      "original document: \n",
      "['Dammit.', 'I', 'completly', 'forgot', 'that', 'you', 'were', 'using', 'a', 'submod!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dammit', 'complet', 'forgot', 'us', 'submod'], ['dammit', 'completly', 'forget', 'use', 'submod'])\n",
      "original document: \n",
      "['r/totallynototters']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rtotallynotot'], ['rtotallynototters'])\n",
      "original document: \n",
      "['Hair.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hair'], ['hair'])\n",
      "original document: \n",
      "['[+Merari01](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnofp3n/):\\n\\nYou', 'definitely', 'should,', \"it's\", 'a', 'great', 'short', 'story!\\n\\nI', 'looked,', 'but', 'it', \"doesn't\", 'seem', 'to', 'be', 'available', 'online', 'in', 'a', 'full', 'version.', 'But', \"I'm\", 'sure', 'you', 'can', 'find', 'a', 'book', 'of', 'short', 'stories', 'that', 'has', 'it', 'for', 'very', 'little', 'money', 'online.', \"It's\", 'been', 'reprinted', 'a', 'lot.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnofp3n\\n\\nyou', 'definit', 'gre', 'short', 'story\\n\\ni', 'look', 'doesnt', 'seem', 'avail', 'onlin', 'ful', 'vert', 'im', 'sur', 'find', 'book', 'short', 'story', 'littl', 'money', 'onlin', 'reprint', 'lot'], ['merari01httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnofp3n\\n\\nyou', 'definitely', 'great', 'short', 'story\\n\\ni', 'look', 'doesnt', 'seem', 'available', 'online', 'full', 'version', 'im', 'sure', 'find', 'book', 'short', 'stories', 'little', 'money', 'online', 'reprint', 'lot'])\n",
      "original document: \n",
      "['Cite', 'a', 'source', 'that', 'says', 'what', 'you', 'are', 'saying.', 'Cite', 'a', 'source', 'that', 'says', 'this', 'was', 'the', 'case', 'in', 'any', 'of', 'the', 'last', '5', '2k', 'games.', 'Or', 'honestly', 'you', 'have', 'nothing', 'to', 'speak', 'on.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cit', 'sourc', 'say', 'say', 'cit', 'sourc', 'say', 'cas', 'last', 'fiv', '2k', 'gam', 'honest', 'noth', 'speak'], ['cite', 'source', 'say', 'say', 'cite', 'source', 'say', 'case', 'last', 'five', '2k', 'game', 'honestly', 'nothing', 'speak'])\n",
      "original document: \n",
      "['NFL.com', 'video:', 'Enemy', 'Intel:', 'The', 'Power', 'Of', 'Play-Action', '[HD](https://phieagles.akamaized.net//PHI/videos/dct/video_audio/2017/09-September/GP17WK04-at-Chargers-Feature-B1B2-Enemy-Intel-and-Breakdown-OFF-5000k.mp4)', '[SD](https://phieagles.akamaized.net//PHI/videos/dct/video_audio/2017/09-September/GP17WK04-at-Chargers-Feature-B1B2-Enemy-Intel-and-Breakdown-OFF-500k.mp4)\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nflcom', 'video', 'enemy', 'intel', 'pow', 'playact', 'hdhttpsphieaglesakamaizednetphivideosdctvideo_audio201709septembergp17wk04atchargersfeatureb1b2enemyintelandbreakdownoff5000kmp4', 'sdhttpsphieaglesakamaizednetphivideosdctvideo_audio201709septembergp17wk04atchargersfeatureb1b2enemyintelandbreakdownoff500kmp4\\n\\n'], ['nflcom', 'video', 'enemy', 'intel', 'power', 'playaction', 'hdhttpsphieaglesakamaizednetphivideosdctvideo_audio201709septembergp17wk04atchargersfeatureb1b2enemyintelandbreakdownoff5000kmp4', 'sdhttpsphieaglesakamaizednetphivideosdctvideo_audio201709septembergp17wk04atchargersfeatureb1b2enemyintelandbreakdownoff500kmp4\\n\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['This', 'content', 'brought', 'to', 'you', 'from', '\"Taiwan', 'Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#off', 'site', 'feed', '\"Taiwan', 'Pool\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'brought', 'taiw', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'sit', 'fee', 'taiw', 'pool\\n'], ['content', 'bring', 'taiwan', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'site', 'fee', 'taiwan', 'pool\\n'])\n",
      "original document: \n",
      "['People', 'keep', 'turning', 'a', 'blind', 'eye', 'to', 'the', 'death', 'toll,', 'as', 'in', 'not', 'even', 'the', 'local', 'newspaper', 'has', 'dared', 'to', 'give', 'a', 'real', 'number', 'yet.', '', '', '', '\\n', '', \"\\nI'm\", 'sorry', \"you're\", 'so', 'angry', 'and', 'bitter.', 'Maybe', 'you', 'should', 'go', 'to', 'that', 'subreddit', 'yourself.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'keep', 'turn', 'blind', 'ey', 'dea', 'tol', 'ev', 'loc', 'newspap', 'dar', 'giv', 'real', 'numb', 'yet', '\\n', '\\nim', 'sorry', 'yo', 'angry', 'bit', 'mayb', 'go', 'subreddit'], ['people', 'keep', 'turn', 'blind', 'eye', 'death', 'toll', 'even', 'local', 'newspaper', 'dare', 'give', 'real', 'number', 'yet', '\\n', '\\nim', 'sorry', 'youre', 'angry', 'bitter', 'maybe', 'go', 'subreddit'])\n",
      "original document: \n",
      "['I', 'just', 'turned', 'on', 'the', 'TV;', 'with', 'all', 'these', 'fucking', 'lines,', 'I', 'keep', 'thinking', 'Devonta', 'Freeman', 'is', 'about', 'to', 'pick', 'up', 'the', 'ball', 'and', 'start', 'running', 'with', 'it\\n\\nEdit:', 'this', 'field', 'is', 'a', 'piece', 'of', 'shit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turn', 'tv', 'fuck', 'lin', 'keep', 'think', 'devont', 'freem', 'pick', 'bal', 'start', 'run', 'it\\n\\nedit', 'field', 'piec', 'shit'], ['turn', 'tv', 'fuck', 'line', 'keep', 'think', 'devonta', 'freeman', 'pick', 'ball', 'start', 'run', 'it\\n\\nedit', 'field', 'piece', 'shit'])\n",
      "original document: \n",
      "['Or', 'you', 'know,', 'EMU', 'might', 'have', 'a', 'good', 'DL...', 'Bad', 'day', 'may', 'have', 'played', 'into', 'it', 'but', 'with', 'the', 'level', 'of', 'recruits', 'that', 'Kentucky', 'gets', 'compared', 'to', 'EMU', \"there's\", 'no', 'excuse', 'for', 'the', 'line', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'emu', 'might', 'good', 'dl', 'bad', 'day', 'may', 'play', 'level', 'recruit', 'kentucky', 'get', 'comp', 'emu', 'ther', 'excus', 'lin'], ['know', 'emu', 'might', 'good', 'dl', 'bad', 'day', 'may', 'play', 'level', 'recruit', 'kentucky', 'get', 'compare', 'emu', 'theres', 'excuse', 'line'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['God', 'I', 'loved', 'me', 'some', 'Jamal', 'Lewis', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'lov', 'jam', 'lew'], ['god', 'love', 'jamal', 'lewis'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['No,', 'currently', 'overseas', 'now', 'its', 'fucking', 'joke']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cur', 'oversea', 'fuck', 'jok'], ['currently', 'overseas', 'fuck', 'joke'])\n",
      "original document: \n",
      "['Love', 'your', 'nipple', 'ring']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'nippl', 'ring'], ['love', 'nipple', 'ring'])\n",
      "original document: \n",
      "['Ah', 'ok.', 'That', 'makes', 'sense.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'ok', 'mak', 'sens'], ['ah', 'ok', 'make', 'sense'])\n",
      "original document: \n",
      "['Vroom', 'vroom', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vroom', 'vroom'], ['vroom', 'vroom'])\n",
      "original document: \n",
      "['This', 'is', 'so', 'good.', '', 'Watching', 'this', 'woman', 'expose', 'her', 'female', 'nature', 'for', 'the', 'whole', 'world', 'to', 'see.', '', '\\n\\nA', 'TO', 'THE', 'MOTHERFUCKING', 'WALT']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'watch', 'wom', 'expos', 'fem', 'nat', 'whol', 'world', 'see', '\\n\\na', 'motherfuck', 'walt'], ['good', 'watch', 'woman', 'expose', 'female', 'nature', 'whole', 'world', 'see', '\\n\\na', 'motherfucking', 'walt'])\n",
      "original document: \n",
      "['Hey', 'sorry,', 'was', 'away', 'for', 'a', 'bit.', 'Yeah', 'sure', \"I'll\", 'take', 'it!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'sorry', 'away', 'bit', 'yeah', 'sur', 'il', 'tak'], ['hey', 'sorry', 'away', 'bite', 'yeah', 'sure', 'ill', 'take'])\n",
      "original document: \n",
      "['Best', 'show', 'on', 'TV.', 'Problem', 'is', \"I've\", 'watched', 'them', 'all', 'multiple', 'times.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'show', 'tv', 'problem', 'iv', 'watch', 'multipl', 'tim'], ['best', 'show', 'tv', 'problem', 'ive', 'watch', 'multiple', 'time'])\n",
      "original document: \n",
      "['But', 'he', 'really', 'does', 'have', 'a', 'swine', 'ass!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'swin', 'ass'], ['really', 'swine', 'ass'])\n",
      "original document: \n",
      "['Hahha', 'you', 'can', 'use', 'it', 'under', 'artistic', 'license.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahh', 'us', 'art', 'licens'], ['hahha', 'use', 'artistic', 'license'])\n",
      "original document: \n",
      "['Lets', 'bet', '1', 'btc?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'bet', 'on', 'btc'], ['let', 'bet', 'one', 'btc'])\n",
      "original document: \n",
      "['It', 'was', 'always', 'fucking', 'stupid']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'fuck', 'stupid'], ['always', 'fuck', 'stupid'])\n",
      "original document: \n",
      "['I', 'was', 'that', 'retarded']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['retard'], ['retard'])\n",
      "original document: \n",
      "['Yeah,', 'but', 'what', 'do', 'you', 'do', 'with', 'the', 'extra', 'pedal?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'extr', 'ped'], ['yeah', 'extra', 'pedal'])\n",
      "original document: \n",
      "['The', 'ultimate', 'jefi']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ultim', 'jef'], ['ultimate', 'jefi'])\n",
      "original document: \n",
      "['All', 'I', 'got', 'was', 'downvoted.', 'Not', 'really', 'sure', 'why.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'downvot', 'real', 'sur'], ['get', 'downvoted', 'really', 'sure'])\n",
      "original document: \n",
      "['It’s', 'a', 'natural', 'response.', '', 'I', 'like', 'it,', 'a', 'lot.', '', 'This', 'is', 'going', 'to', 'be', 'inexcusable', 'for', 'a', 'lot', 'of', 'people,', 'it’s', 'definitely', 'going', 'to', 'turn', 'off', 'people', 'who', 'are', 'on', 'the', 'fence', 'about', 'Trump.', '', 'Remember', 'how', 'bad', 'George', 'W', 'looked', 'after', 'Katrina?', '', 'And', 'he', 'didn’t', 'even', 'say', 'anything', 'that', 'stupid', 'about', 'it.', '\\n\\nWhat’s', 'not', 'cool', 'is', 'walking', 'around', 'like', 'a', 'smug', 'asshole', 'about', 'it.', '', 'Keep', 'that', 'shit', 'to', 'yourself.', '', 'That', 'hurts', 'the', 'cause', 'and', 'perpetuates', 'the', 'stereotype', 'of', 'the', 'smug', 'liberal.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nat', 'respons', 'lik', 'lot', 'going', 'inexcus', 'lot', 'peopl', 'definit', 'going', 'turn', 'peopl', 'fent', 'trump', 'rememb', 'bad', 'georg', 'w', 'look', 'katrin', 'didnt', 'ev', 'say', 'anyth', 'stupid', '\\n\\nwhats', 'cool', 'walk', 'around', 'lik', 'smug', 'asshol', 'keep', 'shit', 'hurt', 'caus', 'perpetu', 'stereotyp', 'smug', 'lib'], ['natural', 'response', 'like', 'lot', 'go', 'inexcusable', 'lot', 'people', 'definitely', 'go', 'turn', 'people', 'fence', 'trump', 'remember', 'bad', 'george', 'w', 'look', 'katrina', 'didnt', 'even', 'say', 'anything', 'stupid', '\\n\\nwhats', 'cool', 'walk', 'around', 'like', 'smug', 'asshole', 'keep', 'shit', 'hurt', 'cause', 'perpetuate', 'stereotype', 'smug', 'liberal'])\n",
      "original document: \n",
      "['I', 'do', 'a', 'similar', 'thing', 'with', 'my', 'Titan.', 'I', 'run', 'the', 'boots', 'that', 'lets', 'me', 'insta', 'draw', 'my', 'trials', 'SMG', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['simil', 'thing', 'tit', 'run', 'boot', 'let', 'inst', 'draw', 'tri', 'smg'], ['similar', 'thing', 'titan', 'run', 'boot', 'let', 'insta', 'draw', 'trials', 'smg'])\n",
      "original document: \n",
      "['After', 'my', 'watch', 'lasting', '6', 'hours', 'yesterday', 'I', 'did', 'some', 'digging', 'around', 'and', 'turned', 'off', 'WiFi', 'and', 'location', 'services', 'and', 'more', 'than', 'doubled', 'my', 'battery', 'life.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['watch', 'last', 'six', 'hour', 'yesterday', 'dig', 'around', 'turn', 'wif', 'loc', 'serv', 'doubl', 'battery', 'lif'], ['watch', 'last', 'six', 'hours', 'yesterday', 'dig', 'around', 'turn', 'wifi', 'location', 'service', 'double', 'battery', 'life'])\n",
      "original document: \n",
      "['Sometimes', 'I', 'use', 'a', 'pomodoro', 'timer', 'just', 'to', 'get', 'started.', 'After', '25', 'minutes', \"I'm\", 'so', 'laser', 'focused', 'and', 'in', 'the', 'zone', 'that', 'I', \"don't\", 'even', 'think', 'about', 'distracting', 'myself', 'from', 'the', 'task.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sometim', 'us', 'pomodoro', 'tim', 'get', 'start', 'twenty-five', 'minut', 'im', 'las', 'focus', 'zon', 'dont', 'ev', 'think', 'distract', 'task'], ['sometimes', 'use', 'pomodoro', 'timer', 'get', 'start', 'twenty-five', 'minutes', 'im', 'laser', 'focus', 'zone', 'dont', 'even', 'think', 'distract', 'task'])\n",
      "original document: \n",
      "['ʎɐpɥʇɹıq', 'ʎddɐɥ']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pq', 'dd'], ['pq', 'dd'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'don’t', 'get', 'the', 'South', 'America', 'part\\n\\nedit', 'TIL', 'South', 'Americans', 'have', 'a', 'reputation', 'for', 'being', 'rude']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'get', 'sou', 'americ', 'part\\n\\nedit', 'til', 'sou', 'am', 'reput', 'rud'], ['dont', 'get', 'south', 'america', 'part\\n\\nedit', 'til', 'south', 'americans', 'reputation', 'rude'])\n",
      "original document: \n",
      "['&gt;so', 'I', 'was', 'wondering', 'if', 'anyone', 'had', 'any', 'idea', 'how', 'much', 'it', 'would', 'roughly', 'cost', 'to', 'wire', '12', 'work', 'stations,', 'cat', '6', 'or', '7.\\n\\n[Depends', 'on', 'how', 'long', 'you', 'want', 'it.]\\n(https://www.amazon.com/slp/shielded-twisted-pair-wire/zjqwn87ea454csk)\\n\\n\\n&gt;', \"I'm\", 'debating', 'on', 'a', 'switch.', 'A', 'router.', 'Very', 'very', 'basic,', 'but', 'I', 'want', 'it', 'to', 'look', 'nice', 'and', 'last', 'the', 'company', 'a', 'long', 'time.\\n\\n[Hmmmm...', 'not', 'that', 'professionaly', 'looking,', 'but', 'the', 'combination', 'is', 'very', \"nice.\\nDont't\", 'mind', 'the', 'gaming', 'marketing]\\n(https://www.amazon.com/dp/B06WGS5RH3/ref=twister_B075ZTJZJM?_encoding=UTF8&amp;psc=1)\\n\\nI', 'have', 'no', 'idea', 'how', 'big', 'the', 'company', 'will', 'be.\\nBut', 'this', 'you', 'can', 'look', 'at', 'the', 'suggestions', 'that', \"i've\", 'mentioned', 'above.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtso', 'wond', 'anyon', 'ide', 'much', 'would', 'rough', 'cost', 'wir', 'twelv', 'work', 'stat', 'cat', 'six', '7\\n\\ndepends', 'long', 'want', 'it\\nhttpswwwamazoncomslpshieldedtwistedpairwirezjqwn87ea454csk\\n\\n\\ngt', 'im', 'deb', 'switch', 'rout', 'bas', 'want', 'look', 'nic', 'last', 'company', 'long', 'time\\n\\nhmmmm', 'professiona', 'look', 'combin', 'nice\\ndontt', 'mind', 'gam', 'marketing\\nhttpswwwamazoncomdpb06wgs5rh3reftwister_b075ztjzjm_encodingutf8amppsc1\\n\\ni', 'ide', 'big', 'company', 'be\\nbut', 'look', 'suggest', 'iv', 'ment'], ['gtso', 'wonder', 'anyone', 'idea', 'much', 'would', 'roughly', 'cost', 'wire', 'twelve', 'work', 'station', 'cat', 'six', '7\\n\\ndepends', 'long', 'want', 'it\\nhttpswwwamazoncomslpshieldedtwistedpairwirezjqwn87ea454csk\\n\\n\\ngt', 'im', 'debate', 'switch', 'router', 'basic', 'want', 'look', 'nice', 'last', 'company', 'long', 'time\\n\\nhmmmm', 'professionaly', 'look', 'combination', 'nice\\ndontt', 'mind', 'game', 'marketing\\nhttpswwwamazoncomdpb06wgs5rh3reftwister_b075ztjzjm_encodingutf8amppsc1\\n\\ni', 'idea', 'big', 'company', 'be\\nbut', 'look', 'suggestions', 'ive', 'mention'])\n",
      "original document: \n",
      "['What', 'about', 'my', 'statements!?', 'Did', 'you', 'give', 'them', 'a', 'chance!?', 'DID', 'YOU!?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stat', 'giv', 'chant'], ['statements', 'give', 'chance'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnofrui/):\\n\\nAwesome,', \"I'll\", 'put', 'it', 'on', 'my', 'to', 'read', 'list!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnofrui\\n\\nawesome', 'il', 'put', 'read', 'list'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnofrui\\n\\nawesome', 'ill', 'put', 'read', 'list'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Head', 'of', 'the', 'epa', \"doesn't\", 'believe', 'in', 'climate', 'change.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['head', 'ep', 'doesnt', 'believ', 'clim', 'chang'], ['head', 'epa', 'doesnt', 'believe', 'climate', 'change'])\n",
      "original document: \n",
      "['Cause', 'Kurt', 'Angle', 'KNOWS']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['caus', 'kurt', 'angl', 'know'], ['cause', 'kurt', 'angle', 'know'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['143414125|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413837\\nI', 'was', 'too', 'young', 'but', 'I', 'probably', 'would', 'have', 'voted', 'for', 'Obama.', 'His', 'campaign', 'spoke', 'to', 'me', 'as', 'a', 'young', 'person', 'in', 'ways', 'that', \"Bernie's\", \"couldn't\", 'in', '2016.', 'Obama', 'was', 'just', 'a', 'special', 'candidate', 'which', 'is', 'probably', 'why', 'I', 'supported', 'him', 'over', 'Hillary.', 'I', 'would', 'have', 'voted', 'for', 'her', 'had', 'she', 'won', 'the', 'nomination.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and twenty-fiv', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413837\\ni', 'young', 'prob', 'would', 'vot', 'obam', 'campaign', 'spok', 'young', 'person', 'way', 'berny', 'couldnt', 'two thousand and sixteen', 'obam', 'spec', 'candid', 'prob', 'support', 'hil', 'would', 'vot', 'nomination\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and twenty-five', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413837\\ni', 'young', 'probably', 'would', 'vote', 'obama', 'campaign', 'speak', 'young', 'person', 'ways', 'bernies', 'couldnt', 'two thousand and sixteen', 'obama', 'special', 'candidate', 'probably', 'support', 'hillary', 'would', 'vote', 'nomination\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['I', \"don't\", 'understand', 'why', \"you'd\", 'play', 'a', 'game', 'like', 'EVE', 'and', 'then', 'just', 'join', 'the', 'largest', 'most', 'powerful', 'alliance', 'in', 'the', 'game', 'in', 'your', 'first', 'month.\\n\\nSo', \"it's\", 'generally', 'hard', 'to', 'respect', 'their', 'line', 'members', 'off', 'the', 'bat.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'understand', 'youd', 'play', 'gam', 'lik', 'ev', 'join', 'largest', 'pow', 'al', 'gam', 'first', 'month\\n\\nso', 'gen', 'hard', 'respect', 'lin', 'memb', 'bat'], ['dont', 'understand', 'youd', 'play', 'game', 'like', 'eve', 'join', 'largest', 'powerful', 'alliance', 'game', 'first', 'month\\n\\nso', 'generally', 'hard', 'respect', 'line', 'members', 'bat'])\n",
      "original document: \n",
      "['I', \"didn't\", 'even', 'have', 'to', 'use', '/s']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'ev', 'us'], ['didnt', 'even', 'use'])\n",
      "original document: \n",
      "['I', 'own', 'a', 'copy', 'of', 'one', 'of', 'the', '111', 'limited', 'editions', 'of', '1Q84', 'by', 'Haruki', 'Murakami!\\n\\nhttp://www.thecurvedhouse.com/portfolio/limited-edition-of-1q84-by-haruki-murakami/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cop', 'on', 'one hundred and eleven', 'limit', 'edit', '1q84', 'haruk', 'murakami\\n\\nhttpwwwthecurvedhousecomportfoliolimitededitionof1q84byharukimurakam'], ['copy', 'one', 'one hundred and eleven', 'limit', 'editions', '1q84', 'haruki', 'murakami\\n\\nhttpwwwthecurvedhousecomportfoliolimitededitionof1q84byharukimurakami'])\n",
      "original document: \n",
      "['Sounds', 'like:\\n\\nWhat', 'are', 'Saturdays', 'for?\\n\\nBoys\\n\\nI', 'dunno', 'I', \"don't\", 'understand', 'twitter', 'memes', 'or', 'whatever', 'this', 'is.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['sound', 'like\\n\\nwhat', 'saturday', 'for\\n\\nboys\\n\\ni', 'dunno', 'dont', 'understand', 'twit', 'mem', 'whatev'], ['sound', 'like\\n\\nwhat', 'saturdays', 'for\\n\\nboys\\n\\ni', 'dunno', 'dont', 'understand', 'twitter', 'memes', 'whatever'])\n",
      "original document: \n",
      "['I', 'don’t', 'discount', 'advanced', 'stats,', 'but', 'in', 'the', 'situation', 'where', 'Rudy', 'Gay', 'has', 'such', 'a', 'small', 'sample', 'size,', 'it’s', 'not', '100%', 'accurate', 'of', 'what', 'he’d', 'produced', 'if', 'he', 'was', 'healthy.', 'Also', 'these', 'specific', 'stats', 'don’t', 'factor', 'in', 'defense,', 'which', 'is', 'where', 'Rudy', 'Gay', 'is', 'a', 'plus', 'compared', 'to', 'Pau', '(purely', 'based', 'off', 'the', 'eye', 'test)', '', 'I’m', 'aware', 'of', 'the', 'injury,', 'which', 'is', 'why', 'I', 'was', 'questioning', 'how', 'you', 'though', 'Pau', 'was', 'better', 'prior', 'to', 'the', 'injury.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'discount', 'adv', 'stat', 'situ', 'rudy', 'gay', 'smal', 'sampl', 'siz', 'one hundred', 'acc', 'hed', 'produc', 'healthy', 'also', 'spec', 'stat', 'dont', 'fact', 'defens', 'rudy', 'gay', 'plu', 'comp', 'pau', 'pur', 'bas', 'ey', 'test', 'im', 'aw', 'injury', 'quest', 'though', 'pau', 'bet', 'pri', 'injury'], ['dont', 'discount', 'advance', 'stats', 'situation', 'rudy', 'gay', 'small', 'sample', 'size', 'one hundred', 'accurate', 'hed', 'produce', 'healthy', 'also', 'specific', 'stats', 'dont', 'factor', 'defense', 'rudy', 'gay', 'plus', 'compare', 'pau', 'purely', 'base', 'eye', 'test', 'im', 'aware', 'injury', 'question', 'though', 'pau', 'better', 'prior', 'injury'])\n",
      "original document: \n",
      "[\"Shi'ar\\n\\nBrood\\n\\nSentinels\", '\\n\\nHellfire', 'club', 'is', 'a', 'threat', 'cause', 'of', 'money', 'and', 'reach,', 'the', 'inner', 'members', \"aren't\", 'always', 'mutants\\n\\nSinister,', 'would', 'still', 'have', 'all', 'his', 'non', 'mutant', 'stuff\\n\\nJuggernaut', \"isn't\", 'a', 'mutant', \"he's\", 'magic\\n\\nPhoenix', 'Force\\n\\nAll', 'the', 'stuff', 'from', 'limbo\\n\\nThe', 'Phalanx']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shiar\\n\\nbrood\\n\\nsentinels', '\\n\\nhellfire', 'club', 'threat', 'caus', 'money', 'reach', 'in', 'memb', 'ar', 'alway', 'mutants\\n\\nsinister', 'would', 'stil', 'non', 'mut', 'stuff\\n\\njuggernaut', 'isnt', 'mut', 'hes', 'magic\\n\\nphoenix', 'force\\n\\nall', 'stuff', 'limbo\\n\\nthe', 'phalanx'], ['shiar\\n\\nbrood\\n\\nsentinels', '\\n\\nhellfire', 'club', 'threat', 'cause', 'money', 'reach', 'inner', 'members', 'arent', 'always', 'mutants\\n\\nsinister', 'would', 'still', 'non', 'mutant', 'stuff\\n\\njuggernaut', 'isnt', 'mutant', 'hes', 'magic\\n\\nphoenix', 'force\\n\\nall', 'stuff', 'limbo\\n\\nthe', 'phalanx'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"That's\", 'so', 'sad', ':(', 'I', \"can't\", 'even', 'imagine', 'what', 'the', 'poor', 'kid', 'was', 'going', 'through.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'sad', 'cant', 'ev', 'imagin', 'poor', 'kid', 'going'], ['thats', 'sad', 'cant', 'even', 'imagine', 'poor', 'kid', 'go'])\n",
      "original document: \n",
      "['Have', 'you', 'considered', 'a', 'speedrun,', \"you've\", 'got', 'a', 'natural', 'talent']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['consid', 'speedrun', 'youv', 'got', 'nat', 'tal'], ['consider', 'speedrun', 'youve', 'get', 'natural', 'talent'])\n",
      "original document: \n",
      "['Supertightwizard,', 'Titan', '285']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['supertightwizard', 'tit', 'two hundred and eighty-five'], ['supertightwizard', 'titan', 'two hundred and eighty-five'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['So', 'they', 'say.', 'Why', 'not', 'stuff', 'stores', 'with', 'them', 'on', 'launch?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'stuff', 'stor', 'launch'], ['say', 'stuff', 'store', 'launch'])\n",
      "original document: \n",
      "['I', 'find', 'that', 'alcohol', 'free', 'cider', 'here', 'is', 'closer', 'to', 'mulled', 'apple', 'juice.', 'Quite', 'often', 'it', 'has', 'cinnamon,', 'cloves', 'and', 'orange', 'peel', 'as', 'part', 'of', 'the', 'spice', 'mix']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['find', 'alcohol', 'fre', 'cid', 'clos', 'mul', 'appl', 'juic', 'quit', 'oft', 'cinnamon', 'clov', 'orang', 'peel', 'part', 'spic', 'mix'], ['find', 'alcohol', 'free', 'cider', 'closer', 'mull', 'apple', 'juice', 'quite', 'often', 'cinnamon', 'cloves', 'orange', 'peel', 'part', 'spice', 'mix'])\n",
      "original document: \n",
      "['I', 'need', 'to', 'beef', 'up', 'my', 'team', 'before', 'I', 'do', 'more', 'Squad', 'Battles', 'after', 'this', 'challenge.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'beef', 'team', 'squad', 'battl', 'challeng'], ['need', 'beef', 'team', 'squad', 'battle', 'challenge'])\n",
      "original document: \n",
      "['Geographic', 'location', 'would', 'have', 'helped', 'but', 'you', 'got', 'it.', 'We', 'googled', 'it', 'and', 'milk', 'snake', 'looks', 'right.', 'We', 'are', 'in', 'MAssachusetts,', 'USA.', 'Thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['geograph', 'loc', 'would', 'help', 'got', 'googl', 'milk', 'snak', 'look', 'right', 'massachuset', 'us', 'thank'], ['geographic', 'location', 'would', 'help', 'get', 'google', 'milk', 'snake', 'look', 'right', 'massachusetts', 'usa', 'thank'])\n",
      "original document: \n",
      "['what', 'the', 'fuck', 'did', 'i', 'just', 'watch?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'watch'], ['fuck', 'watch'])\n",
      "original document: \n",
      "['Was', 'not', 'expecting', 'that.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['expect'], ['expect'])\n",
      "original document: \n",
      "['Stop', 'staring', 'at', 'my', 'screen!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'star', 'screen'], ['stop', 'star', 'screen'])\n",
      "original document: \n",
      "['In', 'continuation', 'of', 'this:\\n\\nAnyone', 'without', 'tscc', '=', 'an', 'enemy', 'of', 'tscc\\n\\nAs', 'we', 'know', 'the', 'natural', 'man', 'is', 'an', 'enemy', 'to', 'god.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['continu', 'this\\n\\nanyone', 'without', 'tscc', 'enemy', 'tscc\\n\\nas', 'know', 'nat', 'man', 'enemy', 'god'], ['continuation', 'this\\n\\nanyone', 'without', 'tscc', 'enemy', 'tscc\\n\\nas', 'know', 'natural', 'man', 'enemy', 'god'])\n",
      "original document: \n",
      "['Young', 'Dolph', 'needs', 'this', 'car']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['young', 'dolph', 'nee', 'car'], ['young', 'dolph', 'need', 'car'])\n",
      "original document: \n",
      "['Colorizebot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['colorizebot'], ['colorizebot'])\n",
      "original document: \n",
      "['Ive', 'had', 'to', 'move', 'in', 'with', 'my', 'mum', 'again', 'for', 'a', 'few', 'months', 'and', 'I', 'have', 'about', '3mb', '(with', 'about', '0.2mb', 'upload)', 'here', 'I', 'want', 'to', 'die.', 'I', 'mean', 'I', 'can', 'just', 'about', 'play', 'pubg', 'on', 'it', 'if', 'no', 'one', 'else', 'is', 'using', 'the', 'internet', 'but', 'god', \"it's\", 'painful', 'to', 'use.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'mov', 'mum', 'month', '3mb', '02mb', 'upload', 'want', 'die', 'mean', 'play', 'pubg', 'on', 'els', 'us', 'internet', 'god', 'pain', 'us'], ['ive', 'move', 'mum', 'months', '3mb', '02mb', 'upload', 'want', 'die', 'mean', 'play', 'pubg', 'one', 'else', 'use', 'internet', 'god', 'painful', 'use'])\n",
      "original document: \n",
      "['Major', 'bummer,', 'Join', 'a', 'category', 'league', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['maj', 'bum', 'join', 'categ', 'leagu'], ['major', 'bummer', 'join', 'category', 'league'])\n",
      "original document: \n",
      "['ASSASSins']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['assassin'], ['assassins'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Now', 'I', 'feel', 'bad', 'realizing', 'how', 'seriously', 'he', 'was', 'hurt.', '', 'Hope', 'he', 'is', 'ok', 'now.', '', '😯']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'bad', 'real', 'sery', 'hurt', 'hop', 'ok'], ['feel', 'bad', 'realize', 'seriously', 'hurt', 'hope', 'ok'])\n",
      "original document: \n",
      "['Kept', 'out', 'so', 'well', 'that', \"they're\", 'not', 'even', 'in', 'the', 'game']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kept', 'wel', 'theyr', 'ev', 'gam'], ['keep', 'well', 'theyre', 'even', 'game'])\n",
      "original document: \n",
      "['A', 'classic', 'household', 'item.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['class', 'household', 'item'], ['classic', 'household', 'item'])\n",
      "original document: \n",
      "['That', 'sucks.', 'Cops', \"can't\", 'just', 'stop', 'and', 'frisk', 'you', 'on', 'the', 'sidewalk', 'for', 'no', 'reason.', 'I', 'know', 'racial', 'profiling', 'happens', 'and', 'there', 'are', 'some', 'real', 'asshole', 'cops', 'out', 'there.', \"I've\", 'heard', 'stories', 'of', 'people', 'getting', 'stopped', 'and', 'patted', 'down', 'because', 'they', '\"fit', 'the', 'description\"', 'of', 'someone', 'who', 'robbed', 'a', 'house.\\n\\nBut', \"I've\", 'never', 'heard', 'of', 'that', 'happening', 'in', 'the', 'city', 'of', 'Atlanta,', 'at', 'least', 'not', 'near', 'the', 'downtown', 'areas.', '\"Walking', 'while', 'black\"', \"isn't\", 'suspicious', 'in', 'neighborhoods', 'that', 'are', '50-100%', 'black.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suck', 'cop', 'cant', 'stop', 'frisk', 'sidewalk', 'reason', 'know', 'rac', 'profil', 'hap', 'real', 'asshol', 'cop', 'iv', 'heard', 'story', 'peopl', 'get', 'stop', 'pat', 'fit', 'describ', 'someon', 'rob', 'house\\n\\nbut', 'iv', 'nev', 'heard', 'hap', 'city', 'atlant', 'least', 'near', 'downtown', 'area', 'walk', 'black', 'isnt', 'suspicy', 'neighb', 'fifty thousand, one hundred', 'black'], ['suck', 'cop', 'cant', 'stop', 'frisk', 'sidewalk', 'reason', 'know', 'racial', 'profile', 'happen', 'real', 'asshole', 'cop', 'ive', 'hear', 'stories', 'people', 'get', 'stop', 'pat', 'fit', 'description', 'someone', 'rob', 'house\\n\\nbut', 'ive', 'never', 'hear', 'happen', 'city', 'atlanta', 'least', 'near', 'downtown', 'areas', 'walk', 'black', 'isnt', 'suspicious', 'neighborhoods', 'fifty thousand, one hundred', 'black'])\n",
      "original document: \n",
      "['I', 'would', 'be', 'happy', 'with', '$30']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'happy', 'thirty'], ['would', 'happy', 'thirty'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'great', 'idea,', 'actually.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'gre', 'ide', 'act'], ['thats', 'great', 'idea', 'actually'])\n",
      "original document: \n",
      "['The', 'fairness', 'doctrine', 'relied', 'on', 'the', 'government', 'having', 'the', 'right', 'to', 'set', 'rules', 'for', 'a', 'licence', 'combined', 'with', 'the', 'pragmatic', 'practice', 'of', 'granting', 'monopolies', 'on', 'broadcast', 'frequencies', 'with', 'such', 'a', 'licence.', '\\n\\nThere', 'are', 'no', 'broadcast', 'frequencies', 'on', 'the', 'internet.', '\\n\\nNobody', 'needs', 'a', 'licence', 'to', 'run', 'a', 'website.', '\\n\\nSo', 'it', 'would', 'fail', 'at', 'the', 'first', 'constitutional', 'challenge.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'doctrin', 'rely', 'govern', 'right', 'set', 'rul', 'lic', 'combin', 'pragm', 'pract', 'grant', 'monopo', 'broadcast', 'frequ', 'lic', '\\n\\nthere', 'broadcast', 'frequ', 'internet', '\\n\\nnobody', 'nee', 'lic', 'run', 'websit', '\\n\\nso', 'would', 'fail', 'first', 'constitut', 'challeng'], ['fairness', 'doctrine', 'rely', 'government', 'right', 'set', 'rule', 'licence', 'combine', 'pragmatic', 'practice', 'grant', 'monopolies', 'broadcast', 'frequencies', 'licence', '\\n\\nthere', 'broadcast', 'frequencies', 'internet', '\\n\\nnobody', 'need', 'licence', 'run', 'website', '\\n\\nso', 'would', 'fail', 'first', 'constitutional', 'challenge'])\n",
      "original document: \n",
      "['Yeah,', 'you', 'guys', 'just', \"don't\", 'understand', 'it', 'for', 'the', 'same', 'reason', 'you', \"don't\", 'understand', 'quantum', 'physics.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'guy', 'dont', 'understand', 'reason', 'dont', 'understand', 'quant', 'phys'], ['yeah', 'guy', 'dont', 'understand', 'reason', 'dont', 'understand', 'quantum', 'physics'])\n",
      "original document: \n",
      "['A', 'driving', 'job', 'then?', 'Just', 'trying', 'to', 'find', 'the', 'connection', 'between', 'a', 'speeding', 'warning', 'and', 'you', 'being', 'let', 'go...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['driv', 'job', 'try', 'find', 'connect', 'spee', 'warn', 'let', 'go'], ['drive', 'job', 'try', 'find', 'connection', 'speed', 'warn', 'let', 'go'])\n",
      "original document: \n",
      "['Fair', 'enough.', 'I', 'just', 'prefer', 'sitting', 'in', 'front', 'of', 'my', 'tv', 'on', 'the', 'couch', 'to', 'my', 'computer.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'enough', 'pref', 'sit', 'front', 'tv', 'couch', 'comput'], ['fair', 'enough', 'prefer', 'sit', 'front', 'tv', 'couch', 'computer'])\n",
      "original document: \n",
      "['Being', 'a', 'gay', 'man,', 'I', 'enjoy', 'reminding', 'straight', 'men', 'that', 'you', 'are', 'what', 'you', 'eat.', \"Who's\", 'the', 'pussy', 'now?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gay', 'man', 'enjoy', 'remind', 'straight', 'men', 'eat', 'who', 'pussy'], ['gay', 'man', 'enjoy', 'remind', 'straight', 'men', 'eat', 'whos', 'pussy'])\n",
      "original document: \n",
      "['Spot', '28', 'or', 'one', 'random', 'please', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spot', 'twenty-eight', 'on', 'random', 'pleas'], ['spot', 'twenty-eight', 'one', 'random', 'please'])\n",
      "original document: \n",
      "['I', 'personally', 'though', 'revelations', 'had', 'some', 'of', 'the', 'best', 'enemy', 'design', 'lol.', 'Drifted', 'from', 'that', 'classic', 'zombie', 'horror', 'but', 'dipped', 'more', 'into', 'body', 'horror,', 'I', 'think', 'it', 'had', 'some', 'of', 'the', 'scariest', 'enemies', 'of', 'the', 'series,', 'the', '\"mayday', 'mayday\"', 'buzzsaw', 'guy', 'yelling', '\"please', 'help', 'me', \"I'm\", 'still', 'human\"', 'while', 'simultaneously', 'trying', 'to', 'kill', 'you', 'really', 'stuck', 'with', 'me.', 'Let', 'me', 'try', 'and', 'find', 'a', 'link.\\n\\nEdit:', '[Found', 'it](https://youtu.be/mVCjtl2jeyg)', 'listen', 'closely', 'to', 'what', 'it', 'says,', 'honestly', 'pretty', 'terrifying', 'since', 'it', 'makes', 'you', 'think', 'he', 'has', 'just', 'enough', 'humanity', 'to', 'cry', 'for', 'help,', 'or', 'that', 'he', 'died', 'screaming', 'for', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['person', 'though', 'revel', 'best', 'enemy', 'design', 'lol', 'drift', 'class', 'zomby', 'hor', 'dip', 'body', 'hor', 'think', 'scariest', 'enemy', 'sery', 'mayday', 'mayday', 'buzzsaw', 'guy', 'yel', 'pleas', 'help', 'im', 'stil', 'hum', 'simult', 'try', 'kil', 'real', 'stuck', 'let', 'try', 'find', 'link\\n\\nedit', 'found', 'ithttpsyoutubemvcjtl2jeyg', 'list', 'clos', 'say', 'honest', 'pretty', 'terr', 'sint', 'mak', 'think', 'enough', 'hum', 'cry', 'help', 'died', 'screaming'], ['personally', 'though', 'revelations', 'best', 'enemy', 'design', 'lol', 'drift', 'classic', 'zombie', 'horror', 'dip', 'body', 'horror', 'think', 'scariest', 'enemies', 'series', 'mayday', 'mayday', 'buzzsaw', 'guy', 'yell', 'please', 'help', 'im', 'still', 'human', 'simultaneously', 'try', 'kill', 'really', 'stick', 'let', 'try', 'find', 'link\\n\\nedit', 'find', 'ithttpsyoutubemvcjtl2jeyg', 'listen', 'closely', 'say', 'honestly', 'pretty', 'terrify', 'since', 'make', 'think', 'enough', 'humanity', 'cry', 'help', 'die', 'scream'])\n",
      "original document: \n",
      "['If', 'you', 'know', 'something', 'about', 'them', 'and', 'the', 'issues', 'the', 'care', 'about,', 'that', 'helps', 'a', 'little', 'bit.', 'For', 'example,', 'my', 'mom', 'was', 'spreading', 'the', 'word', 'about', 'the', 'dangers', 'of', 'the', 'TPP', 'before', 'the', 'campaign', 'started.', 'So', 'I', 'would', 'just', 'ask', 'her', 'things', 'like,', '\"Do', 'know', 'if', 'any', 'of', 'the', 'candidates', 'have', 'taken', 'a', 'position', 'yet', 'against', 'the', 'TPP...???', \"She'd\", 'have', 'to', 'answer', 'Donald', 'Trump.', 'But', 'then', \"she'd\", 'say', 'Rex', 'Tillerson', 'is', 'a', 'billionaire,', 'blah,', 'blah,', 'blah', 'so', 'Trump', \"can't\", 'be', 'trusted', 'to', 'keep', 'his', 'word', 'on', 'that.', '\\n\\nBut', \"you're\", 'asking', 'them', 'to', 'redefine', 'who', 'they', 'are.', \"They've\", 'labelled', 'themselves', 'as', 'liberals,', 'and', 'that', 'comes', 'with', 'all', 'sorts', 'of', 'positive', 'associations', 'for', 'them', 'that', 'proves', 'to', 'themselves', 'that', 'they', 'are', 'a', 'good', 'person.', 'If', 'they', 'let', 'that', 'go,', 'they', 'are', 'now', 'a', 'bad', 'person', 'according', 'to', 'their', 'ideology.', 'So', 'they', 'need', 'to', 'not', 'only', 'see', 'the', 'hypocrisy', 'and', 'lies', 'and', 'failures', 'of', 'progressivism,', 'but', 'they', 'need', 'pointing', 'out,', 'for', 'example,', 'that', 'African', 'American', 'working', 'poor', 'will', 'fare', 'better', 'with', 'less', 'illegal', 'aliens', 'taking', 'jobs', 'for', 'low', 'pay,', 'etc.', 'Or', 'that', 'the', 'welfare', 'state', 'incentivizes', 'fathers', 'not', 'living', 'with', 'their', 'children,', 'perpetuating', 'a', 'cycle', 'of', 'poverty', 'and', 'delinquency.', 'Otherwise,', 'they', 'feel', 'like', 'they', 'are', 'abandoning', 'all', 'of', 'their', 'victims', 'of', 'the', 'evil', 'white', 'man.', '\\n\\nI', 'have', 'to', 'say,', 'though,', 'I', \"haven't\", 'red', 'pilled', 'a', 'single', 'one.', 'Just', 'sown', 'targeted', 'doubts.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'someth', 'issu', 'car', 'help', 'littl', 'bit', 'exampl', 'mom', 'spreading', 'word', 'dang', 'tpp', 'campaign', 'start', 'would', 'ask', 'thing', 'lik', 'know', 'candid', 'tak', 'posit', 'yet', 'tpp', 'shed', 'answ', 'donald', 'trump', 'shed', 'say', 'rex', 'tillerson', 'billionair', 'blah', 'blah', 'blah', 'trump', 'cant', 'trust', 'keep', 'word', '\\n\\nbut', 'yo', 'ask', 'redefin', 'theyv', 'label', 'lib', 'com', 'sort', 'posit', 'assocy', 'prov', 'good', 'person', 'let', 'go', 'bad', 'person', 'accord', 'ideolog', 'nee', 'see', 'hypocrisy', 'lie', 'fail', 'progress', 'nee', 'point', 'exampl', 'afr', 'am', 'work', 'poor', 'far', 'bet', 'less', 'illeg', 'aly', 'tak', 'job', 'low', 'pay', 'etc', 'welf', 'stat', 'int', 'fath', 'liv', 'childr', 'perpetu', 'cyc', 'poverty', 'delinqu', 'otherw', 'feel', 'lik', 'abandon', 'victim', 'evil', 'whit', 'man', '\\n\\ni', 'say', 'though', 'hav', 'red', 'pil', 'singl', 'on', 'sown', 'target', 'doubt'], ['know', 'something', 'issue', 'care', 'help', 'little', 'bite', 'example', 'mom', 'spread', 'word', 'dangers', 'tpp', 'campaign', 'start', 'would', 'ask', 'things', 'like', 'know', 'candidates', 'take', 'position', 'yet', 'tpp', 'shed', 'answer', 'donald', 'trump', 'shed', 'say', 'rex', 'tillerson', 'billionaire', 'blah', 'blah', 'blah', 'trump', 'cant', 'trust', 'keep', 'word', '\\n\\nbut', 'youre', 'ask', 'redefine', 'theyve', 'label', 'liberals', 'come', 'sort', 'positive', 'associations', 'prove', 'good', 'person', 'let', 'go', 'bad', 'person', 'accord', 'ideology', 'need', 'see', 'hypocrisy', 'lie', 'failures', 'progressivism', 'need', 'point', 'example', 'african', 'american', 'work', 'poor', 'fare', 'better', 'less', 'illegal', 'alien', 'take', 'job', 'low', 'pay', 'etc', 'welfare', 'state', 'incentivizes', 'father', 'live', 'children', 'perpetuate', 'cycle', 'poverty', 'delinquency', 'otherwise', 'feel', 'like', 'abandon', 'victims', 'evil', 'white', 'man', '\\n\\ni', 'say', 'though', 'havent', 'red', 'pilled', 'single', 'one', 'sow', 'target', 'doubt'])\n",
      "original document: \n",
      "['To', 'be', 'fair,', 'the', 'idea', \"isn't\", 'mein.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'ide', 'isnt', 'mein'], ['fair', 'idea', 'isnt', 'mein'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"It's\", 'not', 'Oregon,', 'we', 'will', 'put', 'in', 'the', 'backups', 'hopefully', 'at', 'the', 'beginning', 'of', 'the', '3rd', 'quarter']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oregon', 'put', 'backup', 'hop', 'begin', '3rd', 'quart'], ['oregon', 'put', 'backups', 'hopefully', 'begin', '3rd', 'quarter'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['well', 'who', 'would', 'have', 'thunk', 'it...', 'would', 'be', 'interesting', 'if', 'there', 'was', 'a', 'event', 'for', 'this', 'if', 'theres', 'ever', 'a', 'possibility', 'for', 'stalin', 'to', 'take', 'over', 'in', 'Georgia', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'would', 'thunk', 'would', 'interest', 'ev', 'ther', 'ev', 'poss', 'stalin', 'tak', 'georg'], ['well', 'would', 'thunk', 'would', 'interest', 'event', 'theres', 'ever', 'possibility', 'stalin', 'take', 'georgia'])\n",
      "original document: \n",
      "['I’m', 'so', 'sorry', 'for', 'your', 'loss!', 'I’m', 'sure', 'she’s', 'still', 'super', 'proud', 'of', 'you.', 'You', 'look', 'awesome', 'btw!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'loss', 'im', 'sur', 'she', 'stil', 'sup', 'proud', 'look', 'awesom', 'btw'], ['im', 'sorry', 'loss', 'im', 'sure', 'shes', 'still', 'super', 'proud', 'look', 'awesome', 'btw'])\n",
      "original document: \n",
      "['Adding', 'it', 'to', 'my', 'fix', 'it', 'list!', '\\n\\nThanks', 'for', 'letting', 'me', 'know.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad', 'fix', 'list', '\\n\\nthanks', 'let', 'know'], ['add', 'fix', 'list', '\\n\\nthanks', 'let', 'know'])\n",
      "original document: \n",
      "['', '', '\\n[Gideon', 'of', 'the', 'trials](https://img.scryfall.com/cards/normal/en/akh/14.jpg?1496796488)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Gideon%20of%20the%20trials)', '[(SF)](https://scryfall.com/card/akh/14?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!Gideon%20of%20the%20trials)', '', '\\n[axis', 'of', 'mortality](http://mythicspoiler.com/ixa/cards/axisofmortality.jpg)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=axis%20of%20mortality)', '[(SF)](https://scryfall.com/card/xln/3?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!axis%20of%20mortality)', '', '\\n^^^[[cardname]]', '^^^or', '^^^[[cardname|SET]]', '^^^to', '^^^call', '^^^-', '^^^Updated', '^^^images']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\ngideon', 'trialshttpsimgscryfallcomcardsnormalenakh14jpg1496796488', 'ghttpgathererwizardscompagescarddetailsaspxnamegideon20of20the20trials', 'sfhttpsscryfallcomcardakh14utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqgideon20of20the20trials', '\\naxis', 'mortalityhttpmythicspoilercomixacardsaxisofmortalityjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameaxis20of20mortality', 'sfhttpsscryfallcomcardxln3utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqaxis20of20mortality', '\\ncardname', 'cardnameset', 'cal', 'upd', 'im'], ['\\ngideon', 'trialshttpsimgscryfallcomcardsnormalenakh14jpg1496796488', 'ghttpgathererwizardscompagescarddetailsaspxnamegideon20of20the20trials', 'sfhttpsscryfallcomcardakh14utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqgideon20of20the20trials', '\\naxis', 'mortalityhttpmythicspoilercomixacardsaxisofmortalityjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameaxis20of20mortality', 'sfhttpsscryfallcomcardxln3utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqaxis20of20mortality', '\\ncardname', 'cardnameset', 'call', 'update', 'image'])\n",
      "original document: \n",
      "['Plus', \"there's\", 'LeBron', 'James', 'who', 'was', 'front', 'and', 'center', 'of', 'this', 'whole', 'thing', 'last', 'weekend.', 'He', 'had', 'the', 'N', 'word', 'spray', 'painted', 'on', 'his', 'house.', '\\n\\nHe', 'has', 'pretty', 'much', 'given', 'as', 'much', 'as', 'anyone', 'could', 'ask', 'for', 'to', 'the', 'city', 'of', 'Akron', 'to', 'help', 'underpriveledged', 'black', 'youth.', 'http://www.foxsports.com/nba/story/lebron-james-college-scholarships-akron-university-cavaliers-i-promise-081315\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plu', 'ther', 'lebron', 'jam', 'front', 'cent', 'whol', 'thing', 'last', 'weekend', 'n', 'word', 'spray', 'paint', 'hous', '\\n\\nhe', 'pretty', 'much', 'giv', 'much', 'anyon', 'could', 'ask', 'city', 'akron', 'help', 'underpriveledg', 'black', 'you', 'httpwwwfoxsportscomnbastorylebronjamescollegescholarshipsakronuniversitycavaliersipromise081315\\n'], ['plus', 'theres', 'lebron', 'jam', 'front', 'center', 'whole', 'thing', 'last', 'weekend', 'n', 'word', 'spray', 'paint', 'house', '\\n\\nhe', 'pretty', 'much', 'give', 'much', 'anyone', 'could', 'ask', 'city', 'akron', 'help', 'underpriveledged', 'black', 'youth', 'httpwwwfoxsportscomnbastorylebronjamescollegescholarshipsakronuniversitycavaliersipromise081315\\n'])\n",
      "original document: \n",
      "['This', 'content', 'brought', 'to', 'you', 'from', '\"Taiwan', 'Pool\"\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#off', 'site', 'feed', '\"Taiwan', 'Pool\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'brought', 'taiw', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'sit', 'fee', 'taiw', 'pool\\n'], ['content', 'bring', 'taiwan', 'pool\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\noff', 'site', 'fee', 'taiwan', 'pool\\n'])\n",
      "original document: \n",
      "['Actually', 'the', 'comment', 'said', 'NO', 'sugar', '/s']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'com', 'said', 'sug'], ['actually', 'comment', 'say', 'sugar'])\n",
      "original document: \n",
      "['oh,', 'sorry!', 'I', 'havnt', 'posted', 'any', 'of', 'the', 'Chi', 'Cards', 'at', 'all.', 'These', 'are', 'just', 'all', 'of', 'the', 'regulat', 'cards.', 'I', 'assume', 'that', 'you', 'thought', 'the', 'cards', 'after', 'the', 'Chi', 'Card', 'tooltip', 'were', 'all', 'Chi', 'Cards?', 'Because', 'only', '2', 'of', 'the', '10', 'carda', 'utilize', 'the', 'Chi', 'mechanic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sorry', 'havnt', 'post', 'chi', 'card', 'reg', 'card', 'assum', 'thought', 'card', 'chi', 'card', 'tooltip', 'chi', 'card', 'two', 'ten', 'card', 'util', 'chi', 'mech'], ['oh', 'sorry', 'havnt', 'post', 'chi', 'card', 'regulat', 'card', 'assume', 'think', 'card', 'chi', 'card', 'tooltip', 'chi', 'card', 'two', 'ten', 'carda', 'utilize', 'chi', 'mechanic'])\n",
      "original document: \n",
      "['Taro', 'smoothies', 'taste', 'just', 'like', 'this', 'to', 'me.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['taro', 'smoothy', 'tast', 'lik'], ['taro', 'smoothies', 'taste', 'like'])\n",
      "original document: \n",
      "['I', 'propose', 'a', 'moment', 'of', 'silence', 'at', '28:03']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['propos', 'mom', 'sil', 'two thousand, eight hundred and three'], ['propose', 'moment', 'silence', 'two thousand, eight hundred and three'])\n",
      "original document: \n",
      "['Decided,', \"I'm\", 'rooting', 'for', 'the', 'Twins', 'because', 'Bartolo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['decid', 'im', 'root', 'twin', 'bartolo'], ['decide', 'im', 'root', 'twin', 'bartolo'])\n",
      "original document: \n",
      "['Ser', 'Emmon', 'Costayne', '+4\\n\\nLord', 'Addam', 'Costayne\\n\\nSer', 'Ryswin', 'Costayne']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ser', 'emmon', 'costayn', '4\\n\\nlord', 'addam', 'costayne\\n\\nser', 'ryswin', 'costayn'], ['ser', 'emmon', 'costayne', '4\\n\\nlord', 'addam', 'costayne\\n\\nser', 'ryswin', 'costayne'])\n",
      "original document: \n",
      "['But', 'do', 'Rebels', 'appear', 'with', 'a', 'dead', 'Maul?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rebel', 'appear', 'dead', 'maul'], ['rebel', 'appear', 'dead', 'maul'])\n",
      "original document: \n",
      "['So', 'were', 'many.', 'If', 'I', 'recall', 'Cindarella,', 'Snow', 'White', 'and', 'Beauty', 'and', 'the', 'Beast', 'were', 'dark.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'recal', 'cindarell', 'snow', 'whit', 'beauty', 'beast', 'dark'], ['many', 'recall', 'cindarella', 'snow', 'white', 'beauty', 'beast', 'dark'])\n",
      "original document: \n",
      "['Rocket', 'League', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rocket', 'leagu'], ['rocket', 'league'])\n",
      "original document: \n",
      "['[+JavierLoustaunau](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dno8tyg/):\\n\\nMy', 'g.f.', 'is', 'an', 'extremely', 'good', 'writer', 'but', 'the', 'distance', 'between', \"'typing\", 'in', 'a', 'word', \"document'\", 'and', \"'being\", \"published'\", 'seems', 'intimidatingly', 'vast', 'to', 'her.', '\\n\\nWhat', 'would', 'you', 'say', 'where', 'the', 'steps', '(big', 'or', 'small)', 'from', 'aspiring', 'to', 'published?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['javierloustaunauhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdno8tyg\\n\\nmy', 'gf', 'extrem', 'good', 'writ', 'dist', 'typ', 'word', 'docu', 'publ', 'seem', 'intimid', 'vast', '\\n\\nwhat', 'would', 'say', 'step', 'big', 'smal', 'aspir', 'publ'], ['javierloustaunauhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdno8tyg\\n\\nmy', 'gf', 'extremely', 'good', 'writer', 'distance', 'type', 'word', 'document', 'publish', 'seem', 'intimidatingly', 'vast', '\\n\\nwhat', 'would', 'say', 'step', 'big', 'small', 'aspire', 'publish'])\n",
      "original document: \n",
      "['Not', 'sure', 'if', 'Springsteen', 'is', 'under', 'rated', 'but', 'ghost', 'of', 'tom', 'joad', 'is', 'one', 'of', 'the.most', 'haunting', 'albums', 'i', 'have', 'ever', 'heard.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'springsteen', 'rat', 'ghost', 'tom', 'joad', 'on', 'themost', 'haunt', 'album', 'ev', 'heard'], ['sure', 'springsteen', 'rat', 'ghost', 'tom', 'joad', 'one', 'themost', 'haunt', 'albums', 'ever', 'hear'])\n",
      "original document: \n",
      "['I', 'personally', 'feel', 'he', 'nailed', 'the', '\"Emotionless', 'sentient', 'space', 'robot\"', 'he', 'was', 'told', 'to', 'portray.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['person', 'feel', 'nail', 'emotionless', 'senty', 'spac', 'robot', 'told', 'portray'], ['personally', 'feel', 'nail', 'emotionless', 'sentient', 'space', 'robot', 'tell', 'portray'])\n",
      "original document: \n",
      "['I', 'have', 'the', 'same', 'issue.', 'But', 'when', 'I', 'download', 'it', 'and', 'open', 'up', 'the', 'setup', 'log.', 'It', \"doesn't\", 'work.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['issu', 'download', 'op', 'setup', 'log', 'doesnt', 'work'], ['issue', 'download', 'open', 'setup', 'log', 'doesnt', 'work'])\n",
      "original document: \n",
      "['The', 'space', 'dock.', '', 'No', 'better', 'way', 'to', 'seal', 'a', 'friendship']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spac', 'dock', 'bet', 'way', 'seal', 'friend'], ['space', 'dock', 'better', 'way', 'seal', 'friendship'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Umm', 'Salmon', 'when', 'I', 'used', 'to', 'live', 'in', 'Canada', 'Salmon', 'was', 'just', 'something', 'we', 'ate', 'on', 'like', 'weekly', 'basis,', 'been', 'a', 'while', 'since', 'I', 'last', 'ate', 'Salmon.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['um', 'salmon', 'us', 'liv', 'canad', 'salmon', 'someth', 'at', 'lik', 'week', 'bas', 'sint', 'last', 'at', 'salmon'], ['umm', 'salmon', 'use', 'live', 'canada', 'salmon', 'something', 'eat', 'like', 'weekly', 'basis', 'since', 'last', 'eat', 'salmon'])\n",
      "original document: \n",
      "['Happened', 'to', 'me', 'a', 'while', 'back', 'in', 'Chicago.', 'These', 'guys', 'run', 'scams', 'like', 'this', 'with', '\"legit\"', 'tickets', 'all', 'the', 'time', 'on', 'Craigslist.', 'They', 'run', 'multiple', 'listings', 'on', 'Craigslist', 'and', 'typically', 'title', 'them', 'the', 'same', 'way,', 'or', 'substantially', 'similar.', 'The', 'ad', 'on', 'the', 'inside', 'is', 'also', 'generally', 'the', 'same', 'and', 'will', 'use', 'a', 'different', 'name.', '\\n\\nSee', 'if', 'you', 'can', 'find', 'more', 'ads.', 'The', 'number', 'is', 'probably', 'a', 'burner', 'phone,', 'so', 'if', 'you', 'can', 'find', 'another', 'ad', 'and', 'call', 'another', 'burner', 'phone,', 'you', 'can', 'make', 'another', '\"purchase\"', 'and', 'track', 'them', 'down', 'that', 'way.', 'Obviously,', 'that', 'might', 'be', 'dangerous.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hap', 'back', 'chicago', 'guy', 'run', 'scam', 'lik', 'legit', 'ticket', 'tim', 'craigsl', 'run', 'multipl', 'list', 'craigsl', 'typ', 'titl', 'way', 'subst', 'simil', 'ad', 'insid', 'also', 'gen', 'us', 'diff', 'nam', '\\n\\nsee', 'find', 'ad', 'numb', 'prob', 'burn', 'phon', 'find', 'anoth', 'ad', 'cal', 'anoth', 'burn', 'phon', 'mak', 'anoth', 'purchas', 'track', 'way', 'obvy', 'might', 'dang'], ['happen', 'back', 'chicago', 'guy', 'run', 'scam', 'like', 'legit', 'ticket', 'time', 'craigslist', 'run', 'multiple', 'list', 'craigslist', 'typically', 'title', 'way', 'substantially', 'similar', 'ad', 'inside', 'also', 'generally', 'use', 'different', 'name', '\\n\\nsee', 'find', 'ads', 'number', 'probably', 'burner', 'phone', 'find', 'another', 'ad', 'call', 'another', 'burner', 'phone', 'make', 'another', 'purchase', 'track', 'way', 'obviously', 'might', 'dangerous'])\n",
      "original document: \n",
      "['See', 'what', \"you've\", 'just', 'written', 'contradicts', 'your', 'post.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'youv', 'writ', 'contradict', 'post'], ['see', 'youve', 'write', 'contradict', 'post'])\n",
      "original document: \n",
      "['To', 'be', 'fair,', 'you', 'have', 'to', 'have', 'a', 'very', 'high', 'IQ', 'to', 'understand', 'Rick', 'and', 'Morty.', 'The', 'humor', 'is', 'extremely', 'subtle,', 'and', 'without', 'a', 'solid', 'grasp', 'of', 'theoretical', 'physics', 'most', 'of', 'the', 'jokes', 'will', 'go', 'over', 'a', 'typical', \"viewer's\", 'head.', \"There's\", 'also', \"Rick's\", 'nihilistic', 'outlook,', 'which', 'is', 'deftly', 'woven', 'into', 'his', 'characterisation', '-', 'his', 'personal', 'philosophy', 'draws', 'heavily', 'fromNarodnaya', 'Volya', 'literature,', 'for', 'instance.', 'The', 'fans', 'understand', 'this', 'stuff;', 'they', 'have', 'the', 'intellectual', 'capacity', 'to', 'truly', 'appreciate', 'the', 'depths', 'of', 'these', 'jokes,', 'to', 'realize', 'that', \"they're\", 'not', 'just', 'funny-', 'they', 'say', 'something', 'deep', 'about', 'LIFE.', 'As', 'a', 'consequence', 'people', 'who', 'dislike', 'Rick', 'and', 'Morty', 'truly', 'ARE', 'idiots-', 'of', 'course', 'they', \"wouldn't\", 'appreciate,', 'for', 'instance,', 'the', 'humour', 'in', \"Rick's\", 'existencial', 'catchphrase', '\"Wubba', 'Lubba', 'Dub', 'Dub,\"', 'which', 'itself', 'is', 'a', 'cryptic', 'reference', 'to', \"Turgenev's\", 'Russian', 'epic', 'Fathers', 'and', 'Sons', \"I'm\", 'smirking', 'right', 'now', 'just', 'imagining', 'one', 'of', 'those', 'addlepated', 'simpletons', 'scratching', 'their', 'heads', 'in', 'confusion', 'as', 'Dan', \"Harmon's\", 'genius', 'unfolds', 'itself', 'on', 'their', 'television', 'screens.', 'What', 'fools...', 'how', 'I', 'pity', 'them.', '😂', 'And', 'yes', 'by', 'the', 'way,', 'I', 'DO', 'have', 'a', 'Rick', 'and', 'Morty', 'tattoo.', 'And', 'no,', 'you', 'cannot', 'see', 'it.', \"It's\", 'for', 'the', \"ladies'\", 'eyes', 'only-', 'And', 'even', 'they', 'have', 'to', 'demonstrate', 'that', \"they're\", 'within', '5', 'IQ', 'points', 'of', 'my', 'own', '(preferably', 'lower)', 'beforehand.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'high', 'iq', 'understand', 'rick', 'morty', 'hum', 'extrem', 'subtl', 'without', 'solid', 'grasp', 'theoret', 'phys', 'jok', 'go', 'typ', 'view', 'head', 'ther', 'also', 'rick', 'nihil', 'outlook', 'deft', 'wov', 'charact', 'person', 'philosoph', 'draw', 'heavy', 'fromnarodnay', 'voly', 'lit', 'inst', 'fan', 'understand', 'stuff', 'intellect', 'capac', 'tru', 'apprecy', 'depth', 'jok', 'real', 'theyr', 'funny', 'say', 'someth', 'deep', 'lif', 'consequ', 'peopl', 'dislik', 'rick', 'morty', 'tru', 'idiot', 'cours', 'wouldnt', 'apprecy', 'inst', 'humo', 'rick', 'ex', 'catchphras', 'wubb', 'lubb', 'dub', 'dub', 'crypt', 'ref', 'turgenev', 'russ', 'ep', 'fath', 'son', 'im', 'smirk', 'right', 'imagin', 'on', 'addlep', 'simpleton', 'scratching', 'head', 'confus', 'dan', 'harmon', 'geni', 'unfold', 'televid', 'screens', 'fool', 'pity', 'ye', 'way', 'rick', 'morty', 'tattoo', 'cannot', 'see', 'lady', 'ey', 'ev', 'demonst', 'theyr', 'within', 'fiv', 'iq', 'point', 'pref', 'low', 'beforehand'], ['fair', 'high', 'iq', 'understand', 'rick', 'morty', 'humor', 'extremely', 'subtle', 'without', 'solid', 'grasp', 'theoretical', 'physics', 'joke', 'go', 'typical', 'viewers', 'head', 'theres', 'also', 'rick', 'nihilistic', 'outlook', 'deftly', 'weave', 'characterisation', 'personal', 'philosophy', 'draw', 'heavily', 'fromnarodnaya', 'volya', 'literature', 'instance', 'fan', 'understand', 'stuff', 'intellectual', 'capacity', 'truly', 'appreciate', 'depths', 'joke', 'realize', 'theyre', 'funny', 'say', 'something', 'deep', 'life', 'consequence', 'people', 'dislike', 'rick', 'morty', 'truly', 'idiots', 'course', 'wouldnt', 'appreciate', 'instance', 'humour', 'rick', 'existencial', 'catchphrase', 'wubba', 'lubba', 'dub', 'dub', 'cryptic', 'reference', 'turgenevs', 'russian', 'epic', 'father', 'sons', 'im', 'smirk', 'right', 'imagine', 'one', 'addlepated', 'simpletons', 'scratch', 'head', 'confusion', 'dan', 'harmons', 'genius', 'unfold', 'television', 'screen', 'fool', 'pity', 'yes', 'way', 'rick', 'morty', 'tattoo', 'cannot', 'see', 'ladies', 'eye', 'even', 'demonstrate', 'theyre', 'within', 'five', 'iq', 'point', 'preferably', 'lower', 'beforehand'])\n",
      "original document: \n",
      "['I', 'have', 'never', 'heard', 'anyone', 'say', 'anything', 'remotely', 'negative', 'about', 'the', 'possibility', 'of', 'male', 'birth', 'control,', 'either', 'IRL', 'or', 'online.', 'What', 'could', 'the', 'possible', 'downsides', 'be?', 'Of', 'course', 'this', 'is', 'a', 'good', 'idea.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'heard', 'anyon', 'say', 'anyth', 'remot', 'neg', 'poss', 'mal', 'bir', 'control', 'eith', 'irl', 'onlin', 'could', 'poss', 'downsid', 'cours', 'good', 'ide'], ['never', 'hear', 'anyone', 'say', 'anything', 'remotely', 'negative', 'possibility', 'male', 'birth', 'control', 'either', 'irl', 'online', 'could', 'possible', 'downsides', 'course', 'good', 'idea'])\n",
      "original document: \n",
      "['/u/Mikeymc16,', 'your', 'submission', 'has', 'been', 'automatically', 'removed', 'because', 'you', 'do', 'not', 'meet', 'the', 'minimum', 'account', 'age', 'requirement', 'for', 'posting', '(violating', 'rule', '#1).', '[Click', 'here', 'for', 'information](https://www.reddit.com/r/Random_Acts_Of_Pizza/wiki/emergency_assistance)', 'about', 'emergency', 'food', 'assistance.\\n\\n---\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/Random_Acts_Of_Pizza)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['umikeymc16', 'submit', 'autom', 'remov', 'meet', 'minim', 'account', 'ag', 'requir', 'post', 'viol', 'rul', 'on', 'click', 'informationhttpswwwredditcomrrandom_acts_of_pizzawikiemergency_assistance', 'emerg', 'food', 'assistance\\n\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorrandom_acts_of_pizza', 'quest', 'concern'], ['umikeymc16', 'submission', 'automatically', 'remove', 'meet', 'minimum', 'account', 'age', 'requirement', 'post', 'violate', 'rule', 'one', 'click', 'informationhttpswwwredditcomrrandom_acts_of_pizzawikiemergency_assistance', 'emergency', 'food', 'assistance\\n\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorrandom_acts_of_pizza', 'question', 'concern'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Jacob', 'plans', 'stuff', 'for', 'him', 'and', 'tries', 'to', 'give', 'Ice', 'some', 'organization.', \"It's\", 'ultimately', 'up', 'to', 'Ice', 'whether', 'he', 'takes', 'his', 'advice', 'or', 'not,', 'as', 'Jacob', 'has', 'said', 'multiple', 'fucking', 'times.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jacob', 'plan', 'stuff', 'tri', 'giv', 'ic', 'org', 'ultim', 'ic', 'wheth', 'tak', 'adv', 'jacob', 'said', 'multipl', 'fuck', 'tim'], ['jacob', 'plan', 'stuff', 'try', 'give', 'ice', 'organization', 'ultimately', 'ice', 'whether', 'take', 'advice', 'jacob', 'say', 'multiple', 'fuck', 'time'])\n",
      "original document: \n",
      "['Hm.', 'I', 'reckon', 'some', 'familiarity', 'with', 'programming', 'is', 'probably', 'a', 'good', 'idea', '(maybe', 'a', 'basic', 'Python', 'primer', 'or', 'something;', \"it's\", 'arguably', 'more', 'useful', 'than', 'learning', 'how', 'to', 'write', 'code', 'for', 'Arduino', 'depending', 'on', 'what', 'your', 'overall', 'aims', 'are)', 'just', 'so', 'you', 'can', 'understand', 'the', 'general', 'gist', 'of', 'what', 'the', 'code', 'is', 'doing.', '\\n\\nThat', 'said,', 'you', 'could', 'just', 'replicate', 'my', 'whole', 'project', 'and', 'then', 'teach', 'yourself', 'programming/electronics', 'on', 'the', 'fly', 'by', 'trying', 'to', 'add', 'extra', 'features', '(like', 'a', 'buzzer,', 'or', 'an', 'interface', 'that', 'calibrates', 'the', 'time', 'thresholds', 'to', 'suit', 'each', \"user's\", 'keying', 'speeds).', 'The', 'latter', 'is', 'a', 'good', 'way', 'to', 'learn,', 'just', \"don't\", 'be', 'afraid', 'to', 'break', 'stuff!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hm', 'reckon', 'famili', 'program', 'prob', 'good', 'ide', 'mayb', 'bas', 'python', 'prim', 'someth', 'argu', 'us', 'learn', 'writ', 'cod', 'arduino', 'depend', 'overal', 'aim', 'understand', 'gen', 'gist', 'cod', '\\n\\nthat', 'said', 'could', 'reply', 'whol', 'project', 'teach', 'programmingelectron', 'fly', 'try', 'ad', 'extr', 'feat', 'lik', 'buzz', 'interfac', 'calibr', 'tim', 'thresholds', 'suit', 'us', 'key', 'spee', 'lat', 'good', 'way', 'learn', 'dont', 'afraid', 'break', 'stuff'], ['hm', 'reckon', 'familiarity', 'program', 'probably', 'good', 'idea', 'maybe', 'basic', 'python', 'primer', 'something', 'arguably', 'useful', 'learn', 'write', 'code', 'arduino', 'depend', 'overall', 'aim', 'understand', 'general', 'gist', 'code', '\\n\\nthat', 'say', 'could', 'replicate', 'whole', 'project', 'teach', 'programmingelectronics', 'fly', 'try', 'add', 'extra', 'feature', 'like', 'buzzer', 'interface', 'calibrate', 'time', 'thresholds', 'suit', 'users', 'key', 'speed', 'latter', 'good', 'way', 'learn', 'dont', 'afraid', 'break', 'stuff'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Just', 'remember,', 'when', 'going', 'to', 'brothels', 'there', 'is', 'a', 'risk', 'of', 'supporting', 'sex', 'trafficking.', 'You', 'know,', 'women', 'who', 'are', 'being', 'bought', 'and', 'sold', 'to', 'be', 'raped', 'for', 'money..', '\\n\\nOf', 'course,', 'the', 'one', 'you', 'visit', 'might', 'be', 'an', 'exception,', 'but', 'probably', 'not.', 'You', 'could', 'always', 'ask', 'yourself', 'the', 'question', 'risk', 'VS', 'reward,', 'but', 'as', 'always,', 'your', 'cock', 'is', 'almost', 'as', 'important', 'as', 'freedom', 'for', 'women']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'going', 'brothel', 'risk', 'support', 'sex', 'traffick', 'know', 'wom', 'bought', 'sold', 'rap', 'money', '\\n\\nof', 'cours', 'on', 'visit', 'might', 'exceiv', 'prob', 'could', 'alway', 'ask', 'quest', 'risk', 'vs', 'reward', 'alway', 'cock', 'almost', 'import', 'freedom', 'wom'], ['remember', 'go', 'brothels', 'risk', 'support', 'sex', 'traffic', 'know', 'women', 'buy', 'sell', 'rap', 'money', '\\n\\nof', 'course', 'one', 'visit', 'might', 'exception', 'probably', 'could', 'always', 'ask', 'question', 'risk', 'vs', 'reward', 'always', 'cock', 'almost', 'important', 'freedom', 'women'])\n",
      "original document: \n",
      "['Very', 'very', 'much!', 'A', 'real', 'hottie', 'sir', '!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'real', 'hotty', 'sir'], ['much', 'real', 'hottie', 'sir'])\n",
      "original document: \n",
      "['Your', 'post', 'has', 'been', 'automatically', 'removed', 'as', 'an', 'anti-spam', 'measure,', 'as', 'your', 'account', 'is', 'under', 'five', 'days', 'old.', 'You', 'are', 'welcome', 'to', 'repost', 'this', 'content', 'when', 'your', 'account', 'has', 'been', 'active', 'for', 'five', 'days.', 'In', 'that', 'time,', 'please', 'also', 'familiarize', 'yourself', 'with', 'our', '[rules](https://www.reddit.com/r/RoastMe/about/rules/)', 'here', 'on', '/r/RoastMe', 'to', 'avoid', 'any', 'future', 'posts', 'you', 'make', 'from', 'getting', 'removed.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/RoastMe)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'autom', 'remov', 'antispam', 'meas', 'account', 'fiv', 'day', 'old', 'welcom', 'repost', 'cont', 'account', 'act', 'fiv', 'day', 'tim', 'pleas', 'also', 'famili', 'ruleshttpswwwredditcomrroastmeaboutr', 'rroastm', 'avoid', 'fut', 'post', 'mak', 'get', 'removed\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorroastm', 'quest', 'concern'], ['post', 'automatically', 'remove', 'antispam', 'measure', 'account', 'five', 'days', 'old', 'welcome', 'repost', 'content', 'account', 'active', 'five', 'days', 'time', 'please', 'also', 'familiarize', 'ruleshttpswwwredditcomrroastmeaboutrules', 'rroastme', 'avoid', 'future', 'post', 'make', 'get', 'removed\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorroastme', 'question', 'concern'])\n",
      "original document: \n",
      "['Because', 'the', 'shovel', 'pass', 'seemed', 'too', 'obvious']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shovel', 'pass', 'seem', 'obvy'], ['shovel', 'pass', 'seem', 'obvious'])\n",
      "original document: \n",
      "['143412881|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'LOhNy/z9)\\n\\n2008:', 'Elizabeth', 'II\\n2012:', 'Elizabeth', 'II\\n2016:', 'Elizabeth', 'II\\n\\nMake', 'America', 'Great', 'Again!!!\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, eight hundred and eighty-on', 'gt', 'unit', 'stat', 'anonym', 'id', 'lohnyz9\\n\\n2008', 'elizabe', 'ii\\n2012', 'elizabe', 'ii\\n2016', 'elizabe', 'ii\\n\\nmake', 'americ', 'gre', 'again\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, eight hundred and eighty-one', 'gt', 'unite', 'state', 'anonymous', 'id', 'lohnyz9\\n\\n2008', 'elizabeth', 'ii\\n2012', 'elizabeth', 'ii\\n2016', 'elizabeth', 'ii\\n\\nmake', 'america', 'great', 'again\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['', 'Me', 'too!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['What', 'difficulty', 'are', 'you', 'playing', 'on?\\nI', 'would', 'argue', 'that', 'group', 'comp,', 'gear,', 'stats', 'etc', 'matters', 'more', 'on', 'Tactician', '/', 'Honor', 'Mode', 'than', 'the', 'others.', '\\n\\nI', 'did', 'my', 'first', 'run', 'on', 'Classic', 'with', 'a', 'Pure', 'fighter/tank,', 'wizard', 'pyro/geo,', 'another', 'wizard', 'aero/hydro', 'and', 'a', 'Ranger.\\n\\nSome', 'fights', 'I', 'had', 'to', 'come', 'back', 'for,', 'but', 'in', 'general', 'I', 'had', 'no', 'problems.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['difficul', 'play', 'on\\ni', 'would', 'argu', 'group', 'comp', 'gear', 'stat', 'etc', 'mat', 'tact', 'hon', 'mod', 'oth', '\\n\\ni', 'first', 'run', 'class', 'pur', 'fightertank', 'wizard', 'pyrogeo', 'anoth', 'wizard', 'aerohydro', 'ranger\\n\\nsome', 'fight', 'com', 'back', 'gen', 'problem'], ['difficulty', 'play', 'on\\ni', 'would', 'argue', 'group', 'comp', 'gear', 'stats', 'etc', 'matter', 'tactician', 'honor', 'mode', 'others', '\\n\\ni', 'first', 'run', 'classic', 'pure', 'fightertank', 'wizard', 'pyrogeo', 'another', 'wizard', 'aerohydro', 'ranger\\n\\nsome', 'fight', 'come', 'back', 'general', 'problems'])\n",
      "original document: \n",
      "['1:', 'Night', 'King\\n2:', 'Cersei', 'LANNISTER\\n3:', 'Gregor', 'CLEGANE\\n4:', 'Jaime', 'LANNISTER\\n5:', 'Jorah', 'MORMONT\\n6:', 'Daenerys', 'TARGARYEN\\n7:', 'Qyburn\\n8:', 'Beric', 'DONDARION\\n9:', 'Euron', 'GREYJOY\\n10:', 'Jon', 'SNOW\\n\\nFire:', 'Beric', '\\n\\nFamily:', 'Mountain', 'CLEGANEBOWL!\\n\\nSteel:', 'NK\\n\\nMagic:', 'Qyburn\\n\\nLast:', 'Dany']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'night', 'king\\n2', 'cerse', 'lannister\\n3', 'greg', 'clegane\\n4', 'jaim', 'lannister\\n5', 'jorah', 'mormont\\n6', 'daenery', 'targaryen\\n7', 'qyburn\\n8', 'ber', 'dondarion\\n9', 'euron', 'greyjoy\\n10', 'jon', 'snow\\n\\nfire', 'ber', '\\n\\nfamily', 'mountain', 'cleganebowl\\n\\nsteel', 'nk\\n\\nmagic', 'qyburn\\n\\nlast', 'dany'], ['one', 'night', 'king\\n2', 'cersei', 'lannister\\n3', 'gregor', 'clegane\\n4', 'jaime', 'lannister\\n5', 'jorah', 'mormont\\n6', 'daenerys', 'targaryen\\n7', 'qyburn\\n8', 'beric', 'dondarion\\n9', 'euron', 'greyjoy\\n10', 'jon', 'snow\\n\\nfire', 'beric', '\\n\\nfamily', 'mountain', 'cleganebowl\\n\\nsteel', 'nk\\n\\nmagic', 'qyburn\\n\\nlast', 'dany'])\n",
      "original document: \n",
      "['I', 'would', 'probably', 'actually', 'die', 'from', 'the', 'laughter', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'prob', 'act', 'die', 'laught'], ['would', 'probably', 'actually', 'die', 'laughter'])\n",
      "original document: \n",
      "['You', 'have', 'no', 'reason', 'to', 'feel', 'self-conscious.', 'You', 'have', 'a', 'beautiful', 'body.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'feel', 'selfconscy', 'beauty', 'body'], ['reason', 'feel', 'selfconscious', 'beautiful', 'body'])\n",
      "original document: \n",
      "['Yeah', 'I', 'love', 'it.', 'In', 'my', 'top', '10', 'movies', 'of', 'all', 'time', 'honestly.', 'I', 'think', 'a', 'lot', 'of', 'the', 'criticism', 'it', 'gets', 'is', 'kind', 'of', 'insane', 'really.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'lov', 'top', 'ten', 'movy', 'tim', 'honest', 'think', 'lot', 'crit', 'get', 'kind', 'ins', 'real'], ['yeah', 'love', 'top', 'ten', 'movies', 'time', 'honestly', 'think', 'lot', 'criticism', 'get', 'kind', 'insane', 'really'])\n",
      "original document: \n",
      "['False', 'start', 'count:', '5']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fals', 'start', 'count', 'fiv'], ['false', 'start', 'count', 'five'])\n",
      "original document: \n",
      "['This', 'is', 'greatly', 'appreciated,', 'we', 'look', 'forward', 'to', 'working', 'with', 'Lockheed', 'and', 'the', 'United', 'States', 'even', 'more', 'in', 'the', 'future.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'apprecy', 'look', 'forward', 'work', 'lockhee', 'unit', 'stat', 'ev', 'fut'], ['greatly', 'appreciate', 'look', 'forward', 'work', 'lockheed', 'unite', 'state', 'even', 'future'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Plus', 'he', 'has', 'a', 'sick', 'bike.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plu', 'sick', 'bik'], ['plus', 'sick', 'bike'])\n",
      "original document: \n",
      "['I', \"wouldn't\", 'be', 'against', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt'], ['wouldnt'])\n",
      "original document: \n",
      "['I', 'agree.', 'At', '*minimum*', 'this', 'is', 'going', 'to', 'be', 'disputed', 'in', 'the', 'courts', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'minim', 'going', 'disput', 'court', 'though'], ['agree', 'minimum', 'go', 'dispute', 'court', 'though'])\n",
      "original document: \n",
      "['Ready', 'to', 'hear', 'man', 'secretssss!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ready', 'hear', 'man', 'secretssss'], ['ready', 'hear', 'man', 'secretssss'])\n",
      "original document: \n",
      "[\"I'll\", 'most', 'likely', 'be', 'getting', 'an', 'older', 'Jeep', 'Wrangler', 'TJ.', \"I've\", 'heard', 'that', 'dealerships', 'oftentimes', 'under-report', 'issues', 'if', 'they', 'think', 'they', 'can', 'make', 'money', 'from', 'future', 'repairs,', 'which', 'is', 'why', 'I', 'was', 'leaning', 'towards', 'an', 'independent', 'mechanic', '(but', 'maybe', \"I'm\", 'being', 'too', 'cynical).', 'Sounds', 'like', 'it', 'might', 'be', 'worth', 'it', 'to', 'try', 'a', 'Chrysler/Jeep', 'dealership', 'instead.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'lik', 'get', 'old', 'jeep', 'wrangl', 'tj', 'iv', 'heard', 'deal', 'oftentim', 'underreport', 'issu', 'think', 'mak', 'money', 'fut', 'repair', 'lean', 'toward', 'independ', 'mech', 'mayb', 'im', 'cyn', 'sound', 'lik', 'might', 'wor', 'try', 'chryslerjeep', 'deal', 'instead'], ['ill', 'likely', 'get', 'older', 'jeep', 'wrangler', 'tj', 'ive', 'hear', 'dealerships', 'oftentimes', 'underreport', 'issue', 'think', 'make', 'money', 'future', 'repair', 'lean', 'towards', 'independent', 'mechanic', 'maybe', 'im', 'cynical', 'sound', 'like', 'might', 'worth', 'try', 'chryslerjeep', 'dealership', 'instead'])\n",
      "original document: \n",
      "['My', 'friend', '(Dallas', 'fan),', 'always', 'loves', 'to', 'tell', 'me', 'that', 'Jordan', 'Reed', 'sucks', 'because', \"he's\", 'injury', 'prone.', 'I', 'always', 'tell', 'him', 'that', 'it', \"doesn't\", 'matter', 'because', 'Reed', 'always', 'plays', 'and', 'dominates', 'through', 'injuries.', 'HTTR!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['friend', 'dalla', 'fan', 'alway', 'lov', 'tel', 'jord', 'ree', 'suck', 'hes', 'injury', 'pron', 'alway', 'tel', 'doesnt', 'mat', 'ree', 'alway', 'play', 'domin', 'injury', 'httr'], ['friend', 'dallas', 'fan', 'always', 'love', 'tell', 'jordan', 'reed', 'suck', 'hes', 'injury', 'prone', 'always', 'tell', 'doesnt', 'matter', 'reed', 'always', 'play', 'dominate', 'injuries', 'httr'])\n",
      "original document: \n",
      "['Was', 'there', 'an', 'opening', 'cinematic', 'trailer', 'thing', 'like', 'the', 'Oklahoma', 'Ohio', 'State', 'game?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'cinem', 'trail', 'thing', 'lik', 'oklahom', 'ohio', 'stat', 'gam'], ['open', 'cinematic', 'trailer', 'thing', 'like', 'oklahoma', 'ohio', 'state', 'game'])\n",
      "original document: \n",
      "[\"I'll\", 'talk', 'serious', 'and', 'not', 'joking', 'for', 'a', 'minute.', 'I', 'know', 'that', 'sometimes', 'the', 'world', 'is', 'far', 'from', 'kind', 'with', 'us', 'and', 'that', 'leaves', 'us', 'insecure', 'in', 'our', 'own', 'skin,', 'believe', 'me', \"I've\", 'been', 'there', 'before.', 'But', 'I', 'want', 'you', 'to', 'know,', 'kind', 'stranger,', 'that', 'you', 'are', 'very', 'much', 'worthy', 'and', 'just', 'having', 'the', 'courage', 'of', 'going', 'in', 'drag', \"it's\", 'something', 'that', 'you', 'should', 'be', 'proud', 'off.', 'This', 'is', 'not', 'empty', 'discourse,', 'for', 'me', 'anyone', 'who', 'has', 'the', 'courage', 'to', 'dress', 'up', 'on', 'a', 'gender', 'non', 'conforming', 'way', \"it's\", 'a', 'brave', 'motherfucker', 'and', 'deserves', 'applause', 'specially', 'on', 'a', 'world/society', 'where', 'people', 'are', 'still', 'out', 'casted', 'and', 'even', 'killed', 'just', 'because', 'of', 'that.', 'Never', 'forget', 'your', 'self', 'worth', 'sis!', '❤️\\n\\nPs:', 'I', 'hope', 'it', 'will', 'come', 'a', 'day', 'where', 'you', 'are', 'comfortable', 'enough', 'to', 'post', 'both', 'your', 'boy', 'self', 'and', 'your', 'drag', 'persona', 'permanently.', 'You', 'already', 'have', 'a', 'lot', 'of', 'people', 'here', 'who', 'like', 'how', 'you', 'own', 'up', 'the', 'whole', 'Dream', 'Reign', 'thing', 'showing', 'that', 'you', 'are', 'a', 'good', 'sport', 'and', 'have', 'your', 'head', 'in', 'the', 'right', 'place.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'talk', 'sery', 'jok', 'minut', 'know', 'sometim', 'world', 'far', 'kind', 'us', 'leav', 'us', 'insec', 'skin', 'believ', 'iv', 'want', 'know', 'kind', 'stranger', 'much', 'worthy', 'cour', 'going', 'drag', 'someth', 'proud', 'empty', 'discours', 'anyon', 'cour', 'dress', 'gend', 'non', 'conform', 'way', 'brav', 'motherfuck', 'deserv', 'applaus', 'spec', 'worldsocy', 'peopl', 'stil', 'cast', 'ev', 'kil', 'nev', 'forget', 'self', 'wor', 'sis', '\\n\\nps', 'hop', 'com', 'day', 'comfort', 'enough', 'post', 'boy', 'self', 'drag', 'person', 'perm', 'already', 'lot', 'peopl', 'lik', 'whol', 'dream', 'reign', 'thing', 'show', 'good', 'sport', 'head', 'right', 'plac'], ['ill', 'talk', 'serious', 'joke', 'minute', 'know', 'sometimes', 'world', 'far', 'kind', 'us', 'leave', 'us', 'insecure', 'skin', 'believe', 'ive', 'want', 'know', 'kind', 'stranger', 'much', 'worthy', 'courage', 'go', 'drag', 'something', 'proud', 'empty', 'discourse', 'anyone', 'courage', 'dress', 'gender', 'non', 'conform', 'way', 'brave', 'motherfucker', 'deserve', 'applause', 'specially', 'worldsociety', 'people', 'still', 'cast', 'even', 'kill', 'never', 'forget', 'self', 'worth', 'sis', '\\n\\nps', 'hope', 'come', 'day', 'comfortable', 'enough', 'post', 'boy', 'self', 'drag', 'persona', 'permanently', 'already', 'lot', 'people', 'like', 'whole', 'dream', 'reign', 'thing', 'show', 'good', 'sport', 'head', 'right', 'place'])\n",
      "original document: \n",
      "['Remember', 'that', 'game', 'against', 'LSU', 'in', 'Jerry', 'World?', 'That', 'was', 'fun.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'gam', 'lsu', 'jerry', 'world', 'fun'], ['remember', 'game', 'lsu', 'jerry', 'world', 'fun'])\n",
      "original document: \n",
      "[\"I'm\", 'sorry', 'can', 'I', 'ask', 'why', 'is', 'ha', 'sentret', 'no', 'longer', 'obtainable', 'legitimately?', 'I', 'know', 'it', 'will', 'be', 'once', 'bank', 'is', 'updated', 'for', 'gold', 'and', 'silver', 'but', 'why', \"isn't\", 'it', 'right', 'now?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'ask', 'ha', 'sentret', 'long', 'obtain', 'legitim', 'know', 'bank', 'upd', 'gold', 'silv', 'isnt', 'right'], ['im', 'sorry', 'ask', 'ha', 'sentret', 'longer', 'obtainable', 'legitimately', 'know', 'bank', 'update', 'gold', 'silver', 'isnt', 'right'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Ugh,', 'and', 'my', 'Internet', 'decided', 'to', 'die', 'right', 'now...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ugh', 'internet', 'decid', 'die', 'right'], ['ugh', 'internet', 'decide', 'die', 'right'])\n",
      "original document: \n",
      "['Try', 'to', 'squeeze', 'in', 'an', 'SSD', 'in', 'that', 'budget,', 'and', 'not', '100%', 'necessary', 'but', 'if', 'you', 'can', 'afford', 'a', 'higher', 'refresh', 'rate', 'monitor', 'then', 'get', 'that.', 'But', 'an', 'SSD', 'is', '#1', 'priority.\\n\\nEDIT:', 'I', 'saw', 'your', 'post', 'about', 'salvaging', 'an', 'old', 'SSD,', 'sorry.', 'You', 'don’t', 'really', 'need', 'anything', 'else', 'than', 'the', 'stock', 'cooler', 'unless', 'you’re', 'doing', 'anything', 'above', 'a', 'light', 'overclock.', 'You', 'can', 'use', 'the', 'money', 'you', 'saved', 'on', 'other', 'things']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'squeez', 'ssd', 'budget', 'one hundred', 'necess', 'afford', 'high', 'refresh', 'rat', 'monit', 'get', 'ssd', 'on', 'priority\\n\\nedit', 'saw', 'post', 'salv', 'old', 'ssd', 'sorry', 'dont', 'real', 'nee', 'anyth', 'els', 'stock', 'cool', 'unless', 'yo', 'anyth', 'light', 'overclock', 'us', 'money', 'sav', 'thing'], ['try', 'squeeze', 'ssd', 'budget', 'one hundred', 'necessary', 'afford', 'higher', 'refresh', 'rate', 'monitor', 'get', 'ssd', 'one', 'priority\\n\\nedit', 'saw', 'post', 'salvage', 'old', 'ssd', 'sorry', 'dont', 'really', 'need', 'anything', 'else', 'stock', 'cooler', 'unless', 'youre', 'anything', 'light', 'overclock', 'use', 'money', 'save', 'things'])\n",
      "original document: \n",
      "['Severely', 'underrated', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sev', 'under'], ['severely', 'underrate'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dno92nj/):\\n\\nI', 'would', 'say', 'that', 'you', \"can't\", 'look', 'at', 'it', 'and', 'see', 'the', 'end.', 'You', 'have', 'to', 'look', 'at', 'it', 'and', 'see', 'the', 'next', 'step', '-', \"that's\", 'her', 'goal.', 'Only', 'after', 'that', 'she', 'had', 'to', 'move', 'on', 'to', 'the', 'next', 'step.', '', 'She', 'can', 'keep', 'publishing', 'in', 'mind,', 'but', 'the', 'distance', 'will', 'be', 'discouraging.', 'She', 'has', 'to', 'do', 'it', 'step', 'by', 'step.', 'If', 'she', 'has', 'a', 'passion', 'for', 'writing', 'and', 'for', 'what', \"she's\", 'writing', 'she', 'just', 'has', 'to', 'start', 'the', 'journey.', \"It's\", 'vast,', 'but', 'it', 'is', 'SO', 'worth', 'it.', 'Tell', 'her', 'I', 'say', 'good', 'luck!', \"She'll\", 'be', 'there', 'before', 'she', 'knows', 'it!\\n\\nEdit:', 'forgot', 'a', 'word']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdno92nj\\n\\ni', 'would', 'say', 'cant', 'look', 'see', 'end', 'look', 'see', 'next', 'step', 'that', 'goal', 'mov', 'next', 'step', 'keep', 'publ', 'mind', 'dist', 'disco', 'step', 'step', 'pass', 'writ', 'she', 'writ', 'start', 'journey', 'vast', 'wor', 'tel', 'say', 'good', 'luck', 'shel', 'know', 'it\\n\\nedit', 'forgot', 'word'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdno92nj\\n\\ni', 'would', 'say', 'cant', 'look', 'see', 'end', 'look', 'see', 'next', 'step', 'thats', 'goal', 'move', 'next', 'step', 'keep', 'publish', 'mind', 'distance', 'discourage', 'step', 'step', 'passion', 'write', 'shes', 'write', 'start', 'journey', 'vast', 'worth', 'tell', 'say', 'good', 'luck', 'shell', 'know', 'it\\n\\nedit', 'forget', 'word'])\n",
      "original document: \n",
      "[\"I'm\", 'willing', 'to', 'bet', 'Kanye', 'IS', 'pretty', 'ungrateful', 'for', 'everything', 'he', 'has.', \"Isn't\", 'he', 'a', 'known', 'asshole?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'wil', 'bet', 'kany', 'pretty', 'ungr', 'everyth', 'isnt', 'known', 'asshol'], ['im', 'will', 'bet', 'kanye', 'pretty', 'ungrateful', 'everything', 'isnt', 'know', 'asshole'])\n",
      "original document: \n",
      "['....', 'can', 'we', 'not,', 'fox?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fox'], ['fox'])\n",
      "original document: \n",
      "['Dude', 'this', 'is', 'me.', 'I', 'have', 'an', 'inguinal', 'hernia', '(hernia', 'in', 'your', 'groin)', 'and', 'weed', 'makes', 'my', 'fucking', 'dick', 'and', 'balls', 'hurt', 'if', 'there’s', 'a', 'slight', 'pain', 'before', 'I', 'smoke.', 'If', 'I', 'have', 'pain', 'anywhere', 'weed', 'just', 'amplifies', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dud', 'inguin', 'hern', 'hern', 'groin', 'wee', 'mak', 'fuck', 'dick', 'bal', 'hurt', 'ther', 'slight', 'pain', 'smok', 'pain', 'anywh', 'wee', 'ampl'], ['dude', 'inguinal', 'hernia', 'hernia', 'groin', 'weed', 'make', 'fuck', 'dick', 'ball', 'hurt', 'theres', 'slight', 'pain', 'smoke', 'pain', 'anywhere', 'weed', 'amplify'])\n",
      "original document: \n",
      "['Your', 'submission', 'has', 'been', 'automatically', 'removed', 'because', 'it', 'does', 'not', 'include', 'a', 'self', 'review.', 'You', 'must', 'use', 'the', '**exact', 'phrase**', '*self', 'review:*', 'along', 'with', 'a', 'detailed', 'self', 'criticism', 'of', 'your', 'video', 'or', 'channel', 'homepage.', 'See', '/r/youtubers/wiki/index', 'for', 'examples.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/youtubers)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'autom', 'remov', 'includ', 'self', 'review', 'must', 'us', 'exact', 'phrase', 'self', 'review', 'along', 'detail', 'self', 'crit', 'video', 'channel', 'homep', 'see', 'ryoutuberswikiindex', 'examples\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoryoutub', 'quest', 'concern'], ['submission', 'automatically', 'remove', 'include', 'self', 'review', 'must', 'use', 'exact', 'phrase', 'self', 'review', 'along', 'detail', 'self', 'criticism', 'video', 'channel', 'homepage', 'see', 'ryoutuberswikiindex', 'examples\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoryoutubers', 'question', 'concern'])\n",
      "original document: \n",
      "['Because', 'the', 'story', 'frames', 'it', 'as', 'a', 'cover', 'up.', \"It's\", 'a', 'safe', 'class', 'in', 'the', 'data', 'sheet,', 'but', \"that's\", 'because', 'you', \"aren't\", 'supposed', 'to', 'know', 'that', \"it's\", 'thaumiel.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['story', 'fram', 'cov', 'saf', 'class', 'dat', 'sheet', 'that', 'ar', 'suppos', 'know', 'thaumiel'], ['story', 'frame', 'cover', 'safe', 'class', 'data', 'sheet', 'thats', 'arent', 'suppose', 'know', 'thaumiel'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['This', 'is', 'r/RoastMe', 'not', 'r/SpitRoastMe', '...', \"You're\", 'doing', 'it', 'all', 'wrong!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rroastm', 'rspitroastme', 'yo', 'wrong'], ['rroastme', 'rspitroastme', 'youre', 'wrong'])\n",
      "original document: \n",
      "['This', 'is', 'a', 'pretty', 'cool', 'idea!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'cool', 'ide'], ['pretty', 'cool', 'idea'])\n",
      "original document: \n",
      "['It', 'would', 'certainly', 'be', 'viable.', 'Such', 'guns', 'are', 'no', 'less', 'effective', 'than', 'they', 'were', 'a', 'century', 'ago.\\n\\nDefinitely', \"wouldn't\", 'be', 'my', 'first', 'choice', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'certain', 'viabl', 'gun', 'less', 'effect', 'century', 'ago\\n\\ndefinitely', 'wouldnt', 'first', 'cho', 'though'], ['would', 'certainly', 'viable', 'gun', 'less', 'effective', 'century', 'ago\\n\\ndefinitely', 'wouldnt', 'first', 'choice', 'though'])\n",
      "original document: \n",
      "[\"He's\", 'got', '3', 'goals', 'in', '4', 'games', 'and', \"doesn't\", 'look', 'out', 'of', 'place.', 'Actually', 'among', '2017', 'picks', 'I', 'think', 'only', 'Hischier,', 'Yamamoto,', 'and', 'Chytil', 'have', 'done', 'better.', 'Small', 'sample', 'size', 'though.', 'They', 'want', 'him', 'getting', 'a', 'taste', 'so', 'he', 'goes', 'back', 'to', 'captain', 'Tri-City', 'and', 'dominate', 'so', 'he', 'can', 'contend', 'for', 'a', 'spot', 'next', 'year.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'got', 'three', 'goal', 'four', 'gam', 'doesnt', 'look', 'plac', 'act', 'among', 'two thousand and seventeen', 'pick', 'think', 'hischy', 'yamamoto', 'chytil', 'don', 'bet', 'smal', 'sampl', 'siz', 'though', 'want', 'get', 'tast', 'goe', 'back', 'captain', 'tric', 'domin', 'contend', 'spot', 'next', 'year'], ['hes', 'get', 'three', 'goals', 'four', 'game', 'doesnt', 'look', 'place', 'actually', 'among', 'two thousand and seventeen', 'pick', 'think', 'hischier', 'yamamoto', 'chytil', 'do', 'better', 'small', 'sample', 'size', 'though', 'want', 'get', 'taste', 'go', 'back', 'captain', 'tricity', 'dominate', 'contend', 'spot', 'next', 'year'])\n",
      "original document: \n",
      "['No.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['i', 'installed', 'the', 'package', 'but', 'the', 'font', 'is', 'still', 'not', 'showing', 'up', 'on', 'lxapparence,', 'no', 'clue', 'of', 'what', 'to', 'do.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['instal', 'pack', 'font', 'stil', 'show', 'lxapp', 'clu'], ['instal', 'package', 'font', 'still', 'show', 'lxapparence', 'clue'])\n",
      "original document: \n",
      "['Haha', \"don't\", 'worry,', \"I'm\", 'far', 'from', 'bulimic', 'and', \"I've\", 'never', 'done', 'that', 'before.', 'I', \"don't\", 'even', 'want', 'to', 'eat', 'pasta', 'in', 'general,', 'I', 'just', 'wanted', 'that', 'cheese', 'taste.', 'Believe', 'me,', 'I', 'INHALED', 'my', 'dinner', 'after', 'all', 'of', 'that', 'haha.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'dont', 'worry', 'im', 'far', 'bulim', 'iv', 'nev', 'don', 'dont', 'ev', 'want', 'eat', 'past', 'gen', 'want', 'chees', 'tast', 'believ', 'inh', 'din', 'hah'], ['haha', 'dont', 'worry', 'im', 'far', 'bulimic', 'ive', 'never', 'do', 'dont', 'even', 'want', 'eat', 'pasta', 'general', 'want', 'cheese', 'taste', 'believe', 'inhale', 'dinner', 'haha'])\n",
      "original document: \n",
      "['Nah', 'I', 'prefer', 'the', 'calculated', 'one', 'sorry!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nah', 'pref', 'calc', 'on', 'sorry'], ['nah', 'prefer', 'calculate', 'one', 'sorry'])\n",
      "original document: \n",
      "['Before', 'everyone', 'points', 'and', 'laughs', 'at', 'LSU', 'for', 'struggling', 'in', 'this', 'game...\\n\\nTroy', 'is', 'not', 'a', 'pushover.', \"They're\", 'very', 'well', 'coached', 'and', 'one', 'of', 'the', 'best', 'teams', 'in', 'the', 'G5.', \"They'd\", 'give', 'plenty', 'of', 'P5', 'schools', 'a', 'run', 'for', 'their', 'money.', 'Anyone', 'remember', 'how', 'they', 'almost', 'beat', '***Clemson***', 'last', 'year?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['everyon', 'point', 'laugh', 'lsu', 'struggling', 'game\\n\\ntroy', 'pushov', 'theyr', 'wel', 'coach', 'on', 'best', 'team', 'g5', 'theyd', 'giv', 'plenty', 'p5', 'schools', 'run', 'money', 'anyon', 'rememb', 'almost', 'beat', 'clemson', 'last', 'year'], ['everyone', 'point', 'laugh', 'lsu', 'struggle', 'game\\n\\ntroy', 'pushover', 'theyre', 'well', 'coach', 'one', 'best', 'team', 'g5', 'theyd', 'give', 'plenty', 'p5', 'school', 'run', 'money', 'anyone', 'remember', 'almost', 'beat', 'clemson', 'last', 'year'])\n",
      "original document: \n",
      "['And', 'espn3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['espn3'], ['espn3'])\n",
      "original document: \n",
      "['No', 'worries,', 'those', 'heaters', 'was', 'just', 'extra', 'put', 'in', 'by', 'previous', 'owners.', 'We', 'have', 'central', 'heat', 'so', 'we', 'never', 'turn', 'those', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['worry', 'heat', 'extr', 'put', 'prevy', 'own', 'cent', 'heat', 'nev', 'turn'], ['worry', 'heaters', 'extra', 'put', 'previous', 'owners', 'central', 'heat', 'never', 'turn'])\n",
      "original document: \n",
      "['LOL,', 'what?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/NoStupidQuestions/comments/73ht38/how_do_i_get_rid_of_these_virgins/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'], ['httpswwwredditcomrnostupidquestionscomments73ht38how_do_i_get_rid_of_these_virgins'])\n",
      "original document: \n",
      "['If', 'there', \"aren't\", '100', 'ways', 'to', 'intentionally', 'kill', 'myself', 'in', 'the', 'game', \"I'll\", 'be', 'disappointed.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ar', 'one hundred', 'way', 'int', 'kil', 'gam', 'il', 'disappoint'], ['arent', 'one hundred', 'ways', 'intentionally', 'kill', 'game', 'ill', 'disappoint'])\n",
      "original document: \n",
      "['Lame', 'as', 'hell.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lam', 'hel'], ['lame', 'hell'])\n",
      "original document: \n",
      "['Do', 'you', 'know', 'if', 'this', 'is', 'the', 'same', 'card?', 'https://www.amazon.ca/Fenvi-Desktop-Wireless-Hackintosh-Supporting/dp/B01IVIHPBY/ref=lp_7768732011_1_1?srs=7768732011&amp;ie=UTF8&amp;qid=1506816014&amp;sr=8-1\\n\\nTrying', 'to', 'find', 'an', 'equivalent', 'purchase', 'from', 'Canada', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'card', 'httpswwwamazoncafenvidesktopwirelesshackintoshsupportingdpb01ivihpbyreflp_7768732011_1_1srs7768732011ampieutf8ampqid1506816014ampsr81\\n\\ntrying', 'find', 'equ', 'purchas', 'canad'], ['know', 'card', 'httpswwwamazoncafenvidesktopwirelesshackintoshsupportingdpb01ivihpbyreflp_7768732011_1_1srs7768732011ampieutf8ampqid1506816014ampsr81\\n\\ntrying', 'find', 'equivalent', 'purchase', 'canada'])\n",
      "original document: \n",
      "['I', 'broached', 'the', 'conversation', 'again,', 'last', 'night', 'as', 'we', 'were', 'laying', 'down', 'for', 'the', 'night.', 'She', 'agreed', 'that', 'we', 'should', 'choose', 'two', 'night', 'during', 'the', 'week', 'for', 'romance.', 'Thank', 'you', 'for', 'your', 'advice!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['broach', 'convers', 'last', 'night', 'lay', 'night', 'agree', 'choos', 'two', 'night', 'week', 'rom', 'thank', 'adv'], ['broach', 'conversation', 'last', 'night', 'lay', 'night', 'agree', 'choose', 'two', 'night', 'week', 'romance', 'thank', 'advice'])\n",
      "original document: \n",
      "['I’m', 'not', 'sure', 'I’d', 'do', 'anything.', 'I’ll', 'have', 'to', 'give', 'mounted', 'combat', 'another', 'read', 'to', 'refresh', 'my', 'memory.', 'If', 'I', 'made', 'any', 'changes', 'it', 'might', 'be', 'there.', 'As', 'the', 'knight,', 'you', 'need', 'to', 'decide', 'when', 'you', 'should', 'fight', 'from', 'horseback', 'and', 'when', 'you', 'should', 'dismount.', 'It’s', 'pretty', 'campaign', 'dependent', 'how', 'viable', 'mounts', 'are', 'as', 'far', 'as', 'combat', 'situations.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'id', 'anyth', 'il', 'giv', 'mount', 'comb', 'anoth', 'read', 'refresh', 'mem', 'mad', 'chang', 'might', 'knight', 'nee', 'decid', 'fight', 'horseback', 'dismount', 'pretty', 'campaign', 'depend', 'viabl', 'mount', 'far', 'comb', 'situ'], ['im', 'sure', 'id', 'anything', 'ill', 'give', 'mount', 'combat', 'another', 'read', 'refresh', 'memory', 'make', 'change', 'might', 'knight', 'need', 'decide', 'fight', 'horseback', 'dismount', 'pretty', 'campaign', 'dependent', 'viable', 'mount', 'far', 'combat', 'situations'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Nothing', 'specific.', 'All', 'I', 'get', 'is', 'this.', 'https://image.prntscr.com/image/YaQPj863T4ug2Is2O-f9iA.png']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noth', 'spec', 'get', 'httpsimageprntscrcomimageyaqpj863t4ug2is2of9iapng'], ['nothing', 'specific', 'get', 'httpsimageprntscrcomimageyaqpj863t4ug2is2of9iapng'])\n",
      "original document: \n",
      "['Hopefully', 'it', 'will', 'get', 'my', 'boy', 'Alohadance', 'out', 'of', 'a', 'depression', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'get', 'boy', 'alohad', 'depress'], ['hopefully', 'get', 'boy', 'alohadance', 'depression'])\n",
      "original document: \n",
      "['There', 'are', 'many', 'more', 'things', 'that', 'could', 'be', 'done', 'in', 'a', 'hindsight.', 'With', 'a', 'keyword', 'being', '-', '*in', 'a', 'hindsight*.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'thing', 'could', 'don', 'hindsight', 'keyword', 'hindsight'], ['many', 'things', 'could', 'do', 'hindsight', 'keyword', 'hindsight'])\n",
      "original document: \n",
      "['When', 'someone', 'hates', 'your', 'kind', 'and', 'wants', 'you', 'and', 'all', 'your', 'kind', 'genocided,', 'does', 'morality', 'change', 'if', 'they', 'outnumber', 'you?', 'Not', 'really.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someon', 'hat', 'kind', 'want', 'kind', 'genocid', 'mor', 'chang', 'outnumb', 'real'], ['someone', 'hat', 'kind', 'want', 'kind', 'genocided', 'morality', 'change', 'outnumber', 'really'])\n",
      "original document: \n",
      "['&gt;', 'not', 'by', 'the', 'legal', 'way.', 'pretty', 'crazy', 'right?', 'they', 'never', 'tried', 'just', 'once.', 'If', 'they', 'had', 'way', 'less', 'spaniards', 'would', 'be', 'upset.\\n\\nPlease', \"don't\", 'play', 'dumb.', 'You', 'know', 'full', 'well', 'that', 'Rajoy', 'has', 'absolutely', 'no', 'intention', 'of', 'changing', 'the', 'constitution', 'to', 'allow', 'a', 'Catalan', 'vote.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'leg', 'way', 'pretty', 'crazy', 'right', 'nev', 'tri', 'way', 'less', 'spaniard', 'would', 'upset\\n\\nplease', 'dont', 'play', 'dumb', 'know', 'ful', 'wel', 'rajoy', 'absolv', 'int', 'chang', 'constitut', 'allow', 'cat', 'vote\\n\\n'], ['gt', 'legal', 'way', 'pretty', 'crazy', 'right', 'never', 'try', 'way', 'less', 'spaniards', 'would', 'upset\\n\\nplease', 'dont', 'play', 'dumb', 'know', 'full', 'well', 'rajoy', 'absolutely', 'intention', 'change', 'constitution', 'allow', 'catalan', 'vote\\n\\n'])\n",
      "original document: \n",
      "['Fuck,', 'and', 'I', 'forgot', 'to', 'mention', 'that', 'the', 'turret', 'chin', 'is', 'perfectly', 'designed', 'to', 'safely', 'deflect', 'shells', 'directly', 'into', 'the', 'drivers', 'head', 'through', 'through', 'the', 'thin', 'roof', 'armor.\\n\\n\\nOr', 'that', 'the', 'engine', 'could', 'be', 'disabled', 'from', 'being', 'strafed', 'by', 'fiddycals.\\n\\nPerfect', 'storm', 'of', 'ivory', 'tower', 'engineering.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'forgot', 'ment', 'turret', 'chin', 'perfect', 'design', 'saf', 'deflect', 'shel', 'direct', 'driv', 'head', 'thin', 'roof', 'armor\\n\\n\\n', 'engin', 'could', 'dis', 'strafed', 'fiddycals\\n\\nperfect', 'storm', 'iv', 'tow', 'engin'], ['fuck', 'forget', 'mention', 'turret', 'chin', 'perfectly', 'design', 'safely', 'deflect', 'shell', 'directly', 'drivers', 'head', 'thin', 'roof', 'armor\\n\\n\\nor', 'engine', 'could', 'disable', 'strafe', 'fiddycals\\n\\nperfect', 'storm', 'ivory', 'tower', 'engineer'])\n",
      "original document: \n",
      "['Congratulations', 'Vaca!', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congrat', 'vac'], ['congratulations', 'vaca'])\n",
      "original document: \n",
      "['It', 'is', 'the', 'new', 'file', 'format.', 'Having', 'previously', 'worked', 'with', 'vegas,', \"it's\", 'WAY', 'too', 'picky', 'about', 'file', 'formats.\\n\\nFirst,', 'I', 'would', 'try', 'a', 'few', 'tools,', 'one', 'being', 'VLC.', 'Just', 'to', 'see', 'if', 'you', 'can', 'even', 'play', 'the', 'file.', 'Next', 'the', 'portable', 'version', 'of', 'pazera', 'converter', 'suite.', 'Use', 'the', 'whatever', 'to', 'mp4', 'one.', '(it', 'will', 'take', 'any', 'file', 'as', 'input', 'and', 'convert', 'to', '.mp4)\\n\\nYou', 'should', 'now', 'be', 'able', 'to', 'import', 'the', 'footage.', 'Though', 'I', 'would', 'strongly', 'recommend', 'switching', 'to', 'adobe', 'premiere.', \"It's\", 'more', 'compatible', 'with', 'newer', 'formats', 'and', 'once', 'you', 'learn', 'a', 'few', 'shortcuts', 'and', 'workflow.', \"It's\", 'much', 'faster', 'than', 'vegas.', '(and', 'if', 'you', 'are', 'broke,', 'there', 'are...', 'other', 'ways', 'of', 'getting', 'it)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'fil', 'form', 'prevy', 'work', 'vega', 'way', 'picky', 'fil', 'formats\\n\\nfirst', 'would', 'try', 'tool', 'on', 'vlc', 'see', 'ev', 'play', 'fil', 'next', 'port', 'vert', 'pazer', 'convert', 'suit', 'us', 'whatev', 'mp4', 'on', 'tak', 'fil', 'input', 'convert', 'mp4\\n\\nyou', 'abl', 'import', 'foot', 'though', 'would', 'strongly', 'recommend', 'switch', 'adob', 'premy', 'compat', 'new', 'form', 'learn', 'shortcut', 'workflow', 'much', 'fast', 'vega', 'brok', 'way', 'get'], ['new', 'file', 'format', 'previously', 'work', 'vegas', 'way', 'picky', 'file', 'formats\\n\\nfirst', 'would', 'try', 'tool', 'one', 'vlc', 'see', 'even', 'play', 'file', 'next', 'portable', 'version', 'pazera', 'converter', 'suite', 'use', 'whatever', 'mp4', 'one', 'take', 'file', 'input', 'convert', 'mp4\\n\\nyou', 'able', 'import', 'footage', 'though', 'would', 'strongly', 'recommend', 'switch', 'adobe', 'premiere', 'compatible', 'newer', 'format', 'learn', 'shortcuts', 'workflow', 'much', 'faster', 'vegas', 'break', 'ways', 'get'])\n",
      "original document: \n",
      "['Last', 'time', 'I', 'checked', 'there', 'were', 'three', 'different', 'area', 'meetup', 'groups', 'for', 'boardgames.', 'Are', 'they', 'no', 'longer', 'active?', 'The', 'go-to', 'places', 'this', 'sub', 'throws', 'out', 'are', 'Mission', 'Board', 'Games,', 'Pawn', 'and', 'Pint,', 'and', 'Tabletop', 'Game', 'and', 'Hobby.', 'Yours', 'is', 'a', 'pretty', 'common', 'question', 'here.', 'Try', 'the', 'search', 'bar.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['last', 'tim', 'check', 'three', 'diff', 'are', 'meetup', 'group', 'boardgam', 'long', 'act', 'goto', 'plac', 'sub', 'throws', 'miss', 'board', 'gam', 'pawn', 'pint', 'tabletop', 'gam', 'hobby', 'pretty', 'common', 'quest', 'try', 'search', 'bar'], ['last', 'time', 'check', 'three', 'different', 'area', 'meetup', 'group', 'boardgames', 'longer', 'active', 'goto', 'place', 'sub', 'throw', 'mission', 'board', 'game', 'pawn', 'pint', 'tabletop', 'game', 'hobby', 'pretty', 'common', 'question', 'try', 'search', 'bar'])\n",
      "original document: \n",
      "['20', 'year', 'old', 'women', 'are', 'hotter', 'than', '30', 'year', 'old', 'women.', \"That's\", 'life', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty', 'year', 'old', 'wom', 'hot', 'thirty', 'year', 'old', 'wom', 'that', 'lif'], ['twenty', 'year', 'old', 'women', 'hotter', 'thirty', 'year', 'old', 'women', 'thats', 'life'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['heh', 'beagles', 'are', 'the', 'best']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heh', 'beagl', 'best'], ['heh', 'beagles', 'best'])\n",
      "original document: \n",
      "['I', 'found', 'it', 'and', \"couldn't\", 'find', 'the', 'part', 'where', 'he', '1', 'v', '3', 'all', 'of', 'you.\\n\\nAnyways,', 'in', 'this', 'case', 'someone', 'needed', 'plague', 'lord', 'to', 'deal', 'with', 'his', 'Lifesteal.', 'He', 'nearly', 'went', 'full', 'DPS', 'exept', 'for', 'the', 'OP', 'totem.', 'He', 'also', 'kinda', 'snowballed', 'out', 'of', 'control.', 'Dude', 'plays', 'a', 'lot', 'of', 'Kwang.\\n\\nPS', 'Numbing', 'Rogue', 'works', 'until', 'someone', 'can', 'get', 'plague', 'lord', 'online.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['found', 'couldnt', 'find', 'part', 'on', 'v', 'three', 'you\\n\\nanyways', 'cas', 'someon', 'nee', 'plagu', 'lord', 'deal', 'lifest', 'near', 'went', 'ful', 'dps', 'exept', 'op', 'totem', 'also', 'kind', 'snowbal', 'control', 'dud', 'play', 'lot', 'kwang\\n\\nps', 'numb', 'rog', 'work', 'someon', 'get', 'plagu', 'lord', 'onlin'], ['find', 'couldnt', 'find', 'part', 'one', 'v', 'three', 'you\\n\\nanyways', 'case', 'someone', 'need', 'plague', 'lord', 'deal', 'lifesteal', 'nearly', 'go', 'full', 'dps', 'exept', 'op', 'totem', 'also', 'kinda', 'snowball', 'control', 'dude', 'play', 'lot', 'kwang\\n\\nps', 'numb', 'rogue', 'work', 'someone', 'get', 'plague', 'lord', 'online'])\n",
      "original document: \n",
      "['I', 'guess', 'you', 'just', 'take', 'slang', 'literally.', 'It', 'adds', 'to', 'your', 'wonderful', '(imaginary,', 'like', 'my', 'dick)', 'personality.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guess', 'tak', 'slang', 'lit', 'ad', 'wond', 'imagin', 'lik', 'dick', 'person'], ['guess', 'take', 'slang', 'literally', 'add', 'wonderful', 'imaginary', 'like', 'dick', 'personality'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['SD', '|', '[Calgary', 'Flames', 'vs', 'Winnipeg', 'Jets](http://plr.livestreamsonline.net/embed/76)', '|', 'Ad', 'Overlays:', '3', '|', 'Mobile:', 'No', '\\n\\n\\n', '1.', 'If', 'black', 'screen', 'make', 'sure', 'flash', 'player', 'is', 'enabled.', '\\n\\n\\n', '2.', 'Stream', 'Live', '5', '-', '10', 'Before']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sd', 'calg', 'flam', 'vs', 'winnipeg', 'jetshttpplrlivestreamsonlinenetembed76', 'ad', 'overlay', 'three', 'mobl', '\\n\\n\\n', 'on', 'black', 'screen', 'mak', 'sur', 'flash', 'play', 'en', '\\n\\n\\n', 'two', 'stream', 'liv', 'fiv', 'ten'], ['sd', 'calgary', 'flame', 'vs', 'winnipeg', 'jetshttpplrlivestreamsonlinenetembed76', 'ad', 'overlay', 'three', 'mobile', '\\n\\n\\n', 'one', 'black', 'screen', 'make', 'sure', 'flash', 'player', 'enable', '\\n\\n\\n', 'two', 'stream', 'live', 'five', 'ten'])\n",
      "original document: \n",
      "['Congrats', 'on', 'having', 'two', 'of', 'your', 'major', 'cards', 'nerfed', 'and', 'still', 'being', 'the', '4th', 'best', 'class', 'in', 'the', 'game', '(P.S.', \"It's\", 'not', 'druid', \"it's\", 'jade', 'druid).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congr', 'two', 'maj', 'card', 'nerf', 'stil', '4th', 'best', 'class', 'gam', 'ps', 'druid', 'jad', 'druid'], ['congrats', 'two', 'major', 'card', 'nerfed', 'still', '4th', 'best', 'class', 'game', 'ps', 'druid', 'jade', 'druid'])\n",
      "original document: \n",
      "['They', 'also', 'held', 'a', 'office,', 'so', 'yes,', 'he', \"isn't\", 'waiting', 'for', 'a', 'week', 'would', 'be', 'amazing', 'or', 'horrifying.', 'haha', 'true', 'to', 'that', 'sub.', \"It's\", 'just', 'a', 'good', 'addition!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'held', 'off', 'ye', 'isnt', 'wait', 'week', 'would', 'amaz', 'horr', 'hah', 'tru', 'sub', 'good', 'addit'], ['also', 'hold', 'office', 'yes', 'isnt', 'wait', 'week', 'would', 'amaze', 'horrify', 'haha', 'true', 'sub', 'good', 'addition'])\n",
      "original document: \n",
      "['Go', 'tigers.', 'Checking', 'in']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'tig', 'check'], ['go', 'tigers', 'check'])\n",
      "original document: \n",
      "['Thats', 'a', 'lovely', 'accent', 'you', 'have', 'there,', 'New', 'Jersey?...', 'Oh', 'Austria?', '*Good', 'day', 'mate...', \"Let's\", 'throw', 'another', 'shrimp', 'on', 'the', 'barbie!*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'lov', 'acc', 'new', 'jersey', 'oh', 'austr', 'good', 'day', 'mat', 'let', 'throw', 'anoth', 'shrimp', 'barby'], ['thats', 'lovely', 'accent', 'new', 'jersey', 'oh', 'austria', 'good', 'day', 'mate', 'let', 'throw', 'another', 'shrimp', 'barbie'])\n",
      "original document: \n",
      "['Congratulations', \"it's\", 'a', 'great', 'to', 'have', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congrat', 'gre'], ['congratulations', 'great'])\n",
      "original document: \n",
      "['N0tail', 'carried', 'that', 'last', 'game', 'pretty', 'hard.', 'Still', 'a', 'lot', 'of', 'work', 'to', 'be', 'done', 'for', 'OG', 'though,', 'they', 'got', 'outdrafted', 'and', \"would've\", 'lost', 'top', 'rax', 'if', 'HR', \"hadn't\", 'made', 'some', 'crucial', 'mistakes', 'during', 'their', 'highground', 'push.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['n0tail', 'carry', 'last', 'gam', 'pretty', 'hard', 'stil', 'lot', 'work', 'don', 'og', 'though', 'got', 'outdraft', 'wouldv', 'lost', 'top', 'rax', 'hr', 'hadnt', 'mad', 'cruc', 'mistak', 'highground', 'push'], ['n0tail', 'carry', 'last', 'game', 'pretty', 'hard', 'still', 'lot', 'work', 'do', 'og', 'though', 'get', 'outdrafted', 'wouldve', 'lose', 'top', 'rax', 'hr', 'hadnt', 'make', 'crucial', 'mistake', 'highground', 'push'])\n",
      "original document: \n",
      "['Actually,', 'depending', 'on', 'the', 'state,', 'you', 'can.', '', 'Legally', 'anyway.', '', 'Not', 'always', 'or', 'even', 'mostly', 'the', 'proper', 'solution.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'depend', 'stat', 'leg', 'anyway', 'alway', 'ev', 'most', 'prop', 'solv'], ['actually', 'depend', 'state', 'legally', 'anyway', 'always', 'even', 'mostly', 'proper', 'solution'])\n",
      "original document: \n",
      "['Also', 'it', 'has', 'generally', 'less', 'traffic', 'so', 'it', 'helps', 'generate', 'content', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'gen', 'less', 'traff', 'help', 'gen', 'cont'], ['also', 'generally', 'less', 'traffic', 'help', 'generate', 'content'])\n",
      "original document: \n",
      "['Source?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sourc'], ['source'])\n",
      "original document: \n",
      "['add', '2', 'days']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ad', 'two', 'day'], ['add', 'two', 'days'])\n",
      "original document: \n",
      "['A', 'thread', 'on', 'Foran', 'and', 'gambling', 'is', 'more', 'of', 'a', 'mid-season', 'topic', 'tbh.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thread', 'for', 'gambl', 'midseason', 'top', 'tbh'], ['thread', 'foran', 'gamble', 'midseason', 'topic', 'tbh'])\n",
      "original document: \n",
      "['143418498|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'Yx60ZaN/)\\n\\n&gt;&gt;143418325\\nhttps://www.youtube.com/watch?v=dnrT7yZEV-g', '[Embed]\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and ninety-eight', 'gt', 'unit', 'stat', 'anonym', 'id', 'yx60zan\\n\\ngtgt143418325\\nhttpswwwyoutubecomwatchvdnrt7yzevg', 'embed\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and ninety-eight', 'gt', 'unite', 'state', 'anonymous', 'id', 'yx60zan\\n\\ngtgt143418325\\nhttpswwwyoutubecomwatchvdnrt7yzevg', 'embed\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Because', 'there', 'are', 'no', 'citeable', 'claims', 'about', 'it?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cit', 'claim'], ['citeable', 'claim'])\n",
      "original document: \n",
      "['Hahaha', 'you', 'and', 'me', 'both', 'man']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahah', 'man'], ['hahaha', 'man'])\n",
      "original document: \n",
      "['Anyone', 'that', 'says', 'Budweiser', 'anything', 'is', 'better', 'than', 'Miller', 'Lite', 'is', 'welcome', 'to', 'meet', 'me', 'after', 'school', 'in', 'the', 'parking', 'lot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'say', 'budw', 'anyth', 'bet', 'mil', 'lit', 'welcom', 'meet', 'school', 'park', 'lot'], ['anyone', 'say', 'budweiser', 'anything', 'better', 'miller', 'lite', 'welcome', 'meet', 'school', 'park', 'lot'])\n",
      "original document: \n",
      "['Great!', 'It', 'is', 'awesome', 'that', 'you', 'have', 'a', 'car', 'you', 'love!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'awesom', 'car', 'lov'], ['great', 'awesome', 'car', 'love'])\n",
      "original document: \n",
      "['[+existentialadvisor](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol0s6/):\\n\\nSo...what', 'does', 'someone', 'have', 'to', 'do', 'to', 'get', 'published?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['existentialadvisorhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol0s6\\n\\nsowhat', 'someon', 'get', 'publ'], ['existentialadvisorhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol0s6\\n\\nsowhat', 'someone', 'get', 'publish'])\n",
      "original document: \n",
      "['You', 'need', 'another', 'shame', 'flair', 'tbh.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'anoth', 'sham', 'flair', 'tbh'], ['need', 'another', 'shame', 'flair', 'tbh'])\n",
      "original document: \n",
      "['ouch']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ouch'], ['ouch'])\n",
      "original document: \n",
      "['The', 'main', 'reason', \"I'm\", 'getting', 'this', 'game', 'is', 'because', 'it', 'feels', 'like', 'a', 'reskinned', 'BO2,', 'because', \"that's\", 'by', 'far', 'my', 'favourite', 'COD', 'ever.', ':P\\n\\nProbably', 'not', 'great', 'if', \"you're\", 'looking', 'for', 'something', 'like', 'the', 'very', 'early', 'WWII', 'CODs,', 'but', 'I', 'think', 'most', 'people', 'want', 'it', 'to', 'be', 'like', 'BO2,', 'since', \"that's\", 'the', 'most', 'popular', 'COD', 'game', 'to', 'date.', 'I', 'believe', \"it's\", 'even', 'the', 'most', 'played', 'game', 'on', 'PSN,', 'or', 'at', 'least', 'it', 'was', 'at', 'one', 'point.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['main', 'reason', 'im', 'get', 'gam', 'feel', 'lik', 'reskin', 'bo2', 'that', 'far', 'favourit', 'cod', 'ev', 'p\\n\\nprobably', 'gre', 'yo', 'look', 'someth', 'lik', 'ear', 'wwi', 'cod', 'think', 'peopl', 'want', 'lik', 'bo2', 'sint', 'that', 'popul', 'cod', 'gam', 'dat', 'believ', 'ev', 'play', 'gam', 'psn', 'least', 'on', 'point'], ['main', 'reason', 'im', 'get', 'game', 'feel', 'like', 'reskinned', 'bo2', 'thats', 'far', 'favourite', 'cod', 'ever', 'p\\n\\nprobably', 'great', 'youre', 'look', 'something', 'like', 'early', 'wwii', 'cod', 'think', 'people', 'want', 'like', 'bo2', 'since', 'thats', 'popular', 'cod', 'game', 'date', 'believe', 'even', 'play', 'game', 'psn', 'least', 'one', 'point'])\n",
      "original document: \n",
      "['Filtration', 'system', 'marvel', 'to', 'behold.', 'It', 'remove', '80%', 'of', 'human', 'solid', 'waste', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['filt', 'system', 'marvel', 'behold', 'remov', 'eighty', 'hum', 'solid', 'wast'], ['filtration', 'system', 'marvel', 'behold', 'remove', 'eighty', 'human', 'solid', 'waste'])\n",
      "original document: \n",
      "['Oh', 'thank', 'you', 'so', 'much', 'for', 'this', 'post.', 'Do', 'not', 'forget', 'how', 'truly', 'precious', 'this', 'post', 'and', 'yourself', 'are.', 'I', 'really', 'hope', 'the', 'best', 'for', 'you,', 'so', 'goddamn', 'much.\\n\\nPlease', 'respond,', 'I', 'just', 'want', 'to', 'know', 'if', \"you're\", \"okay.\\n\\nWe're\", 'seven', 'goddamn', 'billion', 'people', 'here,', 'people', 'are', 'here', 'for', 'you.', 'Please', 'look', 'at', 'Logic', '-', '1-800-273-8255', '', 'on', 'youtube', 'for', 'me', 'please!!!!!', 'It', 'is', 'such', 'a', 'powerful', 'video,', 'it', 'can', 'make', 'anyone', 'cry.', 'I', 'am', 'just', 'hoping', 'you', 'are', 'doing', 'better,', 'know', 'that', 'the', 'world', 'has', 'so', 'much', 'affection', 'for', 'you,', 'sure', 'there', 'are', 'dicks', 'who', 'are', 'not', 'self', 'aware', 'of', 'their', 'problems,', 'but', 'besides', 'that', 'we', 'are', 'still', 'mortal', 'human', 'beings,', 'just', 'there', 'for', 'each', 'other', 'and', 'hoping', 'for', 'the', 'best.\\n\\nI', 'did', 'have', 'such', 'a', 'good', 'time', 'thanks', 'to', 'your', 'post.\\n\\nIt', 'is', 'often', 'so', 'easy', 'to', 'tell', 'that', 'people', 'are', 'there', 'for', 'you,', 'and', 'I', 'thank', 'you', 'so', 'much', 'for', 'being', 'here', 'for', 'us', 'to', 'remind', 'us', 'that.\\n\\nI', 'just', 'want', 'to', 'know', 'if', 'you', 'feel', 'better', 'for', 'posting', 'that', 'post.', 'It', 'brought', 'me', 'something', 'so', 'pure,', 'that', 'we', 'rarely', 'all', 'feel.\\n\\nI', 'never', 'knew', 'most', 'of', 'my', 'grandparents,', 'what', 'I', 'can', 'tell', 'you', 'though', 'is', 'that', 'your', 'great', 'nan', 'would', 'be', 'so', 'goddamn', 'proud', 'of', 'you', 'for', 'making', 'that', 'post,', 'it', 'brought', 'so', 'much', 'happiness', 'to', 'other', 'people', '&lt;3', '', '', 'I', 'just', 'want', 'you', 'to', 'know', 'that', 'people', 'are', 'here', 'for', 'you,', 'pm', 'me', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'thank', 'much', 'post', 'forget', 'tru', 'precy', 'post', 'real', 'hop', 'best', 'goddamn', 'much\\n\\nplease', 'respond', 'want', 'know', 'yo', 'okay\\n\\nwere', 'sev', 'goddamn', 'bil', 'peopl', 'peopl', 'pleas', 'look', 'log', 'eighteen billion, two million, seven hundred and thirty-eight thousand, two hundred and fifty-five', 'youtub', 'pleas', 'pow', 'video', 'mak', 'anyon', 'cry', 'hop', 'bet', 'know', 'world', 'much', 'affect', 'sur', 'dick', 'self', 'aw', 'problem', 'besid', 'stil', 'mort', 'hum', 'being', 'hop', 'best\\n\\ni', 'good', 'tim', 'thank', 'post\\n\\nit', 'oft', 'easy', 'tel', 'peopl', 'thank', 'much', 'us', 'remind', 'us', 'that\\n\\ni', 'want', 'know', 'feel', 'bet', 'post', 'post', 'brought', 'someth', 'pur', 'rar', 'feel\\n\\ni', 'nev', 'knew', 'grandp', 'tel', 'though', 'gre', 'nan', 'would', 'goddamn', 'proud', 'mak', 'post', 'brought', 'much', 'happy', 'peopl', 'lt3', 'want', 'know', 'peopl', 'pm', 'lt3'], ['oh', 'thank', 'much', 'post', 'forget', 'truly', 'precious', 'post', 'really', 'hope', 'best', 'goddamn', 'much\\n\\nplease', 'respond', 'want', 'know', 'youre', 'okay\\n\\nwere', 'seven', 'goddamn', 'billion', 'people', 'people', 'please', 'look', 'logic', 'eighteen billion, two million, seven hundred and thirty-eight thousand, two hundred and fifty-five', 'youtube', 'please', 'powerful', 'video', 'make', 'anyone', 'cry', 'hop', 'better', 'know', 'world', 'much', 'affection', 'sure', 'dicks', 'self', 'aware', 'problems', 'besides', 'still', 'mortal', 'human', 'be', 'hop', 'best\\n\\ni', 'good', 'time', 'thank', 'post\\n\\nit', 'often', 'easy', 'tell', 'people', 'thank', 'much', 'us', 'remind', 'us', 'that\\n\\ni', 'want', 'know', 'feel', 'better', 'post', 'post', 'bring', 'something', 'pure', 'rarely', 'feel\\n\\ni', 'never', 'know', 'grandparents', 'tell', 'though', 'great', 'nan', 'would', 'goddamn', 'proud', 'make', 'post', 'bring', 'much', 'happiness', 'people', 'lt3', 'want', 'know', 'people', 'pm', 'lt3'])\n",
      "original document: \n",
      "['I', 'mean,', 'you', 'could', 'ult?', 'lol', 'Her', 'boop', 'and', 'cannon', 'dps', \"isn't\", 'generally', 'going', 'to', 'be', 'worth', 'bubbling', 'for', 'in', 'a', 'true', '1v1', 'unless', 'the', 'zarya', 'has', 'literally', '0', 'energy.', 'But', \"it's\", 'still', 'going', 'to', 'be', 'best', 'not', 'to', 'feed', 'her', 'the', 'ult', 'charge', 'even', 'if', 'you', 'can', 'kill', 'her', 'because', 'she', 'has', '0', 'energy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'could', 'ult', 'lol', 'boop', 'cannon', 'dps', 'isnt', 'gen', 'going', 'wor', 'bubbl', 'tru', '1v1', 'unless', 'zary', 'lit', 'zero', 'energy', 'stil', 'going', 'best', 'fee', 'ult', 'charg', 'ev', 'kil', 'zero', 'energy'], ['mean', 'could', 'ult', 'lol', 'boop', 'cannon', 'dps', 'isnt', 'generally', 'go', 'worth', 'bubble', 'true', '1v1', 'unless', 'zarya', 'literally', 'zero', 'energy', 'still', 'go', 'best', 'fee', 'ult', 'charge', 'even', 'kill', 'zero', 'energy'])\n",
      "original document: \n",
      "['The', 'one', 'that', 'stood', 'out', 'to', 'me', 'is', '\"mandatory', 'class', 'on', 'systems', 'of', 'power\".', 'Just...', 'no', 'thank', 'you,', 'no', 'more', 'mandatory', 'heavily', 'politically', 'motivated', 'classes.', 'I', 'still', 'remember', 'my', 'mandatory', 'diversity', 'class', '(I', 'think', 'it', 'was', 'medicine,', 'culture,', 'and', \"society).\\n\\n\\nI'm\", 'really', 'liberal,', 'I', 'was', 'just', 'arguing', 'that', 'if', 'a', 'child', 'is', 'having', 'horrible', 'seizures', 'and', 'parents', 'are', 'repeatedly', 'neglecting', 'appropriate', 'treatment', 'to', 'the', 'point', 'the', 'kid', 'will', 'die,', 'our', 'government', 'should', 'allow', 'doctors', 'to', 'intervene', 'regardless', 'of', 'the', \"parents'\", 'culture', 'of', 'origin.', 'Pretty', 'reasonable,', 'I', \"don't\", 'like', 'kids', 'dying', 'because', 'of', 'neglect.\\n\\n\\nAnother', 'student', 'literally', 'said', '\"Well', \"that's\", 'just', 'your', 'western', 'white', 'male', 'perspective\".', 'It', 'was', 'at', 'the', 'same', 'time', 'obvious,', 'dehumanizing,', 'and', 'a', 'pointless', 'statement', 'and', 'the', 'prof', 'backed', 'them', 'up.', 'I', 'was', 'a', 'little', 'too', 'shocked', 'by', 'how', 'dumb', 'it', 'was', 'to', 'fight', 'back', 'at', 'the', 'time.', 'A', 'real', '\"is', 'this', 'real', 'life\"', 'moment,', 'it', 'was', 'my', 'first', 'time', 'dealing', 'with', 'something', 'like', 'that.', 'Something', 'that', 'became', 'very', 'common', 'in', 'that', 'class...', 'along', 'with', 'a', 'complete', 'lack', 'of', 'any', 'dissenting', 'sources.', 'Really', 'felt', 'like', 'a', 'farce', 'coming', 'from', 'my', 'hard', 'biology', 'background.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'stood', 'mand', 'class', 'system', 'pow', 'thank', 'mand', 'heavy', 'polit', 'mot', 'class', 'stil', 'rememb', 'mand', 'divers', 'class', 'think', 'medicin', 'cult', 'society\\n\\n\\nim', 'real', 'lib', 'argu', 'child', 'horr', 'seiz', 'par', 'rep', 'neglect', 'appropry', 'tre', 'point', 'kid', 'die', 'govern', 'allow', 'doct', 'interv', 'regardless', 'par', 'cult', 'origin', 'pretty', 'reason', 'dont', 'lik', 'kid', 'dying', 'neglect\\n\\n\\nanother', 'stud', 'lit', 'said', 'wel', 'that', 'western', 'whit', 'mal', 'perspect', 'tim', 'obvy', 'dehum', 'pointless', 'stat', 'prof', 'back', 'littl', 'shock', 'dumb', 'fight', 'back', 'tim', 'real', 'real', 'lif', 'mom', 'first', 'tim', 'deal', 'someth', 'lik', 'someth', 'becam', 'common', 'class', 'along', 'complet', 'lack', 'diss', 'sourc', 'real', 'felt', 'lik', 'farc', 'com', 'hard', 'biolog', 'background'], ['one', 'stand', 'mandatory', 'class', 'systems', 'power', 'thank', 'mandatory', 'heavily', 'politically', 'motivate', 'class', 'still', 'remember', 'mandatory', 'diversity', 'class', 'think', 'medicine', 'culture', 'society\\n\\n\\nim', 'really', 'liberal', 'argue', 'child', 'horrible', 'seizures', 'parent', 'repeatedly', 'neglect', 'appropriate', 'treatment', 'point', 'kid', 'die', 'government', 'allow', 'doctor', 'intervene', 'regardless', 'parent', 'culture', 'origin', 'pretty', 'reasonable', 'dont', 'like', 'kid', 'die', 'neglect\\n\\n\\nanother', 'student', 'literally', 'say', 'well', 'thats', 'western', 'white', 'male', 'perspective', 'time', 'obvious', 'dehumanize', 'pointless', 'statement', 'prof', 'back', 'little', 'shock', 'dumb', 'fight', 'back', 'time', 'real', 'real', 'life', 'moment', 'first', 'time', 'deal', 'something', 'like', 'something', 'become', 'common', 'class', 'along', 'complete', 'lack', 'dissent', 'source', 'really', 'felt', 'like', 'farce', 'come', 'hard', 'biology', 'background'])\n",
      "original document: \n",
      "['&gt;', \"What's\", 'your', 'favorite', 'flavor?\\n\\nWatermelon.\\n\\nFor', 'everything', 'else', '(shakes,', 'cakes,', 'whatever)', 'then', 'chocolate.\\n\\nExcept', 'cheesecakes,', 'those', 'are', 'their', 'own', 'flavor', 'and', 'stand', 'alone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'what', 'favorit', 'flavor\\n\\nwatermelon\\n\\nf', 'everyth', 'els', 'shak', 'cak', 'whatev', 'chocolate\\n\\nexcept', 'cheesecak', 'flav', 'stand', 'alon'], ['gt', 'whats', 'favorite', 'flavor\\n\\nwatermelon\\n\\nfor', 'everything', 'else', 'shake', 'cake', 'whatever', 'chocolate\\n\\nexcept', 'cheesecakes', 'flavor', 'stand', 'alone'])\n",
      "original document: \n",
      "['DAE', 'prequel', 'show', '--', 'excuse', 'me,', 'successor', 'show', '--', 'of', 'a', 'slightly-retconned', '(sorry', 'GURM)', 'Ned', 'Stark,', 'Howland', 'Reed,', 'and', 'Arthur', 'Dayne', 'taking', 'care', 'of', 'baby', '\"AeJon\"', 'in', 'a', '\"Three', 'Men', 'and', 'a', 'Baby\"', 'scenario???']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dae', 'prequel', 'show', 'excus', 'success', 'show', 'slightlyretcon', 'sorry', 'gurm', 'ned', 'stark', 'howland', 'ree', 'arth', 'dayn', 'tak', 'car', 'baby', 'aejon', 'three', 'men', 'baby', 'scenario'], ['dae', 'prequel', 'show', 'excuse', 'successor', 'show', 'slightlyretconned', 'sorry', 'gurm', 'ned', 'stark', 'howland', 'reed', 'arthur', 'dayne', 'take', 'care', 'baby', 'aejon', 'three', 'men', 'baby', 'scenario'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['It', 'just', 'lets', 'me', 'play', 'for', 'another', 'minute', 'before', 'crashing', ':/', 'any', 'idea?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'play', 'anoth', 'minut', 'crash', 'ide'], ['let', 'play', 'another', 'minute', 'crash', 'idea'])\n",
      "original document: \n",
      "['Also,', 'drug', 'them', 'and', 'sell', 'their', 'organs,', 'by', 'the', 'time', 'they', 'realize', 'what', 'happened', \"they'll\", 'be', 'dead..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'drug', 'sel', 'org', 'tim', 'real', 'hap', 'theyl', 'dead'], ['also', 'drug', 'sell', 'organs', 'time', 'realize', 'happen', 'theyll', 'dead'])\n",
      "original document: \n",
      "['Both.', 'Sci-fi', 'and', 'fantasy.', 'Or', 'urban', 'fantasy.\\n\\nA', 'better', 'term', 'for', 'it', 'would', 'probably', 'be', 'weird', 'fiction:', 'https://en.wikipedia.org/wiki/Weird_fiction']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['scif', 'fantasy', 'urb', 'fantasy\\n\\na', 'bet', 'term', 'would', 'prob', 'weird', 'fict', 'httpsenwikipediaorgwikiweird_fiction'], ['scifi', 'fantasy', 'urban', 'fantasy\\n\\na', 'better', 'term', 'would', 'probably', 'weird', 'fiction', 'httpsenwikipediaorgwikiweird_fiction'])\n",
      "original document: \n",
      "['Seems', 'like', \"we're\", 'playing', 'a', '4-4-2']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seem', 'lik', 'play', 'four hundred and forty-two'], ['seem', 'like', 'play', 'four hundred and forty-two'])\n",
      "original document: \n",
      "['They', 'should', 'be', 'shut', 'down.', 'Unfortunately', 'the', 'only', 'thing', 'more', 'sacred', 'in', 'the', 'US', 'than', 'a', 'church', 'is', 'a', 'corporation.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shut', 'unfortun', 'thing', 'sacr', 'us', 'church', 'corp'], ['shut', 'unfortunately', 'thing', 'sacred', 'us', 'church', 'corporation'])\n",
      "original document: \n",
      "['She', 'was', 'going', 'to', 'therapy', 'for', 'a', 'while', 'and', 'is', 'currently', 'not.', 'I', 'have', 'mentioned', 'it', 'a', 'few', 'times', 'but', 'I', \"don't\", 'think', 'she', 'wants', 'to', 'go', 'back', 'anytime', 'soon.', '\\n\\n\"but', 'always', 'remember', 'that', 'leaving', 'him', 'for', 'good', 'will', 'be', 'a', 'choice', 'she', 'has', 'to', 'make', 'on', 'her', 'own,', 'and', \"can't\", 'feel', 'like', 'a', 'choice', 'someone', 'else', 'made', 'for', 'her.\"', 'I', 'definitely', 'agree,', 'I', 'just', 'feel', 'guilty', 'with', 'both', 'ignoring', 'him/the', 'relationship', 'or', 'being', 'more', 'accepting,', 'they', 'just', 'both', 'seem', 'like', 'bad', 'options,', 'and', 'I', \"don't\", 'know', 'how', 'to', 'decide', 'which', 'one', 'is', 'better.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'therapy', 'cur', 'ment', 'tim', 'dont', 'think', 'want', 'go', 'back', 'anytim', 'soon', '\\n\\nbut', 'alway', 'rememb', 'leav', 'good', 'cho', 'mak', 'cant', 'feel', 'lik', 'cho', 'someon', 'els', 'mad', 'definit', 'agr', 'feel', 'guil', 'ign', 'himth', 'rel', 'acceiv', 'seem', 'lik', 'bad', 'opt', 'dont', 'know', 'decid', 'on', 'bet'], ['go', 'therapy', 'currently', 'mention', 'time', 'dont', 'think', 'want', 'go', 'back', 'anytime', 'soon', '\\n\\nbut', 'always', 'remember', 'leave', 'good', 'choice', 'make', 'cant', 'feel', 'like', 'choice', 'someone', 'else', 'make', 'definitely', 'agree', 'feel', 'guilty', 'ignore', 'himthe', 'relationship', 'accept', 'seem', 'like', 'bad', 'options', 'dont', 'know', 'decide', 'one', 'better'])\n",
      "original document: \n",
      "['Wow', \"you're\", 'butthole', 'is', '😍😍😍']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'yo', 'butthol'], ['wow', 'youre', 'butthole'])\n",
      "original document: \n",
      "['Devs', 'that', 'homogenize', 'their', 'game', 'and', 'reskin', 'every', 'piece', 'of', 'loot', 'deserve', 'all', 'the', 'toxicity', 'they', 'get.\\n\\nHow', 'dat', 'raid', 'loot', 'bruhs']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dev', 'homog', 'gam', 'reskin', 'every', 'piec', 'loot', 'deserv', 'tox', 'get\\n\\nhow', 'dat', 'raid', 'loot', 'bruh'], ['devs', 'homogenize', 'game', 'reskin', 'every', 'piece', 'loot', 'deserve', 'toxicity', 'get\\n\\nhow', 'dat', 'raid', 'loot', 'bruhs'])\n",
      "original document: \n",
      "['I', 'noticed', 'that', 'you', \"don't\", 'mention', 'test', 'scores.', 'How', 'important', 'are', 'they?', \"I'm\", 'in', 'the', 'process', 'of', 'applying', 'to', 'Masters', 'programs', 'and', 'my', 'test', 'scores', 'are', 'my', 'weak', 'point', 'because', 'I', \"don't\", 'have', 'time', 'to', 'devote/money', 'to', 'take', 'classes.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['not', 'dont', 'ment', 'test', 'scor', 'import', 'im', 'process', 'apply', 'mast', 'program', 'test', 'scor', 'weak', 'point', 'dont', 'tim', 'devotemoney', 'tak', 'class'], ['notice', 'dont', 'mention', 'test', 'score', 'important', 'im', 'process', 'apply', 'master', 'program', 'test', 'score', 'weak', 'point', 'dont', 'time', 'devotemoney', 'take', 'class'])\n",
      "original document: \n",
      "['[Off', 'https://i.imgur.com/mTC76mV.jpg](https://i.imgur.com/mTC76mV.jpg)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsiimgurcommtc76mvjpghttpsiimgurcommtc76mvjpg'], ['httpsiimgurcommtc76mvjpghttpsiimgurcommtc76mvjpg'])\n",
      "original document: \n",
      "[\"That's\", 'cute', 'and', 'all', 'but', \"you're\", 'posting', 'a', 'pic', 'on', 'the', 'expanse', 'of', 'the', 'Internet', 'of', 'a', 'stranger?', 'Creeper.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'cut', 'yo', 'post', 'pic', 'expans', 'internet', 'stranger', 'creep'], ['thats', 'cute', 'youre', 'post', 'pic', 'expanse', 'internet', 'stranger', 'creeper'])\n",
      "original document: \n",
      "['[In', 'the', 'mountains', 'of', 'Colorado', '](https://i.imgur.com/b7ALILi.jpg)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mountain', 'colorado', 'httpsiimgurcomb7alilijpg'], ['mountains', 'colorado', 'httpsiimgurcomb7alilijpg'])\n",
      "original document: \n",
      "['B']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['b'], ['b'])\n",
      "original document: \n",
      "['I', 'might', 'host', 'one', 'if', 'I', 'can', 'get', 'it', 'working.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['might', 'host', 'on', 'get', 'work'], ['might', 'host', 'one', 'get', 'work'])\n",
      "original document: \n",
      "['Fair', 'enough.\\n\\nI', 'see', 'a', 'bit', 'more', 'actual', 'substance', 'to', 'some', 'of', 'his', 'claims', 'than', 'you', 'do,', 'clearly.\\n\\nI', 'agree', 'with', 'you,', 'the', 'hate', 'crime', 'law', \"doesn't\", 'seem', 'to', 'me', 'like', 'a', 'slippery', 'slope', 'to', 'anything.', 'I', 'do', 'agree', 'with', 'his', 'proposition', 'that', 'you', 'cannot', 'codify', 'into', 'law', 'preferred', 'pronoun', 'usage.', 'I', 'would', 'never', 'intentionally', 'misgender', 'someone,', 'and', 'I', 'would', 'do', 'my', 'best', 'to', 'use', 'whatever', 'pronoun', 'a', 'person', 'prefers,', 'but', 'I', 'do', 'not', 'want', 'to', 'live', 'in', 'a', 'society', 'where', \"it's\", 'a', 'crime', 'if', 'I', 'get', 'that', 'wrong.', 'I', 'think', 'there', 'is', 'reasonable', 'room', 'for', 'discussion', 'about', 'where', 'compassion', 'and', 'function', 'meet', 'on', 'something', 'like', 'that.\\n\\nI', \"don't\", 'agree', 'with', 'his', 'stance', 'on', 'the', 'nuclear', 'family', 'and', 'by', 'association', 'his', 'criticism', 'of', 'other', 'models', 'of', 'being.', 'I', 'think', 'the', 'nuclear', 'family', 'is', 'a', 'model', 'that', 'works', 'for', 'many', 'people,', 'but', 'I', 'know', 'in', 'point', 'of', 'fact', 'that', 'it', 'does', 'not', 'work', 'for', 'many', 'others.', 'I', 'think', 'people', 'should', 'be', 'free', 'to', 'do', 'what', 'they', 'see', 'fit.', 'I', 'know', 'of', 'no', 'evidence', 'that', 'suggests', 'gay', 'adoption', 'is', 'in', 'any', 'way', 'inferior', 'to', 'straight', 'adoption.', 'I', \"wasn't\", 'aware', 'he', 'had', 'made', 'statements', 'to', 'that', 'affect,', 'but', 'it', \"doesn't\", 'totally', 'surprise', 'me', 'that', 'he', 'did.\\n\\nI', 'agree', 'that', 'we', 'should', 'encourage', 'people', 'to', 'embrace', 'their', 'gender', 'and', 'sexuality', 'as', 'fluid.', 'More', 'freedom', 'and', 'more', 'ability', 'to', 'be', 'true', 'to', \"one's\", 'reality', 'is', 'good.', '\\n\\nBut', 'I', 'agree', 'with', 'him', 'that', 'there', 'is', 'an', 'anti-science', 'and', 'anti-discourse', 'bent', 'to', 'some', 'of', 'the', 'present', 'political', 'climate', 'on', 'the', 'left.', \"I'm\", 'squarely', 'in', 'the', 'left', 'on', 'the', 'American', 'political', 'spectrum,', 'but', 'I', 'sense', 'more', 'and', 'more', 'that', 'there', 'is', 'a', 'tendency', 'to', 'dismiss', 'anything,', 'whether', 'an', 'idea,', 'a', 'study,', 'or', 'a', 'person,', 'that', 'conflicts', 'with', 'progressive', 'views', 'as', 'so', 'self-evidently', 'wrong,', 'ignorant,', 'and', 'hateful', 'as', 'to', 'not', 'even', 'deserve', 'consideration.', 'I', 'sense', 'that', 'even', 'in', 'myself', 'sometimes,', 'and', 'I', \"don't\", 'like', 'it.', 'Some', 'things', 'are', 'self-evidently', 'wrong,', 'ignorant,', 'and', 'hateful.', 'But', 'the', 'vast', 'majority', 'of', 'things', 'about', 'which', 'people', 'disagree', 'are', 'not', 'that', 'way.', 'I', \"don't\", 'see', 'a', 'lot', 'of', 'people', 'on', 'either', 'side', 'who', 'seem', 'interested', 'in', 'increasing', 'the', 'level', 'of', 'nuance', 'to', 'their', 'opinions.', 'I', 'also', 'feel', 'that', 'there', 'is', 'a', 'chilling', 'effect', 'in', 'much', 'of', 'public', 'discourse.', 'I', \"don't\", 'even', 'like', 'the', 'guy,', 'but', 'by', 'merely', 'talking', 'about', 'him', 'and', 'not', 'just', 'bashing', 'him,', 'I', 'feel', 'like', \"I'm\", 'likely', 'to', 'get', 'downvotes.', 'I', \"don't\", 'care', 'about', 'internet', 'points,', 'but', 'I', 'do', 'think', \"there's\", 'pressure', 'to', 'agree', 'or', 'be', 'silent.', 'That', 'would', 'just', 'be', 'one', 'exceedingly', 'silly', 'example.', 'The', 'left', 'does', 'it', 'to', 'the', 'right.', 'The', 'right', 'does', 'it', 'to', 'the', 'left.', \"That's\", 'probably', 'always', 'been', 'so.', 'But', 'I', 'feel', 'increasingly', 'the', 'far', 'left', 'does', 'it', 'to', 'people', 'who', 'basically', 'share', 'their', 'goals.\\n\\nI', 'studied', 'a', 'fair', 'bit', 'of', 'postmodern', 'philosophy', 'in', 'school,', 'which', 'I', 'admit', 'was', 'several', 'years', 'ago', 'now.', 'I', 'kind', 'of', 'agree', 'with', 'his', 'characterization', 'of', 'its', 'issues', 'and', 'flaws', '(basically', 'that', 'its', 'claims', 'are', 'true', 'only', 'in', 'the', 'abstract', 'and', 'claims', 'that', 'cannot', 'be', 'functionally', 'applied', 'should', 'be', 'treated', 'as', 'false,', 'even', 'if', 'not', 'logically', 'falsifiable-', 'like,', 'for', 'instance,', 'solipsism).', 'I', 'do', 'not', 'share', 'his', 'opinion', 'that', 'the', 'whole', 'thing', 'has', 'an', 'underlying', 'ulterior', 'motive', 'of', 'destroying', 'Western', 'civilization.', 'I', 'would', 'be', 'interested', 'specifically', 'in', 'your', 'take', 'on', 'where', 'he', 'strays', 'regarding', 'postmodern', 'philosophy,', 'because', \"it's\", 'been', 'a', 'while', 'for', 'me.\\n\\nI', 'think', 'certain', 'academic', 'disciplines', 'are', 'rather', 'corrupt.', \"I'm\", 'open', 'to', 'discussion', 'about', 'a', 'lot', 'of', 'things,', 'but', \"it's\", 'wild', 'to', 'me', 'that', 'a', 'Yale', 'professor', 'more', 'or', 'less', 'got', 'run', 'out', 'of', 'town', 'for', 'questioning', 'whether', 'cultural', 'appropriation', 'is', 'for', 'real.', 'Cultural', 'appropriation,', 'microaggressions,', 'patriarchy,', 'white', 'privilege,', 'rape', 'culture...', 'I', 'think', \"there's\", 'real,', 'honest', 'to', 'God', 'truth', 'in', 'this', 'stuff,', 'but', 'when', 'I', 'really', 'think', 'about', 'the', 'world', 'I', 'live', 'in,', 'I', 'have', 'trouble', 'swallowing', 'some', 'of', 'it', 'whole', 'hog', 'without', 'adding', 'at', 'least', 'a', 'pinch', 'of', 'nuance.', 'And', 'if', 'a', 'professor', 'at', 'an', 'institution', 'can', 'lose', 'their', 'job', 'for', 'trying', 'to', 'do', 'so,', 'how', 'can', 'anyone', 'else?', \"Shouldn't\", 'we', 'want', 'to', 'know', 'the', 'truth', 'about', 'those', 'things?', \"Doesn't\", 'that', 'require', 'critical', 'reflection?\\n\\nAnd,', 'for', 'the', 'Christianity', 'piece,', 'I', 'think', 'he', 'makes', 'an', 'excellent', 'observation', '(or', 'takes', 'one,', 'anyway,', 'from', 'Dostoevsky', 'and', 'Neitzsche).', 'The', 'erosion', 'of', 'that', 'as', 'a', 'unifying', 'force', 'has', 'increased', 'the', 'chaos', 'in', 'societies.', 'But', 'I', \"don't\", 'share', 'his', 'pessimism', 'or', 'resentment', 'about', 'it.', 'Our', 'society', 'is', 'more', 'plural.', 'I', 'think', 'the', 'Christian', 'tradition', 'gave', 'societies', 'a', 'toolkit.', 'I', 'believe', 'that', 'any', 'tools', 'from', 'that', 'kit', 'that', 'still', 'have', 'value', 'will', 'be', 'retained,', 'and', 'to', 'hell', 'with', 'the', 'rest.', 'But', 'I', \"don't\", 'disagree', 'with', 'his', 'notion', '(and', 'this', 'was', 'more', 'or', 'less', \"Neitzsche's\", 'take)', 'that', 'you', 'can', 'toss', 'out', 'bad', 'tools', 'from', 'Christianity,', 'and', 'perhaps', 'we', 'must,', 'but', 'that', \"doesn't\", 'mean', 'you', 'immediately', 'have', 'replacements', 'that', 'work', 'better.', 'I', 'believe', \"we'll\", 'find', 'them,', 'but', 'I', 'do', 'think', 'the', 'twentieth', 'century', 'had', \"it's\", 'share', 'of', 'nihilism,', 'and', 'I', 'do', 'think', 'that', 'even', 'today', 'people', 'struggle', 'to', 'find', 'purpose', 'and', 'it', 'can', 'harm', 'them.', 'I', \"don't\", 'mourn', 'Christianity,', 'and', 'I', 'assume', \"we'll\", 'figure', 'that', 'out,', 'but', \"it's\", 'fair', 'to', 'say', 'that,', 'absent', 'an', 'externally', 'imposed', 'value', 'system,', 'people', 'struggle', 'to', 'create', 'an', 'internal', 'one.', \"That's\", 'a', 'real', 'phenomenon', 'in', 'my', 'opinion.\\n\\n\\n\\n\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'enough\\n\\ni', 'see', 'bit', 'act', 'subst', 'claim', 'clearly\\n\\ni', 'agr', 'hat', 'crim', 'law', 'doesnt', 'seem', 'lik', 'slippery', 'slop', 'anyth', 'agr', 'proposit', 'cannot', 'cod', 'law', 'prefer', 'pronoun', 'us', 'would', 'nev', 'int', 'misgend', 'someon', 'would', 'best', 'us', 'whatev', 'pronoun', 'person', 'pref', 'want', 'liv', 'socy', 'crim', 'get', 'wrong', 'think', 'reason', 'room', 'discuss', 'compass', 'funct', 'meet', 'someth', 'lik', 'that\\n\\ni', 'dont', 'agr', 'stant', 'nuclear', 'famy', 'assocy', 'crit', 'model', 'think', 'nuclear', 'famy', 'model', 'work', 'many', 'peopl', 'know', 'point', 'fact', 'work', 'many', 'oth', 'think', 'peopl', 'fre', 'see', 'fit', 'know', 'evid', 'suggest', 'gay', 'adopt', 'way', 'infery', 'straight', 'adopt', 'wasnt', 'aw', 'mad', 'stat', 'affect', 'doesnt', 'tot', 'surpr', 'did\\n\\ni', 'agr', 'enco', 'peopl', 'embrac', 'gend', 'sex', 'fluid', 'freedom', 'abl', 'tru', 'on', 'real', 'good', '\\n\\nbut', 'agr', 'antiscy', 'antidiscours', 'bent', 'pres', 'polit', 'clim', 'left', 'im', 'squ', 'left', 'am', 'polit', 'spectr', 'sens', 'tend', 'dismiss', 'anyth', 'wheth', 'ide', 'study', 'person', 'conflict', 'progress', 'view', 'selfevid', 'wrong', 'ign', 'hat', 'ev', 'deserv', 'consid', 'sens', 'ev', 'sometim', 'dont', 'lik', 'thing', 'selfevid', 'wrong', 'ign', 'hat', 'vast', 'maj', 'thing', 'peopl', 'disagr', 'way', 'dont', 'see', 'lot', 'peopl', 'eith', 'sid', 'seem', 'interest', 'increas', 'level', 'nuant', 'opin', 'also', 'feel', 'chil', 'effect', 'much', 'publ', 'discours', 'dont', 'ev', 'lik', 'guy', 'mer', 'talk', 'bash', 'feel', 'lik', 'im', 'lik', 'get', 'downvot', 'dont', 'car', 'internet', 'point', 'think', 'ther', 'press', 'agr', 'sil', 'would', 'on', 'excess', 'sil', 'exampl', 'left', 'right', 'right', 'left', 'that', 'prob', 'alway', 'feel', 'increas', 'far', 'left', 'peopl', 'bas', 'shar', 'goals\\n\\ni', 'study', 'fair', 'bit', 'postmodern', 'philosoph', 'school', 'admit', 'sev', 'year', 'ago', 'kind', 'agr', 'charact', 'issu', 'flaw', 'bas', 'claim', 'tru', 'abstract', 'claim', 'cannot', 'funct', 'apply', 'tre', 'fals', 'ev', 'log', 'fals', 'lik', 'inst', 'solips', 'shar', 'opin', 'whol', 'thing', 'und', 'ultery', 'mot', 'destroy', 'western', 'civil', 'would', 'interest', 'spec', 'tak', 'strays', 'regard', 'postmodern', 'philosoph', 'me\\n\\ni', 'think', 'certain', 'academ', 'disciplin', 'rath', 'corrupt', 'im', 'op', 'discuss', 'lot', 'thing', 'wild', 'yal', 'profess', 'less', 'got', 'run', 'town', 'quest', 'wheth', 'cult', 'appropry', 'real', 'cult', 'appropry', 'microaggress', 'patriarchy', 'whit', 'privileg', 'rap', 'cult', 'think', 'ther', 'real', 'honest', 'god', 'tru', 'stuff', 'real', 'think', 'world', 'liv', 'troubl', 'swallow', 'whol', 'hog', 'without', 'ad', 'least', 'pinch', 'nuant', 'profess', 'institut', 'los', 'job', 'try', 'anyon', 'els', 'shouldnt', 'want', 'know', 'tru', 'thing', 'doesnt', 'requir', 'crit', 'reflection\\n\\nand', 'christianity', 'piec', 'think', 'mak', 'excel', 'observ', 'tak', 'on', 'anyway', 'dostoevsky', 'neitzsch', 'erod', 'un', 'forc', 'increas', 'chao', 'socy', 'dont', 'shar', 'pessim', 'res', 'socy', 'plur', 'think', 'christian', 'tradit', 'gav', 'socy', 'toolkit', 'believ', 'tool', 'kit', 'stil', 'valu', 'retain', 'hel', 'rest', 'dont', 'disagr', 'not', 'less', 'neitzsch', 'tak', 'toss', 'bad', 'tool', 'christianity', 'perhap', 'must', 'doesnt', 'mean', 'immedy', 'replac', 'work', 'bet', 'believ', 'wel', 'find', 'think', 'twentie', 'century', 'shar', 'nihil', 'think', 'ev', 'today', 'peopl', 'struggle', 'find', 'purpos', 'harm', 'dont', 'mourn', 'christianity', 'assum', 'wel', 'fig', 'fair', 'say', 'abs', 'extern', 'impos', 'valu', 'system', 'peopl', 'struggle', 'cre', 'intern', 'on', 'that', 'real', 'phenomenon', 'opinion\\n\\n\\n\\n\\n'], ['fair', 'enough\\n\\ni', 'see', 'bite', 'actual', 'substance', 'claim', 'clearly\\n\\ni', 'agree', 'hate', 'crime', 'law', 'doesnt', 'seem', 'like', 'slippery', 'slope', 'anything', 'agree', 'proposition', 'cannot', 'codify', 'law', 'prefer', 'pronoun', 'usage', 'would', 'never', 'intentionally', 'misgender', 'someone', 'would', 'best', 'use', 'whatever', 'pronoun', 'person', 'prefer', 'want', 'live', 'society', 'crime', 'get', 'wrong', 'think', 'reasonable', 'room', 'discussion', 'compassion', 'function', 'meet', 'something', 'like', 'that\\n\\ni', 'dont', 'agree', 'stance', 'nuclear', 'family', 'association', 'criticism', 'model', 'think', 'nuclear', 'family', 'model', 'work', 'many', 'people', 'know', 'point', 'fact', 'work', 'many', 'others', 'think', 'people', 'free', 'see', 'fit', 'know', 'evidence', 'suggest', 'gay', 'adoption', 'way', 'inferior', 'straight', 'adoption', 'wasnt', 'aware', 'make', 'statements', 'affect', 'doesnt', 'totally', 'surprise', 'did\\n\\ni', 'agree', 'encourage', 'people', 'embrace', 'gender', 'sexuality', 'fluid', 'freedom', 'ability', 'true', 'ones', 'reality', 'good', '\\n\\nbut', 'agree', 'antiscience', 'antidiscourse', 'bend', 'present', 'political', 'climate', 'leave', 'im', 'squarely', 'leave', 'american', 'political', 'spectrum', 'sense', 'tendency', 'dismiss', 'anything', 'whether', 'idea', 'study', 'person', 'conflict', 'progressive', 'view', 'selfevidently', 'wrong', 'ignorant', 'hateful', 'even', 'deserve', 'consideration', 'sense', 'even', 'sometimes', 'dont', 'like', 'things', 'selfevidently', 'wrong', 'ignorant', 'hateful', 'vast', 'majority', 'things', 'people', 'disagree', 'way', 'dont', 'see', 'lot', 'people', 'either', 'side', 'seem', 'interest', 'increase', 'level', 'nuance', 'opinions', 'also', 'feel', 'chill', 'effect', 'much', 'public', 'discourse', 'dont', 'even', 'like', 'guy', 'merely', 'talk', 'bash', 'feel', 'like', 'im', 'likely', 'get', 'downvotes', 'dont', 'care', 'internet', 'point', 'think', 'theres', 'pressure', 'agree', 'silent', 'would', 'one', 'exceedingly', 'silly', 'example', 'leave', 'right', 'right', 'leave', 'thats', 'probably', 'always', 'feel', 'increasingly', 'far', 'leave', 'people', 'basically', 'share', 'goals\\n\\ni', 'study', 'fair', 'bite', 'postmodern', 'philosophy', 'school', 'admit', 'several', 'years', 'ago', 'kind', 'agree', 'characterization', 'issue', 'flaw', 'basically', 'claim', 'true', 'abstract', 'claim', 'cannot', 'functionally', 'apply', 'treat', 'false', 'even', 'logically', 'falsifiable', 'like', 'instance', 'solipsism', 'share', 'opinion', 'whole', 'thing', 'underlie', 'ulterior', 'motive', 'destroy', 'western', 'civilization', 'would', 'interest', 'specifically', 'take', 'stray', 'regard', 'postmodern', 'philosophy', 'me\\n\\ni', 'think', 'certain', 'academic', 'discipline', 'rather', 'corrupt', 'im', 'open', 'discussion', 'lot', 'things', 'wild', 'yale', 'professor', 'less', 'get', 'run', 'town', 'question', 'whether', 'cultural', 'appropriation', 'real', 'cultural', 'appropriation', 'microaggressions', 'patriarchy', 'white', 'privilege', 'rape', 'culture', 'think', 'theres', 'real', 'honest', 'god', 'truth', 'stuff', 'really', 'think', 'world', 'live', 'trouble', 'swallow', 'whole', 'hog', 'without', 'add', 'least', 'pinch', 'nuance', 'professor', 'institution', 'lose', 'job', 'try', 'anyone', 'else', 'shouldnt', 'want', 'know', 'truth', 'things', 'doesnt', 'require', 'critical', 'reflection\\n\\nand', 'christianity', 'piece', 'think', 'make', 'excellent', 'observation', 'take', 'one', 'anyway', 'dostoevsky', 'neitzsche', 'erosion', 'unify', 'force', 'increase', 'chaos', 'societies', 'dont', 'share', 'pessimism', 'resentment', 'society', 'plural', 'think', 'christian', 'tradition', 'give', 'societies', 'toolkit', 'believe', 'tool', 'kit', 'still', 'value', 'retain', 'hell', 'rest', 'dont', 'disagree', 'notion', 'less', 'neitzsches', 'take', 'toss', 'bad', 'tool', 'christianity', 'perhaps', 'must', 'doesnt', 'mean', 'immediately', 'replacements', 'work', 'better', 'believe', 'well', 'find', 'think', 'twentieth', 'century', 'share', 'nihilism', 'think', 'even', 'today', 'people', 'struggle', 'find', 'purpose', 'harm', 'dont', 'mourn', 'christianity', 'assume', 'well', 'figure', 'fair', 'say', 'absent', 'externally', 'impose', 'value', 'system', 'people', 'struggle', 'create', 'internal', 'one', 'thats', 'real', 'phenomenon', 'opinion\\n\\n\\n\\n\\n'])\n",
      "original document: \n",
      "['SD', 'Streams', '-', '[Ole', 'Miss', 'vs', 'Alabama', '*ESPN*](http://sportstreams.co/live-soccer-wiz-stream-13/)', '|', 'Mobile', 'Compatible', ':', 'Yes', '|', 'Ad', 'Overlays:', '4', '|', 'Use', 'Adblock']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sd', 'streams', 'ol', 'miss', 'vs', 'alabam', 'espnhttpsportstreamscolivesoccerwizstream13', 'mobl', 'compat', 'ye', 'ad', 'overlay', 'four', 'us', 'adblock'], ['sd', 'stream', 'ole', 'miss', 'vs', 'alabama', 'espnhttpsportstreamscolivesoccerwizstream13', 'mobile', 'compatible', 'yes', 'ad', 'overlay', 'four', 'use', 'adblock'])\n",
      "original document: \n",
      "['What', 'are', 'your', 'thoughts', 'on', 'the', 'APA', 'changing', 'definitions', 'due', 'to', 'political', 'pressure?\\n\\nhttps://www.google.com/amp/s/www.lifesitenews.com/mobile/news/former-president-of-apa-says-organization-controlled-by-gay-rights-movement\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'ap', 'chang', 'definit', 'due', 'polit', 'pressure\\n\\nhttpswwwgooglecomampswwwlifesitenewscommobilenewsformerpresidentofapasaysorganizationcontrolledbygayrightsmovement\\n\\n'], ['thoughts', 'apa', 'change', 'definitions', 'due', 'political', 'pressure\\n\\nhttpswwwgooglecomampswwwlifesitenewscommobilenewsformerpresidentofapasaysorganizationcontrolledbygayrightsmovement\\n\\n'])\n",
      "original document: \n",
      "['Why', 'yes', 'it', 'is', '😂😂']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Hey,', 'whatever', 'works', 'for', 'you', '-', 'just', 'call', 'and', 'let', 'me', 'know', 'when', 'EV', 'cars', 'can', 'be', 'fully', 'charged', 'in', '5', 'minutes', 'and', 'hold', 'enough', 'charge', 'for', '300+', 'miles,', 'while', 'costing', 'under', '$30k\\n\\nOh,', 'and', 'have', 'a', 'way', 'to', 'be', 'charged', 'while', 'I', 'live', 'in', 'an', 'apartment.', \"It's\", 'not', 'practical', 'for', 'everyone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'whatev', 'work', 'cal', 'let', 'know', 'ev', 'car', 'ful', 'charg', 'fiv', 'minut', 'hold', 'enough', 'charg', 'three hundred', 'mil', 'cost', '30k\\n\\noh', 'way', 'charg', 'liv', 'apart', 'pract', 'everyon'], ['hey', 'whatever', 'work', 'call', 'let', 'know', 'ev', 'cars', 'fully', 'charge', 'five', 'minutes', 'hold', 'enough', 'charge', 'three hundred', 'miles', 'cost', '30k\\n\\noh', 'way', 'charge', 'live', 'apartment', 'practical', 'everyone'])\n",
      "original document: \n",
      "['LMAO']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lmao'], ['lmao'])\n",
      "original document: \n",
      "['Mycket', 'bra:)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mycket', 'bra'], ['mycket', 'bra'])\n",
      "original document: \n",
      "['Yup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup'], ['yup'])\n",
      "original document: \n",
      "['[+261TurnerLane](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnom6d3/):\\n\\nThis', 'is', 'self-publishing.', 'You', 'literally', 'could', 'self-publish', 'a', 'book', 'on', 'Amazon', 'in', 'ten', 'minutes.', 'And', 'then', 'there', 'are', 'services', 'which', 'you', 'pay', 'to', 'publish', 'your', 'book,', 'like', 'the', 'one', 'OP', 'used.', 'This', 'form', 'of', 'publishing', 'is', 'as', 'open', 'and', 'easy', 'as', 'it', 'gets.', 'If', 'your', 'GF', 'wants', 'to', 'hold', 'her', 'own', 'book', 'in', 'her', 'hand,', 'she', 'could', 'do', 'it,', 'for', 'a', 'price.', 'Traditional', 'publishing', 'is', 'long', 'and', 'hard', 'and', 'discouraging.', 'When', 'I', 'got', 'my', 'first', 'short', 'story', 'published', 'in', 'an', 'anthology', 'you', 'could', 'go', 'to', 'the', 'store', 'and', 'buy,', 'I', 'cried.', 'It', 'was', 'the', 'end', 'result', 'of', 'a', 'long', 'and', 'hard', 'road.', \"I've\", 'recently', 'finished', 'my', 'second', 'full-length', 'novel', '(the', 'first', 'one', \"I'm\", 'trying', 'to', 'sell)', 'and', \"it's\", 'been', 'two', 'months', 'of', 'mostly', 'rejections.', 'But', \"that's\", 'the', 'game.', 'You', 'have', 'to', 'find', 'that', 'right', 'agent.', 'And', 'then', 'they', 'have', 'to', 'find', 'that', 'right', 'publisher,', 'and', 'so', 'on,', 'and', 'three', 'years', 'from', 'now', 'I', 'might', 'be', 'able', 'to', 'walk', 'into', 'a', 'store', 'and', 'buy', 'my', 'book.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['261turnerlanehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnom6d3\\n\\nthis', 'selfpubl', 'lit', 'could', 'selfpubl', 'book', 'amazon', 'ten', 'minut', 'serv', 'pay', 'publ', 'book', 'lik', 'on', 'op', 'us', 'form', 'publ', 'op', 'easy', 'get', 'gf', 'want', 'hold', 'book', 'hand', 'could', 'pric', 'tradit', 'publ', 'long', 'hard', 'disco', 'got', 'first', 'short', 'story', 'publ', 'antholog', 'could', 'go', 'stor', 'buy', 'cri', 'end', 'result', 'long', 'hard', 'road', 'iv', 'rec', 'fin', 'second', 'fullleng', 'novel', 'first', 'on', 'im', 'try', 'sel', 'two', 'month', 'most', 'reject', 'that', 'gam', 'find', 'right', 'ag', 'find', 'right', 'publ', 'three', 'year', 'might', 'abl', 'walk', 'stor', 'buy', 'book'], ['261turnerlanehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnom6d3\\n\\nthis', 'selfpublishing', 'literally', 'could', 'selfpublish', 'book', 'amazon', 'ten', 'minutes', 'service', 'pay', 'publish', 'book', 'like', 'one', 'op', 'use', 'form', 'publish', 'open', 'easy', 'get', 'gf', 'want', 'hold', 'book', 'hand', 'could', 'price', 'traditional', 'publish', 'long', 'hard', 'discourage', 'get', 'first', 'short', 'story', 'publish', 'anthology', 'could', 'go', 'store', 'buy', 'cry', 'end', 'result', 'long', 'hard', 'road', 'ive', 'recently', 'finish', 'second', 'fulllength', 'novel', 'first', 'one', 'im', 'try', 'sell', 'two', 'months', 'mostly', 'rejections', 'thats', 'game', 'find', 'right', 'agent', 'find', 'right', 'publisher', 'three', 'years', 'might', 'able', 'walk', 'store', 'buy', 'book'])\n",
      "original document: \n",
      "['Just', 'let', 'one', 'of', 'the', 'teams', 'be', 'the', 'Cowboys', 'please.', 'Patriots', 'would', 'be', 'ok', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'on', 'team', 'cowboy', 'pleas', 'patriot', 'would', 'ok'], ['let', 'one', 'team', 'cowboys', 'please', 'patriots', 'would', 'ok'])\n",
      "original document: \n",
      "['You', 'owe', 'us', 'one', 'or', 'something.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ow', 'us', 'on', 'someth'], ['owe', 'us', 'one', 'something'])\n",
      "original document: \n",
      "['Is', 'this', 'that', 'surprising?', 'His', 'buyout', 'is', 'doubled', 'as', 'long', 'as', 'Jurich', 'is', 'employed', 'due', 'to', 'some', 'strange', 'contract', 'language.\\n\\nHe', 'probably', \"should've\", 'just', 'gone', 'with', 'no', 'comment.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['surpr', 'buyout', 'doubl', 'long', 'jurich', 'employ', 'due', 'strange', 'contract', 'language\\n\\nh', 'prob', 'shouldv', 'gon', 'com'], ['surprise', 'buyout', 'double', 'long', 'jurich', 'employ', 'due', 'strange', 'contract', 'language\\n\\nhe', 'probably', 'shouldve', 'go', 'comment'])\n",
      "original document: \n",
      "['Fairy', 'Bottle']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fairy', 'bottl'], ['fairy', 'bottle'])\n",
      "original document: \n",
      "['SSD', 'would', 'help', 'with', 'the', 'buildings', 'and', 'such', 'loading', 'slowly', 'when', 'you', 'enter', 'a', 'game.', 'Start', 'with', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ssd', 'would', 'help', 'build', 'load', 'slow', 'ent', 'gam', 'start'], ['ssd', 'would', 'help', 'build', 'load', 'slowly', 'enter', 'game', 'start'])\n",
      "original document: \n",
      "['https://www.reddit.com/r/RocketLeague/comments/73heob/update_on_crashes_from_dirkened_still_crashing/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwredditcomrrocketleaguecomments73heobupdate_on_crashes_from_dirkened_still_crashing'], ['httpswwwredditcomrrocketleaguecomments73heobupdate_on_crashes_from_dirkened_still_crashing'])\n",
      "original document: \n",
      "['\\nIn', 'a', 'private', 'school,', 'sure.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\nin', 'priv', 'school', 'sur'], ['\\nin', 'private', 'school', 'sure'])\n",
      "original document: \n",
      "['Eggcellent']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eggcel'], ['eggcellent'])\n",
      "original document: \n",
      "[\"I'd\", 'say', 'natural', 'boobs', 'and', 'a', 'more', 'matching', 'build', '(now', \"it's\", 'a', 'long', 'slim', 'top', 'with', 'a', 'more', 'filled', 'out', 'bottom)', \"would've\", 'made', 'it', 'better.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'say', 'nat', 'boob', 'match', 'build', 'long', 'slim', 'top', 'fil', 'bottom', 'wouldv', 'mad', 'bet'], ['id', 'say', 'natural', 'boob', 'match', 'build', 'long', 'slim', 'top', 'fill', 'bottom', 'wouldve', 'make', 'better'])\n",
      "original document: \n",
      "['Yes', 'hi,', \"I'll\", 'take', '$20', 'of', 'the', 'OG', 'Kush', 'and', '$15', 'of', 'the', 'Super', 'Blue', 'Dream,', 'thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'hi', 'il', 'tak', 'twenty', 'og', 'kush', 'fifteen', 'sup', 'blu', 'dream', 'thank'], ['yes', 'hi', 'ill', 'take', 'twenty', 'og', 'kush', 'fifteen', 'super', 'blue', 'dream', 'thank'])\n",
      "original document: \n",
      "['No,', 'facts', \"don't\", 'change', 'even', 'if', 'idiots', 'keep', 'posting', 'the', 'same', 'dumb', 'shit', 'so', 'I', 'do', 'not', 'have', 'any', 'reason', 'to', 'alter', 'the', 'replies.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fact', 'dont', 'chang', 'ev', 'idiot', 'keep', 'post', 'dumb', 'shit', 'reason', 'alt', 'reply'], ['facts', 'dont', 'change', 'even', 'idiots', 'keep', 'post', 'dumb', 'shit', 'reason', 'alter', 'reply'])\n",
      "original document: \n",
      "['Yeah', \"I've\", 'literally', 'never', 'purchased', 'a', 'Sivir', 'skin,', 'but', 'I', 'have', '3', 'Sivir', 'skins', '(4', 'w/', 'the', 'victorious', 'skin).', 'All', 'from', 'mystery', 'gifts.\\n\\nAnd', \"it's\", 'not', 'like', 'I', 'have', 'gotten', 'a', 'ton', 'of', 'mystery', 'gifts.', 'Maybe', '10', 'gifts', 'since', 'I', 'started', 'playing', 'this', 'game.', 'Riot', 'just', 'loves', 'giving', 'me', 'Sivir', 'skins', 'I', 'guess.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'iv', 'lit', 'nev', 'purchas', 'sivir', 'skin', 'three', 'sivir', 'skin', 'four', 'w', 'vict', 'skin', 'mystery', 'gifts\\n\\nand', 'lik', 'got', 'ton', 'mystery', 'gift', 'mayb', 'ten', 'gift', 'sint', 'start', 'play', 'gam', 'riot', 'lov', 'giv', 'sivir', 'skin', 'guess'], ['yeah', 'ive', 'literally', 'never', 'purchase', 'sivir', 'skin', 'three', 'sivir', 'skin', 'four', 'w', 'victorious', 'skin', 'mystery', 'gifts\\n\\nand', 'like', 'get', 'ton', 'mystery', 'gift', 'maybe', 'ten', 'gift', 'since', 'start', 'play', 'game', 'riot', 'love', 'give', 'sivir', 'skin', 'guess'])\n",
      "original document: \n",
      "['Then', 'what', 'would', 'you', 'lie', 'about,', 'OP?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lie', 'op'], ['would', 'lie', 'op'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['If', 'I', 'hold', 'my', 'hands', 'by', 'my', 'side,', 'the', 'sleeves', 'end', 'at', 'my', 'knuckles,', 'or', 'right', 'where', 'the', 'bottom', 'of', 'the', 'white', 'strip', 'is', 'on', 'the', 'hip', 'area.', 'I', \"don't\", 'really', 'have', 'the', 'means', 'to', 'take', 'a', 'pic', 'of', 'myself', 'in', 'the', 'jersey,', 'sorry.', 'I', 'usually', 'like', 'a', 'medium', 'jersey', 'so', 'the', '50', 'is', 'perfect', 'for', 'me.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hold', 'hand', 'sid', 'sleev', 'end', 'knuckl', 'right', 'bottom', 'whit', 'strip', 'hip', 'are', 'dont', 'real', 'mean', 'tak', 'pic', 'jersey', 'sorry', 'us', 'lik', 'med', 'jersey', 'fifty', 'perfect'], ['hold', 'hand', 'side', 'sleeves', 'end', 'knuckle', 'right', 'bottom', 'white', 'strip', 'hip', 'area', 'dont', 'really', 'mean', 'take', 'pic', 'jersey', 'sorry', 'usually', 'like', 'medium', 'jersey', 'fifty', 'perfect'])\n",
      "original document: \n",
      "['I', 'should', 'then', 'be', 'able', 'to', 'have', 'the', 'option', 'to', 'toggle', 'all', 'of', 'those', 'features', 'when', 'I', 'choose', 'to', 'do', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['abl', 'opt', 'toggl', 'feat', 'choos'], ['able', 'option', 'toggle', 'feature', 'choose'])\n",
      "original document: \n",
      "['i', 'thought', 'people', 'hated', 'zariss?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'peopl', 'hat', 'zariss'], ['think', 'people', 'hat', 'zariss'])\n",
      "original document: \n",
      "['It', 'looks', 'like', 'its', 'going', 'to', 'be', 'pretty', 'weird']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'going', 'pretty', 'weird'], ['look', 'like', 'go', 'pretty', 'weird'])\n",
      "original document: \n",
      "['Empty', 'seats,', 'many', 'empty', 'seats...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['empty', 'seat', 'many', 'empty', 'seat'], ['empty', 'seat', 'many', 'empty', 'seat'])\n",
      "original document: \n",
      "['Maybe.', 'I', \"don't\", 'know.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'dont', 'know'], ['maybe', 'dont', 'know'])\n",
      "original document: \n",
      "['I', \"don't\", 'know', 'about', 'his', 'elbow,', 'but', 'his', 'heel', 'could', 'use', 'some', 'lotion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'elbow', 'heel', 'could', 'us', 'lot'], ['dont', 'know', 'elbow', 'heel', 'could', 'use', 'lotion'])\n",
      "original document: \n",
      "['yes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['I', 'generally', 'recommend', 'starting', 'with', '1', 'core,', 'and', 'if', 'you', 'like', 'the', 'game', 'picking', 'up', 'a', 'second', 'core', 'along', 'with', 'the', 'Dunwich', 'or', 'Carcossa', 'starter.', 'Having', 'two', 'cores', 'is', 'very', 'helpful', 'in', 'making', 'sure', 'everyone', 'has', 'enough', 'copies', 'of', 'stuff', 'for', 'consistency,', 'especially', 'if', \"you're\", 'playing', 'investigators', 'with', 'any', 'otherlap', 'in', 'available', 'class', 'cards.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gen', 'recommend', 'start', 'on', 'cor', 'lik', 'gam', 'pick', 'second', 'cor', 'along', 'dunwich', 'carcoss', 'start', 'two', 'cor', 'help', 'mak', 'sur', 'everyon', 'enough', 'cop', 'stuff', 'consist', 'espec', 'yo', 'play', 'investig', 'otherlap', 'avail', 'class', 'card'], ['generally', 'recommend', 'start', 'one', 'core', 'like', 'game', 'pick', 'second', 'core', 'along', 'dunwich', 'carcossa', 'starter', 'two', 'core', 'helpful', 'make', 'sure', 'everyone', 'enough', 'copy', 'stuff', 'consistency', 'especially', 'youre', 'play', 'investigators', 'otherlap', 'available', 'class', 'card'])\n",
      "original document: \n",
      "[\"You've\", 'been', 'with', 'him', '3', 'months', 'and', 'it', 'sounds', 'like', \"you've\", 'found', 'a', 'lot', 'of', 'areas', 'where', 'the', 'two', 'of', 'you', \"aren't\", 'compatible.', '', 'It', \"isn't\", 'that', 'either', 'of', 'you', 'did', 'anything', 'wrong,', 'you', 'just', \"aren't\", 'right', 'for', 'one', 'another.', '', 'If', 'you', 'break', 'up', 'it', 'means,', 'for', 'example,', 'he', 'can', 'find', 'a', 'woman', 'who', 'is', 'into', 'his', 'martial', 'arts', 'and', 'you', 'can', 'find', 'a', 'guy', 'who', 'is', 'with', 'you', 'on', 'supporting', 'animals.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youv', 'three', 'month', 'sound', 'lik', 'youv', 'found', 'lot', 'area', 'two', 'ar', 'compat', 'isnt', 'eith', 'anyth', 'wrong', 'ar', 'right', 'on', 'anoth', 'break', 'mean', 'exampl', 'find', 'wom', 'mart', 'art', 'find', 'guy', 'support', 'anim'], ['youve', 'three', 'months', 'sound', 'like', 'youve', 'find', 'lot', 'areas', 'two', 'arent', 'compatible', 'isnt', 'either', 'anything', 'wrong', 'arent', 'right', 'one', 'another', 'break', 'mean', 'example', 'find', 'woman', 'martial', 'arts', 'find', 'guy', 'support', 'animals'])\n",
      "original document: \n",
      "['Lol,', \"there's\", 'this', 'kid', 'named', 'Jake', 'in', 'my', 'class', 'that', 'nobody', 'liked', 'so', 'whenever', 'he', 'would', 'do', 'something', 'I', 'would', 'picture', 'that', 'scene', 'in', 'my', 'head']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'ther', 'kid', 'nam', 'jak', 'class', 'nobody', 'lik', 'whenev', 'would', 'someth', 'would', 'pict', 'scen', 'head'], ['lol', 'theres', 'kid', 'name', 'jake', 'class', 'nobody', 'like', 'whenever', 'would', 'something', 'would', 'picture', 'scene', 'head'])\n",
      "original document: \n",
      "['Totally', 'the', 'pyro', 'of', 'an', 'underdog.', ';p']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tot', 'pyro', 'underdog', 'p'], ['totally', 'pyro', 'underdog', 'p'])\n",
      "original document: \n",
      "[\"I'm\", 'not', '100%', 'sure', 'what', 'you', 'mean,', 'but', 'I', 'see', 'that', 'the', 'DMZ', 'is', 'disabled', 'in', 'my', 'router', 'settings.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'one hundred', 'sur', 'mean', 'see', 'dmz', 'dis', 'rout', 'set'], ['im', 'one hundred', 'sure', 'mean', 'see', 'dmz', 'disable', 'router', 'settings'])\n",
      "original document: \n",
      "[\"Eno's\", 'producing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eno', 'produc'], ['enos', 'produce'])\n",
      "original document: \n",
      "['143412979|', '&gt;', 'Canada', 'Anonymous', '(ID:', 'JohaoXv9)\\n\\n&gt;tfw', 'no', 'matter', 'who', 'i', 'vote', 'for,', 'Canada', 'will', 'always', 'go', 'for', 'Libshit\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, nine hundred and seventy-nin', 'gt', 'canad', 'anonym', 'id', 'johaoxv9\\n\\ngttfw', 'mat', 'vot', 'canad', 'alway', 'go', 'libshit\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, nine hundred and seventy-nine', 'gt', 'canada', 'anonymous', 'id', 'johaoxv9\\n\\ngttfw', 'matter', 'vote', 'canada', 'always', 'go', 'libshit\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Ah,', 'yes,', 'that’s', 'reasonably', 'assholish', 'then.', 'That', 'part', 'wasn’t', 'clear', 'from', 'the', 'post.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'ye', 'that', 'reason', 'asshol', 'part', 'wasnt', 'clear', 'post'], ['ah', 'yes', 'thats', 'reasonably', 'assholish', 'part', 'wasnt', 'clear', 'post'])\n",
      "original document: \n",
      "['How', 'many', 'sex', 'workers', 'do', 'you', 'think', 'are', 'on', 'askreddit', 'right', 'now']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'sex', 'work', 'think', 'askreddit', 'right'], ['many', 'sex', 'workers', 'think', 'askreddit', 'right'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnomsbx/):\\n\\nI', \"didn't\", 'pay', 'anything', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnomsbx\\n\\ni', 'didnt', 'pay', 'anyth'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnomsbx\\n\\ni', 'didnt', 'pay', 'anything'])\n",
      "original document: \n",
      "['The', 'Germans', 'had', 'the', 'Schlieffen', 'Plan,', '(their', 'battle', 'plan', 'used', 'in', 'August', '1914)', 'ready', 'in', '1905.', 'If', 'there', 'were', 'no', 'provocation,', 'they', 'would', 'have', 'created', 'one', 'in', 'order', 'to', 'start', 'the', 'war.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['germ', 'schlieffen', 'plan', 'battl', 'plan', 'us', 'august', 'one thousand, nine hundred and fourteen', 'ready', 'one thousand, nine hundred and fiv', 'provoc', 'would', 'cre', 'on', 'ord', 'start', 'war'], ['germans', 'schlieffen', 'plan', 'battle', 'plan', 'use', 'august', 'one thousand, nine hundred and fourteen', 'ready', 'one thousand, nine hundred and five', 'provocation', 'would', 'create', 'one', 'order', 'start', 'war'])\n",
      "original document: \n",
      "['Hehe😙']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heh'], ['hehe'])\n",
      "original document: \n",
      "['It’s', 'a', 'melee', 'llama', 'hooray', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mel', 'llam', 'hooray'], ['melee', 'llama', 'hooray'])\n",
      "original document: \n",
      "['Markmcfarlane', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['markmcfarl'], ['markmcfarlane'])\n",
      "original document: \n",
      "['Yeah', '1', 'pro', 'player', 'and', 'not', 'even', 'a', 'big', 'one', 'at', 'that.', 'Lets', 'wait', 'and', 'see', 'what', 'the', 'real', 'competitive', 'pros', 'think.', 'Sneaky', 'was', 'getting', 'pissed', 'at', 'his', 'teammates', 'for', 'not', 'playing', 'soloQ', 'enough', 'at', 'bootcamp', 'this', 'year.', 'TSM', 'has', 'said', 'they', 'have', 'put', 'in', 'more', 'hours', 'than', 'ever', 'before.', 'Games', 'on', 'stage', 'are', 'their', 'best', 'practice,', 'do', 'you', 'think', 'they', 'want', 'that', 'practice', 'time', 'to', 'be', 'cut', 'by', 'more', 'than', 'half?', 'These', 'guys', 'want', 'to', 'win', 'worlds,', 'they', 'want', 'to', 'be', 'the', 'best!', 'To', 'be', 'the', 'best', 'you', 'have', 'to', 'beat', 'the', 'best,', 'and', 'to', 'beat', 'the', 'best', 'you', 'have', 'to', 'practice', 'harder', 'than', 'the', 'best.', 'Anyone', 'who', \"doesn't\", 'want', 'to', 'play', 'more', 'is', 'lazy', 'and', 'just', 'in', 'it', 'for', 'the', 'money,', 'they', \"don't\", 'have', 'what', 'it', 'takes', 'to', 'be', 'the', 'best.', 'I', 'dare', 'someone', 'to', 'ask', 'Faker', 'if', 'he', 'thinks', 'going', 'to', 'Bo1', 'format', 'is', 'a', 'good', 'idea', 'for', 'a', 'LCS', 'regular', 'season.', 'He', 'would', 'say', '\"no\"', 'and', 'then', 'go', 'back', 'to', 'playing', 'soloq.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'on', 'pro', 'play', 'ev', 'big', 'on', 'let', 'wait', 'see', 'real', 'competit', 'pro', 'think', 'sneaky', 'get', 'piss', 'team', 'play', 'soloq', 'enough', 'bootcamp', 'year', 'tsm', 'said', 'put', 'hour', 'ev', 'gam', 'stag', 'best', 'pract', 'think', 'want', 'pract', 'tim', 'cut', 'half', 'guy', 'want', 'win', 'world', 'want', 'best', 'best', 'beat', 'best', 'beat', 'best', 'pract', 'hard', 'best', 'anyon', 'doesnt', 'want', 'play', 'lazy', 'money', 'dont', 'tak', 'best', 'dar', 'someon', 'ask', 'fak', 'think', 'going', 'bo1', 'form', 'good', 'ide', 'lcs', 'regul', 'season', 'would', 'say', 'go', 'back', 'play', 'soloq'], ['yeah', 'one', 'pro', 'player', 'even', 'big', 'one', 'let', 'wait', 'see', 'real', 'competitive', 'pros', 'think', 'sneaky', 'get', 'piss', 'teammates', 'play', 'soloq', 'enough', 'bootcamp', 'year', 'tsm', 'say', 'put', 'hours', 'ever', 'game', 'stage', 'best', 'practice', 'think', 'want', 'practice', 'time', 'cut', 'half', 'guy', 'want', 'win', 'worlds', 'want', 'best', 'best', 'beat', 'best', 'beat', 'best', 'practice', 'harder', 'best', 'anyone', 'doesnt', 'want', 'play', 'lazy', 'money', 'dont', 'take', 'best', 'dare', 'someone', 'ask', 'faker', 'think', 'go', 'bo1', 'format', 'good', 'idea', 'lcs', 'regular', 'season', 'would', 'say', 'go', 'back', 'play', 'soloq'])\n",
      "original document: \n",
      "['You', 'play', 'resolutions', 'lower', 'than', '480p', 'on', 'a', '30khz', 'PC', 'CRT', 'by', 'messing', 'with', 'vertical', 'refresh', 'rates.', '', \"Here's\", 'a', 'guide', 'https://www.reddit.com/r/emulation/comments/675wk4/guide_how_to_run_retroarch_at_240p_on_your_vga/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'resolv', 'low', '480p', '30khz', 'pc', 'crt', 'mess', 'vert', 'refresh', 'rat', 'her', 'guid', 'httpswwwredditcomremulationcomments675wk4guide_how_to_run_retroarch_at_240p_on_your_vga'], ['play', 'resolutions', 'lower', '480p', '30khz', 'pc', 'crt', 'mess', 'vertical', 'refresh', 'rat', 'heres', 'guide', 'httpswwwredditcomremulationcomments675wk4guide_how_to_run_retroarch_at_240p_on_your_vga'])\n",
      "original document: \n",
      "['Different', 'in', 'the', 'first,', 'second,', 'and', 'fifth', 'book.', '', \"There's\", 'more', 'continuity', 'in', 'the', 'others.', '', 'And', 'they', 'grow', 'on', 'you,', 'or', 'they', 'did', 'on', 'me,', 'anyway.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['diff', 'first', 'second', 'fif', 'book', 'ther', 'continu', 'oth', 'grow', 'anyway'], ['different', 'first', 'second', 'fifth', 'book', 'theres', 'continuity', 'others', 'grow', 'anyway'])\n",
      "original document: \n",
      "['Ok', 'so', 'why', 'is', 'the', 'procedure', 'to', 'divert?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'proc', 'divert'], ['ok', 'procedure', 'divert'])\n",
      "original document: \n",
      "['We', 'bought', 'last', 'year,', 'a', '2bdrm', 'Unit', 'about', '35', 'minutes', 'by', 'train', 'to', 'the', 'CBD', 'from', 'the', 'South', 'East.', 'A', 'halfway', 'compromise', 'between', 'proximity', 'to', 'work', 'and', 'having', 'a', 'yard', 'for', 'a', 'dog.', 'I', 'did', 'live', 'in', 'CBD', 'apartments', 'before', 'this', 'though', 'for', 'about', '4.5', 'years.', '(With', 'a', 'stint', 'in', 'the', 'outer', 'West', 'in', 'between.)', 'Proximity', 'to', 'everything', 'is', 'pretty', 'hard', 'to', 'compete', 'with,', 'but', 'we', 'wanted', 'the', 'dog', 'and', \"I'm\", 'not', 'going', 'to', 'sit', 'around', 'demanding', \"I'm\", 'entitled', 'to', 'a', '3', 'bdrm', 'townhouse', 'in', 'Richmond.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bought', 'last', 'year', '2bdrm', 'unit', 'thirty-five', 'minut', 'train', 'cbd', 'sou', 'east', 'halfway', 'comprom', 'proxim', 'work', 'yard', 'dog', 'liv', 'cbd', 'apart', 'though', 'forty-five', 'year', 'stint', 'out', 'west', 'proxim', 'everyth', 'pretty', 'hard', 'compet', 'want', 'dog', 'im', 'going', 'sit', 'around', 'demand', 'im', 'entitl', 'three', 'bdrm', 'townh', 'richmond'], ['buy', 'last', 'year', '2bdrm', 'unit', 'thirty-five', 'minutes', 'train', 'cbd', 'south', 'east', 'halfway', 'compromise', 'proximity', 'work', 'yard', 'dog', 'live', 'cbd', 'apartments', 'though', 'forty-five', 'years', 'stint', 'outer', 'west', 'proximity', 'everything', 'pretty', 'hard', 'compete', 'want', 'dog', 'im', 'go', 'sit', 'around', 'demand', 'im', 'entitle', 'three', 'bdrm', 'townhouse', 'richmond'])\n",
      "original document: \n",
      "['There', 'is', 'a', 'very', 'real', 'possibility', 'the', 'NFC', 'west', 'has', 'three', 'teams', 'that', 'are', '1-3.', 'We', 'win', 'against', 'the', 'Cardinals,', 'and', 'the', 'Seahawks', 'lose', 'to', 'the', 'Colts.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'poss', 'nfc', 'west', 'three', 'team', 'thirteen', 'win', 'cardin', 'seahawk', 'los', 'colt'], ['real', 'possibility', 'nfc', 'west', 'three', 'team', 'thirteen', 'win', 'cardinals', 'seahawks', 'lose', 'colts'])\n",
      "original document: \n",
      "['ok']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok'], ['ok'])\n",
      "original document: \n",
      "[\"That's\", 'an', 'awesome', 'idea', 'and', 'I', 'hope', 'they', 'work', 'it', 'out', 'soon.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'awesom', 'ide', 'hop', 'work', 'soon'], ['thats', 'awesome', 'idea', 'hope', 'work', 'soon'])\n",
      "original document: \n",
      "['omg', 'i', 'love', 'this!', 'i', 'share', 'your', 'same', 'joy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['omg', 'lov', 'shar', 'joy'], ['omg', 'love', 'share', 'joy'])\n",
      "original document: \n",
      "['&gt;', 'impossible', 'by', 'the', 'laws', 'of', 'biology.', '\\n\\nIncluding', 'a', 'God.', 'It', 'seems', 'you', 'now', 'agree', 'with', 'me', 'the', 'probability', 'of', 'a', 'God', 'is', 'about', 'the', 'same', 'as', 'flying', 'pink', 'unicorns', '-', '0%']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'imposs', 'law', 'biolog', '\\n\\nincluding', 'god', 'seem', 'agr', 'prob', 'god', 'fly', 'pink', 'unicorn', 'zero'], ['gt', 'impossible', 'laws', 'biology', '\\n\\nincluding', 'god', 'seem', 'agree', 'probability', 'god', 'fly', 'pink', 'unicorns', 'zero'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Yeah', 'but', 'we', 'also', 'know', 'Reddit', 'is', '98%', 'Liberal', 'so....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'also', 'know', 'reddit', 'ninety-eight', 'lib'], ['yeah', 'also', 'know', 'reddit', 'ninety-eight', 'liberal'])\n",
      "original document: \n",
      "['Was', 'it', 'yours?', 'Lol?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['292', 'hunter,', 'psn:', 'chemicologist']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two hundred and ninety-two', 'hunt', 'psn', 'chemicolog'], ['two hundred and ninety-two', 'hunter', 'psn', 'chemicologist'])\n",
      "original document: \n",
      "['International']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['intern'], ['international'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'say', 'it', 'really', \"doesn't\", 'matter', 'if', 'you', 'have', 'a', 'controller', 'or', 'not.Either', 'way', 'its', 'going', 'to', 'be', 'difficult.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'real', 'doesnt', 'mat', 'control', 'noteith', 'way', 'going', 'difficult'], ['say', 'really', 'doesnt', 'matter', 'controller', 'noteither', 'way', 'go', 'difficult'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Damn!!!', 'No', 'need', 'to', 'wear', 'reflective', 'gear', 'I', '', 'Halloween', 'for', 'that', 'one.', '', 'You', 'can', 'see', 'that', 'a', 'mile', 'away']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'nee', 'wear', 'reflect', 'gear', 'halloween', 'on', 'see', 'mil', 'away'], ['damn', 'need', 'wear', 'reflective', 'gear', 'halloween', 'one', 'see', 'mile', 'away'])\n",
      "original document: \n",
      "['I', 'got', 'a', 'feeling', 'LSU', 'will', 'come', 'to', 'regret', 'firing', 'Les', 'Miles', 'by', 'the', 'end', 'of', 'the', 'season.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'feel', 'lsu', 'com', 'regret', 'fir', 'les', 'mil', 'end', 'season'], ['get', 'feel', 'lsu', 'come', 'regret', 'fire', 'les', 'miles', 'end', 'season'])\n",
      "original document: \n",
      "['&gt;Graham,', 'taken', 'with', 'pick', '53', 'in', 'last', 'year’s', 'national', 'draft,', 'didn’t', 'debut', 'until', 'Round', '22', 'and', 'has', 'now', 'played', 'more', 'finals', 'than', 'home-and-away', 'games.', '', '\\n\\nPlays', '5', 'games,', 'kicks', '3', 'goals', 'in', 'the', 'GF', 'and', 'becomes', 'a', 'premiership', 'player.', 'Truly', 'amazing.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtgraham', 'tak', 'pick', 'fifty-three', 'last', 'year', 'nat', 'draft', 'didnt', 'debut', 'round', 'twenty-two', 'play', 'fin', 'homeandaway', 'gam', '\\n\\nplays', 'fiv', 'gam', 'kick', 'three', 'goal', 'gf', 'becom', 'premy', 'play', 'tru', 'amaz'], ['gtgraham', 'take', 'pick', 'fifty-three', 'last', 'years', 'national', 'draft', 'didnt', 'debut', 'round', 'twenty-two', 'play', 'finals', 'homeandaway', 'game', '\\n\\nplays', 'five', 'game', 'kick', 'three', 'goals', 'gf', 'become', 'premiership', 'player', 'truly', 'amaze'])\n",
      "original document: \n",
      "['P.S.', 'Facebook', 'is', 'cancer.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ps', 'facebook', 'cant'], ['ps', 'facebook', 'cancer'])\n",
      "original document: \n",
      "['', 'jot', 'uppjepasst']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jot', 'uppjepasst'], ['jot', 'uppjepasst'])\n",
      "original document: \n",
      "['Oh', 'really?', '', 'I', 'never', 'thought', 'UNP', 'vs', 'CBBE', 'was', 'considered', 'a', 'strictly', 'better', 'sort', 'of', 'deal,', 'I', 'thought', 'it', 'was', 'a', 'matter', 'of', 'preference.', '', 'My', 'main', 'reason', 'for', 'using', 'UNP', 'is', 'because', 'I', 'find', 'it', 'has', 'a', 'wider', 'selection', 'of', 'armours', 'of', 'styles', 'that', 'I', 'like.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'real', 'nev', 'thought', 'unp', 'vs', 'cbbe', 'consid', 'strictly', 'bet', 'sort', 'deal', 'thought', 'mat', 'pref', 'main', 'reason', 'us', 'unp', 'find', 'wid', 'select', 'armo', 'styl', 'lik'], ['oh', 'really', 'never', 'think', 'unp', 'vs', 'cbbe', 'consider', 'strictly', 'better', 'sort', 'deal', 'think', 'matter', 'preference', 'main', 'reason', 'use', 'unp', 'find', 'wider', 'selection', 'armour', 'style', 'like'])\n",
      "original document: \n",
      "['Purolator.', 'And', \"it's\", 'been', 'over', 'a', 'week', 'since', 'it', 'said', 'it', 'was', 'delivered.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['purol', 'week', 'sint', 'said', 'del'], ['purolator', 'week', 'since', 'say', 'deliver'])\n",
      "original document: \n",
      "['Woah!', 'How', 'have', 'I', 'lived', 'until', 'now', 'without', 'knowing', 'this', 'jem?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['woah', 'liv', 'without', 'know', 'jem'], ['woah', 'live', 'without', 'know', 'jem'])\n",
      "original document: \n",
      "['“Toot', 'toot!”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['toot', 'toot'], ['toot', 'toot'])\n",
      "original document: \n",
      "['Yes,', 'for', 'a', 'number', 'of', 'reasons.\\n\\nDominant', 'decks', 'force', 'players', 'to', 'either', 'run', 'the', 'dominant', 'strategy', 'themselves', 'or', 'something', 'that', 'can', 'win', 'reliably', 'against', 'it.', 'This', 'severely', 'limits', 'the', 'amount', 'of', 'strategies', 'instead', 'of', 'diversifying', 'them.', 'If', 'it', 'gets', 'to', 'a', 'point', 'where', 'you', \"don't\", 'have', 'at', 'least', 'one', 'of', 'the', 'basic', 'deck', 'archetypes', '(like', 'no', 'aggro', 'deck)', 'you', 'have', 'a', 'metagame', 'that', 'has', 'warped', 'around', 'that', 'dominant', 'strategy.\\n\\nThere', 'have', 'been', 'many', 'cases', 'of', 'that', 'happening', 'in', 'Vintage', 'over', 'the', 'past', '10', 'years.', 'Legacy', 'too:', 'Mystical', 'Tutor', 'in', 'ANT', 'and', 'Survival', 'of', 'the', 'Fittest', 'are', 'great', 'examples', 'of', 'this', 'happening.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'numb', 'reasons\\n\\ndominant', 'deck', 'forc', 'play', 'eith', 'run', 'domin', 'strategy', 'someth', 'win', 'rely', 'sev', 'limit', 'amount', 'strategies', 'instead', 'divers', 'get', 'point', 'dont', 'least', 'on', 'bas', 'deck', 'archetyp', 'lik', 'aggro', 'deck', 'metagam', 'warp', 'around', 'domin', 'strategy\\n\\nthere', 'many', 'cas', 'hap', 'vint', 'past', 'ten', 'year', 'leg', 'myst', 'tut', 'ant', 'surv', 'fittest', 'gre', 'exampl', 'hap'], ['yes', 'number', 'reasons\\n\\ndominant', 'deck', 'force', 'players', 'either', 'run', 'dominant', 'strategy', 'something', 'win', 'reliably', 'severely', 'limit', 'amount', 'strategies', 'instead', 'diversify', 'get', 'point', 'dont', 'least', 'one', 'basic', 'deck', 'archetypes', 'like', 'aggro', 'deck', 'metagame', 'warp', 'around', 'dominant', 'strategy\\n\\nthere', 'many', 'case', 'happen', 'vintage', 'past', 'ten', 'years', 'legacy', 'mystical', 'tutor', 'ant', 'survival', 'fittest', 'great', 'examples', 'happen'])\n",
      "original document: \n",
      "['Given', 'the', 'context,', 'you', '*are*', 'doing', 'something', 'more', 'than', 'just', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'context', 'someth'], ['give', 'context', 'something'])\n",
      "original document: \n",
      "['This', 'reminds', 'me', 'of', 'when', 'someone', 'takes', 'a', 'scene', 'from', 'a', 'TV', 'show', 'like', 'Family', 'Guy,', 'makes', 'it', 'into', 'a', 'comic,', 'and', 'posts', 'it', 'to', '/r/funny.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remind', 'someon', 'tak', 'scen', 'tv', 'show', 'lik', 'famy', 'guy', 'mak', 'com', 'post', 'rfunny'], ['remind', 'someone', 'take', 'scene', 'tv', 'show', 'like', 'family', 'guy', 'make', 'comic', 'post', 'rfunny'])\n",
      "original document: \n",
      "['Yep', 'basically', 'every', 'game', 'is', 'going', 'to', 'have', 'them', 'these', 'days.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'bas', 'every', 'gam', 'going', 'day'], ['yep', 'basically', 'every', 'game', 'go', 'days'])\n",
      "original document: \n",
      "['Tinder']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tind'], ['tinder'])\n",
      "original document: \n",
      "['A', 'lot', 'of', 'people', 'complain', 'about', 'Diego', 'Fagundez', 'not', 'progressing', 'like', 'we', 'wanted.', 'A', 'big', 'reason', 'is', 'that', 'he', 'some', 'still', 'does', 'not', 'have', 'a', 'weak', 'foot', 'as', 'seen', 'with', 'that', 'chance.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lot', 'peopl', 'complain', 'diego', 'fagundez', 'progress', 'lik', 'want', 'big', 'reason', 'stil', 'weak', 'foot', 'seen', 'chant'], ['lot', 'people', 'complain', 'diego', 'fagundez', 'progress', 'like', 'want', 'big', 'reason', 'still', 'weak', 'foot', 'see', 'chance'])\n",
      "original document: \n",
      "['Imagine', 'eyes', 'that', 'green', 'on', 'a', 'person.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['imagin', 'ey', 'green', 'person'], ['imagine', 'eye', 'green', 'person'])\n",
      "original document: \n",
      "['this', 'looks', 'like', 'a', 'job', 'for', 'the', 'Magic', '8', 'ball', 'bot']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'job', 'mag', 'eight', 'bal', 'bot'], ['look', 'like', 'job', 'magic', 'eight', 'ball', 'bot'])\n",
      "original document: \n",
      "['USI', 'Grad!', '', 'Played', 'that', 'course', 'the', 'year', 'it', 'was', 'just', 'put', 'in.', '', \"Haven't\", 'been', 'back', 'in', 'years,', 'but', 'would', 'love', 'to', 'go', 'back', 'and', 'play.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'grad', 'play', 'cours', 'year', 'put', 'hav', 'back', 'year', 'would', 'lov', 'go', 'back', 'play'], ['usi', 'grad', 'play', 'course', 'year', 'put', 'havent', 'back', 'years', 'would', 'love', 'go', 'back', 'play'])\n",
      "original document: \n",
      "['Yea,', 'Tatum', 'is', 'getting', 'a', 'lot', 'of', 'praise', 'from', 'teammates', 'and', 'coaches,', 'a', 'lot', 'more', 'so', 'than', 'other', 'rookies', 'so', 'its', 'somewhat', 'reassuring', 'but', 'as', 'we', 'all', 'know,', 'compared', 'to', 'most', 'of', 'the', 'other', 'top', 'prospects,', 'Tatum', 'will', 'be', 'the', 'one', 'that', 'has', 'to', 'work', 'the', 'hardest', 'just', 'like', 'Jaylen', 'did', 'last', 'year', 'simply', 'because', 'of', 'the', 'fact', 'that', 'we', 'are', 'already', 'a', 'top', '5', 'team', 'getting', 'top', '5', 'lottery']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'tat', 'get', 'lot', 'pra', 'team', 'coach', 'lot', 'rooky', 'somewh', 'reass', 'know', 'comp', 'top', 'prospect', 'tat', 'on', 'work', 'hardest', 'lik', 'jayl', 'last', 'year', 'simply', 'fact', 'already', 'top', 'fiv', 'team', 'get', 'top', 'fiv', 'lottery'], ['yea', 'tatum', 'get', 'lot', 'praise', 'teammates', 'coach', 'lot', 'rookies', 'somewhat', 'reassure', 'know', 'compare', 'top', 'prospect', 'tatum', 'one', 'work', 'hardest', 'like', 'jaylen', 'last', 'year', 'simply', 'fact', 'already', 'top', 'five', 'team', 'get', 'top', 'five', 'lottery'])\n",
      "original document: \n",
      "['3k?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['3k'], ['3k'])\n",
      "original document: \n",
      "['I', 'agree', '100%', 'and', 'will', 'only', 'add', 'that', 'they', 'must', 'value', 'Mandog', 'considering', 'they', 'used', 'an', 'expansion', 'draft', 'protection', 'on', 'him.', 'I', 'know', 'there', 'really', \"wasn't\", 'anyone', 'better', '(Amac?', 'hahaha)', 'but', 'I', 'think', 'he', 'latches', 'on', 'this', 'year', 'as', 'the', '7th', 'man.', 'Morin', 'stays', 'as', 'does', 'Hagg,', 'FWIW', 'I', 'think', 'Morin', 'will', 'eventually', 'get', 'into', 'the', 'top', '4', 'as', 'well,', 'and', 'I', 'hope', 'that', 'Amac', 'is', 'sent', 'down', 'if', 'not', 'now,', 'eventually', 'during', 'the', 'season', '(but', 'I', \"ain't\", \"holdin'\", 'my', 'breath).', '\\n\\nA', 'pipe', 'dream', 'but', \"I'd\", 'love', 'to', 'see', 'a', 'Provy', '-', 'Gudas', 'top', 'pair,', 'a', 'Morin', '-', 'Ghost', 'second', 'pair,', 'and', 'a', 'Manning', '-', 'Hagg', 'third', 'pair', 'but', 'I', 'also', \"don't\", 'think', 'that', 'will', 'happen.\\n\\nI', 'also', 'extra-agree', 'with', 'getting', 'Sanheim', 'top', 'pairing', 'minutes', 'in', 'the', 'AHL', 'because', 'he', 'should', 'be', 'a', 'top', 'four', 'next', 'year', 'assuming', 'he', 'uses', 'this', 'year', 'to', 'iron', 'out', 'the', 'last', 'of', 'his', 'defensive', 'game.', 'Manning', 'is', 'a', 'UFA', 'next', 'year', 'and', 'I', 'doubt', 'we', 'resign', 'him.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'one hundred', 'ad', 'must', 'valu', 'mandog', 'consid', 'us', 'expand', 'draft', 'protect', 'know', 'real', 'wasnt', 'anyon', 'bet', 'amac', 'hahah', 'think', 'latch', 'year', '7th', 'man', 'morin', 'stay', 'hag', 'fwiw', 'think', 'morin', 'ev', 'get', 'top', 'four', 'wel', 'hop', 'amac', 'sent', 'ev', 'season', 'aint', 'holdin', 'brea', '\\n\\na', 'pip', 'dream', 'id', 'lov', 'see', 'provy', 'guda', 'top', 'pair', 'morin', 'ghost', 'second', 'pair', 'man', 'hag', 'third', 'pair', 'also', 'dont', 'think', 'happen\\n\\ni', 'also', 'extraagr', 'get', 'sanheim', 'top', 'pair', 'minut', 'ahl', 'top', 'four', 'next', 'year', 'assum', 'us', 'year', 'iron', 'last', 'defend', 'gam', 'man', 'uf', 'next', 'year', 'doubt', 'resign'], ['agree', 'one hundred', 'add', 'must', 'value', 'mandog', 'consider', 'use', 'expansion', 'draft', 'protection', 'know', 'really', 'wasnt', 'anyone', 'better', 'amac', 'hahaha', 'think', 'latch', 'year', '7th', 'man', 'morin', 'stay', 'hagg', 'fwiw', 'think', 'morin', 'eventually', 'get', 'top', 'four', 'well', 'hope', 'amac', 'send', 'eventually', 'season', 'aint', 'holdin', 'breath', '\\n\\na', 'pipe', 'dream', 'id', 'love', 'see', 'provy', 'gudas', 'top', 'pair', 'morin', 'ghost', 'second', 'pair', 'man', 'hagg', 'third', 'pair', 'also', 'dont', 'think', 'happen\\n\\ni', 'also', 'extraagree', 'get', 'sanheim', 'top', 'pair', 'minutes', 'ahl', 'top', 'four', 'next', 'year', 'assume', 'use', 'year', 'iron', 'last', 'defensive', 'game', 'man', 'ufa', 'next', 'year', 'doubt', 'resign'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['sorry,', 'my', 'bad', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'bad'], ['sorry', 'bad'])\n",
      "original document: \n",
      "['I', 'think', \"Ushiwakamaru's\", 'Interlude', 'gives', 'some', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'ushiwakamar', 'interlud', 'giv', 'wel'], ['think', 'ushiwakamarus', 'interlude', 'give', 'well'])\n",
      "original document: \n",
      "['This', 'is', 'a', 'great', 'all', 'in', 'one', 'terrain', 'tool', '-', '[Gaia', '-', '$45](https://assetstore.unity.com/packages/tools/terrain/gaia-42618)\\n\\nJust', 'go', 'look', 'before', 'you', 'say', 'it', 'costs', 'too', 'much', '', ':', ')']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'on', 'terrain', 'tool', 'gai', '45httpsassetstoreunitycompackagestoolsterraingaia42618\\n\\njust', 'go', 'look', 'say', 'cost', 'much'], ['great', 'one', 'terrain', 'tool', 'gaia', '45httpsassetstoreunitycompackagestoolsterraingaia42618\\n\\njust', 'go', 'look', 'say', 'cost', 'much'])\n",
      "original document: \n",
      "['okay', 'question', 'What', 'specifically', 'unique', 'to', 'Europe', '..', 'because', 'I', 'would', 'say', 'in', 'Europe', 'the', 'black', 'population', 'is', 'from', 'Africa', 'and', 'it', 'is', 'a', 'smaller', 'minority...', '\\n\\nconversely', 'the', 'Muslim', 'population', 'is', 'exploding', 'and', 'are', 'we', 'counting', 'that', 'as', 'part', 'of', 'black', 'population?...', 'because', 'if', 'you', 'do', 'then', 'I', \"wouldn't\", 'necessarily', 'say', 'definitely', 'Europe']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'quest', 'spec', 'un', 'europ', 'would', 'say', 'europ', 'black', 'pop', 'afric', 'smal', 'min', '\\n\\nconversely', 'muslim', 'pop', 'explod', 'count', 'part', 'black', 'pop', 'wouldnt', 'necess', 'say', 'definit', 'europ'], ['okay', 'question', 'specifically', 'unique', 'europe', 'would', 'say', 'europe', 'black', 'population', 'africa', 'smaller', 'minority', '\\n\\nconversely', 'muslim', 'population', 'explode', 'count', 'part', 'black', 'population', 'wouldnt', 'necessarily', 'say', 'definitely', 'europe'])\n",
      "original document: \n",
      "['If', 'u', 'actually', 'hit', 'it,', 'it', 'explodes', 'bug', 'shit', 'everywhere..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['u', 'act', 'hit', 'explod', 'bug', 'shit', 'everywh'], ['u', 'actually', 'hit', 'explode', 'bug', 'shit', 'everywhere'])\n",
      "original document: \n",
      "['They', 'are', 'planning', 'on', 'introducing', 'stuff', 'like', 'this.', \"It's\", 'just', 'gonna', 'be', 'a', 'few.', 'Just', 'be', 'sure', 'to', 'keep', 'in', 'mind,', 'this', 'is', 'still', 'pretty', 'early', 'game', 'state.', 'Which', 'is', 'a', 'good', 'sign', 'for', 'things', 'to', 'come', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plan', 'introduc', 'stuff', 'lik', 'gonn', 'sur', 'keep', 'mind', 'stil', 'pretty', 'ear', 'gam', 'stat', 'good', 'sign', 'thing', 'com'], ['plan', 'introduce', 'stuff', 'like', 'gonna', 'sure', 'keep', 'mind', 'still', 'pretty', 'early', 'game', 'state', 'good', 'sign', 'things', 'come'])\n",
      "original document: \n",
      "['\"I', 'am', 'not', 'going', 'to', 'stand', 'up', 'to', 'show', 'pride', 'in', 'a', 'flag', 'for', 'a', 'country', 'that', 'oppresses', 'black', 'people', 'and', 'people', 'of', 'color,\"', '', '-', 'Colin', 'Kaepernick', 'on', 'why', 'he', 'started', 'the', 'protest\\n\\nBasically,', 'he', 'believes', 'the', 'flag/national', 'anthem', \"doesn't\", 'deserve', 'to', 'be', 'respected', 'until', 'racial', 'equality', 'is', 'achieved.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'stand', 'show', 'prid', 'flag', 'country', 'oppress', 'black', 'peopl', 'peopl', 'col', 'colin', 'kaepernick', 'start', 'protest\\n\\nbasically', 'believ', 'flagn', 'anthem', 'doesnt', 'deserv', 'respect', 'rac', 'eq', 'achiev'], ['go', 'stand', 'show', 'pride', 'flag', 'country', 'oppress', 'black', 'people', 'people', 'color', 'colin', 'kaepernick', 'start', 'protest\\n\\nbasically', 'believe', 'flagnational', 'anthem', 'doesnt', 'deserve', 'respect', 'racial', 'equality', 'achieve'])\n",
      "original document: \n",
      "['i', 'think', 'its', '600', 'xtals', 'for', 'the', '1pc', 'not', '$50', '\\n\\nthe', '$50', 'is', 'the', 'for', 'set', 'selection', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'six hundred', 'xtal', '1pc', 'fifty', '\\n\\nthe', 'fifty', 'set', 'select'], ['think', 'six hundred', 'xtals', '1pc', 'fifty', '\\n\\nthe', 'fifty', 'set', 'selection'])\n",
      "original document: \n",
      "['I', 'thought', 'that', 'was', 'a', 'makeshift', 'pizza', 'made', 'out', 'of', 'garlic', 'bread', 'and', 'I', 'was', 'about', 'to', 'virtually', 'high', 'five', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'makeshift', 'pizz', 'mad', 'garl', 'bread', 'virt', 'high', 'fiv'], ['think', 'makeshift', 'pizza', 'make', 'garlic', 'bread', 'virtually', 'high', 'five'])\n",
      "original document: \n",
      "['Or', 'you', 'know,', 'you', 'can', 'just', 'take', 'care', 'of', 'your', 'phone.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'tak', 'car', 'phone\\n'], ['know', 'take', 'care', 'phone\\n'])\n",
      "original document: \n",
      "['My', 'feelings', 'are', 'hurt!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'hurt'], ['feel', 'hurt'])\n",
      "original document: \n",
      "['I', 'believe', 'they', 'fit', 'but', \"I'll\", 'confirm', '100%', 'for', 'you', 'when', 'I', 'get', 'home', '.', 'I', 'can', 'tell', 'you', 'the', 'coils', 'that', 'come', 'with', 'the', 'AO', 'are', 'a', 'bit', 'nicer', '.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['believ', 'fit', 'il', 'confirm', 'one hundred', 'get', 'hom', 'tel', 'coil', 'com', 'ao', 'bit', 'nic'], ['believe', 'fit', 'ill', 'confirm', 'one hundred', 'get', 'home', 'tell', 'coil', 'come', 'ao', 'bite', 'nicer'])\n",
      "original document: \n",
      "['Not', 'as', 'bad', 'as', 'I', 'thought.', 'I', 'guess', 'it', 'could', 'be', 'perceived', 'as', 'irreverent,', 'but', \"it's\", 'just', 'kind', 'of', 'cool', 'to', 'me.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bad', 'thought', 'guess', 'could', 'perceiv', 'irrev', 'kind', 'cool'], ['bad', 'think', 'guess', 'could', 'perceive', 'irreverent', 'kind', 'cool'])\n",
      "original document: \n",
      "['This', 'is', 'so', 'true.', 'Judaism', 'came', 'from', 'Sumerian', 'mythology.', 'Elohim', 'is', 'Anu.', 'Lucifer', 'is', 'Enki', 'who', 'is', 'a', 'scientist', 'and', 'the', 'actual', 'Creator', 'of', 'mankind', 'possibly', 'they', \"weren't\", 'even', 'goods', 'but', 'just', 'a', 'more', 'advanced', 'civilization', 'or', 'something...', 'Any', 'how', 'Genesis', 'says', 'we', 'created', 'man', 'in', 'our', 'image', 'not', 'my...', 'So', 'God', 'was', 'not', 'alone.\\n\\nMy', 'guess', 'is', 'Jews', 'usurped', 'a', 'God', 'from', 'Sumerian', 'mythology', 'to', 'be', 'their', 'patron', 'God', 'like', 'Catholics', 'have', 'patron', 'saints.', 'Then', 'he', 'became', 'jealous', 'and', 'to', 'a', 'people', 'who', 'believe', 'in', 'the', 'existence', 'of', 'many', \"God's,\", 'but', 'only', 'one', 'is', \"'their'\", 'personal', 'God,', 'he', 'would', 'semantically', 'be', 'the', 'one', 'true', 'God', 'of', 'this', 'small', 'subsection', 'of', 'humanity.', 'But', 'not', 'necessarily', 'the', 'God', 'of', 'everything', 'ever', 'created.\\n\\nThe', 'garden', 'of', 'Eden', 'myth', 'told', 'by', 'Sumerians', 'is', 'so', 'similar', 'though', 'that', \"it's\", 'hard', 'to', 'believe', \"it's\", 'not', 'the', 'true', 'source,', 'esp', 'since', 'it', 'predates', 'Bible', 'by', 'like', '1500', 'years.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'juda', 'cam', 'sum', 'mytholog', 'elohim', 'anu', 'luc', 'enk', 'sci', 'act', 'cre', 'mankind', 'poss', 'wer', 'ev', 'good', 'adv', 'civil', 'someth', 'genes', 'say', 'cre', 'man', 'im', 'god', 'alone\\n\\nmy', 'guess', 'jew', 'usurp', 'god', 'sum', 'mytholog', 'patron', 'god', 'lik', 'cathol', 'patron', 'saint', 'becam', 'jeal', 'peopl', 'believ', 'ex', 'many', 'god', 'on', 'person', 'god', 'would', 'sem', 'on', 'tru', 'god', 'smal', 'subsect', 'hum', 'necess', 'god', 'everyth', 'ev', 'created\\n\\nthe', 'gard', 'ed', 'myth', 'told', 'sum', 'simil', 'though', 'hard', 'believ', 'tru', 'sourc', 'esp', 'sint', 'pred', 'bibl', 'lik', 'one thousand, five hundred', 'year'], ['true', 'judaism', 'come', 'sumerian', 'mythology', 'elohim', 'anu', 'lucifer', 'enki', 'scientist', 'actual', 'creator', 'mankind', 'possibly', 'werent', 'even', 'goods', 'advance', 'civilization', 'something', 'genesis', 'say', 'create', 'man', 'image', 'god', 'alone\\n\\nmy', 'guess', 'jews', 'usurp', 'god', 'sumerian', 'mythology', 'patron', 'god', 'like', 'catholics', 'patron', 'saint', 'become', 'jealous', 'people', 'believe', 'existence', 'many', 'gods', 'one', 'personal', 'god', 'would', 'semantically', 'one', 'true', 'god', 'small', 'subsection', 'humanity', 'necessarily', 'god', 'everything', 'ever', 'created\\n\\nthe', 'garden', 'eden', 'myth', 'tell', 'sumerians', 'similar', 'though', 'hard', 'believe', 'true', 'source', 'esp', 'since', 'predate', 'bible', 'like', 'one thousand, five hundred', 'years'])\n",
      "original document: \n",
      "['Actually', 'the', 'more', 'transactions', 'on', 'the', 'Tangle', 'the', 'faster', 'it', 'becomes.', '', \"That's\", 'why', 'when', 'they', 'get', 'spammed', 'it', 'speeds', 'up.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'transact', 'tangl', 'fast', 'becom', 'that', 'get', 'spam', 'spee'], ['actually', 'transactions', 'tangle', 'faster', 'become', 'thats', 'get', 'spammed', 'speed'])\n",
      "original document: \n",
      "['Who', 'is', 'going', 'to', 'be', 'teaching', 'Genetics?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'teach', 'genet'], ['go', 'teach', 'genetics'])\n",
      "original document: \n",
      "['Even', 'if', 'you', 'get', 'all', 'of', 'them,', \"there's\", 'a', 'place', 'in', 'the', 'far', 'NE', 'where', \"its'\", 'really', 'cold.', 'You', 'can', 'only', 'go', 'so', 'high', 'up', 'before', 'you', \"can't.\", 'When', 'one', 'of', 'the', 'last', 'few', 'quests', 'starts,', \"you'll\", 'be', 'able', 'to', 'progress.', 'No', 'spoilers', 'but', 'at', 'earliest', 'is', 'this', 'point.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'get', 'ther', 'plac', 'far', 'ne', 'real', 'cold', 'go', 'high', 'cant', 'on', 'last', 'quest', 'start', 'youl', 'abl', 'progress', 'spoil', 'earliest', 'point'], ['even', 'get', 'theres', 'place', 'far', 'ne', 'really', 'cold', 'go', 'high', 'cant', 'one', 'last', 'quest', 'start', 'youll', 'able', 'progress', 'spoilers', 'earliest', 'point'])\n",
      "original document: \n",
      "[\"It's\", 'extremely', 'overly', 'simplistic,', 'but', 'you', 'kind', 'of', 'have', 'to', 'be', 'when', \"you're\", 'talking', 'to', '8', 'year', 'olds.', '\\n\\n&gt;By', 'your', 'logic,', 'they’ll', 'get', 'it', 'sorted', 'out', 'for', 'them', 'later\\n\\nWill', 'they', 'not?', 'Does', 'Canada', 'have', 'a', 'epidemic', 'of', '3rd', 'grade', 'dropouts', \"I'm\", 'not', 'hearing', 'about?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['extrem', 'ov', 'simpl', 'kind', 'yo', 'talk', 'eight', 'year', 'old', '\\n\\ngtby', 'log', 'theyl', 'get', 'sort', 'later\\n\\nwill', 'canad', 'epidem', '3rd', 'grad', 'dropout', 'im', 'hear'], ['extremely', 'overly', 'simplistic', 'kind', 'youre', 'talk', 'eight', 'year', 'olds', '\\n\\ngtby', 'logic', 'theyll', 'get', 'sort', 'later\\n\\nwill', 'canada', 'epidemic', '3rd', 'grade', 'dropouts', 'im', 'hear'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['LOLOL']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lolol'], ['lolol'])\n",
      "original document: \n",
      "['[+261TurnerLane](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnomxii/):\\n\\nOh', 'interesting,', 'the', 'page', 'lists', 'First', 'Choice', 'Books', 'as', 'your', 'publisher,', 'which', 'is', 'a', 'pay', 'to', 'print', 'company,', 'and', 'I', 'assumed', \"that's\", 'how', 'you', 'got', 'the', 'physical', 'copy', \"you're\", 'holding', 'in', 'your', 'picture.', 'Did', 'you', 'make', 'the', 'print', 'copy', 'yourself?', 'It', 'looks', 'very', 'well', 'done', 'if', \"that's\", 'the', 'case.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['261turnerlanehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnomxii\\n\\noh', 'interest', 'pag', 'list', 'first', 'cho', 'book', 'publ', 'pay', 'print', 'company', 'assum', 'that', 'got', 'phys', 'cop', 'yo', 'hold', 'pict', 'mak', 'print', 'cop', 'look', 'wel', 'don', 'that', 'cas'], ['261turnerlanehttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnomxii\\n\\noh', 'interest', 'page', 'list', 'first', 'choice', 'book', 'publisher', 'pay', 'print', 'company', 'assume', 'thats', 'get', 'physical', 'copy', 'youre', 'hold', 'picture', 'make', 'print', 'copy', 'look', 'well', 'do', 'thats', 'case'])\n",
      "original document: \n",
      "['*', 'Username:', '/u/Momo254\\n*', 'Join', 'date:', '2011-12-14', '02:02:53\\n*', 'Link', 'karma:', '407\\n*', 'Comment', 'karma:', '221\\n*', 'Confirmed', 'trades:', '20\\n*', 'Heatware:', '[https://www.heatware.com/eval.php?id=103982](https://www.heatware.com/eval.php?id=103982)\\n\\n^^This', '^^information', '^^does', '^^not', '^^guarantee', '^^a', '^^successful', '^^swap.', '^^It', '^^is', '^^being', '^^provided', '^^to', '^^help', '^^potential', '^^trade', '^^partners', '^^have', '^^more', '^^immediate', '^^background', '^^information', '^^about', '^^with', '^^whom', '^^they', '^^are', '^^swapping.', '^^Please', '^^be', '^^sure', '^^to', '^^familiarize', '^^yourself', '^^with', '^^the', '^^[RULES](https://www.reddit.com/r/hardwareswap/wiki/rules/rules)', '^^and', '^^other', '^^guides', '^^on', '^^the', '^^[WIKI](https://www.reddit.com/r/hardwareswap/wiki/index)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['usernam', 'umomo254\\n', 'join', 'dat', 'twenty million, one hundred and eleven thousand, two hundred and fourteen', '020253\\n', 'link', 'karm', '407\\n', 'com', 'karm', '221\\n', 'confirm', 'trad', '20\\n', 'heatw', 'httpswwwheatwarecomevalphpid103982httpswwwheatwarecomevalphpid103982\\n\\nthis', 'inform', 'guar', 'success', 'swap', 'provid', 'help', 'pot', 'trad', 'partn', 'immedy', 'background', 'inform', 'swap', 'pleas', 'sur', 'famili', 'ruleshttpswwwredditcomrhardwareswapwikirulesr', 'guid', 'wikihttpswwwredditcomrhardwareswapwikiindex'], ['username', 'umomo254\\n', 'join', 'date', 'twenty million, one hundred and eleven thousand, two hundred and fourteen', '020253\\n', 'link', 'karma', '407\\n', 'comment', 'karma', '221\\n', 'confirm', 'trade', '20\\n', 'heatware', 'httpswwwheatwarecomevalphpid103982httpswwwheatwarecomevalphpid103982\\n\\nthis', 'information', 'guarantee', 'successful', 'swap', 'provide', 'help', 'potential', 'trade', 'partner', 'immediate', 'background', 'information', 'swap', 'please', 'sure', 'familiarize', 'ruleshttpswwwredditcomrhardwareswapwikirulesrules', 'guide', 'wikihttpswwwredditcomrhardwareswapwikiindex'])\n",
      "original document: \n",
      "['holy', 'moly']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['holy', 'moly'], ['holy', 'moly'])\n",
      "original document: \n",
      "['The', 'murderers', 'all', 'got', 'off.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['murd', 'got'], ['murderers', 'get'])\n",
      "original document: \n",
      "['Up', 'until', 'last', 'year,', 'a', 'virginity', 'test', 'was', 'required', 'for', 'single', 'women', 'joining', 'the', 'national', 'police', 'force', 'in', 'Indonesia.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['last', 'year', 'virgin', 'test', 'requir', 'singl', 'wom', 'join', 'nat', 'pol', 'forc', 'indones'], ['last', 'year', 'virginity', 'test', 'require', 'single', 'women', 'join', 'national', 'police', 'force', 'indonesia'])\n",
      "original document: \n",
      "['143418340|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'ZKQ7hPa6)\\n\\n&gt;&gt;143412250', '(OP)\\nTrump\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, three hundred and forty', 'gt', 'unit', 'stat', 'anonym', 'id', 'zkq7hpa6\\n\\ngtgt143412250', 'op\\ntrump\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, three hundred and forty', 'gt', 'unite', 'state', 'anonymous', 'id', 'zkq7hpa6\\n\\ngtgt143412250', 'op\\ntrump\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Millenials', '(my', 'generation)', 'grew', 'up', 'with', 'the', 'internet,', 'we', 'barely', 'remember', 'a', 'time', 'when', 'we', 'actually', 'had', 'to', 'buy', 'newspapers,', 'some', 'of', 'us', \"weren't\", 'even', 'alive', 'then.', 'If', 'you', 'wanted', 'the', 'news,', 'you', 'bought', 'a', 'paper', 'with', 'your', 'money', 'or', 'watched', 'it', 'on', 'a', 'tv', 'station', 'that', 'ran', 'ads', 'or', 'was', 'paid', 'for', 'buy', 'our', \"parent's\", 'cable', 'subscriptions.', 'All', 'our', 'lives', 'news', 'has', 'been', 'free,', 'and', 'for', 'that', 'reason', 'we', 'never', 'understood', 'its', 'value.', 'We', \"don't\", 'believe', \"it's\", 'something', \"that's\", 'worth', 'being', 'paid', 'for', 'because', \"we've\", 'never', 'had', 'too.\\n\\nHell', 'in', 'this', 'sub', 'people', 'will', 'copy', 'and', 'paste', 'entire', 'articles', 'from', 'sites', 'and', 'post', 'them', 'in', 'the', 'comments,', 'robbing', 'the', 'site', 'of', 'the', 'page', 'views', 'or', 'if', 'the', 'site', 'is', 'behind', 'a', 'paywall,', 'its', 'just', 'straight', 'up', 'piracy', 'at', 'that', 'point.', 'I', \"can't\", 'post', 'a', 'link', 'to', 'torrent', 'sites', 'on', '/r/television', 'or', '/r/movies', 'but', 'you', 'can', 'copy', 'entire', 'news', 'articles', 'in', 'the', 'comments', 'here', 'and', 'no', 'one', 'bats', 'an', 'eye.', 'Actually', 'think', 'about', 'that', 'for', 'a', 'second.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mil', 'gen', 'grew', 'internet', 'bar', 'rememb', 'tim', 'act', 'buy', 'newspap', 'us', 'wer', 'ev', 'al', 'want', 'new', 'bought', 'pap', 'money', 'watch', 'tv', 'stat', 'ran', 'ad', 'paid', 'buy', 'par', 'cabl', 'subscrib', 'liv', 'new', 'fre', 'reason', 'nev', 'understood', 'valu', 'dont', 'believ', 'someth', 'that', 'wor', 'paid', 'wev', 'nev', 'too\\n\\nhell', 'sub', 'peopl', 'cop', 'past', 'entir', 'artic', 'sit', 'post', 'com', 'rob', 'sit', 'pag', 'view', 'sit', 'behind', 'paywal', 'straight', 'pir', 'point', 'cant', 'post', 'link', 'tor', 'sit', 'rtelevid', 'rmovy', 'cop', 'entir', 'new', 'artic', 'com', 'on', 'bat', 'ey', 'act', 'think', 'second'], ['millenials', 'generation', 'grow', 'internet', 'barely', 'remember', 'time', 'actually', 'buy', 'newspapers', 'us', 'werent', 'even', 'alive', 'want', 'news', 'buy', 'paper', 'money', 'watch', 'tv', 'station', 'run', 'ads', 'pay', 'buy', 'parent', 'cable', 'subscriptions', 'live', 'news', 'free', 'reason', 'never', 'understand', 'value', 'dont', 'believe', 'something', 'thats', 'worth', 'pay', 'weve', 'never', 'too\\n\\nhell', 'sub', 'people', 'copy', 'paste', 'entire', 'article', 'sit', 'post', 'comment', 'rob', 'site', 'page', 'view', 'site', 'behind', 'paywall', 'straight', 'piracy', 'point', 'cant', 'post', 'link', 'torrent', 'sit', 'rtelevision', 'rmovies', 'copy', 'entire', 'news', 'article', 'comment', 'one', 'bat', 'eye', 'actually', 'think', 'second'])\n",
      "original document: \n",
      "['Byoot!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['byoot'], ['byoot'])\n",
      "original document: \n",
      "['You', 'really', \"don't\", 'need', 'to', 'attend', 'Kingdom', 'Halls', 'to', 'understand', 'the', 'thinking', 'of', 'JWs.', '', 'There', \"isn't\", 'much', 'thinking', 'to', 'begin', 'with.', '', 'After', 'all', \"it's\", 'a', 'cult.\\n\\nIf', 'you', 'plan', 'on', 'going', 'however,', 'be', 'prepared', 'for', 'people', 'to', 'try', 'and', 'convert', 'you', 'through', '\"Bible', 'study\"', 'and', 'get', 'invites', 'for', 'doing', 'door', 'to', 'door', 'ministry,', 'or', 'field', 'service', 'as', \"it's\", 'called.', '', 'And', 'when', 'you', 'refuse,', 'you', 'will', 'be', 'judged,', 'avoided,', 'and', 'warned', 'against', 'in', 'secret.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'dont', 'nee', 'attend', 'kingdom', 'hal', 'understand', 'think', 'jws', 'isnt', 'much', 'think', 'begin', 'cult\\n\\nif', 'plan', 'going', 'howev', 'prep', 'peopl', 'try', 'convert', 'bibl', 'study', 'get', 'invit', 'door', 'door', 'min', 'field', 'serv', 'cal', 'refus', 'judg', 'avoid', 'warn', 'secret'], ['really', 'dont', 'need', 'attend', 'kingdom', 'halls', 'understand', 'think', 'jws', 'isnt', 'much', 'think', 'begin', 'cult\\n\\nif', 'plan', 'go', 'however', 'prepare', 'people', 'try', 'convert', 'bible', 'study', 'get', 'invite', 'door', 'door', 'ministry', 'field', 'service', 'call', 'refuse', 'judge', 'avoid', 'warn', 'secret'])\n",
      "original document: \n",
      "['She', 'would', 'probably', 'be', 'more', 'offended', 'by', 'you', 'assuming', \"she's\", 'over', 'forty', 'than', 'being', 'called', 'a', 'girl.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'prob', 'offend', 'assum', 'she', 'forty', 'cal', 'girl'], ['would', 'probably', 'offend', 'assume', 'shes', 'forty', 'call', 'girl'])\n",
      "original document: \n",
      "['better', 'listen.', 'So', '/u/KatyLiedTheBitch,', \"what's\", 'wrong?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'list', 'ukatyliedthebitch', 'what', 'wrong'], ['better', 'listen', 'ukatyliedthebitch', 'whats', 'wrong'])\n",
      "original document: \n",
      "['Anyone', 'got', 'a', 'streamable', 'of', 'the', 'Price', 'break-dance', 'save', 'on', 'Hoffmann?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'got', 'streamable', 'pric', 'breakd', 'sav', 'hoffman'], ['anyone', 'get', 'streamable', 'price', 'breakdance', 'save', 'hoffmann'])\n",
      "original document: \n",
      "['6k?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['6k'], ['6k'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['All', 'my', 'niggas', 'gang', 'bang', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nigga', 'gang', 'bang'], ['niggas', 'gang', 'bang'])\n",
      "original document: \n",
      "['UBW', 'is', 'still', 'my', 'favorite', 'route', 'but', 'I', 'do', 'hope', 'they', 'pace', 'the', 'romance', 'and', 'character', 'building', 'in', 'HF', 'better', 'than', 'they', 'did', 'in', 'the', 'VN.', '\\n\\nOne', 'of', 'the', 'most', 'jarring', 'things', 'was', 'getting', 'character', 'flashbacks', 'of', 'when', 'Shiro', 'and', 'Sakura', 'first', 'meet', 'and', 'when', 'she', 'first', 'started', 'to', 'go', 'to', 'his', 'house', '(and', 'why)', 'was', 'established', 'right', 'at', 'the', 'very', 'end', 'of', 'the', 'normal', 'end', 'of', 'the', 'route', '&lt;.&lt;']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ubw', 'stil', 'favorit', 'rout', 'hop', 'pac', 'rom', 'charact', 'build', 'hf', 'bet', 'vn', '\\n\\none', 'jar', 'thing', 'get', 'charact', 'flashback', 'shiro', 'sakur', 'first', 'meet', 'first', 'start', 'go', 'hous', 'est', 'right', 'end', 'norm', 'end', 'rout', 'ltlt'], ['ubw', 'still', 'favorite', 'route', 'hope', 'pace', 'romance', 'character', 'build', 'hf', 'better', 'vn', '\\n\\none', 'jar', 'things', 'get', 'character', 'flashbacks', 'shiro', 'sakura', 'first', 'meet', 'first', 'start', 'go', 'house', 'establish', 'right', 'end', 'normal', 'end', 'route', 'ltlt'])\n",
      "original document: \n",
      "['Right', 'place', 'right', 'time', 'I', 'guess.', \"It's\", 'about', 'to', 'be', 'really', 'crazy', 'in', 'California.', 'Going', 'to', 'surpass', 'all', 'other', 'rec', 'states', \"I'm\", 'sure']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'plac', 'right', 'tim', 'guess', 'real', 'crazy', 'californ', 'going', 'surpass', 'rec', 'stat', 'im', 'sur'], ['right', 'place', 'right', 'time', 'guess', 'really', 'crazy', 'california', 'go', 'surpass', 'rec', 'state', 'im', 'sure'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Good', 'enough,', 'I', 'guess.', '', 'From', 'my', 'understanding', 'they', 'tossed', 'most', 'of', 'the', 'lore', 'out', 'when', 'they', 'created', 'their', 'new', '\"Age', 'of', 'Sigmar\"', 'table', 'top', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'enough', 'guess', 'understand', 'toss', 'lor', 'cre', 'new', 'ag', 'sigm', 'tabl', 'top', 'gam'], ['good', 'enough', 'guess', 'understand', 'toss', 'lore', 'create', 'new', 'age', 'sigmar', 'table', 'top', 'game'])\n",
      "original document: \n",
      "['So', \"I'm\", 'very', 'new', 'to', 'planted', 'tanks,', 'and', 'this', 'Anubias', 'that', 'I', 'bought', 'was', 'just', 'covered', 'in', 'these', 'black', 'specs', 'one', 'day', '(had', 'it', 'for', 'about', 'a', 'week)', \"\\n\\nI'm\", 'just', 'curious', 'if', 'these', 'are', 'snail', 'eggs', '(or', 'something', 'else)', 'because', 'I', 'have', 'a', 'lot', 'of', 'snails', 'in', 'there', 'already', 'and', 'my', 'other', 'Anubias', 'nana', 'has', 'nothing', 'on', 'it\\n\\nTank', 'is', 'a', '20', 'long', 'with', 'a', 'Nicrew', 'LED', '', 'that', 'runs', 'for', 'about', '7', 'hours', 'a', 'day.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'new', 'plant', 'tank', 'anubia', 'bought', 'cov', 'black', 'spec', 'on', 'day', 'week', '\\n\\nim', 'cury', 'snail', 'eg', 'someth', 'els', 'lot', 'snail', 'already', 'anubia', 'nan', 'noth', 'it\\n\\ntank', 'twenty', 'long', 'nicrew', 'led', 'run', 'sev', 'hour', 'day'], ['im', 'new', 'plant', 'tank', 'anubias', 'buy', 'cover', 'black', 'specs', 'one', 'day', 'week', '\\n\\nim', 'curious', 'snail', 'egg', 'something', 'else', 'lot', 'snail', 'already', 'anubias', 'nana', 'nothing', 'it\\n\\ntank', 'twenty', 'long', 'nicrew', 'lead', 'run', 'seven', 'hours', 'day'])\n",
      "original document: \n",
      "['Agreed', 'not', 'worth', 'the', 'time', 'or', 'money', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'wor', 'tim', 'money'], ['agree', 'worth', 'time', 'money'])\n",
      "original document: \n",
      "['Indeed', 'we', 'have.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indee'], ['indeed'])\n",
      "original document: \n",
      "['Happy', 'Birthday', 'Love!!!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['happy', 'birthday', 'lov'], ['happy', 'birthday', 'love'])\n",
      "original document: \n",
      "['correct.', 'recovering', 'an', 'acct', 'will', '100%', 'log', 'out', 'an', 'account', 'instantly']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['correct', 'recov', 'acct', 'one hundred', 'log', 'account', 'inst'], ['correct', 'recover', 'acct', 'one hundred', 'log', 'account', 'instantly'])\n",
      "original document: \n",
      "['Just', 'a', 'note,', 'you', 'probably', \"don't\", 'need', 'as', 'much', 'sand', 'as', 'you', 'think', 'you', 'do.', '9-10', 'inches', 'is', 'overkill', 'but', 'in', 'the', 'safe', 'range,', 'even', '.50', 'AE', 'will', 'only', 'make', 'it', 'six', 'or', 'seven', '(several', 'YouTubers', 'have', 'confirmed).', 'So', 'you', 'really', 'need', 'about', 'half', 'a', '5', 'gallon', 'bucket', 'of', 'sand,', 'which', 'is', 'way', 'easier', 'to', 'move', 'around.\\n\\nAlternately,', 'while', 'these', 'are', 'only', 'rated', 'for', '.44', 'Magnum', 'and', 'below,', 'if', \"you're\", 'in', 'an', 'apartment', 'and', 'shooting', '9mm,', \"I'd\", 'probably', 'consider', 'this', 'or', 'something', 'similar:\\n\\nhttps://www.range-systems.com/product/guardian-compact-clearing-trap/\\n\\nI', 'live', 'in', 'a', 'concrete', 'and', 'brick', 'house', 'so', 'any', 'exterior', 'wall', 'down', 'towards', 'the', 'ground', 'is', 'a', 'safe', 'direction.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['not', 'prob', 'dont', 'nee', 'much', 'sand', 'think', 'nine hundred and ten', 'inch', 'overkil', 'saf', 'rang', 'ev', 'fifty', 'ae', 'mak', 'six', 'sev', 'sev', 'youtub', 'confirm', 'real', 'nee', 'half', 'fiv', 'gallon', 'bucket', 'sand', 'way', 'easy', 'mov', 'around\\n\\nalternately', 'rat', 'forty-four', 'magn', 'yo', 'apart', 'shoot', '9mm', 'id', 'prob', 'consid', 'someth', 'similar\\n\\nhttpswwwrangesystemscomproductguardiancompactclearingtrap\\n\\ni', 'liv', 'concret', 'brick', 'hous', 'extery', 'wal', 'toward', 'ground', 'saf', 'direct'], ['note', 'probably', 'dont', 'need', 'much', 'sand', 'think', 'nine hundred and ten', 'inch', 'overkill', 'safe', 'range', 'even', 'fifty', 'ae', 'make', 'six', 'seven', 'several', 'youtubers', 'confirm', 'really', 'need', 'half', 'five', 'gallon', 'bucket', 'sand', 'way', 'easier', 'move', 'around\\n\\nalternately', 'rat', 'forty-four', 'magnum', 'youre', 'apartment', 'shoot', '9mm', 'id', 'probably', 'consider', 'something', 'similar\\n\\nhttpswwwrangesystemscomproductguardiancompactclearingtrap\\n\\ni', 'live', 'concrete', 'brick', 'house', 'exterior', 'wall', 'towards', 'grind', 'safe', 'direction'])\n",
      "original document: \n",
      "['thanks', 'for', 'the', 'help', 'everyone', '6mb', 'speeds', 'not', 'as', 'bad', 'as', 'the', 'tmobile', 'subreddit', 'had', 'me', 'thinking', 'and', 'for', 'such', 'a', 'low', 'price!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'help', 'everyon', '6mb', 'spee', 'bad', 'tmobl', 'subreddit', 'think', 'low', 'pric'], ['thank', 'help', 'everyone', '6mb', 'speed', 'bad', 'tmobile', 'subreddit', 'think', 'low', 'price'])\n",
      "original document: \n",
      "[\"I'm\", 'for', 'this', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im'], ['im'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['That', \"kid's\", 'going', 'to', 'die', 'a', 'virgin', 'for', 'sure.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kid', 'going', 'die', 'virgin', 'sur'], ['kid', 'go', 'die', 'virgin', 'sure'])\n",
      "original document: \n",
      "['My', 'parents', 'Chevrolet', 'Caprice', 'with', 'power', 'windows.', 'Only', 'rich', 'people', 'could', 'afford', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['par', 'chevrolet', 'capr', 'pow', 'window', 'rich', 'peopl', 'could', 'afford'], ['parent', 'chevrolet', 'caprice', 'power', 'windows', 'rich', 'people', 'could', 'afford'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"i'm\", 'not', 'gonna', 'advocate', 'blind', 'trust', 'in', 'riot,', 'but', 'i', 'really', 'hate', 'the', 'mentality', 'that', 'a', 'lot', 'of', 'people', 'have', 'where', 'they', 'think', 'they', 'know', \"what's\", 'best', 'for', 'riot', 'as', 'a', 'company', 'better', 'than', 'riot', \"itself.\\n\\nthey're\", 'just', 'going', 'off', 'whatever', 'data', 'is', 'available', 'to', 'them.', 'i', 'highly', 'doubt', 'riot', 'would', 'opt', 'into', 'certain', 'situations', 'while', 'knowing', \"it's\", 'going', 'to', 'cost', 'them', 'more', 'than', 'what', 'they', 'stand', 'to', 'gain\\n\\nthen', 'again,', 'companies', 'make', 'mistakes', 'too']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'gonn', 'advoc', 'blind', 'trust', 'riot', 'real', 'hat', 'ment', 'lot', 'peopl', 'think', 'know', 'what', 'best', 'riot', 'company', 'bet', 'riot', 'itself\\n\\ntheyre', 'going', 'whatev', 'dat', 'avail', 'high', 'doubt', 'riot', 'would', 'opt', 'certain', 'situ', 'know', 'going', 'cost', 'stand', 'gain\\n\\nth', 'company', 'mak', 'mistak'], ['im', 'gonna', 'advocate', 'blind', 'trust', 'riot', 'really', 'hate', 'mentality', 'lot', 'people', 'think', 'know', 'whats', 'best', 'riot', 'company', 'better', 'riot', 'itself\\n\\ntheyre', 'go', 'whatever', 'data', 'available', 'highly', 'doubt', 'riot', 'would', 'opt', 'certain', 'situations', 'know', 'go', 'cost', 'stand', 'gain\\n\\nthen', 'company', 'make', 'mistake'])\n",
      "original document: \n",
      "['Stink', 'bug?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stink', 'bug'], ['stink', 'bug'])\n",
      "original document: \n",
      "['Ha', 'I', 'goofed.', 'Since', 'herb', 'was', 'mentioned', 'I', 'looked', 'at', 'my', 'herb', 'level', 'for', 'my', 'farming', 'level.', 'I', 'have', '40', 'farming...so', 'yeah.', 'my', 'bad.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ha', 'goof', 'sint', 'herb', 'ment', 'look', 'herb', 'level', 'farm', 'level', 'forty', 'farmingso', 'yeah', 'bad'], ['ha', 'goof', 'since', 'herb', 'mention', 'look', 'herb', 'level', 'farm', 'level', 'forty', 'farmingso', 'yeah', 'bad'])\n",
      "original document: \n",
      "['Thought', 'they', 'were', 'CGI.', '\\n\\nFor', 'real', 'though,', 'you', 'have', 'beautiful', 'lips.', 'You', 'should', 'be', 'proud.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thought', 'cgi', '\\n\\nfor', 'real', 'though', 'beauty', 'lip', 'proud'], ['think', 'cgi', '\\n\\nfor', 'real', 'though', 'beautiful', 'lips', 'proud'])\n",
      "original document: \n",
      "['Wheres', 'the', 'cake?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wher', 'cak'], ['wheres', 'cake'])\n",
      "original document: \n",
      "['&gt;', 'Literature', '=\\\\=', 'Manga.', 'You', 'just', 'cant', 'compare', 'the', 'two.\\n\\nTrucks', 'are', 'not', 'pencils', 'but', 'I', 'can', 'say', 'that', 'trucks', 'are', 'on', 'average', 'heavier', 'than', 'pencils.', 'I', 'am', 'not', 'sure', 'why', 'you', 'would', 'say', 'that', 'is', 'not', 'possible', 'to', 'compare', 'two', 'aspect', 'of', 'things', 'simply', 'because', 'they', 'belong', 'to', 'different', 'categories.\\n\\n&gt;', 'It', 'has', 'the', 'comical', 'and', 'action', 'hooks', 'to', 'keep', 'a', 'child', 'interested', 'while', 'being', 'layered', 'with', 'themes', 'of', 'Eastern', 'spirituality', 'and', 'mirrorings', 'of', 'Eastern', 'mythology,', 'all', 'while', 'being', 'a', 'story', 'primarily', 'about', 'self', 'improvment', 'and', 'facing', 'the', 'world', 'with', 'the', 'optimism', 'that', 'you', 'CAN', 'do', 'what', 'you', 'set', 'out', 'to', 'acheive.\\n\\nTypos', 'apart,', 'do', 'you', 'define', 'a', 'work', 'art', 'on', 'the', 'basis', 'on', 'whether', \"it's\", 'interesting,', 'entertaining', 'or', 'positive?', 'Genuine', 'question,', 'no', 'sarcasm.', 'Because', 'to', 'me', \"you're\", 'simply', 'saying', 'that', 'Dragon', 'Ball', 'is', 'nice', 'to', 'read,', 'which', 'I', 'agree', 'on.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'lit', 'mang', 'cant', 'comp', 'two\\n\\ntrucks', 'pencil', 'say', 'truck', 'av', 'heavy', 'pencil', 'sur', 'would', 'say', 'poss', 'comp', 'two', 'aspect', 'thing', 'simply', 'belong', 'diff', 'categories\\n\\ngt', 'com', 'act', 'hook', 'keep', 'child', 'interest', 'lay', 'them', 'eastern', 'spirit', 'mir', 'eastern', 'mytholog', 'story', 'prim', 'self', 'improv', 'fac', 'world', 'optim', 'set', 'acheive\\n\\ntypos', 'apart', 'defin', 'work', 'art', 'bas', 'wheth', 'interest', 'entertain', 'posit', 'genuin', 'quest', 'sarcasm', 'yo', 'simply', 'say', 'dragon', 'bal', 'nic', 'read', 'agr'], ['gt', 'literature', 'manga', 'cant', 'compare', 'two\\n\\ntrucks', 'pencil', 'say', 'truck', 'average', 'heavier', 'pencil', 'sure', 'would', 'say', 'possible', 'compare', 'two', 'aspect', 'things', 'simply', 'belong', 'different', 'categories\\n\\ngt', 'comical', 'action', 'hook', 'keep', 'child', 'interest', 'layer', 'theme', 'eastern', 'spirituality', 'mirror', 'eastern', 'mythology', 'story', 'primarily', 'self', 'improvment', 'face', 'world', 'optimism', 'set', 'acheive\\n\\ntypos', 'apart', 'define', 'work', 'art', 'basis', 'whether', 'interest', 'entertain', 'positive', 'genuine', 'question', 'sarcasm', 'youre', 'simply', 'say', 'dragon', 'ball', 'nice', 'read', 'agree'])\n",
      "original document: \n",
      "['Tasty!', 'Love', 'to', 'see', 'more']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tasty', 'lov', 'see'], ['tasty', 'love', 'see'])\n",
      "original document: \n",
      "['Either', 'way', 'maybe', 'I', 'should', 'bough', 'out.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eith', 'way', 'mayb', 'bough'], ['either', 'way', 'maybe', 'bough'])\n",
      "original document: \n",
      "['His', 'ticket', 'literally', 'requires', 'time', 'travel', 'to', 'close', 'with', 'a', 'solution.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ticket', 'lit', 'requir', 'tim', 'travel', 'clos', 'solv'], ['ticket', 'literally', 'require', 'time', 'travel', 'close', 'solution'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnon5q1/):\\n\\nI', 'got', 'my', 'ISBN', 'number', 'through', 'them', 'when', 'I', 'first', 'finished', 'the', 'book.', 'My', 'parents', 'made', 'my', 'very', 'rough', 'first', 'draft', 'into', 'a', 'physical', 'copy', 'to', 'surprise', 'me.', 'The', 'copy', 'you', 'see', 'there', 'I', 'did', 'not', 'pay', 'to', 'print.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnon5q1\\n\\ni', 'got', 'isbn', 'numb', 'first', 'fin', 'book', 'par', 'mad', 'rough', 'first', 'draft', 'phys', 'cop', 'surpr', 'cop', 'see', 'pay', 'print'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnon5q1\\n\\ni', 'get', 'isbn', 'number', 'first', 'finish', 'book', 'parent', 'make', 'rough', 'first', 'draft', 'physical', 'copy', 'surprise', 'copy', 'see', 'pay', 'print'])\n",
      "original document: \n",
      "['She', 'got', 'sent', 'home', 'before', 'filming', 'started.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['got', 'sent', 'hom', 'film', 'start'], ['get', 'send', 'home', 'film', 'start'])\n",
      "original document: \n",
      "['He', 'is', 'bringing', 'you', 'on', 'his', 'dates', 'with', 'HER,', 'not', 'the', 'other', 'way', 'round,', 'quite', 'frankly.', 'If', 'all', 'you', 'do', 'is', 'sit', 'there', 'and', 'watch', 'him', 'talk', 'at', 'her', 'all', 'night', 'about', 'himself,', \"you're\", 'a', 'part', 'of', 'the', 'audience,', 'not', 'the', 'participants.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bring', 'dat', 'way', 'round', 'quit', 'frank', 'sit', 'watch', 'talk', 'night', 'yo', 'part', 'audy', 'particip'], ['bring', 'date', 'way', 'round', 'quite', 'frankly', 'sit', 'watch', 'talk', 'night', 'youre', 'part', 'audience', 'participants'])\n",
      "original document: \n",
      "['Wow.', 'WTF.', 'I', 'really', 'don’t', 'know', 'how', 'something', 'like', 'this', 'was', 'justified', 'back', 'in', 'the', 'day.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'wtf', 'real', 'dont', 'know', 'someth', 'lik', 'just', 'back', 'day'], ['wow', 'wtf', 'really', 'dont', 'know', 'something', 'like', 'justify', 'back', 'day'])\n",
      "original document: \n",
      "[\"That's\", 'actually', 'a', 'really', 'awesome', 'observation!', 'I', 'also', 'agree', 'with', 'it!', '\\n\\nExtroverted?', 'Maybe', 'do', 'more', 'portrait', 'shots.', 'Introverted,', 'maybe', 'some', 'landscapes', 'and', 'scenary', 'or', 'even', 'wildlife']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'act', 'real', 'awesom', 'observ', 'also', 'agr', '\\n\\nextroverted', 'mayb', 'portrait', 'shot', 'introvert', 'mayb', 'landscap', 'scen', 'ev', 'wildl'], ['thats', 'actually', 'really', 'awesome', 'observation', 'also', 'agree', '\\n\\nextroverted', 'maybe', 'portrait', 'shots', 'introvert', 'maybe', 'landscape', 'scenary', 'even', 'wildlife'])\n",
      "original document: \n",
      "['BBQ', 'is', 'useless', 'without', 'slaw', 'and', 'beans,', \"don't\", 'forget', 'the', 'beans.', '', 'I', 'also', 'like', 'slaw', 'on', 'cheeseburgers', 'sloppy', 'joes', 'and', 'fish', 'sandwiches.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bbq', 'useless', 'without', 'slaw', 'bean', 'dont', 'forget', 'bean', 'also', 'lik', 'slaw', 'cheeseburg', 'sloppy', 'joe', 'fish', 'sandwich'], ['bbq', 'useless', 'without', 'slaw', 'bean', 'dont', 'forget', 'bean', 'also', 'like', 'slaw', 'cheeseburgers', 'sloppy', 'joes', 'fish', 'sandwich'])\n",
      "original document: \n",
      "['As', 'a', 'delivery', 'driver', '2s', 'are', 'always', 'nice', 'to', 'see.', 'Get', 'about', '1', 'a', 'month.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delivery', 'driv', '2s', 'alway', 'nic', 'see', 'get', 'on', 'mon'], ['delivery', 'driver', '2s', 'always', 'nice', 'see', 'get', 'one', 'month'])\n",
      "original document: \n",
      "['&gt;the', 'only', 'cases', 'where', 'a', 'consumer', 'would', 'not', 'have', 'an', 'alternative', 'would', 'be', 'a', 'natural', \"monopoly\\n\\n&gt;That's\", 'naive.\\n\\nExactly,', 'a', 'natural', 'monopoly', \"isn't\", 'the', 'only', 'case.', 'You', 'could', 'be', 'in', 'a', 'market', 'where', \"there's\", 'not', 'enough', 'demand', 'for', 'competition,', 'like', 'in', 'a', 'country', 'town', 'where', \"there's\", 'only', 'one', 'supermarket', 'or', 'bakery', 'in', 'an', 'hour', 'drive.\\n\\nNo', 'amount', 'of', 'tax', 'cuts', 'and', 'removing', 'the', 'minimum', 'wage', 'is', 'going', 'to', 'generate', 'demand', 'in', 'places', 'it', \"doesn't\", 'exist.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthe', 'cas', 'consum', 'would', 'altern', 'would', 'nat', 'monopoly\\n\\ngtthats', 'naive\\n\\nexactly', 'nat', 'monopo', 'isnt', 'cas', 'could', 'market', 'ther', 'enough', 'demand', 'competit', 'lik', 'country', 'town', 'ther', 'on', 'supermarket', 'bakery', 'hour', 'drive\\n\\nno', 'amount', 'tax', 'cut', 'remov', 'minim', 'wag', 'going', 'gen', 'demand', 'plac', 'doesnt', 'ex'], ['gtthe', 'case', 'consumer', 'would', 'alternative', 'would', 'natural', 'monopoly\\n\\ngtthats', 'naive\\n\\nexactly', 'natural', 'monopoly', 'isnt', 'case', 'could', 'market', 'theres', 'enough', 'demand', 'competition', 'like', 'country', 'town', 'theres', 'one', 'supermarket', 'bakery', 'hour', 'drive\\n\\nno', 'amount', 'tax', 'cut', 'remove', 'minimum', 'wage', 'go', 'generate', 'demand', 'place', 'doesnt', 'exist'])\n",
      "original document: \n",
      "['Get', 'TO', '.500']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'five hundred'], ['get', 'five hundred'])\n",
      "original document: \n",
      "['Try', 'Google', 'asshole.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'googl', 'asshol'], ['try', 'google', 'asshole'])\n",
      "original document: \n",
      "[\"that's\", 'understandable,', 'there', 'seems', 'to', 'be', 'a', 'stigma', 'about', 'meeting', 'your', 'SO', 'online??', 'But', 'yeah', 'not', 'a', 'lie', 'heh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'understand', 'seem', 'stigm', 'meet', 'onlin', 'yeah', 'lie', 'heh'], ['thats', 'understandable', 'seem', 'stigma', 'meet', 'online', 'yeah', 'lie', 'heh'])\n",
      "original document: \n",
      "['Your', 'submission', 'or', 'comment', 'was', 'removed', 'for', 'the', 'following', 'reason(s):\\n\\n\\n---\\n\\n---\\n\\n**[Rule', '6:', 'Post', 'Formatting](https://www.reddit.com/r/TumblrInAction/wiki/the_tia_rulebook#wiki_rule_6.3A_post_formatting)**\\n\\n**Screenshots,', 'if', 'used,', 'must', 'be', 'full', 'post', 'and', 'full', 'context.**', '\\n\\n**Just', 'posting', 'the', 'headline', 'of', 'an', 'article', 'will', 'get', 'your', 'post', 'removed**.', 'This', 'includes', 'someone', 'sharing', 'a', 'link', 'on', 'social', 'media', 'without', 'any', 'discussion', 'on', 'that', 'post.\\n\\nWe', 'should', 'be', 'able', 'to', 'see', 'the', 'post', 'in', 'question', 'as', 'well', 'as', 'any', 'necessary', 'context', 'to', 'understand', \"what's\", 'being', 'discussed.\\n\\nYou', 'may', 'censor', 'any', 'blog', 'names', 'to', 'protect', 'the', 'bloggers,', 'but', 'if', 'the', 'post', 'looks', 'like', 'a', 'troll', 'it', 'will', 'be', 'removed', 'and', 'you', 'will', 'be', 'asked', 'to', '**send', 'the', 'mods', 'a', 'link', 'to', 'the', 'blog', 'for', 'verification**.\\n\\n\\n---\\n\\n\\n\\n---\\n\\nIf', 'you', 'have', 'any', 'questions', 'or', 'comments', 'about', 'this', 'action,', '**Use', 'this', 'link', 'to', 'send', 'us', 'a', 'mod', 'mail', 'message**', '[here](https://www.reddit.com/message/compose?to=%2Fr%2FTumblrInAction&amp;subject=About', 'my', 'removed', \"submission&amp;message=I'm\", 'writing', 'to', 'you', 'about', 'the', 'following', 'removal:', 'https://www.reddit.com/r/TumblrInAction/comments/73gf71/say_one_of_these_in_public_i_dare_you/.', '%0D%0DMy', 'issue', 'is:).', '\\n\\n', '\\n**Any', 'PM', 'sent', 'to', 'individual', 'mods', 'about', 'this', 'action', 'will', 'likely', 'be', 'ignored.', '', 'Mod', 'mail', 'is', 'the', 'proper', 'channel', 'and', 'will', 'be', 'your', 'best', 'bet', 'for', 'any', 'appeals,', 'so', 'please', 'use', 'the', 'link', 'above.**']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'com', 'remov', 'follow', 'reasons\\n\\n\\n\\n\\n\\n\\nrule', 'six', 'post', 'formattinghttpswwwredditcomrtumblrinactionwikithe_tia_rulebookwiki_rule_63a_post_formatting\\n\\nscreenshots', 'us', 'must', 'ful', 'post', 'ful', 'context', '\\n\\njust', 'post', 'headlin', 'artic', 'get', 'post', 'remov', 'includ', 'someon', 'shar', 'link', 'soc', 'med', 'without', 'discuss', 'post\\n\\nwe', 'abl', 'see', 'post', 'quest', 'wel', 'necess', 'context', 'understand', 'what', 'discussed\\n\\nyou', 'may', 'cens', 'blog', 'nam', 'protect', 'blog', 'post', 'look', 'lik', 'trol', 'remov', 'ask', 'send', 'mod', 'link', 'blog', 'verification\\n\\n\\n\\n\\n\\n\\n\\n\\nif', 'quest', 'com', 'act', 'us', 'link', 'send', 'us', 'mod', 'mail', 'mess', 'herehttpswwwredditcommessagecomposeto2fr2ftumblrinactionampsubjectabout', 'remov', 'submissionampmessageim', 'writ', 'follow', 'remov', 'httpswwwredditcomrtumblrinactioncomments73gf71say_one_of_these_in_public_i_dare_you', '0d0dmy', 'issu', '\\n\\n', '\\nany', 'pm', 'sent', 'individ', 'mod', 'act', 'lik', 'ign', 'mod', 'mail', 'prop', 'channel', 'best', 'bet', 'ap', 'pleas', 'us', 'link'], ['submission', 'comment', 'remove', 'follow', 'reasons\\n\\n\\n\\n\\n\\n\\nrule', 'six', 'post', 'formattinghttpswwwredditcomrtumblrinactionwikithe_tia_rulebookwiki_rule_63a_post_formatting\\n\\nscreenshots', 'use', 'must', 'full', 'post', 'full', 'context', '\\n\\njust', 'post', 'headline', 'article', 'get', 'post', 'remove', 'include', 'someone', 'share', 'link', 'social', 'media', 'without', 'discussion', 'post\\n\\nwe', 'able', 'see', 'post', 'question', 'well', 'necessary', 'context', 'understand', 'whats', 'discussed\\n\\nyou', 'may', 'censor', 'blog', 'name', 'protect', 'bloggers', 'post', 'look', 'like', 'troll', 'remove', 'ask', 'send', 'mods', 'link', 'blog', 'verification\\n\\n\\n\\n\\n\\n\\n\\n\\nif', 'question', 'comment', 'action', 'use', 'link', 'send', 'us', 'mod', 'mail', 'message', 'herehttpswwwredditcommessagecomposeto2fr2ftumblrinactionampsubjectabout', 'remove', 'submissionampmessageim', 'write', 'follow', 'removal', 'httpswwwredditcomrtumblrinactioncomments73gf71say_one_of_these_in_public_i_dare_you', '0d0dmy', 'issue', '\\n\\n', '\\nany', 'pm', 'send', 'individual', 'mods', 'action', 'likely', 'ignore', 'mod', 'mail', 'proper', 'channel', 'best', 'bet', 'appeal', 'please', 'use', 'link'])\n",
      "original document: \n",
      "['This', 'is', 'interesting', ':0']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'zero'], ['interest', 'zero'])\n",
      "original document: \n",
      "['Way', 'to', 'cash', 'in', 'on', 'some', 'random', \"lady's\", 'grief.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'cash', 'random', 'lady', 'grief'], ['way', 'cash', 'random', 'ladys', 'grief'])\n",
      "original document: \n",
      "['20', 'minutes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty', 'minut'], ['twenty', 'minutes'])\n",
      "original document: \n",
      "['522']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['five hundred and twenty-two'], ['five hundred and twenty-two'])\n",
      "original document: \n",
      "['Yet', \"it's\", 'the', 'richest', 'city', 'in', 'the', 'US', 'and', 'homeless', 'rule', 'the', 'streets.', 'Wtf.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yet', 'richest', 'city', 'us', 'homeless', 'rul', 'streets', 'wtf'], ['yet', 'richest', 'city', 'us', 'homeless', 'rule', 'streets', 'wtf'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Votar', 'tem', 'um', 'custo', 'de', 'oportunidade', 'elevado.\\n\\n\\nTem', 'o', 'deslocamento,', 'a', 'preparação', 'do', 'mesmo,', 'a', 'fila,', 'o', 'constrangimento', 'de', 'ter', 'lá', 'alguém', 'conhecido,', 'o', 'constrangimento', 'de', 'ter', 'de', 'tomar', 'uma', 'decisão', 'difícil,', 'a', 'obrigação', 'moral', 'de', 'não', 'votar', 'de', 'forma', 'desinformada...\\n\\nEu', 'irei', 'votar,', 'mas', 'mesmo', 'assim,', 'e', 'apesar', 'de', 'me', 'ter', 'esforçado', 'ao', 'máximo', 'para', 'estar', 'informado', 'sinto', 'que', 'o', 'meu', 'voto', 'é', 'um', 'bocado', 'inútil.', 'Não', 'percebo', 'o', 'suficiente', 'disto', 'para', 'ter', 'a', 'legitimidade', 'de', 'decidir...', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vot', 'tem', 'um', 'custo', 'de', 'oportunidad', 'elevado\\n\\n\\ntem', 'deslocamento', 'preparacao', 'mesmo', 'fil', 'constrangimento', 'de', 'ter', 'la', 'alguem', 'conhecido', 'constrangimento', 'de', 'ter', 'de', 'tom', 'um', 'decisao', 'dificil', 'obrigacao', 'mor', 'de', 'nao', 'vot', 'de', 'form', 'desinformada\\n\\neu', 'ire', 'vot', 'mas', 'mesmo', 'assim', 'e', 'apes', 'de', 'ter', 'esforcado', 'ao', 'maximo', 'par', 'est', 'informado', 'sinto', 'que', 'meu', 'voto', 'e', 'um', 'bocado', 'inutil', 'nao', 'percebo', 'suficy', 'disto', 'par', 'ter', 'legitimidad', 'de', 'decidir'], ['votar', 'tem', 'um', 'custo', 'de', 'oportunidade', 'elevado\\n\\n\\ntem', 'deslocamento', 'preparacao', 'mesmo', 'fila', 'constrangimento', 'de', 'ter', 'la', 'alguem', 'conhecido', 'constrangimento', 'de', 'ter', 'de', 'tomar', 'uma', 'decisao', 'dificil', 'obrigacao', 'moral', 'de', 'nao', 'votar', 'de', 'forma', 'desinformada\\n\\neu', 'irei', 'votar', 'mas', 'mesmo', 'assim', 'e', 'apesar', 'de', 'ter', 'esforcado', 'ao', 'maximo', 'para', 'estar', 'informado', 'sinto', 'que', 'meu', 'voto', 'e', 'um', 'bocado', 'inutil', 'nao', 'percebo', 'suficiente', 'disto', 'para', 'ter', 'legitimidade', 'de', 'decidir'])\n",
      "original document: \n",
      "['Having', 'read', 'both,', 'the', 'TMM', 'is', 'really', 'an', 'in', 'depth', 'guide', 'on', 'the', 'baby', 'steps,', 'while', 'CGM', 'covers', 'budgeting,', 'communication,', 'bargaining,', 'etc.', 'CGM', 'mentions', 'the', 'baby', 'steps,', 'but', 'focuses', 'more', 'on', 'the', 'bigger', 'picture', 'of', 'your', 'finances.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['read', 'tmm', 'real', 'dep', 'guid', 'baby', 'step', 'cgm', 'cov', 'budget', 'commun', 'bargain', 'etc', 'cgm', 'ment', 'baby', 'step', 'focus', 'big', 'pict', 'fin'], ['read', 'tmm', 'really', 'depth', 'guide', 'baby', 'step', 'cgm', 'cover', 'budget', 'communication', 'bargain', 'etc', 'cgm', 'mention', 'baby', 'step', 'focus', 'bigger', 'picture', 'finance'])\n",
      "original document: \n",
      "['I', 'think', 'a', 'lot', 'of', 'people', 'are', 'overrating', 'action', 'video', 'games', '(Overwatch)', 'due', 'to', 'personal', 'bias,', 'and', 'I', 'think', 'it', 'would', 'be', 'an', 'interesting', 'study', 'to', 'compare', 'this', 'to', 'logic-based', 'exercises.', 'I', 'think', 'the', 'areas', 'of', 'memory/learning', 'would', 'be', 'similar', 'even', 'given', 'the', 'different', 'sensory', 'info.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'lot', 'peopl', 'over', 'act', 'video', 'gam', 'overwatch', 'due', 'person', 'bia', 'think', 'would', 'interest', 'study', 'comp', 'logicbas', 'exerc', 'think', 'area', 'memorylearn', 'would', 'simil', 'ev', 'giv', 'diff', 'sens', 'info'], ['think', 'lot', 'people', 'overrate', 'action', 'video', 'game', 'overwatch', 'due', 'personal', 'bias', 'think', 'would', 'interest', 'study', 'compare', 'logicbased', 'exercise', 'think', 'areas', 'memorylearning', 'would', 'similar', 'even', 'give', 'different', 'sensory', 'info'])\n",
      "original document: \n",
      "['*\"And', 'with', 'that,', 'a', 'mighty', 'cheer', 'went', 'up', 'from', 'the', 'heroes', 'of\\nStamford.', '', 'They', 'had', 'banished', 'the', 'awful', 'WWF', 'name', 'forever,\\nbecause', 'it', 'was', 'haunted.', '', 'Now', \"let's\", 'all', 'celebrate', 'with', 'a', 'cool', 'steak', 'wrap.\"*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mighty', 'che', 'went', 'hero', 'of\\nstamford', 'ban', 'aw', 'wwf', 'nam', 'forever\\nbecause', 'haunt', 'let', 'celebr', 'cool', 'steak', 'wrap'], ['mighty', 'cheer', 'go', 'heroes', 'of\\nstamford', 'banish', 'awful', 'wwf', 'name', 'forever\\nbecause', 'haunt', 'let', 'celebrate', 'cool', 'steak', 'wrap'])\n",
      "original document: \n",
      "['Ahah', 'I', 'fell', 'in', 'love', 'with', 'it', 'at', 'first', 'sight', ';)', 'so', 'much', 'better', 'than', 'blue!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahah', 'fel', 'lov', 'first', 'sight', 'much', 'bet', 'blu'], ['ahah', 'fell', 'love', 'first', 'sight', 'much', 'better', 'blue'])\n",
      "original document: \n",
      "['Agreed', 'fuck', 'micro', 'transactions', 'never', 'defend', 'that', 'shit\\n\\n\\nEdit:', 'wow', 'you', 'guys', 'are', 'just', 'proving', 'my', 'point', 'thanks', 'for', 'accepting', 'them', 'btw']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'fuck', 'micro', 'transact', 'nev', 'defend', 'shit\\n\\n\\nedit', 'wow', 'guy', 'prov', 'point', 'thank', 'acceiv', 'btw'], ['agree', 'fuck', 'micro', 'transactions', 'never', 'defend', 'shit\\n\\n\\nedit', 'wow', 'guy', 'prove', 'point', 'thank', 'accept', 'btw'])\n",
      "original document: \n",
      "['I', 'wanna', 'say', 'it', 'was', 'from', '/u/neuralhandshake']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wann', 'say', 'uneuralhandshak'], ['wanna', 'say', 'uneuralhandshake'])\n",
      "original document: \n",
      "['Thank', 'you!', \"That's\", 'something', 'I', \"hadn't\", 'thought', 'of', 'before.', 'I', 'guess', 'I', 'want', 'to', 'be', 'able', 'to', 'think', 'I', 'beat', 'it', 'and', \"it's\", 'gone', 'forever,', 'but', \"it's\", 'not.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'that', 'someth', 'hadnt', 'thought', 'guess', 'want', 'abl', 'think', 'beat', 'gon', 'forev'], ['thank', 'thats', 'something', 'hadnt', 'think', 'guess', 'want', 'able', 'think', 'beat', 'go', 'forever'])\n",
      "original document: \n",
      "['Well', 'everyone', 'knows', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'everyon', 'know'], ['well', 'everyone', 'know'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Oh,', 'I', 'can', 'be', 'that', 'guy', 'without', 'even', '*trying*', 'but', 'yeah.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'guy', 'without', 'ev', 'try', 'yeah'], ['oh', 'guy', 'without', 'even', 'try', 'yeah'])\n",
      "original document: \n",
      "['Traffic', 'impact!', 'Hah.', '\\n\\nThose', 'people', 'are', 'coming', 'one', 'way', 'or', 'another,', 'the', 'question', 'is', 'do', 'you', 'want', 'them', 'in', 'higher', 'density', 'development', 'right', 'next', 'to', 'the', 'highway', 'or', 'spread', 'out', 'along', 'your', 'city', 'edge', 'in', 'lower', 'density', 'development?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['traff', 'impact', 'hah', '\\n\\nthose', 'peopl', 'com', 'on', 'way', 'anoth', 'quest', 'want', 'high', 'dens', 'develop', 'right', 'next', 'highway', 'spread', 'along', 'city', 'edg', 'low', 'dens', 'develop'], ['traffic', 'impact', 'hah', '\\n\\nthose', 'people', 'come', 'one', 'way', 'another', 'question', 'want', 'higher', 'density', 'development', 'right', 'next', 'highway', 'spread', 'along', 'city', 'edge', 'lower', 'density', 'development'])\n",
      "original document: \n",
      "['Wish', 'I', 'knew.', 'Never', 'seen', 'it', 'before', '.', 'It', 'was', 'sticky', 'and', \"wouldn't\", 'break', 'up.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wish', 'knew', 'nev', 'seen', 'sticky', 'wouldnt', 'break'], ['wish', 'know', 'never', 'see', 'sticky', 'wouldnt', 'break'])\n",
      "original document: \n",
      "['First', 'you', 'learn', 'lots', 'of', 'insane', 'ways', 'to', 'do', 'math', 'that', 'you', 'never', 'imagined', 'even', 'existed,', 'then', 'you', 'apply', 'it', 'in', 'every', 'class', 'you', 'take', 'after', 'that.', 'And', 'I', 'mean', 'every', 'class.', 'There', 'are', 'literally', 'no', 'easy', 'classes', 'in', 'engineering.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'learn', 'lot', 'ins', 'way', 'math', 'nev', 'imagin', 'ev', 'ex', 'apply', 'every', 'class', 'tak', 'mean', 'every', 'class', 'lit', 'easy', 'class', 'engin'], ['first', 'learn', 'lot', 'insane', 'ways', 'math', 'never', 'imagine', 'even', 'exist', 'apply', 'every', 'class', 'take', 'mean', 'every', 'class', 'literally', 'easy', 'class', 'engineer'])\n",
      "original document: \n",
      "['egg', 'confirmed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eg', 'confirm'], ['egg', 'confirm'])\n",
      "original document: \n",
      "[\"It's\", 'like', 'the', 'MSST', 'line', \"hasn't\", 'heard', 'people', 'cheer', 'before.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'msst', 'lin', 'hasnt', 'heard', 'peopl', 'che'], ['like', 'msst', 'line', 'hasnt', 'hear', 'people', 'cheer'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Srry', 'to', 'say', 'bud', 'but', 'that', 'isn’t', 'worth', '2,000', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['srry', 'say', 'bud', 'isnt', 'wor', 'two thousand'], ['srry', 'say', 'bud', 'isnt', 'worth', 'two thousand'])\n",
      "original document: \n",
      "['Ahhhh', 'she’s', 'so', 'cute!', 'My', 'two-month-old’s', 'first', 'game', 'will', 'be', 'October', '14', 'and', 'I’m', 'so', 'excited!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahhhh', 'she', 'cut', 'twomonthold', 'first', 'gam', 'octob', 'fourteen', 'im', 'excit'], ['ahhhh', 'shes', 'cute', 'twomontholds', 'first', 'game', 'october', 'fourteen', 'im', 'excite'])\n",
      "original document: \n",
      "['Well', 'you', 'need', 'to', 'find', 'a', 'place.', \"Don't\", 'put', 'other', \"people's\", 'lives', 'in', 'danger', 'because', 'you', 'want', 'to', 'get', 'high.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'nee', 'find', 'plac', 'dont', 'put', 'peopl', 'liv', 'dang', 'want', 'get', 'high'], ['well', 'need', 'find', 'place', 'dont', 'put', 'people', 'live', 'danger', 'want', 'get', 'high'])\n",
      "original document: \n",
      "['Yes', 'man!!!!', \"I've\", 'been', 'eyeing', 'them', 'for', 'years.', '😍']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'man', 'iv', 'ey', 'year'], ['yes', 'man', 'ive', 'eye', 'years'])\n",
      "original document: \n",
      "['Damn', 'that', 'sucks', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'suck'], ['damn', 'suck'])\n",
      "original document: \n",
      "['The', 'older', 'ones', 'on', 'Steam', \"don't\", 'have', 'uplay', 'afaik,', 'and', 'the', 'only', 'way', 'to', 'get', 'newer', 'assassins', 'creed', 'without', 'uplay', 'is', 'to', 'torrent', 'them', 'so', 'ubisoft', 'makes', 'it', 'difficult', 'for', 'me', 'to', 'pay', 'them', 'even', 'though', \"I'd\", 'like', 'to']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['old', 'on', 'steam', 'dont', 'uplay', 'afaik', 'way', 'get', 'new', 'assassin', 'cree', 'without', 'uplay', 'tor', 'ubisoft', 'mak', 'difficult', 'pay', 'ev', 'though', 'id', 'lik'], ['older', 'ones', 'steam', 'dont', 'uplay', 'afaik', 'way', 'get', 'newer', 'assassins', 'creed', 'without', 'uplay', 'torrent', 'ubisoft', 'make', 'difficult', 'pay', 'even', 'though', 'id', 'like'])\n",
      "original document: \n",
      "['Okay', 'well', 'the', 'best', 'advice', 'a', 'commish', 'could', 'get', 'is', 'to', 'let', 'your', 'league', 'be', 'as', 'organic', 'and', 'natural', 'as', 'possible.', '', \"Don't\", 'interfere,', 'and', 'if', 'other', 'league', 'mates', 'get', 'mad', 'at', 'you', 'and', 'are', 'like', '\"it\\'s', 'not', 'fair', 'you', 'should', 'veto\"', 'you', 'should', 'say', '\"that\\'s', 'your', 'own', 'fault', 'for', 'not', 'getting', 'a', 'trade', 'done', 'before', 'this', 'one', 'was', 'accepted\"\\n\\nBeing', 'a', 'commish', 'is', 'about', 'making', 'sure', 'rules', 'are', 'in', 'place', 'and', 'keeping', 'people', 'involved,', 'and', \"it's\", 'fun.', '', 'You', 'are', 'there', 'to', 'police', 'but', 'only', 'by', 'extreme', 'circumstance,', 'like', 'someone', 'abandoning', 'a', 'team', 'during', 'playoffs.', '', 'A', 'trade', 'mid', 'season?', \"That's\", 'nothing', 'strange,', 'got', 'to', 'let', 'it', 'happen.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'wel', 'best', 'adv', 'com', 'could', 'get', 'let', 'leagu', 'org', 'nat', 'poss', 'dont', 'interf', 'leagu', 'mat', 'get', 'mad', 'lik', 'fair', 'veto', 'say', 'that', 'fault', 'get', 'trad', 'don', 'on', 'accepted\\n\\nbeing', 'com', 'mak', 'sur', 'rul', 'plac', 'keep', 'peopl', 'involv', 'fun', 'pol', 'extrem', 'circumst', 'lik', 'someon', 'abandon', 'team', 'playoff', 'trad', 'mid', 'season', 'that', 'noth', 'strange', 'got', 'let', 'hap'], ['okay', 'well', 'best', 'advice', 'commish', 'could', 'get', 'let', 'league', 'organic', 'natural', 'possible', 'dont', 'interfere', 'league', 'mat', 'get', 'mad', 'like', 'fair', 'veto', 'say', 'thats', 'fault', 'get', 'trade', 'do', 'one', 'accepted\\n\\nbeing', 'commish', 'make', 'sure', 'rule', 'place', 'keep', 'people', 'involve', 'fun', 'police', 'extreme', 'circumstance', 'like', 'someone', 'abandon', 'team', 'playoffs', 'trade', 'mid', 'season', 'thats', 'nothing', 'strange', 'get', 'let', 'happen'])\n",
      "original document: \n",
      "['143412299|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lbsRXVZP)\\n\\ni', 'only', 'voted', 'for', 'trump,', 'which', 'i', 'now', 'regret', 'since', \"i'm\", 'woke', 'about', 'america\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, two hundred and ninety-nin', 'gt', 'unit', 'stat', 'anonym', 'id', 'lbsrxvzp\\n\\ni', 'vot', 'trump', 'regret', 'sint', 'im', 'wok', 'america\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, two hundred and ninety-nine', 'gt', 'unite', 'state', 'anonymous', 'id', 'lbsrxvzp\\n\\ni', 'vote', 'trump', 'regret', 'since', 'im', 'wake', 'america\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['\"I', 'was', 'expecting', 'something', 'better', 'but', 'THIS', 'will', 'do.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['expect', 'someth', 'bet'], ['expect', 'something', 'better'])\n",
      "original document: \n",
      "['he', 'looks', 'nothing', 'like', 'the', 'mascot,', 'what', 'is', 'this?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'noth', 'lik', 'mascot'], ['look', 'nothing', 'like', 'mascot'])\n",
      "original document: \n",
      "['I', 'suppose', \"you're\", 'right.', 'I', 'did', 'find', 'it', 'in', 'a', 'US', 'parking', 'lot', 'after', 'all.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suppos', 'yo', 'right', 'find', 'us', 'park', 'lot'], ['suppose', 'youre', 'right', 'find', 'us', 'park', 'lot'])\n",
      "original document: \n",
      "['Really?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real'], ['really'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['So', 'we', \"don't\", 'have', 'to', 'give', 'it', 'aids', 'names', 'like', 'calling', 'it', '\"the', 'ERFWQ\"', 'combo,', 'just', 'give', 'it', 'a', 'name.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'giv', 'aid', 'nam', 'lik', 'cal', 'erfwq', 'combo', 'giv', 'nam'], ['dont', 'give', 'aid', 'name', 'like', 'call', 'erfwq', 'combo', 'give', 'name'])\n",
      "original document: \n",
      "['Sweet', 'iced', 'tea', 'is', 'my', 'favorite', 'flavor.', 'Unfortunately,', 'I', \"haven't\", 'encountered', 'sweet', 'tea', 'ice', 'cream,', 'sweet', 'tea', 'shakes,', 'sweet', 'tea', 'malts,', 'or', 'sweet', 'tea', 'candy,', 'but', 'it', \"doesn't\", 'matter', 'because', 'cold', 'sweet', 'tea', 'is', 'the', 'best', 'thing', 'to', 'have', 'ever', 'graced', 'my', 'taste', 'buds.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sweet', 'ic', 'tea', 'favorit', 'flav', 'unfortun', 'hav', 'encount', 'sweet', 'tea', 'ic', 'cream', 'sweet', 'tea', 'shak', 'sweet', 'tea', 'malt', 'sweet', 'tea', 'candy', 'doesnt', 'mat', 'cold', 'sweet', 'tea', 'best', 'thing', 'ev', 'grac', 'tast', 'bud'], ['sweet', 'ice', 'tea', 'favorite', 'flavor', 'unfortunately', 'havent', 'encounter', 'sweet', 'tea', 'ice', 'cream', 'sweet', 'tea', 'shake', 'sweet', 'tea', 'malt', 'sweet', 'tea', 'candy', 'doesnt', 'matter', 'cold', 'sweet', 'tea', 'best', 'thing', 'ever', 'grace', 'taste', 'bud'])\n",
      "original document: \n",
      "['At', 'least', 'one', 'guy', 'is', 'working', 'on', 'it(discussions', 'in', 'slack', 'so', 'far),', 'and', 'someone', 'else', 'made', 'a', 'post', 'a', 'week', 'or', 'two', 'ago', 'about', 'one', 'alternative.', '', 'It', \"won't\", 'be', 'coming', 'with', 'the', 'first', 'hardfork', 'though,', 'it', 'will', 'be', 'a', 'ways', 'out.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'on', 'guy', 'work', 'itdiscuss', 'slack', 'far', 'someon', 'els', 'mad', 'post', 'week', 'two', 'ago', 'on', 'altern', 'wont', 'com', 'first', 'hardfork', 'though', 'way'], ['least', 'one', 'guy', 'work', 'itdiscussions', 'slack', 'far', 'someone', 'else', 'make', 'post', 'week', 'two', 'ago', 'one', 'alternative', 'wont', 'come', 'first', 'hardfork', 'though', 'ways'])\n",
      "original document: \n",
      "['Yep.', '', 'Bullying', 'went', 'from', 'an', 'ignored', 'problem,', 'to', 'the', 'diagnosis', 'for', 'every', 'sort', 'of', 'conflict', 'no', 'matter', 'the', 'degree', 'or', 'circumstance.', '\\n\\nFrankly,', \"I'm\", 'tired', 'of', 'hearing', 'the', 'word', 'bullying...', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'bul', 'went', 'ign', 'problem', 'diagnos', 'every', 'sort', 'conflict', 'mat', 'degr', 'circumst', '\\n\\nfrankly', 'im', 'tir', 'hear', 'word', 'bul'], ['yep', 'bully', 'go', 'ignore', 'problem', 'diagnosis', 'every', 'sort', 'conflict', 'matter', 'degree', 'circumstance', '\\n\\nfrankly', 'im', 'tire', 'hear', 'word', 'bully'])\n",
      "original document: \n",
      "['Anyone', 'got', 'links', 'to', 'the', 'last', '2', 'goals', 'and', 'the', 'scuffle', 'at', 'the', 'end?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'got', 'link', 'last', 'two', 'goal', 'scuffl', 'end'], ['anyone', 'get', 'link', 'last', 'two', 'goals', 'scuffle', 'end'])\n",
      "original document: \n",
      "['If', 'you', 'are', 'tightening', 'the', 'cooler', 'too', 'much,', 'it', 'will', 'make', 'the', 'paste', 'too', 'thin.', 'Alternatively', 'if', 'there', 'is', 'too', 'much', 'paste,', 'you', 'will', 'see', 'the', 'gpu', 'temp', 'rise', 'quickly', 'when', 'you', 'launch', 'a', 'game.\\n\\nYou', 'could', 'try', 'using', 'a', 'grease', 'instead', 'of', 'paste.', 'Noctua', 'paste,', 'Phobya', 'Hegrease,', 'or', 'Arctic', 'MX-4.', 'MX-4', 'over', 'arctic', 'silver', '5.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tight', 'cool', 'much', 'mak', 'past', 'thin', 'altern', 'much', 'past', 'see', 'gpu', 'temp', 'ris', 'quick', 'launch', 'game\\n\\nyou', 'could', 'try', 'us', 'greas', 'instead', 'past', 'noctu', 'past', 'phoby', 'hegreas', 'arct', 'mx4', 'mx4', 'arct', 'silv', 'fiv'], ['tighten', 'cooler', 'much', 'make', 'paste', 'thin', 'alternatively', 'much', 'paste', 'see', 'gpu', 'temp', 'rise', 'quickly', 'launch', 'game\\n\\nyou', 'could', 'try', 'use', 'grease', 'instead', 'paste', 'noctua', 'paste', 'phobya', 'hegrease', 'arctic', 'mx4', 'mx4', 'arctic', 'silver', 'five'])\n",
      "original document: \n",
      "['https://twitter.com/TomGulittiNHL/status/909458844505559041']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpstwittercomtomgulittinhlstatus909458844505559041'], ['httpstwittercomtomgulittinhlstatus909458844505559041'])\n",
      "original document: \n",
      "['the', 'fluid', 'simulation', 'is', 'really', 'nice!,', 'keep', 'it', 'up']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fluid', 'sim', 'real', 'nic', 'keep'], ['fluid', 'simulation', 'really', 'nice', 'keep'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Had', 'this', 'on', 'cassette', 'because', 'my', 'car', \"didn't\", 'have', 'a', 'CD', 'player.', 'I', 'ran', 'that', 'tape', 'into', 'the', 'fucking', 'ground', 'playing', 'it', 'so', 'much.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['casset', 'car', 'didnt', 'cd', 'play', 'ran', 'tap', 'fuck', 'ground', 'play', 'much'], ['cassette', 'car', 'didnt', 'cd', 'player', 'run', 'tape', 'fuck', 'grind', 'play', 'much'])\n",
      "original document: \n",
      "['[MRW', 'I', 'try', 'to', 'masturbate', 'to', 'this', 'as', 'a', 'black', 'guy.](https://media.giphy.com/media/xT9KVJLPn98SNxXEME/giphy.gif)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mrw', 'try', 'masturb', 'black', 'guyhttpsmediagiphycommediaxt9kvjlpn98snxxemegiphygif'], ['mrw', 'try', 'masturbate', 'black', 'guyhttpsmediagiphycommediaxt9kvjlpn98snxxemegiphygif'])\n",
      "original document: \n",
      "['I', 'think', 'you', 'just', 'get', 'the', 'regular', 'vaule', 'and', 'go', 'about', 'your', 'day', '', '\\nwhich', 'is', 'kinda', 'dissapointing', 'but', 'reasonable']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'get', 'regul', 'vaul', 'go', 'day', '\\nwhich', 'kind', 'dissapoint', 'reason'], ['think', 'get', 'regular', 'vaule', 'go', 'day', '\\nwhich', 'kinda', 'dissapointing', 'reasonable'])\n",
      "original document: \n",
      "['\"You', 'can', 'probably', 'expect', 'to', 'see', 'myself', 'and', 'some', 'other', 'humans', 'in', 'the', 'cider', 'line,\"', 'I', 'say,', 'taking', 'another', 'drink.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'expect', 'see', 'hum', 'cid', 'lin', 'say', 'tak', 'anoth', 'drink'], ['probably', 'expect', 'see', 'humans', 'cider', 'line', 'say', 'take', 'another', 'drink'])\n",
      "original document: \n",
      "['Your', 'uncle', 'must’ve', 'been', 'happy', 'you', 'finally', 'asked', 'him', 'to', 'homecoming']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unc', 'mustv', 'happy', 'fin', 'ask', 'homecom'], ['uncle', 'mustve', 'happy', 'finally', 'ask', 'homecoming'])\n",
      "original document: \n",
      "['I', \"don't\", 'see', 'them', 'being', 'debrief', 'on', 'that', 'information', 'since', 'they', 'was', 'only', 'tasked', 'to', 'find', 'Blue', 'Team.', 'Locke', 'may', 'known', 'since', 'he', 'was', 'a', 'high', 'ranking', 'ONI', 'agent,', 'but', 'kept', 'it', 'to', 'himself', 'unless', 'it', 'was', 'necessary.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'see', 'debrief', 'inform', 'sint', 'task', 'find', 'blu', 'team', 'lock', 'may', 'known', 'sint', 'high', 'rank', 'on', 'ag', 'kept', 'unless', 'necess'], ['dont', 'see', 'debrief', 'information', 'since', 'task', 'find', 'blue', 'team', 'locke', 'may', 'know', 'since', 'high', 'rank', 'oni', 'agent', 'keep', 'unless', 'necessary'])\n",
      "original document: \n",
      "['It', 'was', 'suppose', 'to.', 'Now', 'they', \"can't\", 'even', 'get', 'that', 'straight.', 'So', 'now,', \"it's\", 'cuz', 'Trump', 'durrrr.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suppos', 'cant', 'ev', 'get', 'straight', 'cuz', 'trump', 'durrr'], ['suppose', 'cant', 'even', 'get', 'straight', 'cuz', 'trump', 'durrrr'])\n",
      "original document: \n",
      "['[+peginus](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo18k/):\\n\\nOh', 'okay,', 'so', 'you', 'paid', 'to', 'publish', 'all', 'of', 'your', 'other', 'copies', 'except', 'the', 'one', \"you're\", 'holding?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peginushttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo18k\\n\\noh', 'okay', 'paid', 'publ', 'cop', 'exceiv', 'on', 'yo', 'hold'], ['peginushttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo18k\\n\\noh', 'okay', 'pay', 'publish', 'copy', 'except', 'one', 'youre', 'hold'])\n",
      "original document: \n",
      "['Because', 'the', 'same', 'number', 'of', 'people', \"don't\", 'play', 'each', 'class.', 'You', 'could', 'have', '8', 'classes', 'with', '100%', 'winrate', 'and', 'one', 'class', 'with', 'just', 'under', '50%', 'winrate', 'if', '(literally)', 'almost', 'everyone', 'plays', 'that', 'one', 'class.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['numb', 'peopl', 'dont', 'play', 'class', 'could', 'eight', 'class', 'one hundred', 'winr', 'on', 'class', 'fifty', 'winr', 'lit', 'almost', 'everyon', 'play', 'on', 'class'], ['number', 'people', 'dont', 'play', 'class', 'could', 'eight', 'class', 'one hundred', 'winrate', 'one', 'class', 'fifty', 'winrate', 'literally', 'almost', 'everyone', 'play', 'one', 'class'])\n",
      "original document: \n",
      "['I', 'don’t', 'dislike', 'her', 'for', 'casually', 'dating', 'because', 'that’s', 'what', 'lots', 'of', 'people', 'do.', 'But', 'I', 'dislike', 'her', 'for', 'calling', 'Taishi', 'out', 'on', 'the', 'very', 'thing', 'she', 'is', 'doing!', 'She', 'half', 'assed', 'her', 'talk', 'with', 'Eric', 'about', 'seeing', 'other', 'guys.', 'Though', 'Taishi', 'isn’t', 'better', 'by', 'going', 'cold', 'turkey', 'with', 'Anna', 'and', 'Niki', 'previously,', 'at', 'least', 'he', 'managed', 'to', 'change', 'when', 'he', 'got', 'called', 'out.\\n\\nAnd', 'her', 'Instagram', 'posts.', 'GIRL.', 'If', 'you’re', 'not', 'bothered', 'with', 'the', 'comments', 'you’re', 'getting,', 'why', 'keep', 'making', 'posts', 'about', 'them?', 'I', 'know', 'she', 'should', 'defend', 'herself', 'but', 'for', 'someone', 'who', 'doesn’t', 'know', 'how', 'reality', 'show', 'works,', 'Cheri', 'has', 'been', 'very', 'careless', 'on', 'how', 'she', 'presented', 'herself.', 'And', 'this', 'is', 'a', 'show', 'without', 'a', 'script', 'so', 'most', 'people', 'perceive', 'it', 'to', 'be', '100%', 'real.', 'So', 'that', 'makes', 'her', 'look', 'really', 'off', 'to', 'the', 'Japanese', 'and', 'Asian', 'viewers,', 'the', 'majority', 'of', 'the', 'Terrace', 'House', 'audience.\\n\\nThat’s', 'just', 'my', 'two', 'cents', 'on', 'her.', 'I', 'just', 'think', 'she’s', 'very', 'hypocritical', 'and', 'obviously', 'so', 'sensitive', 'to', 'the', 'criticism', 'even', 'if', 'she', 'claims', 'she’s', 'stronger', 'than', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'dislik', 'cas', 'dat', 'that', 'lot', 'peopl', 'dislik', 'cal', 'taish', 'thing', 'half', 'ass', 'talk', 'er', 'see', 'guy', 'though', 'taish', 'isnt', 'bet', 'going', 'cold', 'turkey', 'ann', 'nik', 'prevy', 'least', 'man', 'chang', 'got', 'cal', 'out\\n\\nand', 'instagram', 'post', 'girl', 'yo', 'both', 'com', 'yo', 'get', 'keep', 'mak', 'post', 'know', 'defend', 'someon', 'doesnt', 'know', 'real', 'show', 'work', 'cher', 'careless', 'pres', 'show', 'without', 'script', 'peopl', 'perceiv', 'one hundred', 'real', 'mak', 'look', 'real', 'japanes', 'as', 'view', 'maj', 'terrac', 'hous', 'audience\\n\\nthats', 'two', 'cent', 'think', 'she', 'hypocrit', 'obvy', 'sensit', 'crit', 'ev', 'claim', 'she', 'stronger'], ['dont', 'dislike', 'casually', 'date', 'thats', 'lot', 'people', 'dislike', 'call', 'taishi', 'thing', 'half', 'assed', 'talk', 'eric', 'see', 'guy', 'though', 'taishi', 'isnt', 'better', 'go', 'cold', 'turkey', 'anna', 'niki', 'previously', 'least', 'manage', 'change', 'get', 'call', 'out\\n\\nand', 'instagram', 'post', 'girl', 'youre', 'bother', 'comment', 'youre', 'get', 'keep', 'make', 'post', 'know', 'defend', 'someone', 'doesnt', 'know', 'reality', 'show', 'work', 'cheri', 'careless', 'present', 'show', 'without', 'script', 'people', 'perceive', 'one hundred', 'real', 'make', 'look', 'really', 'japanese', 'asian', 'viewers', 'majority', 'terrace', 'house', 'audience\\n\\nthats', 'two', 'cents', 'think', 'shes', 'hypocritical', 'obviously', 'sensitive', 'criticism', 'even', 'claim', 'shes', 'stronger'])\n",
      "original document: \n",
      "['That', 'would', 'just', 'segregate', 'the', 'player', 'base', '*even', 'more*,', 'and', 'make', 'finding', 'a', 'match', 'even', 'harder.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'segreg', 'play', 'bas', 'ev', 'mak', 'find', 'match', 'ev', 'hard'], ['would', 'segregate', 'player', 'base', 'even', 'make', 'find', 'match', 'even', 'harder'])\n",
      "original document: \n",
      "[\"[That's\", 'cute.](http://www.thecanadianencyclopedia.ca/en/article/parizeaus-lobster-flap/)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'cutehttpwwwthecanadianencyclopediacaenarticleparizeauslobsterflap'], ['thats', 'cutehttpwwwthecanadianencyclopediacaenarticleparizeauslobsterflap'])\n",
      "original document: \n",
      "['you', 'sound', 'so', 'entitled,', 'you', 'dont', 'need', 'the', 'skin', 'for', '', 'play', 'you', 'know?you', 'like', 'always', 'only', 'wanna', 'free', 'stuff', 'only']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'entitl', 'dont', 'nee', 'skin', 'play', 'knowyou', 'lik', 'alway', 'wann', 'fre', 'stuff'], ['sound', 'entitle', 'dont', 'need', 'skin', 'play', 'knowyou', 'like', 'always', 'wanna', 'free', 'stuff'])\n",
      "original document: \n",
      "['I', 'love', 'the', 'bubble', 'blower!', 'It', 'can', 'be', 'used', 'as', 'a', 'diversion.', 'It', 'can', 'be', 'an', 'offensive', 'weapon.', 'It', 'can', 'shield', 'you', 'from', 'enemies.', 'It', 'can', 'help', 'control', 'certain', 'areas.', 'I', 'have', 'special', 'power', 'up', 'so', 'the', 'bubbles', 'are', 'huge!', 'I', 'find', 'it', 'to', 'be', 'a', 'fantastic', 'special', 'just', 'got', 'to', 'know', 'when', 'to', 'use', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'bubbl', 'blow', 'us', 'divert', 'offend', 'weapon', 'shield', 'enemy', 'help', 'control', 'certain', 'area', 'spec', 'pow', 'bubbl', 'hug', 'find', 'fantast', 'spec', 'got', 'know', 'us'], ['love', 'bubble', 'blower', 'use', 'diversion', 'offensive', 'weapon', 'shield', 'enemies', 'help', 'control', 'certain', 'areas', 'special', 'power', 'bubble', 'huge', 'find', 'fantastic', 'special', 'get', 'know', 'use'])\n",
      "original document: \n",
      "[\"Pm'ed.\", 'Thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pmed', 'thank'], ['pmed', 'thank'])\n",
      "original document: \n",
      "['I', 'told', 'him', 'I', \"didn't\", 'want', 'to', 'be', 'too', 'intimate', 'because', \"we're\", 'not', 'officially', 'a', 'couple', 'so', 'I', 'did', 'mention', 'it', 'to', 'him', \"that's\", 'the', 'thing.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['told', 'didnt', 'want', 'intim', 'off', 'coupl', 'ment', 'that', 'thing'], ['tell', 'didnt', 'want', 'intimate', 'officially', 'couple', 'mention', 'thats', 'thing'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['At', 'least', 'go', 'through', \"Milton's\", '\"Free', 'to', 'Choose\"', 'video', 'series.\\n\\nhttps://www.youtube.com/watch?v=D3N2sNnGwa4']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'go', 'milton', 'fre', 'choos', 'video', 'series\\n\\nhttpswwwyoutubecomwatchvd3n2snngwa4'], ['least', 'go', 'miltons', 'free', 'choose', 'video', 'series\\n\\nhttpswwwyoutubecomwatchvd3n2snngwa4'])\n",
      "original document: \n",
      "['For', 'me', 'success', 'is', 'more', 'about', 'autonomy,', 'integrity,', 'and', 'people.', \"I'll\", 'never', 'become', 'a', 'millionaire', 'doing', 'construction,', 'but', 'if', 'I', 'have', 'some', 'say', 'in', 'my', 'work,', \"I'm\", 'proud', 'of', 'what', 'I', 'do,', 'and', 'I', 'spend', 'my', 'days', 'around', 'good', 'people', 'then', 'I', 'consider', 'myself', 'successful.\\n\\nKnowing', 'that', 'I', 'have', 'options', 'and', 'can', 'afford', 'to', 'speak', 'my', 'mind,', 'set', 'boundaries,', 'and', 'work', 'under', 'fair', 'conditions', 'means', 'a', 'lot', 'to', 'me.', 'More', 'so', 'than,', 'say,', 'a', '10', 'or', '15%', 'pay', 'bump.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['success', 'autonom', 'integr', 'peopl', 'il', 'nev', 'becom', 'millionair', 'construct', 'say', 'work', 'im', 'proud', 'spend', 'day', 'around', 'good', 'peopl', 'consid', 'successful\\n\\nknowing', 'opt', 'afford', 'speak', 'mind', 'set', 'bound', 'work', 'fair', 'condit', 'mean', 'lot', 'say', 'ten', 'fifteen', 'pay', 'bump'], ['success', 'autonomy', 'integrity', 'people', 'ill', 'never', 'become', 'millionaire', 'construction', 'say', 'work', 'im', 'proud', 'spend', 'days', 'around', 'good', 'people', 'consider', 'successful\\n\\nknowing', 'options', 'afford', 'speak', 'mind', 'set', 'boundaries', 'work', 'fair', 'condition', 'mean', 'lot', 'say', 'ten', 'fifteen', 'pay', 'bump'])\n",
      "original document: \n",
      "['One', 'of', 'the', 'few', 'regrets', 'I', 'have', 'about', 'college', '', 'is', 'not', 'going', 'to', 'Blacksburg', 'for', 'gameday', 'while', 'I', 'had', 'buddies', 'at', 'VT.', 'Enter', 'Sandman', 'is', 'fucking', 'iconic']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'regret', 'colleg', 'going', 'blacksburg', 'gameday', 'buddy', 'vt', 'ent', 'sandm', 'fuck', 'icon'], ['one', 'regret', 'college', 'go', 'blacksburg', 'gameday', 'buddies', 'vt', 'enter', 'sandman', 'fuck', 'iconic'])\n",
      "original document: \n",
      "['its', 'extrapolated', 'via', 'syringe']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['extrapol', 'via', 'syr'], ['extrapolate', 'via', 'syringe'])\n",
      "original document: \n",
      "['I', 'Googled', 'a', 'bit,', 'but', \"I'm\", 'not', 'sure', 'of', 'specific', 'programs', 'or', 'campaigns.', '', '\\n\\nYou', 'could', 'contact', 'the', '[Alabama', 'ACLU](https://www.aclualabama.org/en/voting-rights-restoration)', 'or', 'the', '[Alabama', 'Democratic', 'Party](http://www.aldemocrats.org/)', 'and', 'maybe', \"they'd\", 'be', 'able', 'to', 'find', 'something', 'for', 'you', 'to', 'help', 'with.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['googl', 'bit', 'im', 'sur', 'spec', 'program', 'campaign', '\\n\\nyou', 'could', 'contact', 'alabam', 'acluhttpswwwaclualabamaorgenvotingrightsrest', 'alabam', 'democr', 'partyhttpwwwaldemocratsorg', 'mayb', 'theyd', 'abl', 'find', 'someth', 'help'], ['google', 'bite', 'im', 'sure', 'specific', 'program', 'campaign', '\\n\\nyou', 'could', 'contact', 'alabama', 'acluhttpswwwaclualabamaorgenvotingrightsrestoration', 'alabama', 'democratic', 'partyhttpwwwaldemocratsorg', 'maybe', 'theyd', 'able', 'find', 'something', 'help'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['As', 'a', 'married', 'poly', 'person', 'with', 'children,', 'I', 'would', 'honestly', 'have', 'my', 'brakes', 'screeching', 'right', 'now.', 'Being', 'poly', 'is', 'all', 'about', 'communication', 'and', 'trust.', 'You', 'laid', 'out', 'rules', 'that', 'were', 'agreed', 'to', 'and', 'not', 'even', 'a', 'week', 'later', 'and', \"she's\", 'already', 'dismissed', 'your', 'comfort', 'and', 'boundaries?', '\\n\\nNo.', 'Just', 'no.', 'If', 'My', 'husband', 'had', 'done', 'that', 'to', 'me', 'I', 'would', 'immediately', 'rescind', 'the', 'poly', 'permission', 'and', 'spend', 'some', 'time', 'focusing', 'on', '*us*.', 'Maybe', \"that's\", 'harsh,', 'but', 'from', 'what', 'I', 'can', 'see', 'your', 'wife', 'is', 'going', 'to', 'ignore', 'your', 'needs,', 'and', 'use', 'your', 'permission', 'to', 'be', 'poly', 'as', 'an', 'excuse', 'to', 'do', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['marry', 'poly', 'person', 'childr', 'would', 'honest', 'brak', 'screeching', 'right', 'poly', 'commun', 'trust', 'laid', 'rul', 'agree', 'ev', 'week', 'lat', 'she', 'already', 'dismiss', 'comfort', 'bound', '\\n\\nno', 'husband', 'don', 'would', 'immedy', 'rescind', 'poly', 'permit', 'spend', 'tim', 'focus', 'us', 'mayb', 'that', 'harsh', 'see', 'wif', 'going', 'ign', 'nee', 'us', 'permit', 'poly', 'excus'], ['marry', 'poly', 'person', 'children', 'would', 'honestly', 'brake', 'screech', 'right', 'poly', 'communication', 'trust', 'lay', 'rule', 'agree', 'even', 'week', 'later', 'shes', 'already', 'dismiss', 'comfort', 'boundaries', '\\n\\nno', 'husband', 'do', 'would', 'immediately', 'rescind', 'poly', 'permission', 'spend', 'time', 'focus', 'us', 'maybe', 'thats', 'harsh', 'see', 'wife', 'go', 'ignore', 'need', 'use', 'permission', 'poly', 'excuse'])\n",
      "original document: \n",
      "[\"How'd\", 'you', 'like', 'ME', 'andromeda?', '', '', \"It's\", 'super', 'cheap', 'now.', '', '', 'Heard', 'it', 'was', 'huge', 'disappointment.....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['howd', 'lik', 'andromed', 'sup', 'cheap', 'heard', 'hug', 'disappoint'], ['howd', 'like', 'andromeda', 'super', 'cheap', 'hear', 'huge', 'disappointment'])\n",
      "original document: \n",
      "['when', 'you', 'say', 'assited', 'boot,', 'i', 'assume', 'that', 'means', 'you', 'enter', 'the', 'bios/uefi,', 'from', 'which', 'you', 'can', 'set', 'the', 'boot', 'priority.', 'There', 'youll', 'have', 'the', 'option', 'to', 'set', 'grub/whatever', 'you', 'want', 'as', 'the', 'highest', 'priority.', 'Probably', 'the', 'most', 'important', 'step', 'is', 'that', 'you', 'remove', 'the', 'usb', 'with', 'the', 'iso,', 'once', 'the', 'install', 'is', 'complete', 'it', 'is', 'no', 'longer', 'needed', 'and', 'is', 'likely', 'confusing', 'you.', 'i', 'hope', 'that', 'helps', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'assit', 'boot', 'assum', 'mean', 'ent', 'biosuef', 'set', 'boot', 'pri', 'youl', 'opt', 'set', 'grubwhatev', 'want', 'highest', 'pri', 'prob', 'import', 'step', 'remov', 'usb', 'iso', 'instal', 'complet', 'long', 'nee', 'lik', 'confus', 'hop', 'help'], ['say', 'assited', 'boot', 'assume', 'mean', 'enter', 'biosuefi', 'set', 'boot', 'priority', 'youll', 'option', 'set', 'grubwhatever', 'want', 'highest', 'priority', 'probably', 'important', 'step', 'remove', 'usb', 'iso', 'install', 'complete', 'longer', 'need', 'likely', 'confuse', 'hope', 'help'])\n",
      "original document: \n",
      "['&gt;', 'There', 'are', 'just', 'financial', 'reasons', 'why', 'they', \"don't\\n\\nAnd\", 'player', 'contracts', 'too.', 'Dips', 'into', 'the', 'financials', 'of', 'course', 'but', \"it's\", 'a', 'lot', 'of', 'trouble', 'to', 'break', 'a', \"player's\", 'contract.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'fin', 'reason', 'dont\\n\\nand', 'play', 'contract', 'dip', 'fin', 'cours', 'lot', 'troubl', 'break', 'play', 'contract'], ['gt', 'financial', 'reason', 'dont\\n\\nand', 'player', 'contract', 'dip', 'financials', 'course', 'lot', 'trouble', 'break', 'players', 'contract'])\n",
      "original document: \n",
      "[':(', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Hmmmmm.', 'Understandable', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hmmmmm', 'understand'], ['hmmmmm', 'understandable'])\n",
      "original document: \n",
      "['Short', 'answer,', 'yes.', 'He', 'was', 'tested', 'and', 'monitored', 'throughout', 'the', 'month,', 'as', 'was', 'I.', 'Also,', '***TRIGGER', 'WARNING', 'VOMIT***\\n\\nLong', 'answer,', 'Norovirus', 'is', 'extremely', 'contagious.', 'Just', 'a', 'few', 'parts', 'per', 'million', 'can', 'make', 'you', 'violently', 'ill.', 'You', 'can', 'also', 'get', 'reinfected', 'over', 'and', 'over.', 'Which', 'is', 'what', 'happened', 'to', 'DS', 'and', 'I.', '\\n\\nDS', 'was', 'ok', 'during', 'the', 'day', 'but', 'in', 'the', 'middle', 'of', 'the', 'night', 'he', 'would', 'wake', 'up', 'sick.', 'He', 'has', 'autism', 'so', 'when', 'he', 'would', 'get', 'sick', 'he', 'would', 'panic', 'and', 'try', 'to', 'out', 'run', 'the', 'projectile', 'vomiting.', 'Every', 'night', 'he', 'would', 'run', 'through', 'the', 'house', 'projectile', 'vomiting', 'at', 'least', 'once.', 'Every', 'room', 'in', 'the', 'house', 'was', 'covered', 'in', 'vomit', 'except', 'my', \"daughter's.\", 'My', 'bed,', 'his', 'bed,', 'the', 'couch,', 'the', 'arm', 'chairs,', 'my', 'bookshelf,', 'nothing', 'was', 'left', 'untouched.', 'I', 'would', 'spend', 'hours', 'cleaning', 'every', 'night.\\n\\nHe', 'woke', 'me', 'up', 'one', 'night', 'by', 'puking', 'in', 'my', 'face.\\n\\nAs', 'you', 'can', 'imagine,', 'it', 'took', 'a', 'long', 'time', 'for', 'the', 'kid', 'and', 'I', 'to', 'shake', 'it.', 'Surprisingly,', 'DH', 'and', 'DD', 'never', 'got', 'sick.\\n\\nEDIT:', 'also', 'he', 'refused', 'to', 'take', 'any', 'meds', 'at', 'this', 'point', 'so', 'I', \"couldn't\", 'get', 'anti', 'nausea', 'meds', 'down', 'him.', 'He', 'always', 'ate', 'and', 'drank', 'well,', 'he', 'was', 'pretty', 'much', 'normal', 'all', 'day.', 'He', 'would', 'just', 'wake', 'up', 'every', 'night', 'and', 'spend', 'an', 'hour', 'or', '2', 'puking.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['short', 'answ', 'ye', 'test', 'monit', 'throughout', 'mon', 'also', 'trig', 'warn', 'vomit\\n\\nlong', 'answ', 'norovir', 'extrem', 'contagy', 'part', 'per', 'mil', 'mak', 'viol', 'il', 'also', 'get', 'reinfect', 'hap', 'ds', '\\n\\nds', 'ok', 'day', 'middl', 'night', 'would', 'wak', 'sick', 'aut', 'would', 'get', 'sick', 'would', 'pan', 'try', 'run', 'projectil', 'vomit', 'every', 'night', 'would', 'run', 'hous', 'projectil', 'vomit', 'least', 'every', 'room', 'hous', 'cov', 'vomit', 'exceiv', 'daught', 'bed', 'bed', 'couch', 'arm', 'chair', 'bookshelf', 'noth', 'left', 'untouch', 'would', 'spend', 'hour', 'cle', 'every', 'night\\n\\nhe', 'wok', 'on', 'night', 'puk', 'face\\n\\nas', 'imagin', 'took', 'long', 'tim', 'kid', 'shak', 'surpr', 'dh', 'dd', 'nev', 'got', 'sick\\n\\nedit', 'also', 'refus', 'tak', 'med', 'point', 'couldnt', 'get', 'ant', 'nause', 'med', 'alway', 'at', 'drank', 'wel', 'pretty', 'much', 'norm', 'day', 'would', 'wak', 'every', 'night', 'spend', 'hour', 'two', 'puk'], ['short', 'answer', 'yes', 'test', 'monitor', 'throughout', 'month', 'also', 'trigger', 'warn', 'vomit\\n\\nlong', 'answer', 'norovirus', 'extremely', 'contagious', 'part', 'per', 'million', 'make', 'violently', 'ill', 'also', 'get', 'reinfected', 'happen', 'ds', '\\n\\nds', 'ok', 'day', 'middle', 'night', 'would', 'wake', 'sick', 'autism', 'would', 'get', 'sick', 'would', 'panic', 'try', 'run', 'projectile', 'vomit', 'every', 'night', 'would', 'run', 'house', 'projectile', 'vomit', 'least', 'every', 'room', 'house', 'cover', 'vomit', 'except', 'daughters', 'bed', 'bed', 'couch', 'arm', 'chair', 'bookshelf', 'nothing', 'leave', 'untouched', 'would', 'spend', 'hours', 'clean', 'every', 'night\\n\\nhe', 'wake', 'one', 'night', 'puke', 'face\\n\\nas', 'imagine', 'take', 'long', 'time', 'kid', 'shake', 'surprisingly', 'dh', 'dd', 'never', 'get', 'sick\\n\\nedit', 'also', 'refuse', 'take', 'meds', 'point', 'couldnt', 'get', 'anti', 'nausea', 'meds', 'always', 'eat', 'drink', 'well', 'pretty', 'much', 'normal', 'day', 'would', 'wake', 'every', 'night', 'spend', 'hour', 'two', 'puke'])\n",
      "original document: \n",
      "[\"He's\", 'just', 'working', 'harder,', '', '', '', '', '', '', '', '', '', '', '', '', '\\nto', 'make', 'the', 'abyss', 'better,', '', '', '', '', '', '', '', '', '', '', '\\nso', 'we', 'can', 'travel', 'faster,', '', '', '', '', '', '', '', '\\nand', 'become', 'stronger.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'work', 'hard', '\\nto', 'mak', 'abyss', 'bet', '\\nso', 'travel', 'fast', '\\nand', 'becom', 'stronger'], ['hes', 'work', 'harder', '\\nto', 'make', 'aby', 'better', '\\nso', 'travel', 'faster', '\\nand', 'become', 'stronger'])\n",
      "original document: \n",
      "['Most', 'of', 'the', 'tribes', 'are', 'good', 'in', 'XLN', 'limited,', 'but', 'none', 'of', 'them', 'have', 'enough', 'standard', 'support', 'to', 'be', 'viable.', 'If', 'you', 'want', 'to', 'do', 'a', 'tribe,', 'though,', 'Dinosaurs', 'would', 'be', 'your', 'best', 'bet', 'with', 'things', 'like', \"[[Kinjalli's\", 'Caller]],', '[[Optec', 'Huntmaster]],', '[[Ripjaw', 'Raptor]],', 'and', '[[Carnage', 'Tyrant]].']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trib', 'good', 'xln', 'limit', 'non', 'enough', 'standard', 'support', 'viabl', 'want', 'trib', 'though', 'dinosa', 'would', 'best', 'bet', 'thing', 'lik', 'kinjal', 'cal', 'optec', 'huntmast', 'ripjaw', 'rapt', 'carn', 'tyr'], ['tribes', 'good', 'xln', 'limit', 'none', 'enough', 'standard', 'support', 'viable', 'want', 'tribe', 'though', 'dinosaurs', 'would', 'best', 'bet', 'things', 'like', 'kinjallis', 'caller', 'optec', 'huntmaster', 'ripjaw', 'raptor', 'carnage', 'tyrant'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['IT', 'SHOULD', 'BE', 'MY', 'TURN.', 'I', 'WAS', 'GOING', 'TO', 'DO', 'IT', 'TWO', 'MONTHS', 'AGO', 'AND', 'THEN', 'I', 'DIED', 'NIGHT', '0.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turn', 'going', 'two', 'month', 'ago', 'died', 'night', 'zero'], ['turn', 'go', 'two', 'months', 'ago', 'die', 'night', 'zero'])\n",
      "original document: \n",
      "['Damn...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn'], ['damn'])\n",
      "original document: \n",
      "['Hi', 'there!', 'Your', 'post', 'was', 'removed', 'because', 'it', 'uses', 'the', 'text', 'box.', 'Per', '[rule', '1](/r/AskReddit/wiki/index#wiki_-rule_1-),', 'use', 'of', 'the', 'text', 'box', 'is', 'prohibited.', 'You', 'can', 'resubmit', 'your', 'post', '[here](/r/askreddit/submit?selftext=true&amp;title=Emergency', 'Room', 'workers', 'of', 'Reddit,', 'what', 'are', 'the', 'craziest,', 'most', 'unbelievable,', 'insane', 'stories', 'you', 'have?)', 'without', 'the', 'textbox.\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/AskReddit)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'post', 'remov', 'us', 'text', 'box', 'per', 'rul', '1raskredditwikiindexwiki_rule_1', 'us', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitleemerg', 'room', 'work', 'reddit', 'craziest', 'unbeliev', 'ins', 'story', 'without', 'textbox\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoraskreddit', 'quest', 'concern'], ['hi', 'post', 'remove', 'use', 'text', 'box', 'per', 'rule', '1raskredditwikiindexwiki_rule_1', 'use', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitleemergency', 'room', 'workers', 'reddit', 'craziest', 'unbelievable', 'insane', 'stories', 'without', 'textbox\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoraskreddit', 'question', 'concern'])\n",
      "original document: \n",
      "['Muslims', 'and', 'Sikh', 'are', 'peaceful', 'because', 'I', 'choose', 'to', 'believe', 'people', 'are', 'born', 'generally', 'good.', 'However,', 'having', 'researched', 'a', 'multitude', 'of', 'religions', 'I', 'can', 'tell', 'you', 'that', 'Islam', 'is', 'quite', 'unlike', 'the', 'other', 'abrahamic', 'religions.', 'All', 'have', 'barbarism', 'but', 'that', \"doesn't\", 'mean', 'they', 'exist', 'at', 'uniform', 'degrees', 'of', 'intensity', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['muslim', 'sikh', 'peac', 'choos', 'believ', 'peopl', 'born', 'gen', 'good', 'howev', 'research', 'multitud', 'relig', 'tel', 'islam', 'quit', 'unlik', 'abraham', 'relig', 'barb', 'doesnt', 'mean', 'ex', 'uniform', 'degr', 'intens'], ['muslims', 'sikh', 'peaceful', 'choose', 'believe', 'people', 'bear', 'generally', 'good', 'however', 'research', 'multitude', 'religions', 'tell', 'islam', 'quite', 'unlike', 'abrahamic', 'religions', 'barbarism', 'doesnt', 'mean', 'exist', 'uniform', 'degrees', 'intensity'])\n",
      "original document: \n",
      "['What', 'makes', 'these', 'amazing', 'aside', 'from', 'what', 'actions.', 'Is', 'they', 'prove', 'you', 'can', 'allow', 'them', 'to', 'physically', 'and', 'mentally', 'develop', 'before', 'training', 'and', 'they', 'are', 'more', 'sound.', 'They', 'do', 'not', 'start', 'training', 'ground', 'work', 'until', 'they', 'are', 'four', 'and', 'riding', 'until', 'almost', 'five.', 'I', 'hope', 'soon', 'the', 'world', 'will', 'take', 'suit', 'also', 'that', 'we', 'see', 'the', 'health', 'benefits', 'of', 'not', 'shoeing.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'amaz', 'asid', 'act', 'prov', 'allow', 'phys', 'ment', 'develop', 'train', 'sound', 'start', 'train', 'ground', 'work', 'four', 'rid', 'almost', 'fiv', 'hop', 'soon', 'world', 'tak', 'suit', 'also', 'see', 'heal', 'benefit', 'sho'], ['make', 'amaze', 'aside', 'action', 'prove', 'allow', 'physically', 'mentally', 'develop', 'train', 'sound', 'start', 'train', 'grind', 'work', 'four', 'rid', 'almost', 'five', 'hope', 'soon', 'world', 'take', 'suit', 'also', 'see', 'health', 'benefit', 'shoe'])\n",
      "original document: \n",
      "['Use', 'an', 'incognito', 'tab', 'on', 'chrome']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'incognito', 'tab', 'chrome'], ['use', 'incognito', 'tab', 'chrome'])\n",
      "original document: \n",
      "['Tone', 'down', 'pitcher', 'confidence\\n\\n\\nMake', 'hitting', 'less', 'rng', '\\n\\n\\nFix', 'poor', 'fielding/throwing', 'animations', '(tired', 'of', 'a', 'runner', 'going', 'from', '2nd', 'to', 'home', 'on', 'a', 'ball', 'hit', '4', 'feet', 'in', 'front', 'of', 'home', 'plate', 'that', 'I', 'throw', 'to', 'first)\\n\\nNo', 'way', 'the', 'show', 'touches', 'esports', 'without', '#2', 'and', '#3', 'fixed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ton', 'pitch', 'confidence\\n\\n\\nmak', 'hit', 'less', 'rng', '\\n\\n\\nfix', 'poor', 'fieldingthrow', 'anim', 'tir', 'run', 'going', '2nd', 'hom', 'bal', 'hit', 'four', 'feet', 'front', 'hom', 'plat', 'throw', 'first\\n\\nno', 'way', 'show', 'touch', 'esport', 'without', 'two', 'three', 'fix'], ['tone', 'pitcher', 'confidence\\n\\n\\nmake', 'hit', 'less', 'rng', '\\n\\n\\nfix', 'poor', 'fieldingthrowing', 'animations', 'tire', 'runner', 'go', '2nd', 'home', 'ball', 'hit', 'four', 'feet', 'front', 'home', 'plate', 'throw', 'first\\n\\nno', 'way', 'show', 'touch', 'esports', 'without', 'two', 'three', 'fix'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['*Ugh*,', 'I', 'know', 'though!', 'I', 'want', 'her', 'to', 'come', 'back', 'and', 'slay.', 'She', 'is', 'honestly', 'one', 'of', 'my', 'all-time', 'faves.', 'Her', 'personality', 'is', 'just', 'so', 'refreshing', 'and', 'sweet.', 'I', 'love', 'her!', 'Such', 'a', 'classy,', 'talented', 'lady!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ugh', 'know', 'though', 'want', 'com', 'back', 'slay', 'honest', 'on', 'alltim', 'fav', 'person', 'refresh', 'sweet', 'lov', 'classy', 'tal', 'lady'], ['ugh', 'know', 'though', 'want', 'come', 'back', 'slay', 'honestly', 'one', 'alltime', 'faves', 'personality', 'refresh', 'sweet', 'love', 'classy', 'talented', 'lady'])\n",
      "original document: \n",
      "['Hey,', \"we've\", 'lost', 'to', 'both', 'of', 'those', 'teams', 'at', 'home.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'wev', 'lost', 'team', 'hom'], ['hey', 'weve', 'lose', 'team', 'home'])\n",
      "original document: \n",
      "['Steel', 'type:', '', '\\n', '', '\\n-', 'Start', 'with', 'a', 'sturdy', 'Skarmory.', 'Could', 'set', 'up', 'spikes', 'or', 'toxic', 'before', 'death.', '', '\\n-', 'Air', 'balloon', 'Magnezone.', 'Sturdy', 'again', 'since', 'magnet', 'pull', \"wouldn't\", 'help', 'too', 'much.', '', '\\n-', 'Heatproof', 'Bronzong.', 'Covers', 'fighting', 'and', 'helps', 'against', 'fire.', '', '\\n-', 'Empoleon.', 'A', 'better', 'fire', 'counter.', 'Give', 'it', 'an', 'AV.\\n-', 'Aegislash.', 'No', 'fighting', 'weakness', 'and', 'powerful', 'hitter.', '', '\\n-', 'Ace:', 'Scizor.', 'My', '4th', 'favorite', 'mon', 'and', 'a', 'potential', 'mega', 'for', 'a', '\"rematch\"', 'team.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['steel', 'typ', '\\n', '\\n', 'start', 'sturdy', 'skarm', 'could', 'set', 'spik', 'tox', 'dea', '\\n', 'air', 'balloon', 'magnezon', 'sturdy', 'sint', 'magnet', 'pul', 'wouldnt', 'help', 'much', '\\n', 'heatproof', 'bronzong', 'cov', 'fight', 'help', 'fir', '\\n', 'empoleon', 'bet', 'fir', 'count', 'giv', 'av\\n', 'aegislash', 'fight', 'weak', 'pow', 'hit', '\\n', 'ac', 'sciz', '4th', 'favorit', 'mon', 'pot', 'meg', 'rematch', 'team'], ['steel', 'type', '\\n', '\\n', 'start', 'sturdy', 'skarmory', 'could', 'set', 'spike', 'toxic', 'death', '\\n', 'air', 'balloon', 'magnezone', 'sturdy', 'since', 'magnet', 'pull', 'wouldnt', 'help', 'much', '\\n', 'heatproof', 'bronzong', 'cover', 'fight', 'help', 'fire', '\\n', 'empoleon', 'better', 'fire', 'counter', 'give', 'av\\n', 'aegislash', 'fight', 'weakness', 'powerful', 'hitter', '\\n', 'ace', 'scizor', '4th', 'favorite', 'mon', 'potential', 'mega', 'rematch', 'team'])\n",
      "original document: \n",
      "['With', '2cb', 'every', 'mg', 'counts.', 'A', 'dose', 'of', '13mg', 'for', 'someone', 'could', 'be', 'very', 'different', 'to', '18mg', 'dose']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['2cb', 'every', 'mg', 'count', 'dos', '13mg', 'someon', 'could', 'diff', '18mg', 'dos'], ['2cb', 'every', 'mg', 'count', 'dose', '13mg', 'someone', 'could', 'different', '18mg', 'dose'])\n",
      "original document: \n",
      "[\"You've\", 'not', 'been', 'out', 'clubbing', 'for', 'a', 'while,', 'have', 'you?!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youv', 'club'], ['youve', 'club'])\n",
      "original document: \n",
      "[\"[+msanteler](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoomeq/):\\n\\nShe's\", 'saying', 'her', 'parents', 'paid', 'for', 'the', 'first', 'one,', 'which', 'assigned', 'an', 'ISBN', 'number.', 'Every', 'copy', 'since', 'then', '(including', 'the', 'one', \"she's\", 'holding', 'in', 'the', 'photo)', 'has', 'not', 'been', 'pay-to-print']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['msantelerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoomeq\\n\\nshe', 'say', 'par', 'paid', 'first', 'on', 'assign', 'isbn', 'numb', 'every', 'cop', 'sint', 'includ', 'on', 'she', 'hold', 'photo', 'paytoprint'], ['msantelerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoomeq\\n\\nshes', 'say', 'parent', 'pay', 'first', 'one', 'assign', 'isbn', 'number', 'every', 'copy', 'since', 'include', 'one', 'shes', 'hold', 'photo', 'paytoprint'])\n",
      "original document: \n",
      "[\"it's\", 'a', 'masterpiece.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['masterpiec'], ['masterpiece'])\n",
      "original document: \n",
      "['Just', 'save', 'your', 'rp', 'for', 'the', 'next', 'time', 'they', 'have', 'special', 'loot.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sav', 'rp', 'next', 'tim', 'spec', 'loot'], ['save', 'rp', 'next', 'time', 'special', 'loot'])\n",
      "original document: \n",
      "['Well', 'duh', 'they', 'can', 'keep', 'the', 'brown', 'people', 'out', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'duh', 'keep', 'brown', 'peopl'], ['well', 'duh', 'keep', 'brown', 'people'])\n",
      "original document: \n",
      "['Go', 'Rebs.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'reb'], ['go', 'rebs'])\n",
      "original document: \n",
      "['Ok', \"I'll\", 'bite.', 'Based', 'on', 'my', 'original', 'comment', 'your', 'initial', 'reply', 'had', 'nothing', 'to', 'do', 'with', 'my', 'post.', 'You', 'are', 'just', 'inventing', 'a', 'secondary', 'argument.', 'If', 'you', 'do', 'require', 'my', 'political', 'stance:', 'I', 'am', 'not', 'an', 'advocate', 'for', 'Trump', 'and', 'I', 'did', 'not', 'vote', 'for', 'him.', 'I', 'guess', 'I', 'really', 'do', 'not', 'understand', 'what', 'triggered', 'you.', 'Again', 'though,', 'I', 'really', 'do', 'hope', 'you', 'have', 'a', 'good', 'night.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'il', 'bit', 'bas', 'origin', 'com', 'init', 'reply', 'noth', 'post', 'inv', 'second', 'argu', 'requir', 'polit', 'stant', 'advoc', 'trump', 'vot', 'guess', 'real', 'understand', 'trig', 'though', 'real', 'hop', 'good', 'night'], ['ok', 'ill', 'bite', 'base', 'original', 'comment', 'initial', 'reply', 'nothing', 'post', 'invent', 'secondary', 'argument', 'require', 'political', 'stance', 'advocate', 'trump', 'vote', 'guess', 'really', 'understand', 'trigger', 'though', 'really', 'hope', 'good', 'night'])\n",
      "original document: \n",
      "['I', 'GET', 'TO', 'GET', 'DRUNK', 'BEFORE', 'THE', '3RD', 'INNING!!!!', 'THANKS', 'CARDBROS!!!!', '\\n\\nEdit:', 'GO', 'CU!!!!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'get', 'drunk', '3rd', 'in', 'thank', 'cardbro', '\\n\\nedit', 'go', 'cu'], ['get', 'get', 'drink', '3rd', 'inning', 'thank', 'cardbros', '\\n\\nedit', 'go', 'cu'])\n",
      "original document: \n",
      "['I', 'bet', 'she', 'dried', 'them', 'on', 'the', 'clothes', 'line', 'or', 'with', 'low', 'heat', 'in', 'the', 'dryer.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'dri', 'cloth', 'lin', 'low', 'heat', 'dry'], ['bet', 'dry', 'clothe', 'line', 'low', 'heat', 'dryer'])\n",
      "original document: \n",
      "[\"I'm\", 'in.', 'Go', 'Tigers\\n\\nEdit:', 'Would', 'it', 'be', 'a', 'haiku', 'for', 'comments', 'on', 'every', 'subreddit', 'or', 'just', 'r/cfb?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'go', 'tigers\\n\\nedit', 'would', 'haiku', 'com', 'every', 'subreddit', 'rcfb'], ['im', 'go', 'tigers\\n\\nedit', 'would', 'haiku', 'comment', 'every', 'subreddit', 'rcfb'])\n",
      "original document: \n",
      "['So', 'can', 'I', 'post', 'a', 'meta', 'link', 'to', 'it', 'in', 'another', 'top', 'comment?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'met', 'link', 'anoth', 'top', 'com'], ['post', 'meta', 'link', 'another', 'top', 'comment'])\n",
      "original document: \n",
      "['I', 'think', \"it's\", 'a', 'special', 'way', 'to', 'cook', 'an', 'egg.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'spec', 'way', 'cook', 'eg'], ['think', 'special', 'way', 'cook', 'egg'])\n",
      "original document: \n",
      "['This', 'might', 'be', 'true', 'for', 'your', 'most', 'basic', 'White', 'Oaks,', 'but', 'you', 'can', 'spot', 'a', 'pair', 'of', 'super', 'slubby', 'japanese', 'selvedge,', 'or', 'rainbow', 'weft,', 'or', 'N&amp;F', 'vugar', 'selvedge', 'from', 'a', 'mile', 'away.\\n\\nThe', 'build', 'quality', 'is', 'also', 'often', 'better,', 'turn', 'a', 'pair', 'of', 'Ciano', 'Farmer', 'inside', 'out', 'next', 'to', 'a', 'pair', 'of', 'Uniqlo', 'selvedge', 'and', 'tell', 'me', 'which', 'is', 'better', 'built.\\n\\nThere', 'is', 'also', 'a', 'matter', 'of', 'inherent', 'value', 'in', 'the', 'materials.', 'Shuttle', 'looms', 'put', 'out', 'much', 'less', 'fabric', 'than', 'their', 'modern', 'counterparts.', 'Boutique', 'mills', 'will', 'use', 'specific', 'strains', 'of', 'cotton', 'to', 'achieve', 'desired', 'textures', 'and', 'fades.', 'The', 'dying', 'process', 'is', 'often', 'more', 'labor', 'intensive', '', 'and', 'uses', 'real', 'indigo', '(most', 'dept.', 'stores', 'use', 'artificial', 'dyes).', 'Also,', 'a', 'rare', 'deadstock', 'fabric', 'that', 'only', 'has', '&lt;100', 'bolts', 'in', 'existence', 'is', 'going', 'to', 'cost', 'more', 'than', 'a', 'run', 'of', 'the', 'mill', '(literally)', 'fabric', 'that', 'is', 'being', 'pumped', 'out', 'in', 'the', '1000s', 'of', 'yards', 'per', 'day.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['might', 'tru', 'bas', 'whit', 'oak', 'spot', 'pair', 'sup', 'slubby', 'japanes', 'selvedg', 'rainbow', 'weft', 'nampf', 'vug', 'selvedg', 'mil', 'away\\n\\nthe', 'build', 'qual', 'also', 'oft', 'bet', 'turn', 'pair', 'ciano', 'farm', 'insid', 'next', 'pair', 'uniqlo', 'selvedg', 'tel', 'bet', 'built\\n\\nthere', 'also', 'mat', 'inh', 'valu', 'mat', 'shuttl', 'loom', 'put', 'much', 'less', 'fabr', 'modern', 'counterpart', 'bout', 'mil', 'us', 'spec', 'strains', 'cotton', 'achiev', 'desir', 'text', 'fad', 'dying', 'process', 'oft', 'lab', 'intend', 'us', 'real', 'indigo', 'dept', 'stor', 'us', 'art', 'dye', 'also', 'rar', 'deadstock', 'fabr', 'lt100', 'bolt', 'ex', 'going', 'cost', 'run', 'mil', 'lit', 'fabr', 'pump', '1000s', 'yard', 'per', 'day'], ['might', 'true', 'basic', 'white', 'oaks', 'spot', 'pair', 'super', 'slubby', 'japanese', 'selvedge', 'rainbow', 'weft', 'nampf', 'vugar', 'selvedge', 'mile', 'away\\n\\nthe', 'build', 'quality', 'also', 'often', 'better', 'turn', 'pair', 'ciano', 'farmer', 'inside', 'next', 'pair', 'uniqlo', 'selvedge', 'tell', 'better', 'built\\n\\nthere', 'also', 'matter', 'inherent', 'value', 'materials', 'shuttle', 'loom', 'put', 'much', 'less', 'fabric', 'modern', 'counterparts', 'boutique', 'mill', 'use', 'specific', 'strain', 'cotton', 'achieve', 'desire', 'textures', 'fade', 'die', 'process', 'often', 'labor', 'intensive', 'use', 'real', 'indigo', 'dept', 'store', 'use', 'artificial', 'dye', 'also', 'rare', 'deadstock', 'fabric', 'lt100', 'bolt', 'existence', 'go', 'cost', 'run', 'mill', 'literally', 'fabric', 'pump', '1000s', 'yards', 'per', 'day'])\n",
      "original document: \n",
      "['Lol', 'at', 'chaisson', 'sidestepping']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'chaisson', 'sidestep'], ['lol', 'chaisson', 'sidestep'])\n",
      "original document: \n",
      "['Flair', 'up']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flair'], ['flair'])\n",
      "original document: \n",
      "['Yes', 'but', 'not', 'the', 'condoms.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'condom'], ['yes', 'condoms'])\n",
      "original document: \n",
      "['Oh', 'yeah,', 'I', 'hate', 'sticking-out', 'tongues.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'yeah', 'hat', 'stickingout', 'tongu'], ['oh', 'yeah', 'hate', 'stickingout', 'tongue'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Dun', 'worry', 'Meta', 'Coola', 'will', 'one', 'shot', 'Goks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dun', 'worry', 'met', 'cool', 'on', 'shot', 'gok'], ['dun', 'worry', 'meta', 'coola', 'one', 'shoot', 'goks'])\n",
      "original document: \n",
      "['Our', 'boi', 'Bamboe', 'coming', 'through', 'in', 'the', \"clutch.\\n\\nCan't\", 'say', \"I'm\", 'too', 'relieved', 'with', 'them', 'qualifying', 'for', 'a', 'LAN', 'thanks', 'to', 'trademark', 'Sexy', 'Bamboe', 'plays', '[but](http://68.media.tumblr.com/ab5ee52bbbd86b1ffa2069e7758fe820/tumblr_ojbd1zglWn1qim9hqo3_400.gif)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['boi', 'bambo', 'com', 'clutch\\n\\ncant', 'say', 'im', 'reliev', 'qual', 'lan', 'thank', 'trademark', 'sexy', 'bambo', 'play', 'buthttp68mediatumblrcomab5ee52bbbd86b1ffa2069e7758fe820tumblr_ojbd1zglwn1qim9hqo3_400gif'], ['boi', 'bamboe', 'come', 'clutch\\n\\ncant', 'say', 'im', 'relieve', 'qualify', 'lan', 'thank', 'trademark', 'sexy', 'bamboe', 'play', 'buthttp68mediatumblrcomab5ee52bbbd86b1ffa2069e7758fe820tumblr_ojbd1zglwn1qim9hqo3_400gif'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['&gt;', 'I', 'suffer', 'from', 'severe', 'anxiety,', 'and', 'my', 'first', \"kitten's\", 'behavior', 'is', 'pushing', 'me', 'to', 'the', 'limit,', 'to', 'the', 'point', 'where', 'I', 'dislike', 'him.\\n\\nYou', \"don't\", 'need', 'to', 'apologize', 'for', 'taking', 'care', 'of', 'your', 'mental', 'health.', 'I', 'think', 'the', 'best', 'thing', 'for', 'you', 'to', 'do', 'is', 'to', 'rehome', 'them', 'pronto.', 'Cats', 'under', '1yo', 'are', 'pretty', 'easy', 'to', 'find', 'homes', 'for', '-', \"they're\", 'cute!', 'You', 'might', 'do', 'well', 'adopting', 'an', 'older', 'cat.', 'Older', 'cats', 'are', 'harder', 'for', 'find', 'homes', 'for,', 'so', 'there', 'are', 'a', 'lot', 'of', 'great', 'cats', 'looking', 'for', 'homes.', 'The', 'best', 'thing', 'about', 'mature', 'cats', 'that', 'their', 'personalities', 'are', 'already', 'developed', 'so', 'you', 'can', 'talk', 'to', 'whoever', 'is', 'caring', 'for', 'them', 'now', 'and', 'find', 'out', 'if', 'they', 'are', 'a', 'good', 'fit', 'for', 'your', 'home.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'suff', 'sev', 'anxy', 'first', 'kit', 'behavy', 'push', 'limit', 'point', 'dislik', 'him\\n\\nyou', 'dont', 'nee', 'apolog', 'tak', 'car', 'ment', 'heal', 'think', 'best', 'thing', 'rehom', 'pronto', 'cat', '1yo', 'pretty', 'easy', 'find', 'hom', 'theyr', 'cut', 'might', 'wel', 'adopt', 'old', 'cat', 'old', 'cat', 'hard', 'find', 'hom', 'lot', 'gre', 'cat', 'look', 'hom', 'best', 'thing', 'mat', 'cat', 'person', 'already', 'develop', 'talk', 'whoev', 'car', 'find', 'good', 'fit', 'hom'], ['gt', 'suffer', 'severe', 'anxiety', 'first', 'kitten', 'behavior', 'push', 'limit', 'point', 'dislike', 'him\\n\\nyou', 'dont', 'need', 'apologize', 'take', 'care', 'mental', 'health', 'think', 'best', 'thing', 'rehome', 'pronto', 'cat', '1yo', 'pretty', 'easy', 'find', 'home', 'theyre', 'cute', 'might', 'well', 'adopt', 'older', 'cat', 'older', 'cat', 'harder', 'find', 'home', 'lot', 'great', 'cat', 'look', 'home', 'best', 'thing', 'mature', 'cat', 'personalities', 'already', 'develop', 'talk', 'whoever', 'care', 'find', 'good', 'fit', 'home'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[', '**text', 'here**', '](', '**link', 'here**', ').\\n\\nWithout', 'the', 'spaces.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['text', 'link', '\\n\\nwithout', 'spac'], ['text', 'link', '\\n\\nwithout', 'space'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['fun', 'fact', 'i', 'got', 'a', 'Noctua', 'NH-U9B', 'SE2', ';P\\nnoctua', 'coolers', 'are', 'not', 'scrap', 'FYI,', 'heck', 'just', 'change', 'the', 'fans', 'to', 'something', 'non-poo-ish', 'else\\n\\ni', 'got', 'PRIME', 'X370-PRO,', 'Corsair', 'Vengenace', 'LPX', '3200@16']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fun', 'fact', 'got', 'noctu', 'nhu9b', 'se2', 'p\\nnoctua', 'cool', 'scrap', 'fyi', 'heck', 'chang', 'fan', 'someth', 'nonpoo', 'else\\n\\ni', 'got', 'prim', 'x370pro', 'corsair', 'vengenac', 'lpx', 'three hundred and twenty thousand and sixteen'], ['fun', 'fact', 'get', 'noctua', 'nhu9b', 'se2', 'p\\nnoctua', 'coolers', 'scrap', 'fyi', 'heck', 'change', 'fan', 'something', 'nonpooish', 'else\\n\\ni', 'get', 'prime', 'x370pro', 'corsair', 'vengenace', 'lpx', 'three hundred and twenty thousand and sixteen'])\n",
      "original document: \n",
      "['I', 'can', 'Promise', 'you', 'one', 'thing.', 'These', 'people', 'dont', 'want', 'to', 'be', 'greedy.', 'But', 'if', 'the', 'only', 'thing', 'that', 'makes', 'you', 'happy', 'is', 'milking', 'that', '~~thong~~', 'money,', 'then', 'you', 'are', 'gonna', 'do', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prom', 'on', 'thing', 'peopl', 'dont', 'want', 'greedy', 'thing', 'mak', 'happy', 'milk', 'thong', 'money', 'gonn'], ['promise', 'one', 'thing', 'people', 'dont', 'want', 'greedy', 'thing', 'make', 'happy', 'milk', 'thong', 'money', 'gonna'])\n",
      "original document: \n",
      "['143418414|', '&gt;', 'Canada', 'Anonymous', '(ID:', '+DelM8Tk)\\n\\n&gt;&gt;143418344\\n&gt;still', 'voted', 'hillary\\nCuckservative,', 'through', 'and', 'through\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and fourteen', 'gt', 'canad', 'anonym', 'id', 'delm8tk\\n\\ngtgt143418344\\ngtstill', 'vot', 'hillary\\ncuckservative', 'through\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and fourteen', 'gt', 'canada', 'anonymous', 'id', 'delm8tk\\n\\ngtgt143418344\\ngtstill', 'vote', 'hillary\\ncuckservative', 'through\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Is', 'there', 'really', 'equal', 'rights', 'for', 'homosexuals?', 'Also', 'they', \"didn't\", 'disrespect', 'the', 'flag', 'they', 'kneeled', 'to', 'it', 'which', 'in', 'a', 'lot', 'of', 'ways', 'is', 'almost', 'more', 'respectful.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'eq', 'right', 'homosex', 'also', 'didnt', 'disrespect', 'flag', 'kneel', 'lot', 'way', 'almost', 'respect'], ['really', 'equal', 'right', 'homosexuals', 'also', 'didnt', 'disrespect', 'flag', 'kneel', 'lot', 'ways', 'almost', 'respectful'])\n",
      "original document: \n",
      "['\"I', 'frequently', 'dedicate', 'more', 'time', 'to', 'my', 'career', 'than', 'self', 'or', 'family\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['frequ', 'ded', 'tim', 'car', 'self', 'famy'], ['frequently', 'dedicate', 'time', 'career', 'self', 'family'])\n",
      "original document: \n",
      "['What', 'if', 'it', \"wasn't\", 'that', 'they', 'died', 'to', 'pass', 'on', 'rulership', 'but', 'it', 'changed', 'hands.', 'So', 'that', 'each', 'one', 'was', 'responsible', 'for', 'a', 'certain', 'amount', 'of', 'time', 'before', 'they', 'let', 'the', 'next', 'take', 'over.', 'Also', 'interesting', 'that', 'there', 'are', 'currently', '8', 'super', 'entities', 'that', 'about', '95%', 'of', 'economic', 'wealth', 'flows', 'through.\\n\\nProbably', 'just', 'coincidence', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wasnt', 'died', 'pass', 'rul', 'chang', 'hand', 'on', 'respons', 'certain', 'amount', 'tim', 'let', 'next', 'tak', 'also', 'interest', 'cur', 'eight', 'sup', 'ent', 'ninety-five', 'econom', 'weal', 'flow', 'through\\n\\nprobably', 'coincid', 'though'], ['wasnt', 'die', 'pass', 'rulership', 'change', 'hand', 'one', 'responsible', 'certain', 'amount', 'time', 'let', 'next', 'take', 'also', 'interest', 'currently', 'eight', 'super', 'entities', 'ninety-five', 'economic', 'wealth', 'flow', 'through\\n\\nprobably', 'coincidence', 'though'])\n",
      "original document: \n",
      "['Why?', 'Not', 'arguing', 'just', 'curious', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['argu', 'cury'], ['argue', 'curious'])\n",
      "original document: \n",
      "['Thank', 'you!', 'Ill', 'definitely', 'follow', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'il', 'definit', 'follow'], ['thank', 'ill', 'definitely', 'follow'])\n",
      "original document: \n",
      "['It', 'looks', 'like', 'an', 'ironstone', 'concretion.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'ironston', 'concret'], ['look', 'like', 'ironstone', 'concretion'])\n",
      "original document: \n",
      "['[I', 'decided', 'to', 'post', 'it', 'myself', 'after', 'all.](https://www.reddit.com/r/anime/comments/73ifnk/spoilers_action_heroine_cheer_fruits_episode_12/)', 'The', 'episode', 'is', 'out,', \"it's\", 'just', 'only', 'available', 'in', 'a', 'batch', 'on', 'the', 'torrent', 'site', '(at', 'least', 'it', 'was', 'last', 'time', 'I', 'checked),', 'so', 'maybe', \"that's\", 'why', 'the', 'thread', \"hadn't\", 'been', 'posted', 'yet.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['decid', 'post', 'allhttpswwwredditcomranimecomments73ifnkspoilers_action_heroine_cheer_fruits_episode_12', 'episod', 'avail', 'batch', 'tor', 'sit', 'least', 'last', 'tim', 'check', 'mayb', 'that', 'thread', 'hadnt', 'post', 'yet'], ['decide', 'post', 'allhttpswwwredditcomranimecomments73ifnkspoilers_action_heroine_cheer_fruits_episode_12', 'episode', 'available', 'batch', 'torrent', 'site', 'least', 'last', 'time', 'check', 'maybe', 'thats', 'thread', 'hadnt', 'post', 'yet'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Something', 'along', 'these', 'lines', 'perhaps?', '\\n\\nhttps://i.imgur.com/K1cEUn7.png']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someth', 'along', 'lin', 'perhap', '\\n\\nhttpsiimgurcomk1ceun7png'], ['something', 'along', 'line', 'perhaps', '\\n\\nhttpsiimgurcomk1ceun7png'])\n",
      "original document: \n",
      "['Misplacing', 'stuff,', 'the', 'time', 'spent', 'finding', 'it', 'again,', 'the', 'mess', 'created', 'while', 'trying', 'to', 'find', 'it,', 'which', 'makes', 'me', 'lose', 'more', 'stuff,', 'and', 'I', \"don't\", 'have', 'time', 'to', 'clean', 'it', 'up', 'because', \"I'm\", 'spending', 'so', 'much', 'time', 'looking', 'for', 'stuff!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['misplac', 'stuff', 'tim', 'spent', 'find', 'mess', 'cre', 'try', 'find', 'mak', 'los', 'stuff', 'dont', 'tim', 'cle', 'im', 'spend', 'much', 'tim', 'look', 'stuff'], ['misplace', 'stuff', 'time', 'spend', 'find', 'mess', 'create', 'try', 'find', 'make', 'lose', 'stuff', 'dont', 'time', 'clean', 'im', 'spend', 'much', 'time', 'look', 'stuff'])\n",
      "original document: \n",
      "['You', 'are', 'missing', 'your', 'LSU', 'flair,', 'breaux.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['miss', 'lsu', 'flair', 'breaux'], ['miss', 'lsu', 'flair', 'breaux'])\n",
      "original document: \n",
      "['I', 'dont', 'think', 'so.', 'I', 'got', 'quite', 'broad', 'shoulders', '(they', 'annoy', 'me', 'when', 'shopping', 'for', 'clothes', 'since', 'I', 'am', 'skinny', 'yet', 'my', 'shoulders', 'make', 'small', 'sizes', 'not', 'fit)', 'yet', 'my', 'wrists', 'are', 'quite', 'small', '(not', 'the', 'most', 'jacked', 'person', 'but', 'my', 'wrist', 'is', 'like', '4/5', 'times', 'smaller', 'than', 'my', 'biceps', 'in', 'diameter.', 'Muscles', 'dont', 'change', 'it', 'either', 'since', 'most', 'of', 'the', 'bulk', 'ends', 'up', 'in', 'the', 'upper', 'part', 'of', 'the', 'forarms,', 'not', 'the', 'wrists.', 'Even', 'then', 'the', 'only', 'thing', 'it', 'affects', 'are', 'clothes', 'sizes', 'and', 'fit.', 'Noone', 'else', 'cares.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'got', 'quit', 'broad', 'should', 'annoy', 'shop', 'cloth', 'sint', 'skinny', 'yet', 'should', 'mak', 'smal', 'siz', 'fit', 'yet', 'wrist', 'quit', 'smal', 'jack', 'person', 'wrist', 'lik', 'forty-five', 'tim', 'smal', 'bicep', 'diamet', 'musc', 'dont', 'chang', 'eith', 'sint', 'bulk', 'end', 'up', 'part', 'forarm', 'wrist', 'ev', 'thing', 'affect', 'cloth', 'siz', 'fit', 'noon', 'els', 'car'], ['dont', 'think', 'get', 'quite', 'broad', 'shoulder', 'annoy', 'shop', 'clothe', 'since', 'skinny', 'yet', 'shoulder', 'make', 'small', 'size', 'fit', 'yet', 'wrists', 'quite', 'small', 'jack', 'person', 'wrist', 'like', 'forty-five', 'time', 'smaller', 'biceps', 'diameter', 'muscle', 'dont', 'change', 'either', 'since', 'bulk', 'end', 'upper', 'part', 'forarms', 'wrists', 'even', 'thing', 'affect', 'clothe', 'size', 'fit', 'noone', 'else', 'care'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop21i/):\\n\\nThat', 'is', 'what', \"I'm\", 'saying,', \"I'm\", 'sorry', 'for', 'the', 'confusion.', 'My', 'parents', 'bought', 'copies', 'and', 'the', 'ISBN', 'number', 'when', 'I', 'was', 'younger,', 'but', 'the', 'copy', \"I'm\", 'holding', 'and', 'any', 'others', 'were', 'not', 'pay-to-print']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop21i\\n\\nthat', 'im', 'say', 'im', 'sorry', 'confus', 'par', 'bought', 'cop', 'isbn', 'numb', 'young', 'cop', 'im', 'hold', 'oth', 'paytoprint'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop21i\\n\\nthat', 'im', 'say', 'im', 'sorry', 'confusion', 'parent', 'buy', 'copy', 'isbn', 'number', 'younger', 'copy', 'im', 'hold', 'others', 'paytoprint'])\n",
      "original document: \n",
      "['Imo', 'it', 'has', 'a', 'much', 'bigger', 'impact', 'than', 'that.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['imo', 'much', 'big', 'impact'], ['imo', 'much', 'bigger', 'impact'])\n",
      "original document: \n",
      "['That', 'is', 'an', 'impressive', 'collection', 'but', 'I', 'feel', 'like', 'you', 'are', 'missing', 'more', 'than', 'a', 'couple', 'from', 'the', 'collection.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['impress', 'collect', 'feel', 'lik', 'miss', 'coupl', 'collect'], ['impressive', 'collection', 'feel', 'like', 'miss', 'couple', 'collection'])\n",
      "original document: \n",
      "['&gt;The', 'Media', 'Needs', 'To', 'Stop', 'Rationalizing', '~~President', 'Trump’s~~', 'Right-Wing', 'Behavior\\n\\nFTFY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthe', 'med', 'nee', 'stop', 'rat', 'presid', 'trump', 'rightw', 'behavior\\n\\nftfy'], ['gtthe', 'media', 'need', 'stop', 'rationalize', 'president', 'trump', 'rightwing', 'behavior\\n\\nftfy'])\n",
      "original document: \n",
      "['I', 'still', 'hate', 'this', 'fucking', 'stage.', 'I', 'discovered,', 'though,', 'that', 'if', 'you', 'have', 'a', 'cape', 'and', 'bring', 'a', 'blue', 'Yoshi', 'with', 'you,', 'and', 'you’re', 'really', 'careful,', 'you', 'can', 'get', 'to', 'the', 'first', 'koopas,', 'grab', 'one,', 'and', 'fly', 'the', 'rest', 'of', 'the', 'way.', 'It’s', 'not', 'easy', 'but', 'it', 'works.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'hat', 'fuck', 'stag', 'discov', 'though', 'cap', 'bring', 'blu', 'yosh', 'yo', 'real', 'car', 'get', 'first', 'koopa', 'grab', 'on', 'fly', 'rest', 'way', 'easy', 'work'], ['still', 'hate', 'fuck', 'stage', 'discover', 'though', 'cape', 'bring', 'blue', 'yoshi', 'youre', 'really', 'careful', 'get', 'first', 'koopas', 'grab', 'one', 'fly', 'rest', 'way', 'easy', 'work'])\n",
      "original document: \n",
      "['I', 'mean,', 'does', 'every', 'team', 'really', 'need', 'a', 'name?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'every', 'team', 'real', 'nee', 'nam'], ['mean', 'every', 'team', 'really', 'need', 'name'])\n",
      "original document: \n",
      "[\"I'm\", 'a', 'bit', 'worried', 'with', 'Mi5s', 'since', 'lots', 'of', 'users', 'reported', 'that', 'they', 'have', 'problems', 'with', 'the', 'fingerprint', 'sensor.', 'Looks', 'like', 'I', 'will', 'narrow', 'down', 'my', 'choice', 'to', 'Mi5', 'or', 'A1.', 'Thanks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'bit', 'worry', 'mi5s', 'sint', 'lot', 'us', 'report', 'problem', 'fingerprint', 'sens', 'look', 'lik', 'narrow', 'cho', 'mi5', 'a1', 'thank'], ['im', 'bite', 'worry', 'mi5s', 'since', 'lot', 'users', 'report', 'problems', 'fingerprint', 'sensor', 'look', 'like', 'narrow', 'choice', 'mi5', 'a1', 'thank'])\n",
      "original document: \n",
      "['Quick', 'question:', 'does', 'ethermine', 'API', 'offer', 'a', 'way', 'to', 'notify', 'you', 'if', 'your', 'rig', 'is', 'not', 'working,', 'or', 'you', 'can', 'only', 'get', 'the', 'calculated', 'and', 'reported', 'hashrate?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quick', 'quest', 'ethermin', 'ap', 'off', 'way', 'not', 'rig', 'work', 'get', 'calc', 'report', 'hashr'], ['quick', 'question', 'ethermine', 'api', 'offer', 'way', 'notify', 'rig', 'work', 'get', 'calculate', 'report', 'hashrate'])\n",
      "original document: \n",
      "['Still', '/u/Yurika_BLADE', 'is', 'right;', 'the', 'new', 'dailies', 'do', 'have', 'a', 'lower', 'drop', 'rate', 'considering', 'the', 'AP', 'cost.\\n\\nOn', 'the', 'JP', 'table,', 'it', 'lists', 'Chaldea', '(Wed)', '30', 'AP', 'with', 'a', '6.8%', 'rate', 'for', 'an', 'average', 'of', '439.6', 'AP', 'per', 'drop.\\n\\nMeanwhile', 'on', 'the', 'NA', 'side', 'Germania', 'is', 'at', 'a', '3.6%', 'drop', 'rate', 'but', 'with', 'half', 'the', 'AP', 'cost', '(15)', 'while', 'is', 'a', '417.7', 'AP', 'cost', 'per', 'drop,', '*slightly*', 'better', 'than', 'the', 'new', 'daily.', '\\n\\nOn', 'top', 'of', 'that,', 'the', '20', 'AP', 'Monster', 'Hunt', 'Daily', 'is', 'a', '5.3%', 'rate', 'at', '20', 'AP', 'cost', '', 'for', 'a', '380.3', 'cost/drop.\\n\\n---\\n\\nNow', \"I'm\", 'not', 'sure', 'why', 'the', '40', 'AP', 'Wednesday', 'quest', 'is', 'not', 'listed', 'for', 'Claws,', 'but', 'given', 'the', 'information', 'present', 'in', 'the', 'drop', 'table,', 'you', 'actually', 'have', '*more*', 'chances', 'to', 'get', 'Claws', 'if', 'you', 'farm', 'for', 'them', 'before', 'the', '3rd.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['stil', 'uyurika_blade', 'right', 'new', 'dai', 'low', 'drop', 'rat', 'consid', 'ap', 'cost\\n\\non', 'jp', 'tabl', 'list', 'chalde', 'wed', 'thirty', 'ap', 'sixty-eight', 'rat', 'av', 'four thousand, three hundred and ninety-six', 'ap', 'per', 'drop\\n\\nmeanwhile', 'na', 'sid', 'german', 'thirty-six', 'drop', 'rat', 'half', 'ap', 'cost', 'fifteen', 'four thousand, one hundred and seventy-seven', 'ap', 'cost', 'per', 'drop', 'slight', 'bet', 'new', 'dai', '\\n\\non', 'top', 'twenty', 'ap', 'monst', 'hunt', 'dai', 'fifty-three', 'rat', 'twenty', 'ap', 'cost', 'three thousand, eight hundred and three', 'costdrop\\n\\n\\n\\nnow', 'im', 'sur', 'forty', 'ap', 'wednesday', 'quest', 'list', 'claw', 'giv', 'inform', 'pres', 'drop', 'tabl', 'act', 'chant', 'get', 'claw', 'farm', '3rd'], ['still', 'uyurika_blade', 'right', 'new', 'dailies', 'lower', 'drop', 'rate', 'consider', 'ap', 'cost\\n\\non', 'jp', 'table', 'list', 'chaldea', 'wed', 'thirty', 'ap', 'sixty-eight', 'rate', 'average', 'four thousand, three hundred and ninety-six', 'ap', 'per', 'drop\\n\\nmeanwhile', 'na', 'side', 'germania', 'thirty-six', 'drop', 'rate', 'half', 'ap', 'cost', 'fifteen', 'four thousand, one hundred and seventy-seven', 'ap', 'cost', 'per', 'drop', 'slightly', 'better', 'new', 'daily', '\\n\\non', 'top', 'twenty', 'ap', 'monster', 'hunt', 'daily', 'fifty-three', 'rate', 'twenty', 'ap', 'cost', 'three thousand, eight hundred and three', 'costdrop\\n\\n\\n\\nnow', 'im', 'sure', 'forty', 'ap', 'wednesday', 'quest', 'list', 'claw', 'give', 'information', 'present', 'drop', 'table', 'actually', 'chance', 'get', 'claw', 'farm', '3rd'])\n",
      "original document: \n",
      "['Sprint', 'attack?', 'Her', 'only', 'sprint', 'attack', 'is', 'a', 'gb.', 'If', 'you', 'mean', 'dodge', 'attacks', 'then', \"it's\", 'because', 'her', 'dodge', 'attacks', 'are', 'heavies.', 'Berzerker,', 'warlord,', 'and', 'highlander', 'can', 'execute', 'from', 'their', 'dodge', 'attacks', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sprint', 'attack', 'sprint', 'attack', 'gb', 'mean', 'dodg', 'attack', 'dodg', 'attack', 'heavy', 'berzerk', 'warlord', 'highland', 'execut', 'dodg', 'attack'], ['sprint', 'attack', 'sprint', 'attack', 'gb', 'mean', 'dodge', 'attack', 'dodge', 'attack', 'heavies', 'berzerker', 'warlord', 'highlander', 'execute', 'dodge', 'attack'])\n",
      "original document: \n",
      "['No', 'love', 'for', 'SoundCloud', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'soundcloud'], ['love', 'soundcloud'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Neat.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['neat'], ['neat'])\n",
      "original document: \n",
      "['Manningham', 'had', 'a', 'great', '15', 'minutes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['manningham', 'gre', 'fifteen', 'minut'], ['manningham', 'great', 'fifteen', 'minutes'])\n",
      "original document: \n",
      "['Antiqua', 'is', 'a', 'classy', 'typeface.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['antiqu', 'classy', 'typefac'], ['antiqua', 'classy', 'typeface'])\n",
      "original document: \n",
      "['Thank', 'you', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'lt3'], ['thank', 'lt3'])\n",
      "original document: \n",
      "['But', 'why?', 'Why', \"wouldn't\", 'someone', 'like', 'Xpecial', 'or', 'say,', 'Apollo/Gate', 'support', 'the', 'new', 'system?\\n\\nBefore', 'franchising-', 'less', 'people', 'watching', 'their', 'games,', 'barely', 'any', 'chances', 'of', 'winning', 'the', 'split\\n\\nAfter', 'franchising-', 'possibly', 'more', 'people', 'watching', 'their', 'games', 'and', 'get', 'more', 'money', 'through', 'revenue', 'sharing\\n\\nSounds', 'like', 'a', 'win-win', 'for', 'a', 'lot', 'of', 'players', 'and', 'orgs.', '\\n\\nEven', 'if', 'NA', 'does', 'fall', 'behind', 'I', 'doubt', \"it'll\", 'happen', 'immediately', 'after', 'the', 'change', 'and', 'with', 'EU', 'breaking', 'its', 'league', 'into', 'idk', 'how', 'many', 'and', 'having', 'around', '20', 'teams,', \"they'll\", 'probably', 'turn', 'to', 'a', 'BO1', 'format', 'too.', 'But', 'Riot', 'knew', 'how', 'people', 'would', 'react', 'if', 'they', 'announced', 'BO1', 'for', 'EU', 'first', 'so', 'here', 'we', 'are.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'someon', 'lik', 'xpec', 'say', 'apollog', 'support', 'new', 'system\\n\\nbefore', 'franch', 'less', 'peopl', 'watch', 'gam', 'bar', 'chant', 'win', 'split\\n\\nafter', 'franch', 'poss', 'peopl', 'watch', 'gam', 'get', 'money', 'revenu', 'sharing\\n\\nsounds', 'lik', 'winwin', 'lot', 'play', 'org', '\\n\\neven', 'na', 'fal', 'behind', 'doubt', 'itl', 'hap', 'immedy', 'chang', 'eu', 'break', 'leagu', 'idk', 'many', 'around', 'twenty', 'team', 'theyl', 'prob', 'turn', 'bo1', 'form', 'riot', 'knew', 'peopl', 'would', 'react', 'annount', 'bo1', 'eu', 'first'], ['wouldnt', 'someone', 'like', 'xpecial', 'say', 'apollogate', 'support', 'new', 'system\\n\\nbefore', 'franchise', 'less', 'people', 'watch', 'game', 'barely', 'chance', 'win', 'split\\n\\nafter', 'franchise', 'possibly', 'people', 'watch', 'game', 'get', 'money', 'revenue', 'sharing\\n\\nsounds', 'like', 'winwin', 'lot', 'players', 'orgs', '\\n\\neven', 'na', 'fall', 'behind', 'doubt', 'itll', 'happen', 'immediately', 'change', 'eu', 'break', 'league', 'idk', 'many', 'around', 'twenty', 'team', 'theyll', 'probably', 'turn', 'bo1', 'format', 'riot', 'know', 'people', 'would', 'react', 'announce', 'bo1', 'eu', 'first'])\n",
      "original document: \n",
      "['Our', 'points', 'are', 'not', 'mutually', 'exclusive.', \"\\n\\nI'm\", 'saying', 'he', \"wasn't\", 'deficient.', \"\\n\\nYou're\", 'saying', 'he', 'was', 'better', 'at', 'math.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['point', 'mut', 'exclud', '\\n\\nim', 'say', 'wasnt', 'deficy', '\\n\\nyoure', 'say', 'bet', 'math'], ['point', 'mutually', 'exclusive', '\\n\\nim', 'say', 'wasnt', 'deficient', '\\n\\nyoure', 'say', 'better', 'math'])\n",
      "original document: \n",
      "['Wow,', 'do', 'u', 'have', 'a', 'video', 'to', 'share', '?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'u', 'video', 'shar'], ['wow', 'u', 'video', 'share'])\n",
      "original document: \n",
      "['I', 'normally', \"don't\", 'really', 'find', 'him', 'funny', 'at', 'all...but', 'this', 'is', 'just', 'hilarious.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['norm', 'dont', 'real', 'find', 'funny', 'allbut', 'hil'], ['normally', 'dont', 'really', 'find', 'funny', 'allbut', 'hilarious'])\n",
      "original document: \n",
      "['Yeah,', 'it', 'does.', 'That', \"doesn't\", 'mean', \"I'll\", 'be', 'able', 'to', 'scrape', 'up', 'the', 'cash', 'to', 'buy', 'a', 'Wii', 'U', 'in', 'time,', 'though.', \"I'd\", 'rather', 'not', 'be', 'on', 'a', 'time', 'limit', 'to', 'do', 'that,', 'so', \"I'm\", 'trying', 'to', 'find', 'out', 'if', \"that's\", 'the', 'case.', 'If', 'I', 'can', 'download', 'the', 'app', 'now', 'and', 'use', 'it', 'whenever', 'I', 'like,', 'that', 'would', 'be', 'ideal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'doesnt', 'mean', 'il', 'abl', 'scrape', 'cash', 'buy', 'wii', 'u', 'tim', 'though', 'id', 'rath', 'tim', 'limit', 'im', 'try', 'find', 'that', 'cas', 'download', 'ap', 'us', 'whenev', 'lik', 'would', 'id'], ['yeah', 'doesnt', 'mean', 'ill', 'able', 'scrape', 'cash', 'buy', 'wii', 'u', 'time', 'though', 'id', 'rather', 'time', 'limit', 'im', 'try', 'find', 'thats', 'case', 'download', 'app', 'use', 'whenever', 'like', 'would', 'ideal'])\n",
      "original document: \n",
      "[\"Let's\", 'say', 'you', 'own', 'an', 'apple', 'orchard.', 'You', 'sell', 'the', 'apples', 'you', 'grow.', 'But', 'instead', 'of', 'branding', 'them', 'as', 'what', 'they', 'are', '(apples),', 'you', 'brand', 'them', 'as', 'bananas.', \"They're\", 'still', 'apples,', 'no', 'matter', 'what', 'you', 'call', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'say', 'appl', 'orchard', 'sel', 'appl', 'grow', 'instead', 'brand', 'appl', 'brand', 'banana', 'theyr', 'stil', 'appl', 'mat', 'cal'], ['let', 'say', 'apple', 'orchard', 'sell', 'apples', 'grow', 'instead', 'brand', 'apples', 'brand', 'bananas', 'theyre', 'still', 'apples', 'matter', 'call'])\n",
      "original document: \n",
      "['[Same', 'dude', ':)](https://puu.sh/xMOA5/aa49a4e9e0.jpg)', \"I've\", 'never', 'tried', 'a', 'controller', 'though...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dud', 'httpspuushxmoa5aa49a4e9e0jpg', 'iv', 'nev', 'tri', 'control', 'though'], ['dude', 'httpspuushxmoa5aa49a4e9e0jpg', 'ive', 'never', 'try', 'controller', 'though'])\n",
      "original document: \n",
      "['of', 'course.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cours'], ['course'])\n",
      "original document: \n",
      "['The', 'president', 'of', 'the', 'United', 'States', 'of', 'America', 'is', 'Donald', 'Trump.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['presid', 'unit', 'stat', 'americ', 'donald', 'trump'], ['president', 'unite', 'state', 'america', 'donald', 'trump'])\n",
      "original document: \n",
      "['may', 'not', 'need', 'to', 'be,', 'but', 'it', 'is.', 'thank', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'nee', 'thank'], ['may', 'need', 'thank'])\n",
      "original document: \n",
      "['It', 'was', 'so', 'gross.', 'It', 'was', 'like', 'dark', 'red/Brown', 'and', 'had', 'lumps', 'of', 'what', 'they', 'said', 'was', 'the', 'meat,', 'and', 'the', 'rest', 'was', 'blood', 'sauce.', 'I', 'guess', 'it', 'just', 'like', 'their', \"culture's\", 'version', 'of', 'British', 'black', 'pudding', 'or', 'Chinese', 'blood', 'sausage.', 'They', 'had', 'it', 'with', 'rice.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gross', 'lik', 'dark', 'redbrown', 'lump', 'said', 'meat', 'rest', 'blood', 'sauc', 'guess', 'lik', 'cult', 'vert', 'brit', 'black', 'pud', 'chines', 'blood', 'saus', 'ric'], ['gross', 'like', 'dark', 'redbrown', 'lump', 'say', 'meat', 'rest', 'blood', 'sauce', 'guess', 'like', 'culture', 'version', 'british', 'black', 'pudding', 'chinese', 'blood', 'sausage', 'rice'])\n",
      "original document: \n",
      "['Sounds', 'good!', 'When', 'do', 'we', 'begin?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'good', 'begin'], ['sound', 'good', 'begin'])\n",
      "original document: \n",
      "['You', 'do', 'scrappy', 'comments.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['scrappy', 'com'], ['scrappy', 'comment'])\n",
      "original document: \n",
      "['Hmm', 'seems', 'like', \"that's\", 'how', 'it', 'is', 'for', 'some', 'others', 'here', 'too', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmm', 'seem', 'lik', 'that', 'oth'], ['hmm', 'seem', 'like', 'thats', 'others'])\n",
      "original document: \n",
      "['Then', 'you', 'bring', 'in', 'a', 'Silverback', 'Gorilla', 'to', 'eat', 'the', 'paper', 'towel?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bring', 'silverback', 'gorill', 'eat', 'pap', 'towel'], ['bring', 'silverback', 'gorilla', 'eat', 'paper', 'towel'])\n",
      "original document: \n",
      "['Yes']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['Is', 'there', 'anyone', 'ever', 'who', 'looked', 'at', 'this', 'picture', 'and', 'said,', '“That', 'looks', 'like', 'a', 'smart', 'guy', 'who', 'can', 'handle', 'the', 'complexities', 'of', 'being', 'a', 'modern', 'college', 'football', 'coach.”\\n\\nhttp://image.cdnllnwnl.xosnetwork.com/pics33/400/JW/JWULRFVBLTSLXMZ.20150213165120.jpg\\n\\n\\nI', 'would', 'pay', 'money', 'to', 'see', 'him', 'and', 'Butch', 'Jones', 'play', 'tic-tac-toe.', '', 'They', 'would', 'both', 'be', 'so', 'confused', 'and', 'frustrated.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'ev', 'look', 'pict', 'said', 'look', 'lik', 'smart', 'guy', 'handl', 'complex', 'modern', 'colleg', 'footbal', 'coach\\n\\nhttpimagecdnllnwnlxosnetworkcompics33400jwjwulrfvbltslxmz20150213165120jpg\\n\\n\\ni', 'would', 'pay', 'money', 'see', 'butch', 'jon', 'play', 'tictacto', 'would', 'confus', 'frust'], ['anyone', 'ever', 'look', 'picture', 'say', 'look', 'like', 'smart', 'guy', 'handle', 'complexities', 'modern', 'college', 'football', 'coach\\n\\nhttpimagecdnllnwnlxosnetworkcompics33400jwjwulrfvbltslxmz20150213165120jpg\\n\\n\\ni', 'would', 'pay', 'money', 'see', 'butch', 'jones', 'play', 'tictactoe', 'would', 'confuse', 'frustrate'])\n",
      "original document: \n",
      "[\"It's\", 'just', 'that', 'Okada/Tana', 'will', 'one', 'day', 'be', 'revisited', 'again,', \"that's\", 'a', 'given.', \"We've\", 'already', 'seen', 'it', 'for', 'the', 'title', 'and', 'at', 'every', 'big', 'event,', 'the', 'only', 'one', 'left', 'is', 'G1', 'Finals', 'so', 'you', 'might', 'as', 'well', 'do', 'it', 'there,', 'especially', 'in', 'a', 'G1', 'where', 'Okada', 'winning', 'is', 'most', 'likely', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okadatan', 'on', 'day', 'revisit', 'that', 'giv', 'wev', 'already', 'seen', 'titl', 'every', 'big', 'ev', 'on', 'left', 'g1', 'fin', 'might', 'wel', 'espec', 'g1', 'okad', 'win', 'lik'], ['okadatana', 'one', 'day', 'revisit', 'thats', 'give', 'weve', 'already', 'see', 'title', 'every', 'big', 'event', 'one', 'leave', 'g1', 'finals', 'might', 'well', 'especially', 'g1', 'okada', 'win', 'likely'])\n",
      "original document: \n",
      "[\"She's\", 'definitely', 'not,', 'so', 'she', 'should', 'be', 'safe', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['she', 'definit', 'saf'], ['shes', 'definitely', 'safe'])\n",
      "original document: \n",
      "['Here', 'you', 'go:', \"https://c2.staticflickr.com/8/7172/6711904025_a02d7cafdc_b.jpg\\n\\nI'll\", 'take', 'all', 'your', 'money', 'now', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'httpsc2staticflickrcom871726711904025_a02d7cafdc_bjpg\\n\\nill', 'tak', 'money', 'pleas'], ['go', 'httpsc2staticflickrcom871726711904025_a02d7cafdc_bjpg\\n\\nill', 'take', 'money', 'please'])\n",
      "original document: \n",
      "['Well,', '4DD345HRIN3', 'means', 'added', 'a', 'shrine.', 'You', 'can', 'find', 'stick', 'by', 'going', 'to', 'top', 'left', 'corner', 'on', 'empty', 'server', 'and', 'keeping', 'walking', 'up', 'and', 'right.', 'If', 'you', 'found', 'green', 'Donald', 'Trump', 'without', 'stick', 'try', 'changing', 'the', 'server.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', '4dd345hrin3', 'mean', 'ad', 'shrine', 'find', 'stick', 'going', 'top', 'left', 'corn', 'empty', 'serv', 'keep', 'walk', 'right', 'found', 'green', 'donald', 'trump', 'without', 'stick', 'try', 'chang', 'serv'], ['well', '4dd345hrin3', 'mean', 'add', 'shrine', 'find', 'stick', 'go', 'top', 'leave', 'corner', 'empty', 'server', 'keep', 'walk', 'right', 'find', 'green', 'donald', 'trump', 'without', 'stick', 'try', 'change', 'server'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Watch', 'fox', 'for', 'the', 'comedic', 'value?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['watch', 'fox', 'com', 'valu'], ['watch', 'fox', 'comedic', 'value'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Can', 'i', 'please', 'not', 'be', 'stressed', 'for', 'the', 'entire', 'game', 'this', 'year?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'stressed', 'entir', 'gam', 'year'], ['please', 'stress', 'entire', 'game', 'year'])\n",
      "original document: \n",
      "[\"I'm\", 'hoping', 'that', 'SpaceX', 'makes', 'fuel', 'costs', 'a', 'major', 'capital', 'concern', 'in', 'the', 'development', '&amp;', 'testing', 'of', 'the', 'BFR.', '', 'The', 'only', 'company', \"I've\", 'ever', 'heard', 'had', 'that', 'as', 'a', 'major', 'issue', 'was', 'Armadillo', 'Aerospace,', 'where', 'I', 'heard', 'that', 'fuel', 'was', 'about', 'a', 'third', 'of', 'their', 'capital', 'costs', 'in', 'the', 'development', 'of', 'the', 'Mod', 'vehicle', 'that', 'they', 'used', 'in', 'the', 'Lunar', 'Lander', 'Challenge.', '', 'That', 'was', 'a', 'whole', 'lot', 'of', 'actual', 'flying', 'time', 'for', 'the', 'vehicle,', 'and', 'it', 'showed', 'in', 'the', 'end', 'with', 'a', 'very', 'sturdy', 'and', 'reliable', 'vehicle.', '', 'It', 'is', 'also', 'something', 'pretty', 'common', 'in', 'terms', 'of', 'research', 'teams', 'doing', 'VTOL', 'rocket', 'research', 'to', 'have', 'a', 'whole', 'lot', 'of', 'flights.\\n\\nThat', 'is', 'something', 'you', 'have', 'the', 'luxury', 'to', 'do', 'when', 'you', 'have', 'a', 'reusable', 'vehicle....', 'which', 'fortunately', 'the', 'BFR', 'is', 'going', 'to', 'be.', '', 'It', \"wouldn't\", 'surprise', 'me', 'if', 'there', 'were', 'over', 'a', 'thousand', 'flights', '(perhaps', 'just', 'suborbital)', 'before', 'any', 'passengers', 'ever', 'went', 'on', \"board.\\n\\nI'm\", 'hoping', 'that', 'the', 'constellation', 'will', 'start', 'going', 'up', 'before', 'the', 'BFR', 'is', 'ready', 'for', 'revenue', 'service', 'with', 'satellites,', 'but', 'that', 'certainly', 'is', 'a', 'possibility', 'too.', '', 'I', 'really', 'want', 'to', 'see', 'a', 'bunch', 'of', 'videos', '[just', 'like', 'this](https://www.youtube.com/watch?v=HXdjxPY2j_0)', 'with', 'the', 'BFR', 'in', 'the', 'meantime.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'hop', 'spacex', 'mak', 'fuel', 'cost', 'maj', 'capit', 'concern', 'develop', 'amp', 'test', 'bfr', 'company', 'iv', 'ev', 'heard', 'maj', 'issu', 'armadillo', 'aerospac', 'heard', 'fuel', 'third', 'capit', 'cost', 'develop', 'mod', 'vehic', 'us', 'lun', 'land', 'challeng', 'whol', 'lot', 'act', 'fly', 'tim', 'vehic', 'show', 'end', 'sturdy', 'rely', 'vehic', 'also', 'someth', 'pretty', 'common', 'term', 'research', 'team', 'vtol', 'rocket', 'research', 'whol', 'lot', 'flights\\n\\nthat', 'someth', 'luxury', 'reus', 'vehic', 'fortun', 'bfr', 'going', 'wouldnt', 'surpr', 'thousand', 'flight', 'perhap', 'suborbit', 'passeng', 'ev', 'went', 'board\\n\\nim', 'hop', 'constel', 'start', 'going', 'bfr', 'ready', 'revenu', 'serv', 'satellit', 'certain', 'poss', 'real', 'want', 'see', 'bunch', 'video', 'lik', 'thishttpswwwyoutubecomwatchvhxdjxpy2j_0', 'bfr', 'meantim'], ['im', 'hop', 'spacex', 'make', 'fuel', 'cost', 'major', 'capital', 'concern', 'development', 'amp', 'test', 'bfr', 'company', 'ive', 'ever', 'hear', 'major', 'issue', 'armadillo', 'aerospace', 'hear', 'fuel', 'third', 'capital', 'cost', 'development', 'mod', 'vehicle', 'use', 'lunar', 'lander', 'challenge', 'whole', 'lot', 'actual', 'fly', 'time', 'vehicle', 'show', 'end', 'sturdy', 'reliable', 'vehicle', 'also', 'something', 'pretty', 'common', 'term', 'research', 'team', 'vtol', 'rocket', 'research', 'whole', 'lot', 'flights\\n\\nthat', 'something', 'luxury', 'reusable', 'vehicle', 'fortunately', 'bfr', 'go', 'wouldnt', 'surprise', 'thousand', 'flight', 'perhaps', 'suborbital', 'passengers', 'ever', 'go', 'board\\n\\nim', 'hop', 'constellation', 'start', 'go', 'bfr', 'ready', 'revenue', 'service', 'satellite', 'certainly', 'possibility', 'really', 'want', 'see', 'bunch', 'videos', 'like', 'thishttpswwwyoutubecomwatchvhxdjxpy2j_0', 'bfr', 'meantime'])\n",
      "original document: \n",
      "['You', 'look', 'amazing!', 'I', 'love', 'your', 'abs']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'amaz', 'lov', 'ab'], ['look', 'amaze', 'love', 'abs'])\n",
      "original document: \n",
      "['Sturm', 'is', 'really', 'good', 'in', 'crucible,', 'just', \"don't\", 'use', 'it', 'in', 'PVE', 'because', 'there', 'are', 'too', 'many', 'exotics', 'with', 'better', 'perks.', 'Sadly', 'I', 'think', 'the', 'tractor', 'Cannon', 'is', 'better,', 'and', 'that', 'says', 'something']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sturm', 'real', 'good', 'cruc', 'dont', 'us', 'pve', 'many', 'exot', 'bet', 'perk', 'sad', 'think', 'tract', 'cannon', 'bet', 'say', 'someth'], ['sturm', 'really', 'good', 'crucible', 'dont', 'use', 'pve', 'many', 'exotics', 'better', 'perk', 'sadly', 'think', 'tractor', 'cannon', 'better', 'say', 'something'])\n",
      "original document: \n",
      "['Print', 'is', 'way', 'too', 'dark,', 'but', 'I', \"don't\", 'think', 'people', 'will', 'call', 'you', 'out', 'for', 'this.', \"It's\", 'quite', 'an', 'obscure', 'piece', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['print', 'way', 'dark', 'dont', 'think', 'peopl', 'cal', 'quit', 'obsc', 'piec'], ['print', 'way', 'dark', 'dont', 'think', 'people', 'call', 'quite', 'obscure', 'piece'])\n",
      "original document: \n",
      "['Thanks,', 'I', 'think', 'I', 'may', 'have', 'picked', 'one', 'out', 'when', 'I', 'received', 'it', 'but', \"haven't\", 'used', 'it', 'in', 'over', 'a', 'year', 'so', \"I'll\", 'definitely', 'look', 'into', 'that!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'think', 'may', 'pick', 'on', 'receiv', 'hav', 'us', 'year', 'il', 'definit', 'look'], ['thank', 'think', 'may', 'pick', 'one', 'receive', 'havent', 'use', 'year', 'ill', 'definitely', 'look'])\n",
      "original document: \n",
      "['In', 'the', 'context', 'of', 'referendums', 'and', 'whether', 'they', 'have', 'the', 'right', 'to', 'do', 'it,', 'self-determination', '&gt;', 'all.', 'They', 'can', 'also', 'easily', 'declare', 'independence.\\n\\nIn', 'the', 'context', 'of', 'actual,', 'successful', 'implementation,', 'however', 'force', '&gt;', 'all,', 'obviously.', 'Though', 'I', 'can', 'hardly', 'imagine', 'Spain', 'pulling', 'a', '1991', 'Yugoslavia', 'and', 'bombing', 'Barcelona,', 'especially', 'since', 'that', 'would', 'make', 'the', 'situation', 'worse', 'for', 'Spain', 'in', 'the', 'long', 'run.', 'Then', 'again,', '50%', 'wanting', 'something', \"shouldn't\", 'be', 'even', 'close', 'to', 'enough', 'for', 'anything', 'of', 'such', 'major', 'importance.\\n\\nAnd', 'as', 'for', 'geopolitics,', 'military', 'power', \"isn't\", 'even', 'close', 'to', '&gt;', 'all,', 'otherwise', 'North', 'Korea', 'would', 'long', 'since', 'have', 'stopped', 'existing', 'and', 'the', 'US', 'would', 'own', 'all', 'the', 'oil', 'in', 'the', 'Middle', 'East.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['context', 'referendum', 'wheth', 'right', 'selfdetermin', 'gt', 'also', 'easy', 'decl', 'independence\\n\\nin', 'context', 'act', 'success', 'impl', 'howev', 'forc', 'gt', 'obvy', 'though', 'hard', 'imagin', 'spain', 'pul', 'one thousand, nine hundred and ninety-on', 'yugoslav', 'bomb', 'barcelon', 'espec', 'sint', 'would', 'mak', 'situ', 'wors', 'spain', 'long', 'run', 'fifty', 'want', 'someth', 'shouldnt', 'ev', 'clos', 'enough', 'anyth', 'maj', 'importance\\n\\nand', 'geopolit', 'milit', 'pow', 'isnt', 'ev', 'clos', 'gt', 'otherw', 'nor', 'kore', 'would', 'long', 'sint', 'stop', 'ex', 'us', 'would', 'oil', 'middl', 'east'], ['context', 'referendums', 'whether', 'right', 'selfdetermination', 'gt', 'also', 'easily', 'declare', 'independence\\n\\nin', 'context', 'actual', 'successful', 'implementation', 'however', 'force', 'gt', 'obviously', 'though', 'hardly', 'imagine', 'spain', 'pull', 'one thousand, nine hundred and ninety-one', 'yugoslavia', 'bomb', 'barcelona', 'especially', 'since', 'would', 'make', 'situation', 'worse', 'spain', 'long', 'run', 'fifty', 'want', 'something', 'shouldnt', 'even', 'close', 'enough', 'anything', 'major', 'importance\\n\\nand', 'geopolitics', 'military', 'power', 'isnt', 'even', 'close', 'gt', 'otherwise', 'north', 'korea', 'would', 'long', 'since', 'stop', 'exist', 'us', 'would', 'oil', 'middle', 'east'])\n",
      "original document: \n",
      "['My', 'ex', 'had', 'a', 'cat', 'who', 'loved', 'riding', 'with', 'us', 'in', 'the', 'car.', '', 'One', 'time', 'she', 'was', 'camping', 'out', 'in', 'the', 'back', 'and', 'we', 'stopped', 'at', 'Burger', 'King', 'and', 'got', 'some', 'chicken', 'tenders.', '', 'We', 'went', 'out', 'to', 'the', 'car', 'and', 'decided', 'to', 'eat', 'them', 'while', 'we', 'were', 'still', 'parked', 'there;', 'we', 'just', 'set', 'them', 'in', 'the', 'center', 'console.', '', 'She', 'smelled', 'them,', 'wandered', 'up', 'to', 'the', 'box,', 'and', 'just', 'casually', 'took', 'one.', '', \"Didn't\", 'eat', 'it,', 'but', 'we', 'thought', 'that', 'was', 'so', 'adorable.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ex', 'cat', 'lov', 'rid', 'us', 'car', 'on', 'tim', 'camp', 'back', 'stop', 'burg', 'king', 'got', 'chick', 'tend', 'went', 'car', 'decid', 'eat', 'stil', 'park', 'set', 'cent', 'consol', 'smel', 'wand', 'box', 'cas', 'took', 'on', 'didnt', 'eat', 'thought', 'ad'], ['ex', 'cat', 'love', 'rid', 'us', 'car', 'one', 'time', 'camp', 'back', 'stop', 'burger', 'king', 'get', 'chicken', 'tender', 'go', 'car', 'decide', 'eat', 'still', 'park', 'set', 'center', 'console', 'smell', 'wander', 'box', 'casually', 'take', 'one', 'didnt', 'eat', 'think', 'adorable'])\n",
      "original document: \n",
      "['143418153|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', '7aSzBYHR)\\n\\n2012:', 'Gary', 'Johnson\\n2016:', 'Gary', 'Johnson\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, one hundred and fifty-thr', 'gt', 'unit', 'stat', 'anonym', 'id', '7aszbyhr\\n\\n2012', 'gary', 'johnson\\n2016', 'gary', 'johnson\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, one hundred and fifty-three', 'gt', 'unite', 'state', 'anonymous', 'id', '7aszbyhr\\n\\n2012', 'gary', 'johnson\\n2016', 'gary', 'johnson\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"There's\", 'so', 'many', 'crappy', 'looking', 'Yugioh', 'cards', 'that', 'ended', 'up', 'being', 'part', 'of', 'some', 'really', 'niche', 'broken', 'strategy', 'though', 'and', \"that's\", 'the', 'fun.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'many', 'crappy', 'look', 'yugioh', 'card', 'end', 'part', 'real', 'nich', 'brok', 'strategy', 'though', 'that', 'fun'], ['theres', 'many', 'crappy', 'look', 'yugioh', 'card', 'end', 'part', 'really', 'niche', 'break', 'strategy', 'though', 'thats', 'fun'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['No', 'reason', 'for', 'that', 'kind', 'of', 'tone,', 'you', 'certainly', \"wouldn't\", 'be', 'in', 'any', 'danger']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'kind', 'ton', 'certain', 'wouldnt', 'dang'], ['reason', 'kind', 'tone', 'certainly', 'wouldnt', 'danger'])\n",
      "original document: \n",
      "['I', 'know,', \"i'm\", 'not', '100%', 'sure', 'about', 'it', 'either', 'But', \"i'll\", 'let', 'it', 'be', 'for', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'im', 'one hundred', 'sur', 'eith', 'il', 'let'], ['know', 'im', 'one hundred', 'sure', 'either', 'ill', 'let'])\n",
      "original document: \n",
      "[\"that's\", 'true.', 'fury', 'was', 'remorselessly', 'grim', 'but', 'it', 'felt', 'maybe', 'less', 'grounded', '(?)', 'than', 'his', 'other', 'darker', 'films.', 'training', 'day', 'is,', 'imo,', 'as', 'close', 'to', 'a', 'perfect', \"'life\", 'fucking', \"sucks'\", 'movie', 'as', 'there', 'is', 'because', 'the', 'leads', 'are', 'dark,', 'twisted,', 'but', 'emotionally', 'relatable.', 'in', 'fury', 'it', 'felt', 'like', 'each', 'character', 'was', 'relentlessly', 'dark', 'which', 'is,', 'perhaps,', 'how', 'life', 'was', 'for', 'soldiers', 'during', 'ww2.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'tru', 'fury', 'remorseless', 'grim', 'felt', 'mayb', 'less', 'ground', 'dark', 'film', 'train', 'day', 'imo', 'clos', 'perfect', 'lif', 'fuck', 'suck', 'movy', 'lead', 'dark', 'twist', 'emot', 'rel', 'fury', 'felt', 'lik', 'charact', 'relentless', 'dark', 'perhap', 'lif', 'soldy', 'ww2'], ['thats', 'true', 'fury', 'remorselessly', 'grim', 'felt', 'maybe', 'less', 'ground', 'darker', 'film', 'train', 'day', 'imo', 'close', 'perfect', 'life', 'fuck', 'suck', 'movie', 'lead', 'dark', 'twist', 'emotionally', 'relatable', 'fury', 'felt', 'like', 'character', 'relentlessly', 'dark', 'perhaps', 'life', 'soldier', 'ww2'])\n",
      "original document: \n",
      "['**One**', 'DAY', '**Six**', 'HOURS', '!!!!!!!!\\n\\nOur', '2nd', 'longest', 'time', 'yet!\\n\\nCLOCK', 'RESET!!!!!', 'WOOO', 'GO', 'BROWNS!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'day', 'six', 'hour', '\\n\\nour', '2nd', 'longest', 'tim', 'yet\\n\\nclock', 'reset', 'wooo', 'go', 'brown'], ['one', 'day', 'six', 'hours', '\\n\\nour', '2nd', 'longest', 'time', 'yet\\n\\nclock', 'reset', 'wooo', 'go', 'brown'])\n",
      "original document: \n",
      "['Lol', 'well', 'when', 'I', 'see', 'u', 'in', 'hell', \"let's\", 'get', 'a', 'beer']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'wel', 'see', 'u', 'hel', 'let', 'get', 'beer'], ['lol', 'well', 'see', 'u', 'hell', 'let', 'get', 'beer'])\n",
      "original document: \n",
      "['ah', 'okay..', \"i've\", 'just', 'hit', 'level', '20.', 'So', 'do', 'I', 'wait', 'or', 'decrypt', 'straight', 'up', 'once', \"i've\", 'got', 'some?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'okay', 'iv', 'hit', 'level', 'twenty', 'wait', 'decrypt', 'straight', 'iv', 'got'], ['ah', 'okay', 'ive', 'hit', 'level', 'twenty', 'wait', 'decrypt', 'straight', 'ive', 'get'])\n",
      "original document: \n",
      "['And', 'to', 'some,', 'including', 'myself,', 'it', 'is', 'kind', 'of', 'retarded', 'to', 'ask', 'someone', 'to', 'stand', 'for', 'something', 'they', 'have', 'no', 'obligation', 'to.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['includ', 'kind', 'retard', 'ask', 'someon', 'stand', 'someth', 'oblig'], ['include', 'kind', 'retard', 'ask', 'someone', 'stand', 'something', 'obligation'])\n",
      "original document: \n",
      "['I', 'agree', 'it', 'was', 'a', 'good', 'movie', 'but', 'it', 'was', 'not', 'an', 'original', 'concept.', \"There's\", 'very', 'few', 'movies', 'like', 'it,', 'but', 'almost', 'every', 'writer', 'or', 'aspiring', 'producer/director', 'Ik', 'has', 'had', 'a', 'similar', 'idea.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'good', 'movy', 'origin', 'conceiv', 'ther', 'movy', 'lik', 'almost', 'every', 'writ', 'aspir', 'producerdirect', 'ik', 'simil', 'ide'], ['agree', 'good', 'movie', 'original', 'concept', 'theres', 'movies', 'like', 'almost', 'every', 'writer', 'aspire', 'producerdirector', 'ik', 'similar', 'idea'])\n",
      "original document: \n",
      "['*Neo', 'Yokio*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['neo', 'yokio'], ['neo', 'yokio'])\n",
      "original document: \n",
      "['Concentrate', 'and', 'ask', 'again']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'ask'], ['concentrate', 'ask'])\n",
      "original document: \n",
      "['yeah.', 'wait', 'for', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'wait'], ['yeah', 'wait'])\n",
      "original document: \n",
      "['Guys', 'loved', 'listening.', 'First', 'time', 'very', 'impressed,', 'quality', 'and', 'content.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guy', 'lov', 'list', 'first', 'tim', 'impress', 'qual', 'cont'], ['guy', 'love', 'listen', 'first', 'time', 'impress', 'quality', 'content'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop0dm/):\\n\\nAll', 'the', 'other', 'copies', 'are', 'not', 'paid.', 'My', 'parents', 'paid', 'to', 'print', 'some', 'copies', 'when', 'I', 'was', 'younger.', 'After', 'getting', 'my', 'book', 'professionally', 'edited', 'I', 'went', 'through', 'CreateSpace', 'and', 'Amazon', '(which', 'is', 'free)', 'to', 'publish', 'and', 'distribute', 'my', 'book']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop0dm\\n\\nall', 'cop', 'paid', 'par', 'paid', 'print', 'cop', 'young', 'get', 'book', 'profess', 'edit', 'went', 'createspac', 'amazon', 'fre', 'publ', 'distribut', 'book'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop0dm\\n\\nall', 'copy', 'pay', 'parent', 'pay', 'print', 'copy', 'younger', 'get', 'book', 'professionally', 'edit', 'go', 'createspace', 'amazon', 'free', 'publish', 'distribute', 'book'])\n",
      "original document: \n",
      "['Dad,', 'you', 'are', 'fucked!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dad', 'fuck'], ['dad', 'fuck'])\n",
      "original document: \n",
      "['Nope.', \"It's\", 'a', 'singular', 'noun', 'for', 'a', 'group', 'of', 'nouns,', 'and', 'therefore', 'would', 'need', 'a', 'singular', 'verb.', 'It', 'describes', 'the', 'group', 'of', 'nouns', 'as', 'a', 'singular', 'entity', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nop', 'singul', 'noun', 'group', 'noun', 'theref', 'would', 'nee', 'singul', 'verb', 'describ', 'group', 'noun', 'singul', 'ent'], ['nope', 'singular', 'noun', 'group', 'nouns', 'therefore', 'would', 'need', 'singular', 'verb', 'describe', 'group', 'nouns', 'singular', 'entity'])\n",
      "original document: \n",
      "['True', 'enough,', 'but', \"it's\", 'slightly', 'more', 'of', 'a', 'pain', 'in', 'the', 'ass', 'to', 'build', 'a', 'mailing', 'list', 'that', 'way.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'enough', 'slight', 'pain', 'ass', 'build', 'mail', 'list', 'way'], ['true', 'enough', 'slightly', 'pain', 'ass', 'build', 'mail', 'list', 'way'])\n",
      "original document: \n",
      "['I', 'just', 'it', 'for', 'Rank', '4', 'this', 'morning.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rank', 'four', 'morn'], ['rank', 'four', 'morning'])\n",
      "original document: \n",
      "['really?', 'the', 'only', 'subaru', 'issues', 'i', 'would', 'regularly', 'see', 'are', 'valve', 'cover', 'leaksm', 'head', 'gasket', 'leakage', 'at', '100k', 'miles,', 'and', 'timing', 'cover', 'leaks', '(on', '6', 'cylinders).', 'other', 'than', 'that,', 'never', 'seen', 'any', 'braking', 'issues', 'or', 'electrical', 'issues.', 'Worked', 'as', 'a', 'subaru', 'mechanic', 'for', '5', 'years...\\n\\nthat', 'being', 'said,', 'ill', 'buy', 'the', 'car', 'off', 'of', 'you.', 'How', 'close', 'to', '$500', 'are', 'we?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'subaru', 'issu', 'would', 'regul', 'see', 'valv', 'cov', 'leaksm', 'head', 'gasket', 'leak', '100k', 'mil', 'tim', 'cov', 'leak', 'six', 'cylind', 'nev', 'seen', 'brak', 'issu', 'elect', 'issu', 'work', 'subaru', 'mech', 'fiv', 'years\\n\\nthat', 'said', 'il', 'buy', 'car', 'clos', 'five hundred'], ['really', 'subaru', 'issue', 'would', 'regularly', 'see', 'valve', 'cover', 'leaksm', 'head', 'gasket', 'leakage', '100k', 'miles', 'time', 'cover', 'leak', 'six', 'cylinders', 'never', 'see', 'brake', 'issue', 'electrical', 'issue', 'work', 'subaru', 'mechanic', 'five', 'years\\n\\nthat', 'say', 'ill', 'buy', 'car', 'close', 'five hundred'])\n",
      "original document: \n",
      "[\"That's\", 'pretty', 'much', 'how', 'I', 'feel.', 'If', 'I', 'had', 'a', 'second', 'team,', \"it'd\", 'be', 'Texas.', 'I', 'loved', 'watching', 'them', 'as', 'a', 'kid,', 'and', 'their', 'orange', 'is', 'one', 'of', 'my', 'favorite', 'colors.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'pretty', 'much', 'feel', 'second', 'team', 'itd', 'texa', 'lov', 'watch', 'kid', 'orang', 'on', 'favorit', 'col'], ['thats', 'pretty', 'much', 'feel', 'second', 'team', 'itd', 'texas', 'love', 'watch', 'kid', 'orange', 'one', 'favorite', 'color'])\n",
      "original document: \n",
      "['Girl,', 'look', 'how', 'fucking', 'orange', 'you', 'look,', 'girl!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['girl', 'look', 'fuck', 'orang', 'look', 'girl'], ['girl', 'look', 'fuck', 'orange', 'look', 'girl'])\n",
      "original document: \n",
      "['Odc,', 'Turbo,', 'and', 'a', 'couple', 'nitro']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['odc', 'turbo', 'coupl', 'nitro'], ['odc', 'turbo', 'couple', 'nitro'])\n",
      "original document: \n",
      "['Asicboost', 'does', 'not', 'reduce', 'the', 'CO2', 'emissions', 'of', 'bitcoin.', 'It', 'just', 'changes', 'the', '%', 'allocation', 'of', 'hash', 'power', 'to', 'those', 'that', 'have', 'it', 'enabled', 'from', 'those', 'that', \"don't.\", 'The', 'whole', 'stability', 'of', 'bitcoin', 'relies', 'on', 'the', 'amount', 'of', 'hash', 'power', 'being', 'committed', 'to', 'the', 'network.', 'The', 'more', 'committed', 'hashpower,', 'the', 'stronger', 'the', 'network.', 'Asicboost', 'games', 'the', 'system', 'to', 'the', 'advantage', 'of', 'a', 'select', 'few.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['asicboost', 'reduc', 'co2', 'emit', 'bitcoin', 'chang', 'alloc', 'hash', 'pow', 'en', 'dont', 'whol', 'stabl', 'bitcoin', 'rely', 'amount', 'hash', 'pow', 'commit', 'network', 'commit', 'hashpow', 'stronger', 'network', 'asicboost', 'gam', 'system', 'adv', 'select'], ['asicboost', 'reduce', 'co2', 'emissions', 'bitcoin', 'change', 'allocation', 'hash', 'power', 'enable', 'dont', 'whole', 'stability', 'bitcoin', 'rely', 'amount', 'hash', 'power', 'commit', 'network', 'commit', 'hashpower', 'stronger', 'network', 'asicboost', 'game', 'system', 'advantage', 'select'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Very', 'nice', 'choice,', 'what', 'else', 'did', 'it', 'have?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'cho', 'els'], ['nice', 'choice', 'else'])\n",
      "original document: \n",
      "['Judge', 'has', 'been', 'average', 'in', 'RF', 'according', 'to', 'his', 'defensive', 'WAR.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['judg', 'av', 'rf', 'accord', 'defend', 'war'], ['judge', 'average', 'rf', 'accord', 'defensive', 'war'])\n",
      "original document: \n",
      "['\"Turn', 'around.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turn', 'around'], ['turn', 'around'])\n",
      "original document: \n",
      "['Do', 'you', 'think', 'manual', 'labor', 'positions', 'will', 'become', 'reallocated', 'in', 'a', 'way?', \"I'd\", 'with', 'our', 'advancements', 'we', 'will', 'also', 'open', 'up', 'new', 'doors', 'for', 'workers.', '', '\\n\\nI', 'also', 'feel', 'like', 'humanity', 'will', 'be', 'able', 'to', 'adapt', 'to', 'change', 'in', 'the', 'workforce', 'like', 'in', 'the', 'past', 'but', 'maybe', \"I'm\", 'being', 'too', 'idealistic', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'man', 'lab', 'posit', 'becom', 'realloc', 'way', 'id', 'adv', 'also', 'op', 'new', 'door', 'work', '\\n\\ni', 'also', 'feel', 'lik', 'hum', 'abl', 'adapt', 'chang', 'workforc', 'lik', 'past', 'mayb', 'im', 'id'], ['think', 'manual', 'labor', 'position', 'become', 'reallocate', 'way', 'id', 'advancements', 'also', 'open', 'new', 'doors', 'workers', '\\n\\ni', 'also', 'feel', 'like', 'humanity', 'able', 'adapt', 'change', 'workforce', 'like', 'past', 'maybe', 'im', 'idealistic'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['They', 'both', 'contain', 'rodents', 'dying.', \"That's\", 'a', 'fairly', 'close', 'correlation.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['contain', 'rod', 'dying', 'that', 'fair', 'clos', 'correl'], ['contain', 'rodents', 'die', 'thats', 'fairly', 'close', 'correlation'])\n",
      "original document: \n",
      "['It', 'looks', 'like', 'your', 'original', 'comment', 'was', 'removed,', 'but', 'I', 'am', 'still', 'fine', 'in', 'trading']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'origin', 'com', 'remov', 'stil', 'fin', 'trad'], ['look', 'like', 'original', 'comment', 'remove', 'still', 'fine', 'trade'])\n",
      "original document: \n",
      "['Teach', 'an', 'adult', 'aged', 'woman', 'how', 'to', 'whistle', 'if', 'she', 'has', 'not', 'yet', 'found', 'the', 'talent.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['teach', 'adult', 'ag', 'wom', 'whistl', 'yet', 'found', 'tal'], ['teach', 'adult', 'age', 'woman', 'whistle', 'yet', 'find', 'talent'])\n",
      "original document: \n",
      "['It', 'was', 'OK']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok'], ['ok'])\n",
      "original document: \n",
      "['First', 'of', 'many', 'I', 'hope!', 'Let', 'me', 'know', 'if', 'you', 'take', 'requests']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'many', 'hop', 'let', 'know', 'tak', 'request'], ['first', 'many', 'hope', 'let', 'know', 'take', 'request'])\n",
      "original document: \n",
      "['Emmelyne', 'Costayne']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['emmelyn', 'costayn'], ['emmelyne', 'costayne'])\n",
      "original document: \n",
      "['Where', 'can', 'I', 'find', 'more', 'information', 'on', 'that?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['find', 'inform'], ['find', 'information'])\n",
      "original document: \n",
      "['Relax,', 'everything', 'will', 'be', 'alright.', 'I', 'mean,', 'I', \"don't\", 'know', 'what', 'will', 'happen,', 'and', 'I', 'honeslty', 'never', 'bothered', 'to', 'check', 'what', 'that', 'leak', 'business', 'was', 'about,', 'but', \"I'm\", 'sure', \"there's\", 'no', 'need', 'to', 'stress', 'over', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['relax', 'everyth', 'alright', 'mean', 'dont', 'know', 'hap', 'honesl', 'nev', 'both', 'check', 'leak', 'busy', 'im', 'sur', 'ther', 'nee', 'stress'], ['relax', 'everything', 'alright', 'mean', 'dont', 'know', 'happen', 'honeslty', 'never', 'bother', 'check', 'leak', 'business', 'im', 'sure', 'theres', 'need', 'stress'])\n",
      "original document: \n",
      "['Blood', 'donor', 'of', 'three', 'years', 'here.', 'My', 'suggestion', 'is', 'looking', 'away.', 'The', 'pain', 'is', 'only', 'for', 'a', 'split', 'second', 'and', 'does', 'not', 'last.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['blood', 'don', 'three', 'year', 'suggest', 'look', 'away', 'pain', 'split', 'second', 'last'], ['blood', 'donor', 'three', 'years', 'suggestion', 'look', 'away', 'pain', 'split', 'second', 'last'])\n",
      "original document: \n",
      "['Thank', 'you', 'Shannistration', 'for', 'your', 'submission', 'to', '/r/thatHappened!', 'Unfortunately', 'it', 'was', 'removed', 'for', 'the', 'following', 'reason(s):\\n\\n\\n*', '**This', 'is', 'either', 'a', 'recent', 'repost', 'or', 'this', 'specific', 'story', 'is', 'being', 'or', 'has', 'been', 'posted', 'repeatedly.**\\n\\n', '', 'Avoid', 'reposting', 'by', 'using', 'the', 'search', 'function', 'and', 'browsing', 'the', 'Top', 'of', 'All', 'Time.', 'Users', 'who', 'frequently', 'repost', 'may', 'be', 'permanently', 'banned.\\n\\n\\n\\n\\n\\nPlease', '[message', 'the', 'moderators](https://www.reddit.com/message/compose?to=/r/thatHappened)', 'if', 'you', 'have', 'any', 'questions.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'shan', 'submit', 'rthathappened', 'unfortun', 'remov', 'follow', 'reasons\\n\\n\\n', 'eith', 'rec', 'repost', 'spec', 'story', 'post', 'repeatedly\\n\\n', 'avoid', 'repost', 'us', 'search', 'funct', 'brows', 'top', 'tim', 'us', 'frequ', 'repost', 'may', 'perm', 'banned\\n\\n\\n\\n\\n\\nplease', 'mess', 'moderatorshttpswwwredditcommessagecomposetorthathap', 'quest'], ['thank', 'shannistration', 'submission', 'rthathappened', 'unfortunately', 'remove', 'follow', 'reasons\\n\\n\\n', 'either', 'recent', 'repost', 'specific', 'story', 'post', 'repeatedly\\n\\n', 'avoid', 'reposting', 'use', 'search', 'function', 'browse', 'top', 'time', 'users', 'frequently', 'repost', 'may', 'permanently', 'banned\\n\\n\\n\\n\\n\\nplease', 'message', 'moderatorshttpswwwredditcommessagecomposetorthathappened', 'question'])\n",
      "original document: \n",
      "['Welp.', 'I', 'guess', 'bye', 'Brewers']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welp', 'guess', 'bye', 'brew'], ['welp', 'guess', 'bye', 'brewers'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['People', 'making', 'smash', 'displays', 'or', 'MvC', \"displays...\\n\\nI'm\", 'over', 'here', 'making', 'my', 'MUGEN', 'display', 'smh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'mak', 'smash', 'display', 'mvc', 'displays\\n\\nim', 'mak', 'mug', 'display', 'smh'], ['people', 'make', 'smash', 'display', 'mvc', 'displays\\n\\nim', 'make', 'mugen', 'display', 'smh'])\n",
      "original document: \n",
      "['In', 'Australia,', 'Eb', 'games', 'is', 'giving', 'a', 'cappy', 'hat', 'with', 'every', 'preorder.', 'Not', 'sure', 'about', 'other', 'locations', 'but', 'it', \"doesn't\", 'seem', 'like', 'there', 'are', 'any', 'in', 'game', 'bonuses.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['austral', 'eb', 'gam', 'giv', 'cappy', 'hat', 'every', 'preord', 'sur', 'loc', 'doesnt', 'seem', 'lik', 'gam', 'bonus'], ['australia', 'eb', 'game', 'give', 'cappy', 'hat', 'every', 'preorder', 'sure', 'locations', 'doesnt', 'seem', 'like', 'game', 'bonuses'])\n",
      "original document: \n",
      "['Literally', 'spent', 'an', 'hour', 'testing', 'it.', 'Below', '190', \"didn't\", 'crash', 'at', 'all', 'for', 'me.', 'At', '195', 'literally', \"couldn't\", 'load', 'into', 'the', 'tower', 'after', '10', 'attempts.', \"Can't\", 'just', 'pass', 'that', 'off', 'as', 'RNG.', 'It', 'definitely', 'became', 'more', 'difficult', 'to', 'get', 'into', 'the', 'tower', 'as', 'I', 'added', 'more', 'to', 'my', 'vault.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'spent', 'hour', 'test', 'one hundred and ninety', 'didnt', 'crash', 'one hundred and ninety-fiv', 'lit', 'couldnt', 'load', 'tow', 'ten', 'attempt', 'cant', 'pass', 'rng', 'definit', 'becam', 'difficult', 'get', 'tow', 'ad', 'vault'], ['literally', 'spend', 'hour', 'test', 'one hundred and ninety', 'didnt', 'crash', 'one hundred and ninety-five', 'literally', 'couldnt', 'load', 'tower', 'ten', 'attempt', 'cant', 'pass', 'rng', 'definitely', 'become', 'difficult', 'get', 'tower', 'add', 'vault'])\n",
      "original document: \n",
      "['Quick', 'calculation', 'shows', 'about', '8', 'or', '9', 'carbs', 'total', 'for', 'the', 'glaze.', 'Can', 'probably', 'be', 'omitted.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['quick', 'calc', 'show', 'eight', 'nin', 'carb', 'tot', 'glaz', 'prob', 'omit'], ['quick', 'calculation', 'show', 'eight', 'nine', 'carbs', 'total', 'glaze', 'probably', 'omit'])\n",
      "original document: \n",
      "['No', 'worries!', 'Good', 'luck!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['worry', 'good', 'luck'], ['worry', 'good', 'luck'])\n",
      "original document: \n",
      "['Jet', 'sweep', 'to', 'the', 'TE....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jet', 'sweep', 'te'], ['jet', 'sweep', 'te'])\n",
      "original document: \n",
      "['Lmao', 'if', 'only', 'the', 'founders', 'of', 'this', 'country', 'could', 'see', 'how', 'sensitive', '“patriots”', 'have', 'done', 'a', 'complete', '180', 'on', 'what', 'was', 'supposed', 'to', 'be', 'the', 'most', 'important', 'right', 'we', 'have.', '\\n\\nNot', 'saying', 'the', 'first', 'amendment', 'is', 'supposed', 'to', 'protect', 'people', 'from', 'all', 'the', 'consequences', 'of', 'their', 'speech,', 'but', 'to', 'badmouth', 'people', 'using', 'it', 'and', 'to', 'seemingly', 'relish', 'in', 'punishment', 'for', 'a', 'peaceful,', 'non-disruptive', 'protest', 'is', 'an', 'absolute', 'absurdity.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lmao', 'found', 'country', 'could', 'see', 'sensit', 'patriot', 'don', 'complet', 'one hundred and eighty', 'suppos', 'import', 'right', '\\n\\nnot', 'say', 'first', 'amend', 'suppos', 'protect', 'peopl', 'consequ', 'speech', 'badmou', 'peopl', 'us', 'seem', 'rel', 'pun', 'peac', 'nondisrupt', 'protest', 'absolv', 'absurd'], ['lmao', 'founder', 'country', 'could', 'see', 'sensitive', 'patriots', 'do', 'complete', 'one hundred and eighty', 'suppose', 'important', 'right', '\\n\\nnot', 'say', 'first', 'amendment', 'suppose', 'protect', 'people', 'consequences', 'speech', 'badmouth', 'people', 'use', 'seemingly', 'relish', 'punishment', 'peaceful', 'nondisruptive', 'protest', 'absolute', 'absurdity'])\n",
      "original document: \n",
      "['Hey.', 'I', 'want', 'One', 'Piece', 'if', \"that's\", 'cool?', 'It', 'is', 'the', 'full', 'game,', 'yes?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'want', 'on', 'piec', 'that', 'cool', 'ful', 'gam', 'ye'], ['hey', 'want', 'one', 'piece', 'thats', 'cool', 'full', 'game', 'yes'])\n",
      "original document: \n",
      "['Aho', 'Girl', 'and', 'Made', 'In', 'Abyss', 'were', 'worth', 'watching,', 'but', 'they', \"didn't\", 'exactly', 'have', 'stiff', 'competition.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aho', 'girl', 'mad', 'abyss', 'wor', 'watch', 'didnt', 'exact', 'stiff', 'competit'], ['aho', 'girl', 'make', 'aby', 'worth', 'watch', 'didnt', 'exactly', 'stiff', 'competition'])\n",
      "original document: \n",
      "['&gt;The', 'ranking', 'in', 'Forza', '7', 'is', 'very', 'unusual.', 'Nvidia', 'has', 'confirmed', 'ComputerBase,', 'however,', 'that', 'the', 'results', 'are', 'so', 'correct,', 'so', 'there', 'is', 'no', 'problem', 'with', 'the', 'system', 'in', 'the', 'editorial', 'regarding', 'GeForce.\\n\\n\\nI', 'am', 'so', 'confused']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtthe', 'rank', 'forz', 'sev', 'unus', 'nvid', 'confirm', 'computerbas', 'howev', 'result', 'correct', 'problem', 'system', 'edit', 'regard', 'geforce\\n\\n\\ni', 'confus'], ['gtthe', 'rank', 'forza', 'seven', 'unusual', 'nvidia', 'confirm', 'computerbase', 'however', 'result', 'correct', 'problem', 'system', 'editorial', 'regard', 'geforce\\n\\n\\ni', 'confuse'])\n",
      "original document: \n",
      "['I', 'like', '\"descriptive', 'annotation\"', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'describ', 'annot'], ['like', 'descriptive', 'annotation'])\n",
      "original document: \n",
      "['Halfmoon', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['halfmoon', 'lt3'], ['halfmoon', 'lt3'])\n",
      "original document: \n",
      "['Traps', 'are', 'gay', 'tbh']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trap', 'gay', 'tbh'], ['trap', 'gay', 'tbh'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Ephraim', 'gauntlet', 'was', 'bigger']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ephraim', 'gauntlet', 'big'], ['ephraim', 'gauntlet', 'bigger'])\n",
      "original document: \n",
      "['Why', 'not', 'just', 'milk', 'an', 'hour', 'earlier,', 'so', \"they're\", 'routine', \"isn't\", 'messed', 'up?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['milk', 'hour', 'ear', 'theyr', 'routin', 'isnt', 'mess'], ['milk', 'hour', 'earlier', 'theyre', 'routine', 'isnt', 'mess'])\n",
      "original document: \n",
      "['Is', 'it', 'only', 'opiates', 'though?', '', 'I', 'told', 'them', 'I', 'smoke', 'weed', 'like', 'every', 'day', 'and', 'they', 'just', 'told', 'me', 'not', 'to', 'come', 'in', 'high.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'though', 'told', 'smok', 'wee', 'lik', 'every', 'day', 'told', 'com', 'high'], ['opiates', 'though', 'tell', 'smoke', 'weed', 'like', 'every', 'day', 'tell', 'come', 'high'])\n",
      "original document: \n",
      "['Salve,', 'to', 'keep', 'it', 'moist.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['salv', 'keep', 'moist'], ['salve', 'keep', 'moist'])\n",
      "original document: \n",
      "['Yes.', \"That's\", 'why', 'ISFPs', 'are', 'better', 'at', 'handling', 'conflict', 'even', 'though', 'we', 'abhor', 'it', 'too.', 'If', 'it', \"doesn't\", 'happen', 'in', 'real', 'life,', 'it', \"doesn't\", 'happen', 'in', 'our', 'mind.', 'Unless', 'the', 'ISFP', 'is', 'in', 'Ni', 'loop,', 'in', 'which', 'case', 'the', 'ISFP', 'sort', 'of', 'becomes', 'paranoid.\\n\\nBut', 'healthy', 'INFPs', \"don't\", 'have', 'this', 'problem', 'either', 'because', 'their', 'Ne', 'sees', 'other', 'things', 'to', 'focus', 'on', 'soon.', \"It's\", 'when', 'your', 'tertiary', 'Si', 'is', 'overactive,', 'that', 'it', 'forces', 'you', 'to', 'keep', 'reliving', 'the', 'past,', 'analyzing', 'it.', 'INTPs', 'are', 'like', 'that', 'too,', 'except', 'they', 'are', 'more', 'into', 'analysis', 'rather', 'than', 'feeling', 'emotions', 'about', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'that', 'isfp', 'bet', 'handl', 'conflict', 'ev', 'though', 'abh', 'doesnt', 'hap', 'real', 'lif', 'doesnt', 'hap', 'mind', 'unless', 'isfp', 'ni', 'loop', 'cas', 'isfp', 'sort', 'becom', 'paranoid\\n\\nbut', 'healthy', 'infp', 'dont', 'problem', 'eith', 'ne', 'see', 'thing', 'foc', 'soon', 'terty', 'si', 'overact', 'forc', 'keep', 'rel', 'past', 'analys', 'intp', 'lik', 'exceiv', 'analys', 'rath', 'feel', 'emot'], ['yes', 'thats', 'isfps', 'better', 'handle', 'conflict', 'even', 'though', 'abhor', 'doesnt', 'happen', 'real', 'life', 'doesnt', 'happen', 'mind', 'unless', 'isfp', 'ni', 'loop', 'case', 'isfp', 'sort', 'become', 'paranoid\\n\\nbut', 'healthy', 'infps', 'dont', 'problem', 'either', 'ne', 'see', 'things', 'focus', 'soon', 'tertiary', 'si', 'overactive', 'force', 'keep', 'relive', 'past', 'analyze', 'intps', 'like', 'except', 'analysis', 'rather', 'feel', 'emotions'])\n",
      "original document: \n",
      "['1g', 'ket', 'was', 'in', 'mylar']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['1g', 'ket', 'myl'], ['1g', 'ket', 'mylar'])\n",
      "original document: \n",
      "['Roughly', '9', 'days.', 'I', 'applied', '9/20', 'and', 'received', 'them', '9/29.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rough', 'nin', 'day', 'apply', 'nine hundred and twenty', 'receiv', 'nine hundred and twenty-nin'], ['roughly', 'nine', 'days', 'apply', 'nine hundred and twenty', 'receive', 'nine hundred and twenty-nine'])\n",
      "original document: \n",
      "['You', \"don't\", 'even', 'need', 'to', 'jog', 'if', 'you', 'just', 'want', 'to', 'lose', 'weight.', 'Just', 'eat', 'whatever', 'you', 'want,', 'but', 'less', 'of', 'what', 'your', 'usual', 'serving', 'sizes', 'are,', 'it', \"doesn't\", 'have', 'to', 'be', 'drastic', 'either.', \"I've\", 'lost', 'around', '20', 'pounds', 'over', 'the', 'last', '20', 'weeks,', 'so', \"that's\", 'around', 'a', 'pound', 'per', 'week.', 'Or', 'if', 'you', 'want', 'to', 'take', 'it', 'a', 'bit', 'further,', 'download', 'myfitnesspal', 'and', 'count', 'macros.\\n\\nI', 'personally', 'recommend', 'some', 'kind', 'of', 'strength', 'training', 'if', 'you', 'can', 'commit', 'to', 'a', 'gym,', 'if', 'not', 'maybe', 'start', 'with', 'bodyweight', 'exercises.', 'Not', 'a', 'coach', 'or', 'anything,', 'just', 'really', 'interested', 'in', 'fitness', 'haha.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'ev', 'nee', 'jog', 'want', 'los', 'weight', 'eat', 'whatev', 'want', 'less', 'us', 'serv', 'siz', 'doesnt', 'drast', 'eith', 'iv', 'lost', 'around', 'twenty', 'pound', 'last', 'twenty', 'week', 'that', 'around', 'pound', 'per', 'week', 'want', 'tak', 'bit', 'download', 'myfitnessp', 'count', 'macros\\n\\ni', 'person', 'recommend', 'kind', 'strength', 'train', 'commit', 'gym', 'mayb', 'start', 'bodyweight', 'exerc', 'coach', 'anyth', 'real', 'interest', 'fit', 'hah'], ['dont', 'even', 'need', 'jog', 'want', 'lose', 'weight', 'eat', 'whatever', 'want', 'less', 'usual', 'serve', 'size', 'doesnt', 'drastic', 'either', 'ive', 'lose', 'around', 'twenty', 'pound', 'last', 'twenty', 'weeks', 'thats', 'around', 'pound', 'per', 'week', 'want', 'take', 'bite', 'download', 'myfitnesspal', 'count', 'macros\\n\\ni', 'personally', 'recommend', 'kind', 'strength', 'train', 'commit', 'gym', 'maybe', 'start', 'bodyweight', 'exercise', 'coach', 'anything', 'really', 'interest', 'fitness', 'haha'])\n",
      "original document: \n",
      "['Fuck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck'], ['fuck'])\n",
      "original document: \n",
      "['a', 'guidline', 'I', 'follow', 'when', 'it', 'comes', 'to', 'fetishizing:', 'get', 'the', 'opinion', 'of', 'the', 'actual', 'people', 'being', 'represented', 'as', 'to', 'whether', 'or', 'not', 'the', 'representation', 'is', 'good.', 'in', 'the', 'example', 'above,', 'if', 'women', 'are', 'going', 'nuts', 'about', 'the', 'gay', 'male', 'characters,', 'but', 'actual', 'gay', 'men', 'hate', 'how', 'they', 'are', 'portrayed', 'than', 'that', 'is', 'probably', 'fetishization.\\n\\nas', 'a', 'side', 'note,', 'I', 'find', 'emotional', 'stuff', 'sexy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guidlin', 'follow', 'com', 'fet', 'get', 'opin', 'act', 'peopl', 'repres', 'wheth', 'repres', 'good', 'exampl', 'wom', 'going', 'nut', 'gay', 'mal', 'charact', 'act', 'gay', 'men', 'hat', 'portray', 'prob', 'fetishization\\n\\nas', 'sid', 'not', 'find', 'emot', 'stuff', 'sexy'], ['guidline', 'follow', 'come', 'fetishize', 'get', 'opinion', 'actual', 'people', 'represent', 'whether', 'representation', 'good', 'example', 'women', 'go', 'nut', 'gay', 'male', 'character', 'actual', 'gay', 'men', 'hate', 'portray', 'probably', 'fetishization\\n\\nas', 'side', 'note', 'find', 'emotional', 'stuff', 'sexy'])\n",
      "original document: \n",
      "['No', 'cuz', 'Miami', 'respects', 'him', 'and', 'he', 'said', 'he', 'said', 'he', 'wants', 'to', 'play', 'in', 'miami', 'again']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cuz', 'miam', 'respect', 'said', 'said', 'want', 'play', 'miam'], ['cuz', 'miami', 'respect', 'say', 'say', 'want', 'play', 'miami'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'fun', 'way', 'of', 'thinking', 'about', 'it!', '\\n\\nStill', 'though,', 'it', 'adds', 'a', 'whole', 'different', 'layer', 'if', 'you', 'have', 'to', 'decide', 'how', 'much', 'you', 'can', 'afford', 'to', 'carry.', '\\n\\nMore', 'importantly,', 'the', 'fact', 'that', 'you', 'only', 'have', 'a', 'limited', 'amount', 'of', 'arrows', 'means', 'every', 'shot', 'has', 'to', 'count.', '\\n\\nBut', 'yeah', 'in', 'Skyrim', \"it's\", 'not', 'an', 'issue...', 'So', 'next', 'time', 'I', 'do', 'play', 'it,', \"I'll\", 'be', 'thinking', 'of', 'your', 'method', 'haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'fun', 'way', 'think', '\\n\\nstill', 'though', 'ad', 'whol', 'diff', 'lay', 'decid', 'much', 'afford', 'carry', '\\n\\nmore', 'import', 'fact', 'limit', 'amount', 'arrow', 'mean', 'every', 'shot', 'count', '\\n\\nbut', 'yeah', 'skyrim', 'issu', 'next', 'tim', 'play', 'il', 'think', 'method', 'hah'], ['thats', 'fun', 'way', 'think', '\\n\\nstill', 'though', 'add', 'whole', 'different', 'layer', 'decide', 'much', 'afford', 'carry', '\\n\\nmore', 'importantly', 'fact', 'limit', 'amount', 'arrows', 'mean', 'every', 'shoot', 'count', '\\n\\nbut', 'yeah', 'skyrim', 'issue', 'next', 'time', 'play', 'ill', 'think', 'method', 'haha'])\n",
      "original document: \n",
      "['I', 'want', 'to', 'take', 'a', 'gap', 'year', 'to', 'chill', 'and', 'get', 'money', 'up', 'first.', 'Friends', 'are', 'roasting', 'me', 'for', 'it', 'tho']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'tak', 'gap', 'year', 'chil', 'get', 'money', 'first', 'friend', 'roast', 'tho'], ['want', 'take', 'gap', 'year', 'chill', 'get', 'money', 'first', 'friends', 'roast', 'tho'])\n",
      "original document: \n",
      "['I', 'swear', 'this', 'same', 'exact', 'post', 'has', 'been', 'posted', 'before']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['swear', 'exact', 'post', 'post'], ['swear', 'exact', 'post', 'post'])\n",
      "original document: \n",
      "['Hey', 'selling', 'my', 'magni', 'and', 'modi', '2', 'Uber', 'you', 'can', 'check', 'my', 'post', 'history', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'sel', 'magn', 'mod', 'two', 'ub', 'check', 'post', 'hist'], ['hey', 'sell', 'magni', 'modi', 'two', 'uber', 'check', 'post', 'history'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['If', 'people', \"don't\", 'want', 'to', 'buy', 'my', 'slaves,', 'then', 'my', 'slave', 'selling', 'business', 'will', 'just', 'go', 'under.', \"It's\", 'the', 'will', 'of', 'the', 'free', 'market.', 'If', 'people', 'want', 'slaves,', 'I', \"don't\", 'see', 'who', 'you', 'think', 'you', 'are', 'that', \"you're\", 'in', 'a', 'position', 'to', 'deny', 'them', 'this', 'liberty.', 'Are', 'you', 'anti', 'liberty?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'dont', 'want', 'buy', 'slav', 'slav', 'sel', 'busy', 'go', 'fre', 'market', 'peopl', 'want', 'slav', 'dont', 'see', 'think', 'yo', 'posit', 'deny', 'liberty', 'ant', 'liberty'], ['people', 'dont', 'want', 'buy', 'slave', 'slave', 'sell', 'business', 'go', 'free', 'market', 'people', 'want', 'slave', 'dont', 'see', 'think', 'youre', 'position', 'deny', 'liberty', 'anti', 'liberty'])\n",
      "original document: \n",
      "['Countries', 'only', 'exist', 'for', 'the', 'benefit', 'of', 'their', 'peoples', '-', \"there's\", 'no', 'intrinsic', 'value', 'in', 'preserving', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['country', 'ex', 'benefit', 'peopl', 'ther', 'intrins', 'valu', 'preserv'], ['countries', 'exist', 'benefit', 'people', 'theres', 'intrinsic', 'value', 'preserve'])\n",
      "original document: \n",
      "['I', 'have', 'videos', 'of', 'my', 'wedding', 'and', 'from', 'the', 'birth', 'of', 'both', 'my', 'children', 'so', 'I', 'can', 'relive', 'those', 'days', 'easily', 'enough.\\n\\nIf', 'I', 'could', 'relive', 'just', 'one', 'day,', 'it', 'would', 'be', 'a', 'day', 'in', 'July', 'of', 'last', 'year', 'when', 'after', 'more', 'than', '25', 'years', 'I', 'sat', 'down', 'to', 'have', 'lunch', 'with', 'my', 'best', 'childhood', 'friend.', 'We', 'talked', 'about', 'all', 'that', 'had', 'happened', 'over', 'the', 'many', 'years', 'in-between.', 'He', 'looked', 'so', 'different.', 'If', 'you', 'saw', 'him', 'walking', 'down', 'the', 'street,', 'his', 'old', 'clothes,', 'his', 'long', 'tangled', 'and', 'unwashed', 'hair,', 'nearly', 'all', 'his', 'exposed', 'skin', 'covered', 'in', 'tattoos', 'and', 'a', 'cigarette', 'hanging', 'from', 'his', 'mouth,', \"you'd\", 'have', 'wanted', 'to', 'avoid', 'him.', 'He', 'looked', 'like', 'an', 'absolute', 'badass', 'who', 'might', 'just', 'be', 'completely', 'and', 'unpredictably', 'violent.', 'Except', \"that's\", 'not', 'who', 'he', 'really', 'was', 'on', 'the', 'inside.', 'He', 'was', 'still', 'the', 'same', 'kind', 'and', 'thoughtful', 'guy', 'I', 'knew', 'from', 'my', 'childhood.', \"He'd\", 'had', 'more', 'than', 'his', 'fair', 'share', 'of', 'misfortune.', 'Some', 'of', 'it', 'was', 'his', 'fault', 'and', 'some', \"wasn't\", 'but', 'he', 'owned', 'every', 'bit', 'of', 'it.', 'He', 'never', 'blamed', 'anyone', 'but', 'himself.', 'When', 'I', 'asked', 'him', 'how', 'he', 'manages', 'it', 'all', 'he', 'said,', '\"It\\'s', 'my', 'life.', 'What', 'else', 'can', 'I', 'do?\"\\n\\nAfterwards', 'we', 'said', 'our', 'goodbyes', 'and', 'went', 'our', 'separate', 'ways,', 'back', 'to', 'our', 'families', 'and', 'our', 'lives.', 'I', 'was', 'so', 'happy', 'to', 'have', 'reconnected', 'with', 'him', 'and', 'I', 'thought', \"we'd\", 'continue', 'to', 'be', 'a', 'part', 'of', 'each', \"other's\", 'lives', 'for', 'many', 'years', 'to', 'come.', '', 'A', 'month', 'later,', 'he', 'died', 'unexpectedly', 'and', 'all', 'alone', 'of', 'a', 'heart', 'attack.', 'Oh', 'how', 'I', 'wish', 'I', 'could', 'have', 'spent', 'more', 'time', 'with', 'him.', 'I', 'just', \"didn't\", 'know', 'there', \"wasn't\", 'any', 'time', 'left.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['video', 'wed', 'bir', 'childr', 'rel', 'day', 'easy', 'enough\\n\\nif', 'could', 'rel', 'on', 'day', 'would', 'day', 'july', 'last', 'year', 'twenty-five', 'year', 'sat', 'lunch', 'best', 'child', 'friend', 'talk', 'hap', 'many', 'year', 'inbetween', 'look', 'diff', 'saw', 'walk', 'street', 'old', 'cloth', 'long', 'tangl', 'unwash', 'hair', 'near', 'expos', 'skin', 'cov', 'tattoo', 'cigaret', 'hang', 'mou', 'youd', 'want', 'avoid', 'look', 'lik', 'absolv', 'badass', 'might', 'complet', 'unpredict', 'viol', 'exceiv', 'that', 'real', 'insid', 'stil', 'kind', 'thought', 'guy', 'knew', 'child', 'hed', 'fair', 'shar', 'misfortun', 'fault', 'wasnt', 'own', 'every', 'bit', 'nev', 'blam', 'anyon', 'ask', 'man', 'said', 'lif', 'els', 'do\\n\\nafterwards', 'said', 'goodby', 'went', 'sep', 'way', 'back', 'famy', 'liv', 'happy', 'reconnect', 'thought', 'wed', 'continu', 'part', 'oth', 'liv', 'many', 'year', 'com', 'mon', 'lat', 'died', 'unexpect', 'alon', 'heart', 'attack', 'oh', 'wish', 'could', 'spent', 'tim', 'didnt', 'know', 'wasnt', 'tim', 'left'], ['videos', 'wed', 'birth', 'children', 'relive', 'days', 'easily', 'enough\\n\\nif', 'could', 'relive', 'one', 'day', 'would', 'day', 'july', 'last', 'year', 'twenty-five', 'years', 'sit', 'lunch', 'best', 'childhood', 'friend', 'talk', 'happen', 'many', 'years', 'inbetween', 'look', 'different', 'saw', 'walk', 'street', 'old', 'clothe', 'long', 'tangle', 'unwashed', 'hair', 'nearly', 'expose', 'skin', 'cover', 'tattoo', 'cigarette', 'hang', 'mouth', 'youd', 'want', 'avoid', 'look', 'like', 'absolute', 'badass', 'might', 'completely', 'unpredictably', 'violent', 'except', 'thats', 'really', 'inside', 'still', 'kind', 'thoughtful', 'guy', 'know', 'childhood', 'hed', 'fair', 'share', 'misfortune', 'fault', 'wasnt', 'own', 'every', 'bite', 'never', 'blame', 'anyone', 'ask', 'manage', 'say', 'life', 'else', 'do\\n\\nafterwards', 'say', 'goodbyes', 'go', 'separate', 'ways', 'back', 'families', 'live', 'happy', 'reconnected', 'think', 'wed', 'continue', 'part', 'others', 'live', 'many', 'years', 'come', 'month', 'later', 'die', 'unexpectedly', 'alone', 'heart', 'attack', 'oh', 'wish', 'could', 'spend', 'time', 'didnt', 'know', 'wasnt', 'time', 'leave'])\n",
      "original document: \n",
      "['...wait', 'is', 'there', 'an', 'inside', 'joke', \"I'm\", 'missing', 'or', 'are', 'you', 'being', 'serious.\\n\\nNot', 'that', \"there's\", 'anything', 'wrong', 'if', 'you', 'were,', 'but', 'just', 'wanna', 'make', 'sure', 'im', 'not', 'missing', 'out', 'on', 'something.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'insid', 'jok', 'im', 'miss', 'serious\\n\\nnot', 'ther', 'anyth', 'wrong', 'wann', 'mak', 'sur', 'im', 'miss', 'someth'], ['wait', 'inside', 'joke', 'im', 'miss', 'serious\\n\\nnot', 'theres', 'anything', 'wrong', 'wanna', 'make', 'sure', 'im', 'miss', 'something'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Yes.', 'This', 'is', 'talking', 'about', 'how', 'they', 'have', 'edited', 'the', 'description', 'without', 'notifying', 'anyone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'talk', 'edit', 'describ', 'without', 'not', 'anyon'], ['yes', 'talk', 'edit', 'description', 'without', 'notify', 'anyone'])\n",
      "original document: \n",
      "['I', 'have', 'no', 'examples', 'but', 'I', 'just', 'wanted', 'to', 'say', 'this', 'is', 'a', 'terrific', 'question']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exampl', 'want', 'say', 'ter', 'quest'], ['examples', 'want', 'say', 'terrific', 'question'])\n",
      "original document: \n",
      "['He', 'did', 'indeed,', 'my', 'b,', 'brain', 'fart']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['indee', 'b', 'brain', 'fart'], ['indeed', 'b', 'brain', 'fart'])\n",
      "original document: \n",
      "['Am', 'i', 'the', 'only', 'thinking', '\"A', 'Piece', 'of', 'Shit?\"\\nGreat', 'abbreviation']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'piec', 'shit\\ngre', 'abbrevy'], ['think', 'piece', 'shit\\ngreat', 'abbreviation'])\n",
      "original document: \n",
      "['Second', 'the', 'recommendation', 'on', 'Mans', 'Best', 'Friend.', '', 'My', 'partner', 'and', 'I', 'used', 'that', 'to', 'get', 'our', 'systems', 'down.', '', 'You', \"won't\", 'have', 'to', 'think', 'about', 'the', 'climbing', 'or', 'gear', 'since', \"it's\", 'bolted.\\n\\nAnother', 'good', 'area', 'might', 'be', 'the', 'riding', 'hood', 'wall.', '', 'Physical', 'Graffiti', 'is', 'a', '2', 'pitch', 'climb', 'on', 'gear', 'with', 'a', 'bolted', 'anchor', 'midway.', '', 'Right', 'next', 'door', 'is', 'Big', 'Bad', 'Wolf,', '3', 'pitches', 'of', 'sport', 'that', 'goes', 'at', '5.9.\\n\\nWhatever', 'route', 'you', 'choose', 'make', 'sure', \"you've\", 'thought', 'out', 'the', 'plan', 'to', 'get', 'back', 'down', '-', 'both', 'after', 'the', 'climb', 'or', 'if', 'you', 'need', 'to', 'bail', 'part', 'way.', '', 'Have', 'a', 'great', 'trip.', '', 'RR', 'is', 'paradise.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['second', 'recommend', 'man', 'best', 'friend', 'partn', 'us', 'get', 'system', 'wont', 'think', 'climb', 'gear', 'sint', 'bolted\\n\\nanother', 'good', 'are', 'might', 'rid', 'hood', 'wal', 'phys', 'graffit', 'two', 'pitch', 'climb', 'gear', 'bolt', 'anch', 'midway', 'right', 'next', 'door', 'big', 'bad', 'wolf', 'three', 'pitch', 'sport', 'goe', '59\\n\\nwhatever', 'rout', 'choos', 'mak', 'sur', 'youv', 'thought', 'plan', 'get', 'back', 'climb', 'nee', 'bail', 'part', 'way', 'gre', 'trip', 'rr', 'parad'], ['second', 'recommendation', 'man', 'best', 'friend', 'partner', 'use', 'get', 'systems', 'wont', 'think', 'climb', 'gear', 'since', 'bolted\\n\\nanother', 'good', 'area', 'might', 'rid', 'hood', 'wall', 'physical', 'graffiti', 'two', 'pitch', 'climb', 'gear', 'bolt', 'anchor', 'midway', 'right', 'next', 'door', 'big', 'bad', 'wolf', 'three', 'pitch', 'sport', 'go', '59\\n\\nwhatever', 'route', 'choose', 'make', 'sure', 'youve', 'think', 'plan', 'get', 'back', 'climb', 'need', 'bail', 'part', 'way', 'great', 'trip', 'rr', 'paradise'])\n",
      "original document: \n",
      "['Oh,', \"what're\", 'the', 'valvetrain', 'issues?', 'Weak', 'pushrods,', 'poor', 'quality', 'springs?\\n\\nIf', \"you're\", 'doing', 'heads', 'and', 'cam,', 'can', 'you', 'buy', 'a', '\"cam', 'package\"', 'with', 'all', 'new', 'valvetrain', 'parts', 'to', 'fix', 'it?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'what', 'valvetrain', 'issu', 'weak', 'pushrod', 'poor', 'qual', 'springs\\n\\nif', 'yo', 'head', 'cam', 'buy', 'cam', 'pack', 'new', 'valvetrain', 'part', 'fix'], ['oh', 'whatre', 'valvetrain', 'issue', 'weak', 'pushrods', 'poor', 'quality', 'springs\\n\\nif', 'youre', 'head', 'cam', 'buy', 'cam', 'package', 'new', 'valvetrain', 'part', 'fix'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol8ke/):\\n\\nDepends', 'if', 'they', 'decide', 'to', 'go', 'the', 'traditional', 'route', 'or', 'not.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol8ke\\n\\ndepend', 'decid', 'go', 'tradit', 'rout'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol8ke\\n\\ndepends', 'decide', 'go', 'traditional', 'route'])\n",
      "original document: \n",
      "['143418952|', '&gt;', 'None', 'Anonymous', '(ID:', 'L6+x+Lup)\\n\\n&gt;&gt;143412250', '(OP)\\n2010:', 'Liberal', 'Democrat\\n2015:', 'Conservative\\n2016:', 'Remain\\n2017:', 'Liberal', 'Democrat\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, nine hundred and fifty-two', 'gt', 'non', 'anonym', 'id', 'l6xlup\\n\\ngtgt143412250', 'op\\n2010', 'lib', 'democrat\\n2015', 'conservative\\n2016', 'remain\\n2017', 'lib', 'democrat\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, nine hundred and fifty-two', 'gt', 'none', 'anonymous', 'id', 'l6xlup\\n\\ngtgt143412250', 'op\\n2010', 'liberal', 'democrat\\n2015', 'conservative\\n2016', 'remain\\n2017', 'liberal', 'democrat\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Best', 'all', 'around', 'player', 'in', 'my', 'books.', \"He's\", 'clutch', 'on', 'offence', 'and', 'always', 'solid', 'on', 'defence.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['best', 'around', 'play', 'book', 'hes', 'clutch', 'off', 'alway', 'solid', 'def'], ['best', 'around', 'player', 'book', 'hes', 'clutch', 'offence', 'always', 'solid', 'defence'])\n",
      "original document: \n",
      "['so', 'you', 'are', 'who', 'infomercials', 'are', 'targeting.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['infomerc', 'target'], ['infomercials', 'target'])\n",
      "original document: \n",
      "['Why', 'do', 'you', 'ask', 'people', 'if', 'they', 'have', 'allergies', 'at', 'least', 'once', 'a', 'week?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ask', 'peopl', 'allergy', 'least', 'week'], ['ask', 'people', 'allergies', 'least', 'week'])\n",
      "original document: \n",
      "['Yeah,', 'I', 'have', 'mine', 'at', 'home', 'with', 'the', 'door', 'open.', 'Sometimes', 'I′ll', 'put', 'food', 'in', 'it,', 'or', 'hide', 'a', 'toy.', 'That', 'way', 'it', 'isn′t', 'a', 'big', 'deal', 'when', 'I', 'close', 'the', 'door', 'and', 'we', 'go', 'somewhere.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'min', 'hom', 'door', 'op', 'sometim', 'il', 'put', 'food', 'hid', 'toy', 'way', 'isnt', 'big', 'deal', 'clos', 'door', 'go', 'somewh'], ['yeah', 'mine', 'home', 'door', 'open', 'sometimes', 'ill', 'put', 'food', 'hide', 'toy', 'way', 'isnt', 'big', 'deal', 'close', 'door', 'go', 'somewhere'])\n",
      "original document: \n",
      "['I', \"don't\", 'get', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'get'], ['dont', 'get'])\n",
      "original document: \n",
      "['#Welcome', 'to', 'r/LateStageCapitalism\\n***\\n\\n#***Please', 'remember', 'that', 'this', 'subreddit', 'is', 'a', 'SAFE', 'SPACE', 'for', 'leftist', 'discussion.', 'Any', 'Liberalism,', 'capitalist', 'apologia,', 'or', 'attempts', 'to', 'debate', 'socialism', 'will', 'be', 'met', 'with', 'an', 'immediate', 'ban.', 'Take', 'it', 'to', 'r/DebateCommunism.', 'Bigotry,', '[ableism](http://www.autistichoya.com/p/ableist-words-and-terms-to-avoid.html)', 'and', 'hate', 'speech', 'will', 'also', 'be', 'met', 'with', 'immediate', 'bans;', 'Socialism', 'is', 'an', 'intrinsically', 'inclusive', 'system.***\\n\\nIf', 'you', 'are', 'new', 'to', 'socialism,', 'please', 'check', 'out', 'our', 'socialism', 'crash', 'course', '[here](https://github.com/dessalines/essays/blob/master/crash_course_socialism.md).\\n\\nIf', 'you', 'are', 'curious', 'to', 'what', 'our', 'leftist', 'terminology', 'means,', 'then', 'please', 'check', 'out', 'our', 'glossary', '[here](https://github.com/dessalines/essays/blob/master/glossary_of_socialist_terms.md).\\n\\nIn', 'addition,', 'here', 'are', 'some', 'introductory', 'links', 'about', 'socialism:\\n\\n-', '[Albert', 'Einstein', '-', '*Why', 'Socialism?*](http://monthlyreview.org/2009/05/01/why-socialism/)\\n\\n-', '[Pyotr', 'Kropotkin', '-', '*The', 'Conquest', 'of', 'Bread*](https://theanarchistlibrary.org/library/petr-kropotkin-the-conquest-of-bread)\\n\\n-', '[Friedrich', 'Engels', '-', '*The', 'Principles', 'of', 'Communism*](https://www.marxists.org/archive/marx/works/1847/11/prin-com.htm)\\n\\n-', '[Vladimir', 'Lenin', '-', '*The', 'State', '&amp;', 'Revolution*](https://www.marxists.org/archive/lenin/works/1917/staterev/)\\n\\n-', '[Rosa', 'Luxemburg', '-', '*Reform', 'or', 'Revolution*](https://www.marxists.org/archive/luxemburg/1900/reform-revolution/)\\n\\n-', '[Karl', 'Marx', '&amp;', 'Friedrich', 'Engels', '-', '*The', 'Communist', 'Manifesto*](https://www.marxists.org/archive/marx/works/1848/communist-manifesto/)\\n\\nFor', 'an', 'extended', 'list', 'of', 'works,', 'check', 'out', '[our', 'wiki](https://www.reddit.com/r/LateStageCapitalism/wiki/index)', 'or', '[this', 'masterlist.](http://pastebin.com/raw/7xNqzd85)\\n\\n#☭☭☭\\n\\n***\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/LateStageCapitalism)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welcom', 'rlatestagecapitalism\\n\\n\\nplease', 'rememb', 'subreddit', 'saf', 'spac', 'left', 'discuss', 'lib', 'capit', 'apolog', 'attempt', 'deb', 'soc', 'met', 'immedy', 'ban', 'tak', 'rdebatecommun', 'bigotry', 'ableismhttpwwwautistichoyacompableistwordsandtermstoavoidhtml', 'hat', 'speech', 'also', 'met', 'immedy', 'ban', 'soc', 'intrins', 'includ', 'system\\n\\nif', 'new', 'soc', 'pleas', 'check', 'soc', 'crash', 'cours', 'herehttpsgithubcomdessalinesessaysblobmastercrash_course_socialismmd\\n\\nif', 'cury', 'left', 'terminolog', 'mean', 'pleas', 'check', 'gloss', 'herehttpsgithubcomdessalinesessaysblobmasterglossary_of_socialist_termsmd\\n\\nin', 'addit', 'introduc', 'link', 'socialism\\n\\n', 'albert', 'einstein', 'socialismhttpmonthlyrevieworg20090501whysocialism\\n\\n', 'pyot', 'kropotkin', 'conquest', 'breadhttpstheanarchistlibraryorglibrarypetrkropotkintheconquestofbread\\n\\n', 'friedrich', 'engel', 'principl', 'communismhttpswwwmarxistsorgarchivemarxworks184711princomhtm\\n\\n', 'vladimir', 'lenin', 'stat', 'amp', 'revolutionhttpswwwmarxistsorgarchiveleninworks1917staterev\\n\\n', 'ros', 'luxemburg', 'reform', 'revolutionhttpswwwmarxistsorgarchiveluxemburg1900reformrevolution\\n\\n', 'karl', 'marx', 'amp', 'friedrich', 'engel', 'commun', 'manifestohttpswwwmarxistsorgarchivemarxworks1848communistmanifesto\\n\\nfor', 'extend', 'list', 'work', 'check', 'wikihttpswwwredditcomrlatestagecapitalismwikiindex', 'masterlisthttppastebincomraw7xnqzd85\\n\\n\\n\\n\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorlatestagecapit', 'quest', 'concern'], ['welcome', 'rlatestagecapitalism\\n\\n\\nplease', 'remember', 'subreddit', 'safe', 'space', 'leftist', 'discussion', 'liberalism', 'capitalist', 'apologia', 'attempt', 'debate', 'socialism', 'meet', 'immediate', 'ban', 'take', 'rdebatecommunism', 'bigotry', 'ableismhttpwwwautistichoyacompableistwordsandtermstoavoidhtml', 'hate', 'speech', 'also', 'meet', 'immediate', 'ban', 'socialism', 'intrinsically', 'inclusive', 'system\\n\\nif', 'new', 'socialism', 'please', 'check', 'socialism', 'crash', 'course', 'herehttpsgithubcomdessalinesessaysblobmastercrash_course_socialismmd\\n\\nif', 'curious', 'leftist', 'terminology', 'mean', 'please', 'check', 'glossary', 'herehttpsgithubcomdessalinesessaysblobmasterglossary_of_socialist_termsmd\\n\\nin', 'addition', 'introductory', 'link', 'socialism\\n\\n', 'albert', 'einstein', 'socialismhttpmonthlyrevieworg20090501whysocialism\\n\\n', 'pyotr', 'kropotkin', 'conquest', 'breadhttpstheanarchistlibraryorglibrarypetrkropotkintheconquestofbread\\n\\n', 'friedrich', 'engels', 'principles', 'communismhttpswwwmarxistsorgarchivemarxworks184711princomhtm\\n\\n', 'vladimir', 'lenin', 'state', 'amp', 'revolutionhttpswwwmarxistsorgarchiveleninworks1917staterev\\n\\n', 'rosa', 'luxemburg', 'reform', 'revolutionhttpswwwmarxistsorgarchiveluxemburg1900reformrevolution\\n\\n', 'karl', 'marx', 'amp', 'friedrich', 'engels', 'communist', 'manifestohttpswwwmarxistsorgarchivemarxworks1848communistmanifesto\\n\\nfor', 'extend', 'list', 'work', 'check', 'wikihttpswwwredditcomrlatestagecapitalismwikiindex', 'masterlisthttppastebincomraw7xnqzd85\\n\\n\\n\\n\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorlatestagecapitalism', 'question', 'concern'])\n",
      "original document: \n",
      "['Using', 'your', 'logic:\\n\\nAnd', 'intelligence', 'increases', 'spell', 'damage.', 'So', 'the', 'necro', 'spells', 'that', 'do', 'spell', 'damage', 'will', 'do', '5%', 'more', 'damage', 'for', 'every', 'point', 'in', 'intelligence.\\n\\nBut', 'necro', 'does', 'not', '*scale*', 'with', 'intelligence\\n\\nOh', 'wait,', \"that's\", 'complete', 'and', 'utter', 'nonsense,', 'as', 'is', 'your', 'post.', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'logic\\n\\nand', 'intellig', 'increas', 'spel', 'dam', 'necro', 'spel', 'spel', 'dam', 'fiv', 'dam', 'every', 'point', 'intelligence\\n\\nbut', 'necro', 'scal', 'intelligence\\n\\noh', 'wait', 'that', 'complet', 'ut', 'nonsens', 'post'], ['use', 'logic\\n\\nand', 'intelligence', 'increase', 'spell', 'damage', 'necro', 'spell', 'spell', 'damage', 'five', 'damage', 'every', 'point', 'intelligence\\n\\nbut', 'necro', 'scale', 'intelligence\\n\\noh', 'wait', 'thats', 'complete', 'utter', 'nonsense', 'post'])\n",
      "original document: \n",
      "['Will', 'answer', 'upon', 'getting', 'home', ':p', '(working', 'atm)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['answ', 'upon', 'get', 'hom', 'p', 'work', 'atm'], ['answer', 'upon', 'get', 'home', 'p', 'work', 'atm'])\n",
      "original document: \n",
      "['Lol', 'after', 'a', '100', 'comment', 'thread', 'with', 'majority', 'ripping', 'him', 'I', 'bet', 'there', 'will', 'be', 'a', 'turn', 'around', 'in', 'opinion.', '\"Oh', 'we', 'believed', 'him', 'him', 'he', 'just', 'needed', 'to', 'show', 'it\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'one hundred', 'com', 'thread', 'maj', 'rip', 'bet', 'turn', 'around', 'opin', 'oh', 'believ', 'nee', 'show'], ['lol', 'one hundred', 'comment', 'thread', 'majority', 'rip', 'bet', 'turn', 'around', 'opinion', 'oh', 'believe', 'need', 'show'])\n",
      "original document: \n",
      "['I', 'feel', 'like', 'Lt.', 'Dan', 'on', 'the', 'mast', 'of', 'that', 'shrimping', 'boat', 'during', 'a', 'hurricane', 'watching', 'this', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel', 'lik', 'lt', 'dan', 'mast', 'shrimping', 'boat', 'hur', 'watch', 'gam'], ['feel', 'like', 'lt', 'dan', 'mast', 'shrimp', 'boat', 'hurricane', 'watch', 'game'])\n",
      "original document: \n",
      "['How', 'much', 'have', 'you', 'donated?\\n\\nSince', 'if', 'memory', 'serves', \"you're\", 'a', 'Sanders', 'supporter,', 'how', 'much', 'has', 'he?', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['much', 'donated\\n\\nsince', 'mem', 'serv', 'yo', 'sand', 'support', 'much', '\\n'], ['much', 'donated\\n\\nsince', 'memory', 'serve', 'youre', 'sanders', 'supporter', 'much', '\\n'])\n",
      "original document: \n",
      "['Yeah', \"it's\", 'not', 'necessary', 'then.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'necess'], ['yeah', 'necessary'])\n",
      "original document: \n",
      "['Your', 'post', 'seems', 'at', 'odds', 'with', 'itself.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'seem', 'od'], ['post', 'seem', 'odds'])\n",
      "original document: \n",
      "['They', 'want', 'you', 'to', 'pay', 'full', 'price', '-', 'the', 'freebies', 'are', 'over!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'pay', 'ful', 'pric', 'freeby'], ['want', 'pay', 'full', 'price', 'freebies'])\n",
      "original document: \n",
      "['You', 'can', 'never', 'go', 'wrong', 'with', \"Vanille's\", 'TMR.', 'It', 'is', 'not', 'a', 'TM', 'that', 'would', 'go', 'to', 'waste', 'if', 'you', 'get', 'Genji', 'Glove.', \"It'll\", 'be', 'BiS', 'for', 'whatever', 'healer', 'you', 'get.\\n\\nI', 'would', 'say,', 'yeah,', 'go', 'ahead', 'with', 'it,', 'you', \"won't\", 'regret', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'go', 'wrong', 'vanil', 'tmr', 'tm', 'would', 'go', 'wast', 'get', 'genj', 'glov', 'itl', 'bis', 'whatev', 'heal', 'get\\n\\ni', 'would', 'say', 'yeah', 'go', 'ahead', 'wont', 'regret'], ['never', 'go', 'wrong', 'vanilles', 'tmr', 'tm', 'would', 'go', 'waste', 'get', 'genji', 'glove', 'itll', 'bis', 'whatever', 'healer', 'get\\n\\ni', 'would', 'say', 'yeah', 'go', 'ahead', 'wont', 'regret'])\n",
      "original document: \n",
      "['Better', 'stones', 'than', 'batteries...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'ston', 'battery'], ['better', 'stone', 'batteries'])\n",
      "original document: \n",
      "['The', 'thing', 'with', 'going', 'to', 'places', 'is', 'that', 'there', \"isn't\", 'really', 'any', 'where', 'I', 'live', 'just', 'a', 'couple', 'pubs', 'that', 'are', 'mostly', 'filled', 'with', 'people', 'who', 'are', 'a', 'little', 'too', 'old', 'to', 'be', 'out', 'getting', 'hammered', 'every', 'weekend.', 'I', 'want', 'to', 'start', 'going', 'places', 'but', 'I', 'basically', 'have', 'to', 'travel', '2h', 'to', 'go', 'any', 'where', 'decent.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thing', 'going', 'plac', 'isnt', 'real', 'liv', 'coupl', 'pub', 'most', 'fil', 'peopl', 'littl', 'old', 'get', 'ham', 'every', 'weekend', 'want', 'start', 'going', 'plac', 'bas', 'travel', '2h', 'go', 'dec'], ['thing', 'go', 'place', 'isnt', 'really', 'live', 'couple', 'pubs', 'mostly', 'fill', 'people', 'little', 'old', 'get', 'hammer', 'every', 'weekend', 'want', 'start', 'go', 'place', 'basically', 'travel', '2h', 'go', 'decent'])\n",
      "original document: \n",
      "['Just', 'wait', 'until', 'the', 'LR', 'Trunks', 'banner.', 'Best', 'LR', 'banner', 'rates', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'lr', 'trunk', 'ban', 'best', 'lr', 'ban', 'rat'], ['wait', 'lr', 'trunks', 'banner', 'best', 'lr', 'banner', 'rat'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['whats', 'stopping', 'you?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'stop'], ['whats', 'stop'])\n",
      "original document: \n",
      "['I', 'used', 'to', 'have', 'a', 'friend', 'like', 'Ross', 'several', 'years', 'ago.', 'You', 'get', 'used', 'to', 'it.', 'You', 'start', 'developing', 'a', 'sense', 'of', 'when', \"they're\", 'going', 'to', 'pull', 'shit', 'and', 'then', 'you', 'both', 'laugh', 'about', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'friend', 'lik', 'ross', 'sev', 'year', 'ago', 'get', 'us', 'start', 'develop', 'sens', 'theyr', 'going', 'pul', 'shit', 'laugh'], ['use', 'friend', 'like', 'ross', 'several', 'years', 'ago', 'get', 'use', 'start', 'develop', 'sense', 'theyre', 'go', 'pull', 'shit', 'laugh'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Huhuh', 'good', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['huhuh', 'good', 'on'], ['huhuh', 'good', 'one'])\n",
      "original document: \n",
      "['ghetto', 'has', 'nothing', 'to', 'do', 'with', 'race,', 'you', 'can', 'be', 'white,', 'black,', 'Hispanic,', 'or', 'arabic', 'and', 'still', 'be', 'ghetto.', 'ghetto', 'has', 'to', 'do', 'with', 'being', 'ignorant', 'as', 'fuck,', 'like', 'the', 'girl', 'being', 'ignorant', 'and', 'obnoxious', 'as', 'hell,', 'and', 'then', 'getting', 'upset', 'when', 'she', 'got', 'called', 'out', 'on', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ghetto', 'noth', 'rac', 'whit', 'black', 'hisp', 'arab', 'stil', 'ghetto', 'ghetto', 'ign', 'fuck', 'lik', 'girl', 'ign', 'obnoxy', 'hel', 'get', 'upset', 'got', 'cal'], ['ghetto', 'nothing', 'race', 'white', 'black', 'hispanic', 'arabic', 'still', 'ghetto', 'ghetto', 'ignorant', 'fuck', 'like', 'girl', 'ignorant', 'obnoxious', 'hell', 'get', 'upset', 'get', 'call'])\n",
      "original document: \n",
      "['Hey', 'there.', 'I', 'just', 'tried', 'a', 'randomizer', '(Heartgold)', 'for', 'the', 'first', 'time.', 'It', 'was', 'a', 'lot', 'of', 'fun,', 'but', 'I', 'wanted', 'to', 'start', 'over', 'and', 'try', 'a', 'new', 'one.The', 'problem', 'is,', 'whenever', 'I', 'try', 'to', 'make', 'a', 'new', 'game,', 'my', 'randomizer', 'file', 'loads', 'an', 'identical', 'pokemon', 'layout.', 'Same', 'starters', 'as', 'the', 'first', 'time,', 'and', 'same', 'encounters.\\n\\nIs', 'this', 'a', 'common', 'issue', 'with', 'randomizers?', 'Is', 'there', 'some', 'way', 'to', 'make', 'it', 'fully', 'randomize', 'on', 'a', 'new', 'game?', 'Thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'tri', 'random', 'heartgold', 'first', 'tim', 'lot', 'fun', 'want', 'start', 'try', 'new', 'oneth', 'problem', 'whenev', 'try', 'mak', 'new', 'gam', 'random', 'fil', 'load', 'id', 'pokemon', 'layout', 'start', 'first', 'tim', 'encounters\\n\\n', 'common', 'issu', 'random', 'way', 'mak', 'ful', 'random', 'new', 'gam', 'thank'], ['hey', 'try', 'randomizer', 'heartgold', 'first', 'time', 'lot', 'fun', 'want', 'start', 'try', 'new', 'onethe', 'problem', 'whenever', 'try', 'make', 'new', 'game', 'randomizer', 'file', 'load', 'identical', 'pokemon', 'layout', 'starters', 'first', 'time', 'encounters\\n\\nis', 'common', 'issue', 'randomizers', 'way', 'make', 'fully', 'randomize', 'new', 'game', 'thank'])\n",
      "original document: \n",
      "['Are', 'you', 'going', 'to', 'keep', 'doing', 'these', 'when', 'teams', 'lose', 'in', 'the', 'postseason,', 'or', 'was', 'this', 'the', 'last', 'one?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'keep', 'team', 'los', 'postseason', 'last', 'on'], ['go', 'keep', 'team', 'lose', 'postseason', 'last', 'one'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Okay', '--', \"that's\", 'what', 'I', 'thought,', 'but', 'a', 'cursory', 'google', 'search', 'basically', 'said', 'that', 'eBay', 'sends', 'out', 'tax', 'forms', 'so', 'I', \"wasn't\", 'sure.', 'Thank', 'you!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'that', 'thought', 'curs', 'googl', 'search', 'bas', 'said', 'ebay', 'send', 'tax', 'form', 'wasnt', 'sur', 'thank'], ['okay', 'thats', 'think', 'cursory', 'google', 'search', 'basically', 'say', 'ebay', 'send', 'tax', 'form', 'wasnt', 'sure', 'thank'])\n",
      "original document: \n",
      "['Link', 'them', 'the', 'DynastyFF', 'Tools', 'website']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['link', 'dynastyff', 'tool', 'websit'], ['link', 'dynastyff', 'tool', 'website'])\n",
      "original document: \n",
      "['Props', 'to', 'Noah', 'making', 'himself', 'look', 'slightly', 'more', 'like', 'Steven', 'Adams', 'to', 'help', \"Kanter's\", 'transition.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prop', 'noah', 'mak', 'look', 'slight', 'lik', 'stev', 'adam', 'help', 'kant', 'transit'], ['prop', 'noah', 'make', 'look', 'slightly', 'like', 'steven', 'adams', 'help', 'kanters', 'transition'])\n",
      "original document: \n",
      "['You', 'can', 'hide', 'those', 'under', 'forum', 'settings.', \"Can't\", 'remember', 'how', 'because', 'I', 'set', 'mine', 'in', '2009.', 'Stopped', 'going', 'mostly', 'after', 'I', 'joined', 'reddit', 'tho.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hid', 'for', 'set', 'cant', 'rememb', 'set', 'min', 'two thousand and nine', 'stop', 'going', 'most', 'join', 'reddit', 'tho'], ['hide', 'forum', 'settings', 'cant', 'remember', 'set', 'mine', 'two thousand and nine', 'stop', 'go', 'mostly', 'join', 'reddit', 'tho'])\n",
      "original document: \n",
      "['I', 'was', 'in', 'a', 'relationship', 'with', 'a', 'guy', 'for', 'two', 'years,', 'we', 'lived', 'together', 'and', 'everything.', 'Long', 'story', 'short,', 'he', \"wasn't\", 'the', 'guy', 'for', 'me', 'and', 'I', 'broke', 'up', 'with', 'him', 'and', 'moved', 'back', 'in', 'with', 'my', 'parents.', 'I', 'totally', \"didn't\", 'expect', 'his', 'reaction,', 'I', 'thought', 'he', 'would', 'shrug', 'and', 'pretend', 'he', \"didn't\", 'care.', 'He', 'did', 'care.', 'He', 'cried', 'his', 'eyes', 'out', 'and', 'begged', 'me', 'to', 'stay,', 'asked', 'me', 'over', 'and', 'over', 'to', 'please', 'stay', 'and', 'give', 'him', 'one', 'more', 'chance.', 'Even', 'clung', 'to', 'me', 'and', 'kept', 'saying', 'please', 'please', 'please,', \"I'll\", 'do', 'everything', 'different!', 'It', 'was', 'the', 'hardest', 'thing', 'I', 'had', 'to', 'do', 'in', 'my', 'life,', 'walking', 'away', 'and', 'purposely', 'hurting', 'someone', 'to', 'save', 'my', 'own', 'happiness.', 'It', 'was', 'the', 'hardest', 'but', 'also', 'the', 'best', 'decision', 'I', 'have', 'ever', 'made.', 'It', 'has', 'been', 'years', 'since', 'then', 'and', \"I'm\", 'sure', 'he', 'had', 'a', 'hard', 'time', 'at', 'first.', 'There', 'was', 'just', 'nothing', 'I', 'could', 'do', 'for', 'him,', 'every', 'conversation', 'we', 'had', 'just', 'gave', 'him', 'hope', 'we', 'would', 'be', 'okay', 'again.', \"I'm\", 'glad', 'he', 'could', 'rely', 'on', 'his', 'parents', 'for', 'support.', 'I', 'felt', 'so', 'guilty', 'at', 'first,', 'especially', 'because', 'I', 'started', 'dating', 'someone', 'new', 'a', 'lot', 'earlier', 'than', 'him.', 'However', 'years', 'later', 'we', 'are', 'both', 'in', 'relationships', 'with', 'different', 'people', 'who', 'suit', 'us', 'much', 'better.', 'I', 'honestly', 'think', 'he', 'would', 'agree', 'now', 'that', 'it', 'was', 'for', 'the', 'best.', 'This', 'was', 'my', 'personal', 'anecdote,', 'I', 'hope', 'it', 'gives', 'you', 'hope', 'that', 'down', 'the', 'line', 'everything', 'will', 'be', 'okay', '😊', 'the', 'heavy', 'feeling', 'and', 'the', 'guilt', 'will', 'lessen', 'over', 'time,', 'you', 'are', 'not', 'responsible', 'for', 'his', 'happiness.', 'You', 'are', 'not', 'obligated', 'to', 'stay', 'with', 'him', 'so', 'he', 'is', 'comfortable.', 'Sending', 'you', 'strength', 'and', 'happiness', 'and', 'if', 'you', 'want', 'to', 'talk', \"I'm\", 'here!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rel', 'guy', 'two', 'year', 'liv', 'togeth', 'everyth', 'long', 'story', 'short', 'wasnt', 'guy', 'brok', 'mov', 'back', 'par', 'tot', 'didnt', 'expect', 'react', 'thought', 'would', 'shrug', 'pretend', 'didnt', 'car', 'car', 'cri', 'ey', 'beg', 'stay', 'ask', 'pleas', 'stay', 'giv', 'on', 'chant', 'ev', 'clung', 'kept', 'say', 'pleas', 'pleas', 'pleas', 'il', 'everyth', 'diff', 'hardest', 'thing', 'lif', 'walk', 'away', 'purpos', 'hurt', 'someon', 'sav', 'happy', 'hardest', 'also', 'best', 'decid', 'ev', 'mad', 'year', 'sint', 'im', 'sur', 'hard', 'tim', 'first', 'noth', 'could', 'every', 'convers', 'gav', 'hop', 'would', 'okay', 'im', 'glad', 'could', 'rely', 'par', 'support', 'felt', 'guil', 'first', 'espec', 'start', 'dat', 'someon', 'new', 'lot', 'ear', 'howev', 'year', 'lat', 'rel', 'diff', 'peopl', 'suit', 'us', 'much', 'bet', 'honest', 'think', 'would', 'agr', 'best', 'person', 'anecdot', 'hop', 'giv', 'hop', 'lin', 'everyth', 'okay', 'heavy', 'feel', 'guilt', 'less', 'tim', 'respons', 'happy', 'oblig', 'stay', 'comfort', 'send', 'strength', 'happy', 'want', 'talk', 'im'], ['relationship', 'guy', 'two', 'years', 'live', 'together', 'everything', 'long', 'story', 'short', 'wasnt', 'guy', 'break', 'move', 'back', 'parent', 'totally', 'didnt', 'expect', 'reaction', 'think', 'would', 'shrug', 'pretend', 'didnt', 'care', 'care', 'cry', 'eye', 'beg', 'stay', 'ask', 'please', 'stay', 'give', 'one', 'chance', 'even', 'cling', 'keep', 'say', 'please', 'please', 'please', 'ill', 'everything', 'different', 'hardest', 'thing', 'life', 'walk', 'away', 'purposely', 'hurt', 'someone', 'save', 'happiness', 'hardest', 'also', 'best', 'decision', 'ever', 'make', 'years', 'since', 'im', 'sure', 'hard', 'time', 'first', 'nothing', 'could', 'every', 'conversation', 'give', 'hope', 'would', 'okay', 'im', 'glad', 'could', 'rely', 'parent', 'support', 'felt', 'guilty', 'first', 'especially', 'start', 'date', 'someone', 'new', 'lot', 'earlier', 'however', 'years', 'later', 'relationships', 'different', 'people', 'suit', 'us', 'much', 'better', 'honestly', 'think', 'would', 'agree', 'best', 'personal', 'anecdote', 'hope', 'give', 'hope', 'line', 'everything', 'okay', 'heavy', 'feel', 'guilt', 'lessen', 'time', 'responsible', 'happiness', 'obligate', 'stay', 'comfortable', 'send', 'strength', 'happiness', 'want', 'talk', 'im'])\n",
      "original document: \n",
      "['Yes,', 'much', 'better', 'to', 'unquestioningly', 'believe', 'every', 'narrative', 'presented', 'to', 'you.\\n\\nAlso,', 'by', 'definition,', 'the', 'narrative', 'of', 'a', 'conspiracy', 'theory', 'is', '*in*convenient.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'much', 'bet', 'unquest', 'believ', 'every', 'nar', 'pres', 'you\\n\\nalso', 'definit', 'nar', 'conspir', 'the', 'inconveny'], ['yes', 'much', 'better', 'unquestioningly', 'believe', 'every', 'narrative', 'present', 'you\\n\\nalso', 'definition', 'narrative', 'conspiracy', 'theory', 'inconvenient'])\n",
      "original document: \n",
      "[\"There's\", 'a', 'difference', 'in', 'forcing', 'a', 'turnover', 'and', 'recovering', 'a', 'fumble.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'diff', 'forc', 'turnov', 'recov', 'fumbl'], ['theres', 'difference', 'force', 'turnover', 'recover', 'fumble'])\n",
      "original document: \n",
      "['If', 'any', 'individual', 'submits', 'in', 'obedience', 'to', 'another,', 'they', 'do', 'so', 'for', 'a', 'variety', 'of', 'reasons:', 'love,', 'trust,', 'monetary', 'matters,', 'and', 'fear', 'being', 'the', 'first', 'that', 'come', 'to', 'mind.', 'Of', 'these,', 'fear', 'is', 'that', 'only', 'one', 'that', 'makes', 'sense', 'in', 'context,', 'yet', 'there', 'really', \"isn't\", 'any', 'hint', 'of', 'that', 'in', 'the', 'story.', 'And', 'unfortunately', 'Blizzard', 'has', 'kind', 'of', 'left', 'us', 'hanging', 'in', 'the', '\"so', 'what', 'the', 'hell', 'is', 'Sylvanas', 'doing', 'now\"', 'category,', 'so', 'as', 'consumers', 'of', 'the', 'story,', 'we', 'are', 'left', 'to', 'make', 'sense', 'of', 'our', 'place', 'in', 'it.', '(As', 'you', 'can', 'tell,', \"I'm\", 'thinking', 'of', 'this', 'in', 'the', 'larger', 'sense', 'of', 'the', 'overall', 'story', 'and', 'how', 'my', 'actions', 'fit', 'into', 'it,', 'not', 'just', 'some', 'dude', 'sitting', 'at', 'a', 'computer).', 'Of', 'all', 'the', 'Horde', 'races,', 'the', 'Tauren', 'have', 'perhaps', 'the', 'least', 'checkered', 'past,', 'in', 'terms', 'of', 'unprovoked', 'violence', 'or', 'delving', 'into', 'black', 'magic', 'fuckery.', 'It', 'just', 'seems...', 'false', 'that', 'my', 'tauren', 'shaman,', 'with', 'his', 'own', 'history', 'and', 'the', 'history', 'of', 'his', 'race,', 'would', 'willingly', 'follow', 'Sylvanas.', 'And', 'her', 'actions', 'thus', 'far', 'as', 'Warchief', 'have', 'not', 'gone', 'beyond', 'this', 'single,', 'apparently', 'self-serving', 'mission.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['individ', 'submit', 'obedy', 'anoth', 'vary', 'reason', 'lov', 'trust', 'monet', 'mat', 'fear', 'first', 'com', 'mind', 'fear', 'on', 'mak', 'sens', 'context', 'yet', 'real', 'isnt', 'hint', 'story', 'unfortun', 'blizzard', 'kind', 'left', 'us', 'hang', 'hel', 'sylvana', 'categ', 'consum', 'story', 'left', 'mak', 'sens', 'plac', 'tel', 'im', 'think', 'larg', 'sens', 'overal', 'story', 'act', 'fit', 'dud', 'sit', 'comput', 'hord', 'rac', 'taur', 'perhap', 'least', 'check', 'past', 'term', 'unprovok', 'viol', 'delv', 'black', 'mag', 'fuckery', 'seem', 'fals', 'taur', 'sham', 'hist', 'hist', 'rac', 'would', 'wil', 'follow', 'sylvana', 'act', 'thu', 'far', 'warchief', 'gon', 'beyond', 'singl', 'app', 'selfserv', 'miss'], ['individual', 'submit', 'obedience', 'another', 'variety', 'reason', 'love', 'trust', 'monetary', 'matter', 'fear', 'first', 'come', 'mind', 'fear', 'one', 'make', 'sense', 'context', 'yet', 'really', 'isnt', 'hint', 'story', 'unfortunately', 'blizzard', 'kind', 'leave', 'us', 'hang', 'hell', 'sylvanas', 'category', 'consumers', 'story', 'leave', 'make', 'sense', 'place', 'tell', 'im', 'think', 'larger', 'sense', 'overall', 'story', 'action', 'fit', 'dude', 'sit', 'computer', 'horde', 'race', 'tauren', 'perhaps', 'least', 'checker', 'past', 'term', 'unprovoked', 'violence', 'delve', 'black', 'magic', 'fuckery', 'seem', 'false', 'tauren', 'shaman', 'history', 'history', 'race', 'would', 'willingly', 'follow', 'sylvanas', 'action', 'thus', 'far', 'warchief', 'go', 'beyond', 'single', 'apparently', 'selfserving', 'mission'])\n",
      "original document: \n",
      "['I', 'guess', 'after', 'today', 'Oaks', 'has', 'now', 'surpassed', 'Martin', 'Shrekli', 'in', 'that', 'desire.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guess', 'today', 'oak', 'surpass', 'martin', 'shrekli', 'desir'], ['guess', 'today', 'oaks', 'surpass', 'martin', 'shrekli', 'desire'])\n",
      "original document: \n",
      "['LOL']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['Arguable.', 'I', 'was', 'hyping', 'Fury', 'Road', 'up', 'for', 'years', 'and', 'no', 'one', 'I', 'talked', 'to', 'cared', 'about', 'it.', 'Until', 'they', 'saw', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['argu', 'hyp', 'fury', 'road', 'year', 'on', 'talk', 'car', 'saw'], ['arguable', 'hype', 'fury', 'road', 'years', 'one', 'talk', 'care', 'saw'])\n",
      "original document: \n",
      "['Perhaps', 'this', 'can', 'be', 'the', 'next', 'project', 'for', 'the', 'makers', 'of', '[PancakeBot](http://www.pancakebot.com/).']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['perhap', 'next', 'project', 'mak', 'pancakebothttpwwwpancakebotcom'], ['perhaps', 'next', 'project', 'makers', 'pancakebothttpwwwpancakebotcom'])\n",
      "original document: \n",
      "['Please', 'run', 'for', 'President', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'run', 'presid'], ['please', 'run', 'president'])\n",
      "original document: \n",
      "['get', 'the', 'mdr1000x', 'instead', ',', 'you', 'can', 'get', 'it', 'refurbished', 'for', '200', 'or', 'new', 'for', '250']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'mdr1000x', 'instead', 'get', 'refurb', 'two hundred', 'new', 'two hundred and fifty'], ['get', 'mdr1000x', 'instead', 'get', 'refurbish', 'two hundred', 'new', 'two hundred and fifty'])\n",
      "original document: \n",
      "['Knock', 'Knock']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['knock', 'knock'], ['knock', 'knock'])\n",
      "original document: \n",
      "['In', 'the', 'printer', 'profile', 'find', 'the', 'starting', 'gcode.', 'Cut', 'the', 'two', 'lines', 'having', 'to', 'do', 'with', 'heating', 'the', 'nozzle', 'and', 'paste', 'them', 'just', 'after', 'the', 'mesh', 'leveling', 'line.\\n\\nProblem', 'solved,', 'remember', 'to', 'clean', 'your', 'nozzle', 'after', 'each', 'print.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['print', 'profil', 'find', 'start', 'gcod', 'cut', 'two', 'lin', 'heat', 'nozzl', 'past', 'mesh', 'level', 'line\\n\\nproblem', 'solv', 'rememb', 'cle', 'nozzl', 'print'], ['printer', 'profile', 'find', 'start', 'gcode', 'cut', 'two', 'line', 'heat', 'nozzle', 'paste', 'mesh', 'level', 'line\\n\\nproblem', 'solve', 'remember', 'clean', 'nozzle', 'print'])\n",
      "original document: \n",
      "[\"It's\", 'not', 'the', 'actual', 'muscles,', 'but', 'the', 'demonstration', 'of', 'being', 'able', 'to', 'dedicate', 'yourself', 'to', 'getting', 'muscular', \"that's\", 'attractive.', 'Getting', 'in', 'shape', 'is', '-hard-', 'for', 'most', 'people,', 'and', 'the', 'kind', 'of', 'people', 'who', 'can', 'keep', 'up', 'that', 'kind', 'of', 'lifestyle', 'tend', 'to', 'be', 'good', 'at', 'committing', 'to', 'things.', '\\n\\nThen', \"there's\", 'the', 'crazies', 'who', 'jack', 'themselves', 'up', 'to', 'the', 'point', 'they', 'look', 'like', 'they', 'can', 'barely', 'turn', 'their', 'heads,', 'and', 'that', 'looks', 'like', 'obsession', '(in', 'my', 'opinion)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'musc', 'demonst', 'abl', 'ded', 'get', 'muscul', 'that', 'attract', 'get', 'shap', 'hard', 'peopl', 'kind', 'peopl', 'keep', 'kind', 'lifestyl', 'tend', 'good', 'commit', 'thing', '\\n\\nthen', 'ther', 'crazy', 'jack', 'point', 'look', 'lik', 'bar', 'turn', 'head', 'look', 'lik', 'obsess', 'opin'], ['actual', 'muscle', 'demonstration', 'able', 'dedicate', 'get', 'muscular', 'thats', 'attractive', 'get', 'shape', 'hard', 'people', 'kind', 'people', 'keep', 'kind', 'lifestyle', 'tend', 'good', 'commit', 'things', '\\n\\nthen', 'theres', 'crazies', 'jack', 'point', 'look', 'like', 'barely', 'turn', 'head', 'look', 'like', 'obsession', 'opinion'])\n",
      "original document: \n",
      "['He', 'does.', 'But', 'OP', 'has', '(1)', 'given', 'her', 'number', 'when', 'she', 'didn’t', 'want', 'to', '(2)', 'answered', 'his', 'calls/replied', 'to', 'messages', 'when', 'she', 'didn’t', 'want', 'to', '(3)', 'got', 'off', 'the', 'bus', 'or', 'missed', 'it', 'to', 'avoid', 'talking', 'to', 'him', '(4)', 'felt', 'she', 'needed', 'to', 'explain', 'why', 'she', 'didn’t', 'message', 'back', '(5)', 'didn’t', 'kick', 'him', 'in', 'the', 'balls', 'when', 'he', 'hugged/', 'touched', 'etc.\\n\\nMost', 'people', 'don’t', 'need', 'this', 'kind', 'of', 'practice', 'but', 'sounds', 'like', 'op', 'needs', 'to', 'get', 'used', 'to', 'saying', 'no', 'as', 'well.', 'The', 'phone', 'number', 'especially', 'should', 'never', 'have', 'been', 'shared']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'on', 'giv', 'numb', 'didnt', 'want', 'two', 'answ', 'callsreply', 'mess', 'didnt', 'want', 'three', 'got', 'bus', 'miss', 'avoid', 'talk', 'four', 'felt', 'nee', 'explain', 'didnt', 'mess', 'back', 'fiv', 'didnt', 'kick', 'bal', 'hug', 'touch', 'etc\\n\\nmost', 'peopl', 'dont', 'nee', 'kind', 'pract', 'sound', 'lik', 'op', 'nee', 'get', 'us', 'say', 'wel', 'phon', 'numb', 'espec', 'nev', 'shar'], ['op', 'one', 'give', 'number', 'didnt', 'want', 'two', 'answer', 'callsreplied', 'message', 'didnt', 'want', 'three', 'get', 'bus', 'miss', 'avoid', 'talk', 'four', 'felt', 'need', 'explain', 'didnt', 'message', 'back', 'five', 'didnt', 'kick', 'ball', 'hug', 'touch', 'etc\\n\\nmost', 'people', 'dont', 'need', 'kind', 'practice', 'sound', 'like', 'op', 'need', 'get', 'use', 'say', 'well', 'phone', 'number', 'especially', 'never', 'share'])\n",
      "original document: \n",
      "['Not', 'going', 'to', 'resubmit,', 'just', 'inquiring,', 'would', 'it', 'have', 'been', 'accepted', 'if', 'the', 'title', 'was', 'something', 'like:', 'this', 'man', 'on', 'a', 'carrousel?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'resubmit', 'inquir', 'would', 'acceiv', 'titl', 'someth', 'lik', 'man', 'carrousel'], ['go', 'resubmit', 'inquire', 'would', 'accept', 'title', 'something', 'like', 'man', 'carrousel'])\n",
      "original document: \n",
      "['Sorry,', 'meant', 'to', 'say', 'not', 'scared.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'meant', 'say', 'scar'], ['sorry', 'mean', 'say', 'scar'])\n",
      "original document: \n",
      "['PU', 'and', 'Bluehole', 'are', 'too', 'busy', 'counting', 'their', 'money,', \"can't\", 'work', 'on', 'fixing', 'their', 'game.', 'Try', 'again', 'in', 'a', 'few', 'months', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pu', 'bluehol', 'busy', 'count', 'money', 'cant', 'work', 'fix', 'gam', 'try', 'month'], ['pu', 'bluehole', 'busy', 'count', 'money', 'cant', 'work', 'fix', 'game', 'try', 'months'])\n",
      "original document: \n",
      "['hahahahaha', 'dén', 'var', 'sgu', 'god']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahahahah', 'den', 'var', 'sgu', 'god'], ['hahahahaha', 'den', 'var', 'sgu', 'god'])\n",
      "original document: \n",
      "['OG', 'Ultraboost', 'or', 'Bait', 'EQT?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['og', 'ultraboost', 'bait', 'eqt'], ['og', 'ultraboost', 'bait', 'eqt'])\n",
      "original document: \n",
      "['Malarkey', 'was', 'the', 'funniest', 'character', 'on', 'BoB.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['malarkey', 'funniest', 'charact', 'bob'], ['malarkey', 'funniest', 'character', 'bob'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Love', 'u', '2', 'bb']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'u', 'two', 'bb'], ['love', 'u', 'two', 'bb'])\n",
      "original document: \n",
      "['Cold.', 'Wet.', 'Flaky.', 'I', \"don't\", 'remember', 'much,', 'I', 'was', 'a', 'baby,', 'but', \"it's\", 'lost', 'its', 'authenticity', 'by', 'now-', 'now', 'its', 'just', 'a', 'nuisance']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cold', 'wet', 'flaky', 'dont', 'rememb', 'much', 'baby', 'lost', 'auth', 'nuis'], ['cold', 'wet', 'flaky', 'dont', 'remember', 'much', 'baby', 'lose', 'authenticity', 'nuisance'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['[Very', 'interesting.](https://www.youtube.com/watch?v=pkunxmvunxI&amp;t=113s)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interestinghttpswwwyoutubecomwatchvpkunxmvunxiampt113s'], ['interestinghttpswwwyoutubecomwatchvpkunxmvunxiampt113s'])\n",
      "original document: \n",
      "[\"That's\", 'jacket', 'money']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'jacket', 'money'], ['thats', 'jacket', 'money'])\n",
      "original document: \n",
      "['i', 'have', 'american', '3ds', 'so', 'i', \"can't\", 'play', 'mhxx,', 'but', \"I'll\", 'get', 'it', 'when', 'i', 'get', 'a', 'switch.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['am', '3ds', 'cant', 'play', 'mhxx', 'il', 'get', 'get', 'switch\\n'], ['american', '3ds', 'cant', 'play', 'mhxx', 'ill', 'get', 'get', 'switch\\n'])\n",
      "original document: \n",
      "['In', 'a', 'very', 'concise', 'thought:\\n\\nImagine', 'your', 'victory', 'condition.', 'Do', 'you', 'win', 'with', 'the', 'opponent', 'having', 'a', 'full', 'hand', 'of', 'cards', 'he', \"didn't\", 'have', 'time', 'to', 'cast?', \"You're\", 'the', 'beatdown.', 'In', 'contrast,', 'do', 'you', 'win', 'with', 'the', 'opponent', 'having', 'exhausted', 'his', 'resources', 'while', 'you', 'simply', 'overpower', 'him', 'after', 'absorbing', 'his', 'threats?', \"You're\", 'the', 'control.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'thought\\n\\nimagine', 'vict', 'condit', 'win', 'oppon', 'ful', 'hand', 'card', 'didnt', 'tim', 'cast', 'yo', 'beatdown', 'contrast', 'win', 'oppon', 'exhaust', 'resourc', 'simply', 'overpow', 'absorb', 'threats', 'yo', 'control\\n'], ['concise', 'thought\\n\\nimagine', 'victory', 'condition', 'win', 'opponent', 'full', 'hand', 'card', 'didnt', 'time', 'cast', 'youre', 'beatdown', 'contrast', 'win', 'opponent', 'exhaust', 'resources', 'simply', 'overpower', 'absorb', 'threats', 'youre', 'control\\n'])\n",
      "original document: \n",
      "['Yes,', 'you', 'are', 'an', 'asshole.\\n\\nYou', 'had', 'one', 'job', 'to', 'do.', '', 'Feed', 'the', 'dog', 'and', 'cat.\\n\\nThey', 'trusted', 'you', 'with', 'a', 'key', 'to', 'their', 'home', 'to', 'do', 'that', 'job.', '', 'Instead', 'of', 'doing', 'just', 'that,', 'you', 'went', 'through', 'all', 'the', 'corners', 'of', 'their', 'home', 'and', 'figuratively', 'rubbed', 'your', 'scent', 'in', 'it.\\n\\nYou', 'showed', 'that', 'you', 'have', 'no', 'consideration', 'or', 'respect', 'for', 'your', 'adult', 'son', 'and', 'his', 'adult', 'wife', 'and', 'their', 'decisions', 'about', 'their', 'life.', '', 'Had', 'you', 'been', 'considerate,', 'you', 'would', 'have', 'offered', 'to', 'do', 'a', 'deep', 'cleaning,', 'listing', 'out', 'the', 'things', 'that', 'you', 'could', 'do', 'for', 'their', 'home.', '', 'This', 'could', 'have', 'been', 'done', 'through', 'a', 'phone', 'call', 'or', 'a', 'text', 'message', 'and', 'you', 'should', 'wait', 'for', 'an', 'answer', 'before', 'doing', 'anything', 'else.', '', '\\n\\nI', 'am', 'very', 'leery', 'that', 'you', 'say', 'you', 'organized.', '', 'By', 'that,', 'did', 'you', 'straighten', 'up', 'a', 'pile', 'of', 'papers', 'on', 'the', 'table?', '', 'Or', 'did', 'you', 'rearrange', 'cupboards', 'and', 'furniture?', '', 'If', 'your', 'cleaning', 'included', 'more', 'than', 'squaring', 'away', 'a', 'pile', 'of', 'papers', 'so', 'they', 'are', 'not', 'on', 'the', 'verge', 'of', 'falling', 'off', 'the', 'pile,', 'you', 'overstepped', 'big', 'time.', '', 'By', 'the', 'way', 'your', 'son', 'described', 'your', 'actions', 'and', 'called', 'you', 'presumptuous', 'and', 'controlling,', 'I', 'suspect', 'you', 'did', 'a', 'lot', 'more', 'than', 'cleaning', 'surfaces', 'that', 'were', 'dirty,', 'other', 'than', 'doing', 'laundry.\\n\\nIf', 'you', 'wanted', 'to', 'help,', 'you', 'need', 'to', 'ask', 'in', 'what', 'form', 'they', 'would', 'accept', 'your', 'help.', '', 'You', \"don't\", 'barge', 'in', 'and', 'start', 'doing', 'things', 'without', 'their', 'knowledge', 'and', 'consent.\\n\\nI', 'have', 'a', 'followup', 'question', 'for', 'you?', '', 'Did', 'you', 'keep', 'the', 'key', 'or', 'did', 'you', 'give', 'it', 'back?', '', 'If', 'you', \"didn't\", 'give', 'it', 'back', '(why', 'not?),', 'have', 'you', 'USED', 'the', 'key', 'without', 'their', 'prior', 'knowledge?', '', 'If', \"you've\", 'used', 'the', 'key,', 'you', 'are', 'more', 'than', 'an', 'asshole', 'very', 'much', 'deserving', 'of', 'the', 'cold', 'shoulders', 'thrown', 'your', 'way.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'asshole\\n\\nyou', 'on', 'job', 'fee', 'dog', 'cat\\n\\nthey', 'trust', 'key', 'hom', 'job', 'instead', 'went', 'corn', 'hom', 'fig', 'rub', 'scent', 'it\\n\\nyou', 'show', 'consid', 'respect', 'adult', 'son', 'adult', 'wif', 'decid', 'lif', 'consid', 'would', 'off', 'deep', 'cle', 'list', 'thing', 'could', 'hom', 'could', 'don', 'phon', 'cal', 'text', 'mess', 'wait', 'answ', 'anyth', 'els', '\\n\\ni', 'leery', 'say', 'org', 'straighten', 'pil', 'pap', 'tabl', 'rearrang', 'cupboard', 'furnit', 'cle', 'includ', 'squ', 'away', 'pil', 'pap', 'verg', 'fal', 'pil', 'overstep', 'big', 'tim', 'way', 'son', 'describ', 'act', 'cal', 'presumptu', 'control', 'suspect', 'lot', 'cle', 'surfac', 'dirty', 'laundry\\n\\nif', 'want', 'help', 'nee', 'ask', 'form', 'would', 'acceiv', 'help', 'dont', 'barg', 'start', 'thing', 'without', 'knowledg', 'consent\\n\\ni', 'followup', 'quest', 'keep', 'key', 'giv', 'back', 'didnt', 'giv', 'back', 'us', 'key', 'without', 'pri', 'knowledg', 'youv', 'us', 'key', 'asshol', 'much', 'deserv', 'cold', 'should', 'thrown', 'way'], ['yes', 'asshole\\n\\nyou', 'one', 'job', 'fee', 'dog', 'cat\\n\\nthey', 'trust', 'key', 'home', 'job', 'instead', 'go', 'corner', 'home', 'figuratively', 'rub', 'scent', 'it\\n\\nyou', 'show', 'consideration', 'respect', 'adult', 'son', 'adult', 'wife', 'decisions', 'life', 'considerate', 'would', 'offer', 'deep', 'clean', 'list', 'things', 'could', 'home', 'could', 'do', 'phone', 'call', 'text', 'message', 'wait', 'answer', 'anything', 'else', '\\n\\ni', 'leery', 'say', 'organize', 'straighten', 'pile', 'paper', 'table', 'rearrange', 'cupboards', 'furniture', 'clean', 'include', 'square', 'away', 'pile', 'paper', 'verge', 'fall', 'pile', 'overstep', 'big', 'time', 'way', 'son', 'describe', 'action', 'call', 'presumptuous', 'control', 'suspect', 'lot', 'clean', 'surface', 'dirty', 'laundry\\n\\nif', 'want', 'help', 'need', 'ask', 'form', 'would', 'accept', 'help', 'dont', 'barge', 'start', 'things', 'without', 'knowledge', 'consent\\n\\ni', 'followup', 'question', 'keep', 'key', 'give', 'back', 'didnt', 'give', 'back', 'use', 'key', 'without', 'prior', 'knowledge', 'youve', 'use', 'key', 'asshole', 'much', 'deserve', 'cold', 'shoulder', 'throw', 'way'])\n",
      "original document: \n",
      "['Came', 'here', 'to', 'say', 'that.', 'The', 'chorus', 'is', 'strong', 'as', 'fuck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cam', 'say', 'chor', 'strong', 'fuck'], ['come', 'say', 'chorus', 'strong', 'fuck'])\n",
      "original document: \n",
      "['Drink', 'plenty', 'of', 'water', 'before', 'hand.', '', 'The', 'better', 'hydrated', 'you', 'are', 'the', 'easier', 'for', 'them', 'to', 'find', 'the', 'vein,', 'and', 'the', 'easier', 'the', 'blood', 'will', 'flow.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['drink', 'plenty', 'wat', 'hand', 'bet', 'hydr', 'easy', 'find', 'vein', 'easy', 'blood', 'flow'], ['drink', 'plenty', 'water', 'hand', 'better', 'hydrate', 'easier', 'find', 'vein', 'easier', 'blood', 'flow'])\n",
      "original document: \n",
      "['Why', 'would', 'expect', 'people', 'to', 'change', 'how', 'they', 'are', 'born?', 'I', \"didn't\", 'choose', 'my', 'skin', 'color,', 'sex,', 'or', 'sexuality', 'either', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'expect', 'peopl', 'chang', 'born', 'didnt', 'choos', 'skin', 'col', 'sex', 'sex', 'eith'], ['would', 'expect', 'people', 'change', 'bear', 'didnt', 'choose', 'skin', 'color', 'sex', 'sexuality', 'either'])\n",
      "original document: \n",
      "['This', 'sounds', 'great,', 'but', 'I', 'guarantee', 'you', 'the', 'moment', 'that', 'one', 'of', 'James', '/', 'Curry', '/', 'Durant', '/', 'etc.', 'decides', 'to', 'be', 'a', 'goober', 'and', 'take', 'a', 'knee,', 'this', 'memo', 'will', 'suddenly', 'have', 'no', 'teeth.', 'NBA', 'bends', 'over', 'backwards', 'all', 'the', 'time.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'gre', 'guar', 'mom', 'on', 'jam', 'curry', 'dur', 'etc', 'decid', 'goob', 'tak', 'kne', 'memo', 'sud', 'tee', 'nba', 'bend', 'backward', 'tim'], ['sound', 'great', 'guarantee', 'moment', 'one', 'jam', 'curry', 'durant', 'etc', 'decide', 'goober', 'take', 'knee', 'memo', 'suddenly', 'teeth', 'nba', 'bend', 'backwards', 'time'])\n",
      "original document: \n",
      "['[+i_stay_turnt](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo19t/):\\n\\nHow', 'long', 'did', 'it', 'take', 'for', 'your', 'book', 'to', 'publish?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['i_stay_turnthttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo19t\\n\\nhow', 'long', 'tak', 'book', 'publ'], ['i_stay_turnthttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo19t\\n\\nhow', 'long', 'take', 'book', 'publish'])\n",
      "original document: \n",
      "['Lusk', '182']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lusk', 'one hundred and eighty-two'], ['lusk', 'one hundred and eighty-two'])\n",
      "original document: \n",
      "[\"Here's\", 'what', 'I', 'came', 'up', 'with:', 'https://i.imgur.com/2HGOTps.png\\n\\n^^^^^***bleep***', '^^^^^***bloop***']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['her', 'cam', 'httpsiimgurcom2hgotpspng\\n\\nbleep', 'bloop'], ['heres', 'come', 'httpsiimgurcom2hgotpspng\\n\\nbleep', 'bloop'])\n",
      "original document: \n",
      "['&gt;', 'EVE', 'is', 'literally', 'known', 'as', 'excel', 'in', 'space', 'and', 'as', 'an', 'old', 'player', 'of', 'it', 'I', 'can', 'attest', 'to', 'it.\\n\\nIf', 'you', 'choose', 'to', 'play', 'excel', 'in', 'space,', 'you', 'are', 'welcome', 'to.', '', 'I', \"don't\", 'because', \"it's\", 'kind', 'of', 'boring.', '', 'I', 'like', 'to', 'PvP.', '', 'If', 'you', 'never', 'left', 'high', 'sec', 'and', 'never', 'progressed', 'past', 'spreadsheets', 'in', 'space,', 'I', 'see', 'why', 'you', 'would', 'quit.', 'However,', 'that', 'was', 'your', 'decision', 'to', 'play', 'that', 'way', 'and', 'it', \"isn't\", 'required', 'at', 'all.', '', 'Fortunately,', 'many', 'players', 'progress', 'past', 'those', 'things', 'into', 'the', 'more', 'interesting', 'parts', 'of', 'the', 'game', 'very', 'quickly.\\n\\n&gt;', 'It', 'literally', 'rewards', 'betrayal', 'and', 'treachery', 'and', 'corruption', 'as', 'well', 'as', 'other', 'factors\\n\\nThe', 'game', 'rewards', 'being', 'good', 'at', 'whatever', 'it', 'is', 'that', 'interests', 'you.', '', 'Some', 'people', 'scam,', 'and', 'most', 'of', 'them', \"don't\", 'benefit', 'from', 'it', 'at', 'all.', '', \"It's\", 'a', 'very', 'minor', 'part', 'of', 'the', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'ev', 'lit', 'known', 'excel', 'spac', 'old', 'play', 'attest', 'it\\n\\nif', 'choos', 'play', 'excel', 'spac', 'welcom', 'dont', 'kind', 'bor', 'lik', 'pvp', 'nev', 'left', 'high', 'sec', 'nev', 'progress', 'past', 'spreadsheets', 'spac', 'see', 'would', 'quit', 'howev', 'decid', 'play', 'way', 'isnt', 'requir', 'fortun', 'many', 'play', 'progress', 'past', 'thing', 'interest', 'part', 'gam', 'quickly\\n\\ngt', 'lit', 'reward', 'betray', 'treachery', 'corrupt', 'wel', 'factors\\n\\nthe', 'gam', 'reward', 'good', 'whatev', 'interest', 'peopl', 'scam', 'dont', 'benefit', 'min', 'part', 'gam'], ['gt', 'eve', 'literally', 'know', 'excel', 'space', 'old', 'player', 'attest', 'it\\n\\nif', 'choose', 'play', 'excel', 'space', 'welcome', 'dont', 'kind', 'bore', 'like', 'pvp', 'never', 'leave', 'high', 'sec', 'never', 'progress', 'past', 'spreadsheets', 'space', 'see', 'would', 'quit', 'however', 'decision', 'play', 'way', 'isnt', 'require', 'fortunately', 'many', 'players', 'progress', 'past', 'things', 'interest', 'part', 'game', 'quickly\\n\\ngt', 'literally', 'reward', 'betrayal', 'treachery', 'corruption', 'well', 'factors\\n\\nthe', 'game', 'reward', 'good', 'whatever', 'interest', 'people', 'scam', 'dont', 'benefit', 'minor', 'part', 'game'])\n",
      "original document: \n",
      "['done']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['don'], ['do'])\n",
      "original document: \n",
      "[\"It's\", 'sad', 'to', 'think', 'that', 'people', 'feel', 'this', 'extreme', 'but', 'it', 'is', 'important', 'to', 'remember', 'this', \"isn't\", 'representative', 'of', 'the', 'majority', 'of', 'left', 'wingers.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'think', 'peopl', 'feel', 'extrem', 'import', 'rememb', 'isnt', 'repres', 'maj', 'left', 'wing'], ['sad', 'think', 'people', 'feel', 'extreme', 'important', 'remember', 'isnt', 'representative', 'majority', 'leave', 'wingers'])\n",
      "original document: \n",
      "['Not', 'really,', 'my', 'damage', 'has', 'been', 'very', 'consistent', 'again', 'the', 'world', 'boss.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'dam', 'consist', 'world', 'boss'], ['really', 'damage', 'consistent', 'world', 'boss'])\n",
      "original document: \n",
      "['143418468|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'j6fQZDZD)\\n\\n&gt;&gt;143417408\\n*puts', 'her', 'mocha', 'latte', 'down*\\n*takes', 'her', 'poodle', 'off', 'her', 'lap*\\nUM,', 'excuse', 'me', 'sweety-pie.', 'You', 'think', \"it's\", 'okay', 'to', 'just', 'go', 'around', 'shitposting', 'like', 'you', 'do?', 'Maybe', 'you', \"don't\", 'really', 'understand...', '*flips', 'her', 'hair*', 'After', 'all,', 'you', 'rural', 'and', 'suburban', 'retards', 'are', 'uncivilized', 'folk.', 'Oopsie,', 'did', 'I', 'hurt', 'your', 'feelings?', '*crouches', 'down', 'and', 'undoes', 'your', 'zipper*', \"ahahaha--you're\", 'blushing?', 'How', 'pathetic...', 'I', 'guess', 'you', 'Trumpkin', 'farmhands', \"aren't\", 'used', 'to', 'REAL', 'women', 'after', 'all', 'those', 'hours', 'you', 'spend', 'on', 'the', 'farm.', \"What's\", 'that,', 'honeybuns?', 'No--let', 'me', 'guess.', \"That's\", 'why', 'you', 'carry', 'such', 'big', 'guns', 'around,', 'you', 'tiny-cocked', 'bigot--', '*stares,', 'shocked,', 'at', 'your', 'thundercock*', 'Ahaha..', 'ahah...', 'hah...', 'Er,', 'listen', 'b-baby', 'dick...', 'J-Just', 'lie', 'back', 'and', 'relax.', \"It'll\", 'make', 'it', 'easier', 'to', 'accept', 'your', 'privilege.', '*strokes', 'your', 'cock', 'up', 'and', 'down,', 'then', 'starts', 'blowing', 'you', 'gently*', \"It'll...\", 'mmph...', 'all', 'be', 'over', 'soon...', 'mmm...', 'snookems.', 'After', 'all,', 'even', 'your', 'dick', 'admits', 'that', 'female', 'is', 'the', 'superior', 'gender.', 'Look', 'how', 'hard', \"I've\", 'made', 'you.', \"You're\", 'pathetic.', 'Now...', 'mmmph...', 'once', \"we're\", 'finished', 'here,', \"we're\", 'going', 'to', 'go', 'back', 'to', 'my', 'room', 'and', 'you', 'can', 'atone', 'for', 'all', 'those', 'horrible', 'years', 'of', 'institutional', 'privilege', 'by', 'submitting', 'to', 'my', 'vagina,', 'which,', 'by', 'the', 'way,', 'has', 'its', 'own', 'voice\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and sixty-eight', 'gt', 'unit', 'stat', 'anonym', 'id', 'j6fqzdzd\\n\\ngtgt143417408\\nputs', 'moch', 'lat', 'down\\ntakes', 'poodl', 'lap\\num', 'excus', 'sweetypy', 'think', 'okay', 'go', 'around', 'shitpost', 'lik', 'mayb', 'dont', 'real', 'understand', 'flip', 'hair', 'rur', 'suburb', 'retard', 'uncivil', 'folk', 'oopsy', 'hurt', 'feel', 'crouch', 'undo', 'zip', 'ahahahayo', 'blush', 'pathet', 'guess', 'trumpkin', 'farmhand', 'ar', 'us', 'real', 'wom', 'hour', 'spend', 'farm', 'what', 'honeybun', 'nolet', 'guess', 'that', 'carry', 'big', 'gun', 'around', 'tinycock', 'bigot', 'star', 'shock', 'thundercock', 'ahah', 'ahah', 'hah', 'er', 'list', 'bbaby', 'dick', 'jjust', 'lie', 'back', 'relax', 'itl', 'mak', 'easy', 'acceiv', 'privileg', 'strokes', 'cock', 'start', 'blow', 'gent', 'itl', 'mmph', 'soon', 'mmm', 'snookem', 'ev', 'dick', 'admit', 'fem', 'supery', 'gend', 'look', 'hard', 'iv', 'mad', 'yo', 'pathet', 'mmmph', 'fin', 'going', 'go', 'back', 'room', 'aton', 'horr', 'year', 'institut', 'privileg', 'submit', 'vagin', 'way', 'voice\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and eighteen thousand, four hundred and sixty-eight', 'gt', 'unite', 'state', 'anonymous', 'id', 'j6fqzdzd\\n\\ngtgt143417408\\nputs', 'mocha', 'latte', 'down\\ntakes', 'poodle', 'lap\\num', 'excuse', 'sweetypie', 'think', 'okay', 'go', 'around', 'shitposting', 'like', 'maybe', 'dont', 'really', 'understand', 'flip', 'hair', 'rural', 'suburban', 'retard', 'uncivilized', 'folk', 'oopsie', 'hurt', 'feel', 'crouch', 'undo', 'zipper', 'ahahahayoure', 'blush', 'pathetic', 'guess', 'trumpkin', 'farmhands', 'arent', 'use', 'real', 'women', 'hours', 'spend', 'farm', 'whats', 'honeybuns', 'nolet', 'guess', 'thats', 'carry', 'big', 'gun', 'around', 'tinycocked', 'bigot', 'star', 'shock', 'thundercock', 'ahaha', 'ahah', 'hah', 'er', 'listen', 'bbaby', 'dick', 'jjust', 'lie', 'back', 'relax', 'itll', 'make', 'easier', 'accept', 'privilege', 'stroke', 'cock', 'start', 'blow', 'gently', 'itll', 'mmph', 'soon', 'mmm', 'snookems', 'even', 'dick', 'admit', 'female', 'superior', 'gender', 'look', 'hard', 'ive', 'make', 'youre', 'pathetic', 'mmmph', 'finish', 'go', 'go', 'back', 'room', 'atone', 'horrible', 'years', 'institutional', 'privilege', 'submit', 'vagina', 'way', 'voice\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['that', 'would', 'be', 'cool']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'cool'], ['would', 'cool'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'trying', 'to', 'defend', 'anything', 'here,', 'just', 'trying', 'to', 'clarify', 'the', 'stats.', 'In', 'fact', 'I', 'also', 'think', 'Mercy', 'is', 'busted', 'as', 'hell', 'rn.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'try', 'defend', 'anyth', 'try', 'clar', 'stat', 'fact', 'also', 'think', 'mercy', 'bust', 'hel', 'rn'], ['im', 'try', 'defend', 'anything', 'try', 'clarify', 'stats', 'fact', 'also', 'think', 'mercy', 'bust', 'hell', 'rn'])\n",
      "original document: \n",
      "['No', 'more', 'field', 'goals,', 'period.', 'Always', 'go', 'for', 'it', 'on', '4th', 'down.', 'Hell,', 'no', 'more', 'kicking', 'for', 'the', 'point', 'after', 'either.', 'Always', 'go', 'for', '2.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['field', 'goal', 'period', 'alway', 'go', '4th', 'hel', 'kick', 'point', 'eith', 'alway', 'go', 'two'], ['field', 'goals', 'period', 'always', 'go', '4th', 'hell', 'kick', 'point', 'either', 'always', 'go', 'two'])\n",
      "original document: \n",
      "['Good', 'gamer', 'tags', 'are', 'always', 'great.', 'My', 'husband', 'and', 'I', 'came', 'across', 'a', 'guy', 'in', 'halo', 'reach', 'a', 'few', 'times', 'whose', 'tag', 'was', 'About', '729', 'Jews', '(Iirc', 'the', 'number', 'might', 'be', 'different)', 'but', 'it', 'made', 'us', 'lol', 'every', 'time', 'it', 'popped', 'up', '“you', 'just', 'killed', 'about', '729', 'Jews....', '“']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'gam', 'tag', 'alway', 'gre', 'husband', 'cam', 'across', 'guy', 'halo', 'reach', 'tim', 'whos', 'tag', 'seven hundred and twenty-nine', 'jew', 'iirc', 'numb', 'might', 'diff', 'mad', 'us', 'lol', 'every', 'tim', 'pop', 'kil', 'seven hundred and twenty-nine', 'jew'], ['good', 'gamer', 'tag', 'always', 'great', 'husband', 'come', 'across', 'guy', 'halo', 'reach', 'time', 'whose', 'tag', 'seven hundred and twenty-nine', 'jews', 'iirc', 'number', 'might', 'different', 'make', 'us', 'lol', 'every', 'time', 'pop', 'kill', 'seven hundred and twenty-nine', 'jews'])\n",
      "original document: \n",
      "['You', 'were', 'doing', 'something', 'that', 'could', 'break', 'your', 'arm', 'without', 'an', 'EMT', 'on', 'sight?', 'What', 'barbarians', 'do', 'you', 'have', 'for', 'parents!\\n\\n-', 'Too', 'many', 'people', 'these', 'days.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someth', 'could', 'break', 'arm', 'without', 'emt', 'sight', 'barb', 'parents\\n\\n', 'many', 'peopl', 'day'], ['something', 'could', 'break', 'arm', 'without', 'emt', 'sight', 'barbarians', 'parents\\n\\n', 'many', 'people', 'days'])\n",
      "original document: \n",
      "['Muhammad', 'was', 'a', 'bitch!', 'Do', 'I', 'win!?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['muhammad', 'bitch', 'win'], ['muhammad', 'bitch', 'win'])\n",
      "original document: \n",
      "[\"It's\", 'nice', 'to', 'see', 'Hosmer', 'buy', 'tickets', 'behind', 'home', 'tonight', 'for', 'all', 'the', 'hearts', 'he', 'broke', 'in', 'KC.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'see', 'hosm', 'buy', 'ticket', 'behind', 'hom', 'tonight', 'heart', 'brok', 'kc'], ['nice', 'see', 'hosmer', 'buy', 'ticket', 'behind', 'home', 'tonight', 'hearts', 'break', 'kc'])\n",
      "original document: \n",
      "['Not', 'sure', 'how', 'defending.', 'What', 'I', 'am', 'saying', 'is', 'that', 'Mercy', 'changes', 'the', 'game', 'for', 'sure', 'and', 'what', 'it', 'changed', 'it', 'from', 'is', 'no', 'longer', 'can', 'you', 'get', 'a', 'pick', 'and', 'then', 'roll', 'a', 'fight.', 'Instead', 'people', 'need', 'to', 'force', 'her', 'rez,', 'take', 'her', 'out', 'and', 'win', 'the', 'team', 'battle.', 'In', 'reality', 'we', 'already', 'killed', 'support', 'first', 'and', 'more', 'so', 'with', 'Mercy.', 'Mercy', 'isnt', 'broken', 'she', 'is', 'strong', 'and', 'needs', 'a', 'different', 'tactic.', 'Much', 'of', 'what', 'she', 'does', 'isnt', 'different', 'to', 'what', 'she', 'used', 'to', 'do', 'which', 'is', 'rez', 'and', 'shift', 'a', 'battle.', '\\n\\nand', 'then', 'I', 'said', 'Ana', 'though', 'needs', 'some', 'changes', 'because', 'she', 'doesnt', 'do', 'as', 'much', 'as', 'what', 'other', 'supports', 'do.', 'Like', 'how', '76', 'kicked', 'McCree', 'out.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'defend', 'say', 'mercy', 'chang', 'gam', 'sur', 'chang', 'long', 'get', 'pick', 'rol', 'fight', 'instead', 'peopl', 'nee', 'forc', 'rez', 'tak', 'win', 'team', 'battl', 'real', 'already', 'kil', 'support', 'first', 'mercy', 'mercy', 'isnt', 'brok', 'strong', 'nee', 'diff', 'tact', 'much', 'isnt', 'diff', 'us', 'rez', 'shift', 'battl', '\\n\\nand', 'said', 'an', 'though', 'nee', 'chang', 'doesnt', 'much', 'support', 'lik', 'seventy-six', 'kick', 'mccree'], ['sure', 'defend', 'say', 'mercy', 'change', 'game', 'sure', 'change', 'longer', 'get', 'pick', 'roll', 'fight', 'instead', 'people', 'need', 'force', 'rez', 'take', 'win', 'team', 'battle', 'reality', 'already', 'kill', 'support', 'first', 'mercy', 'mercy', 'isnt', 'break', 'strong', 'need', 'different', 'tactic', 'much', 'isnt', 'different', 'use', 'rez', 'shift', 'battle', '\\n\\nand', 'say', 'ana', 'though', 'need', 'change', 'doesnt', 'much', 'support', 'like', 'seventy-six', 'kick', 'mccree'])\n",
      "original document: \n",
      "['\"I', \"don't\", 'want', 'a', 'F2P', 'trash', 'store', 'with', 'combat', 'decks', 'but', \"I'll\", 'buy', 'an', 'expansion', 'with', 'one', 'for', 'the', 'same', 'price.\"\\n\\nDo', 'you', 'not', 'see', 'how', \"there's\", 'zero', 'difference', 'in', 'what', 'you', 'just', 'said', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'want', 'f2p', 'trash', 'stor', 'comb', 'deck', 'il', 'buy', 'expand', 'on', 'price\\n\\ndo', 'see', 'ther', 'zero', 'diff', 'said', 'lol'], ['dont', 'want', 'f2p', 'trash', 'store', 'combat', 'deck', 'ill', 'buy', 'expansion', 'one', 'price\\n\\ndo', 'see', 'theres', 'zero', 'difference', 'say', 'lol'])\n",
      "original document: \n",
      "['&gt;', 'Feels', 'dehumanizing', 'and', 'makes', 'me', 'ponder', 'how', 'they', 'treat', 'others', 'that', 'they', \"aren't\", 'trying', 'to', 'get', 'something', 'out', 'of.', 'In', 'the', 'end,', 'yes', \"it's\", 'better', 'to', 'know,', 'bullet', 'dodged', 'and', 'all', 'that.', 'Still', 'feels', 'bad.\\n\\n**BINGO!!!**\\n\\nThis', 'is', 'it', 'exactly.', \"It's\", 'OK', 'not', 'to', 'be', 'interested', 'anymore,', 'but', 'especially', 'in', 'my', 'case,', 'where', 'the', 'rapport', 'is', 'long', 'established', 'professionally', 'and', 'I', 'thought,', 'a', 'least', 'platonically,', 'I', 'figured', 'you', 'can', 'be', 'cool', 'instead', 'of', 'treating', 'me', 'like', \"I'm\", 'a', 'leper', 'asking', 'you', 'for', 'spare', 'change', 'all', 'of', 'a', 'sudden.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'feel', 'dehum', 'mak', 'pond', 'tre', 'oth', 'ar', 'try', 'get', 'someth', 'end', 'ye', 'bet', 'know', 'bullet', 'dodg', 'stil', 'feel', 'bad\\n\\nbingo\\n\\nthis', 'exact', 'ok', 'interest', 'anym', 'espec', 'cas', 'rapport', 'long', 'est', 'profess', 'thought', 'least', 'platon', 'fig', 'cool', 'instead', 'tre', 'lik', 'im', 'lep', 'ask', 'spar', 'chang', 'sud'], ['gt', 'feel', 'dehumanize', 'make', 'ponder', 'treat', 'others', 'arent', 'try', 'get', 'something', 'end', 'yes', 'better', 'know', 'bullet', 'dodge', 'still', 'feel', 'bad\\n\\nbingo\\n\\nthis', 'exactly', 'ok', 'interest', 'anymore', 'especially', 'case', 'rapport', 'long', 'establish', 'professionally', 'think', 'least', 'platonically', 'figure', 'cool', 'instead', 'treat', 'like', 'im', 'leper', 'ask', 'spare', 'change', 'sudden'])\n",
      "original document: \n",
      "['In', 'that', 'Behind', 'the', 'Scenes', 'video,', 'we', 'see', 'what', 'looks', 'like', 'a', 'Resistance', 'fighter', 'falling', 'from', 'a', 'sizeable', 'height.', 'I', 'know', 'some', 'have', 'speculated', 'that', 'this', 'is', 'Paige', 'Tico.', 'Is', 'it?', 'Or', 'is', 'it', 'someone', 'else?', 'Or', 'does', 'it', 'hold', 'no', 'important', 'in', 'the', 'actual', 'film?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['behind', 'scen', 'video', 'see', 'look', 'lik', 'resist', 'fight', 'fal', 'siz', 'height', 'know', 'spec', 'paig', 'tico', 'someon', 'els', 'hold', 'import', 'act', 'film'], ['behind', 'scenes', 'video', 'see', 'look', 'like', 'resistance', 'fighter', 'fall', 'sizeable', 'height', 'know', 'speculate', 'paige', 'tico', 'someone', 'else', 'hold', 'important', 'actual', 'film'])\n",
      "original document: \n",
      "['Thanks!', \"it's\", 'the', 'thought', 'that', 'counts']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'thought', 'count'], ['thank', 'think', 'count'])\n",
      "original document: \n",
      "['Well', 'in', 'a', 'points', 'league', 'you', \"don't\", 'need', 'to', 'worry', 'about', 'categories', 'as', 'much.', 'Just', 'play', 'the', 'guys', 'with', 'the', 'highest', 'point', 'averages.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'point', 'leagu', 'dont', 'nee', 'worry', 'categ', 'much', 'play', 'guy', 'highest', 'point', 'av'], ['well', 'point', 'league', 'dont', 'need', 'worry', 'categories', 'much', 'play', 'guy', 'highest', 'point', 'average'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['bta', 'corn']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bta', 'corn'], ['bta', 'corn'])\n",
      "original document: \n",
      "['Well', 'at', 'least', 'our', 'defense', 'showed', 'up', 'today', ':D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'least', 'defens', 'show', 'today'], ['well', 'least', 'defense', 'show', 'today'])\n",
      "original document: \n",
      "['https://i.imgur.com/gyb9sAR.png']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsiimgurcomgyb9sarpng'], ['httpsiimgurcomgyb9sarpng'])\n",
      "original document: \n",
      "['Pretty', 'much', 'what', 'i', 'was', 'expecting.', 'Should', 'i', 'bother', 'even', 'trying', 'to', 'remain', 'friends', 'though?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'much', 'expect', 'both', 'ev', 'try', 'remain', 'friend', 'though'], ['pretty', 'much', 'expect', 'bother', 'even', 'try', 'remain', 'friends', 'though'])\n",
      "original document: \n",
      "['When', 'you', 'are', 'on', 'steam,', 'click', 'your', 'profile', 'picture', 'on', 'the', 'bottom', 'right', 'corner.', 'It', 'will', 'take', 'you', 'to', 'your', 'Steam', 'Profile.', 'Near', 'the', 'top', 'left,', 'there', 'will', 'be', 'a', 'website', 'address.', 'That', 'is', 'your', 'Steam', 'Profiel', 'URL.', 'You', 'can', 'copy-paste', 'it', 'to', 'Reddit,', 'so', 'I', 'can', 'add', 'ya!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['steam', 'click', 'profil', 'pict', 'bottom', 'right', 'corn', 'tak', 'steam', 'profil', 'near', 'top', 'left', 'websit', 'address', 'steam', 'profiel', 'url', 'copypast', 'reddit', 'ad', 'ya'], ['steam', 'click', 'profile', 'picture', 'bottom', 'right', 'corner', 'take', 'steam', 'profile', 'near', 'top', 'leave', 'website', 'address', 'steam', 'profiel', 'url', 'copypaste', 'reddit', 'add', 'ya'])\n",
      "original document: \n",
      "['Your', 'parents', 'did', 'work', 'at', 'one', 'time', 'to', 'gain', 'the', 'security', 'not', 'to', 'in', 'the', 'future.', \"You're\", 'very', 'well', 'aware', 'of', 'what', 'I', 'mean,', \"you're\", 'being', 'willfully', 'ignorant', 'because', 'your', 'argument', 'was', 'already', 'crappy.', 'Nice', 'job,', 'thank', 'you', 'for', 'exposing', 'that.', '\\n\\nI', 'literally', 'shared', 'the', 'definition', 'of', 'crony', 'capitalism,', 'if', 'you', \"don't\", 'like', 'it', 'then', \"that's\", 'OK', 'but', \"you're\", 'still', 'wrong.', \"I'm\", 'sorry', 'that', 'upsets', 'you.', 'You', 'have', 'been', 'able', 'to', 'get', 'away', 'with', 'saying', 'things', 'are', 'your', 'own', 'way', 'at', 'other', 'points', 'in', 'your', 'life', 'but', 'this', 'is', 'not', 'one', 'where', 'that', 'works.', 'Crony', 'capitalism', 'is', 'a', 'thing,', 'its', 'THE', 'thing', 'we', 'are', 'discussing,', 'and', 'you', \"don't\", 'get', 'to', 'just', 'define', 'things', 'in', 'your', 'own', 'twisted', 'ways', 'to', 'fit', 'your', 'argument.', \"You're\", 'not', 'very', 'good', 'at', 'this.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['par', 'work', 'on', 'tim', 'gain', 'sec', 'fut', 'yo', 'wel', 'aw', 'mean', 'yo', 'wil', 'ign', 'argu', 'already', 'crappy', 'nic', 'job', 'thank', 'expos', '\\n\\ni', 'lit', 'shar', 'definit', 'crony', 'capit', 'dont', 'lik', 'that', 'ok', 'yo', 'stil', 'wrong', 'im', 'sorry', 'upset', 'abl', 'get', 'away', 'say', 'thing', 'way', 'point', 'lif', 'on', 'work', 'crony', 'capit', 'thing', 'thing', 'discuss', 'dont', 'get', 'defin', 'thing', 'twist', 'way', 'fit', 'argu', 'yo', 'good'], ['parent', 'work', 'one', 'time', 'gain', 'security', 'future', 'youre', 'well', 'aware', 'mean', 'youre', 'willfully', 'ignorant', 'argument', 'already', 'crappy', 'nice', 'job', 'thank', 'expose', '\\n\\ni', 'literally', 'share', 'definition', 'crony', 'capitalism', 'dont', 'like', 'thats', 'ok', 'youre', 'still', 'wrong', 'im', 'sorry', 'upset', 'able', 'get', 'away', 'say', 'things', 'way', 'point', 'life', 'one', 'work', 'crony', 'capitalism', 'thing', 'thing', 'discuss', 'dont', 'get', 'define', 'things', 'twist', 'ways', 'fit', 'argument', 'youre', 'good'])\n",
      "original document: \n",
      "['It', \"doesn't\", 'matter', 'to', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'mat'], ['doesnt', 'matter'])\n",
      "original document: \n",
      "['I', 'completely', 'agree', ',', 'some', 'turns', \"I'd\", 'get', '8+', 'of', 'them', 'and', \"it's\", 'just', 'irritating', ',', 'I', 'wish', 'we', 'had', 'the', 'option', 'to', 'turn', 'them', 'off.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['complet', 'agr', 'turn', 'id', 'get', 'eight', 'irrit', 'wish', 'opt', 'turn'], ['completely', 'agree', 'turn', 'id', 'get', 'eight', 'irritate', 'wish', 'option', 'turn'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['I', 'have', 'a', 'sk', 'Hynix', '250', 'for', '50$']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sk', 'hynix', 'two hundred and fifty', 'fifty'], ['sk', 'hynix', 'two hundred and fifty', 'fifty'])\n",
      "original document: \n",
      "['Congrats', 'on', 'the', 'purchase.', '', 'watchbuys', 'is', 'doing', 'a', 'road', 'show', 'in', 'San', 'Francisco', 'this', 'weekend.', '', \"I'll\", 'be', 'there', 'to', 'check', 'it', 'out', 'before', 'I', 'make', 'the', 'final', 'plunge.', '', 'Super', 'excited!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congr', 'purchas', 'watchbuy', 'road', 'show', 'san', 'francisco', 'weekend', 'il', 'check', 'mak', 'fin', 'plung', 'sup', 'excit'], ['congrats', 'purchase', 'watchbuys', 'road', 'show', 'san', 'francisco', 'weekend', 'ill', 'check', 'make', 'final', 'plunge', 'super', 'excite'])\n",
      "original document: \n",
      "['Pass', 'the', 'mic', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pass', 'mic'], ['pass', 'mic'])\n",
      "original document: \n",
      "['&gt;But', 'this', 'singular', 'focus', 'on', 'the', 'white', 'working', 'class', '—', 'rather', 'than', 'the', 'working', 'class', 'as', 'a', 'whole,', 'in', 'all', 'its', 'hues', '—', 'has', '(perhaps', 'unintentionally)', 'aided', 'and', 'abetted', 'neoliberalism’s', 'ascension', 'in', 'the', 'Democratic', 'Party.', 'The', 'fate', 'of', 'workers', 'has', 'been', 'lost', 'in', 'the', 'shuffle,', 'undermining', 'both', 'the', 'material', 'wellbeing', 'and', 'the', 'morale', 'of', 'what', 'should', 'be', 'the', 'party’s', 'voting', 'base.\\n\\n\\n&gt;By', 'focusing', 'on', 'the', 'role', 'of', 'white', 'voters', 'in', 'Clinton’s', 'defeat,', 'rather', 'than', 'the', 'failure', 'of', 'the', 'Democrats’', 'neoliberal', 'strategy,', 'liberal', 'pundits', 'and', 'party', 'leaders', 'are', 'drawing', 'the', 'wrong', 'conclusions', 'from', 'Trump’s', 'victory.', 'Instead', 'of', 'debating', 'how', 'to', 'win', 'white', 'workers', 'or', 'doubling', 'down', 'on', 'the', 'misguided', 'strategy', 'of', 'courting', 'upscale', 'whites,', 'Democrats', 'must', 'train', 'their', 'attention', 'on', 'the', 'needs', 'of', 'the', 'working', 'class', 'as', 'a', 'whole.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtbut', 'singul', 'foc', 'whit', 'work', 'class', 'rath', 'work', 'class', 'whol', 'hue', 'perhap', 'unint', 'aid', 'abet', 'neolib', 'ascend', 'democr', 'party', 'fat', 'work', 'lost', 'shuffl', 'undermin', 'mat', 'wellb', 'mor', 'party', 'vot', 'base\\n\\n\\ngtby', 'focus', 'rol', 'whit', 'vot', 'clinton', 'def', 'rath', 'fail', 'democr', 'neolib', 'strategy', 'lib', 'pundit', 'party', 'lead', 'draw', 'wrong', 'conclud', 'trump', 'vict', 'instead', 'deb', 'win', 'whit', 'work', 'doubl', 'misguid', 'strategy', 'court', 'upsc', 'whit', 'democr', 'must', 'train', 'at', 'nee', 'work', 'class', 'whol'], ['gtbut', 'singular', 'focus', 'white', 'work', 'class', 'rather', 'work', 'class', 'whole', 'hue', 'perhaps', 'unintentionally', 'aid', 'abet', 'neoliberalisms', 'ascension', 'democratic', 'party', 'fate', 'workers', 'lose', 'shuffle', 'undermine', 'material', 'wellbeing', 'morale', 'party', 'vote', 'base\\n\\n\\ngtby', 'focus', 'role', 'white', 'voters', 'clintons', 'defeat', 'rather', 'failure', 'democrats', 'neoliberal', 'strategy', 'liberal', 'pundits', 'party', 'leaders', 'draw', 'wrong', 'conclusions', 'trump', 'victory', 'instead', 'debate', 'win', 'white', 'workers', 'double', 'misguide', 'strategy', 'court', 'upscale', 'white', 'democrats', 'must', 'train', 'attention', 'need', 'work', 'class', 'whole'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoqn8c/):\\n\\nUsing', 'CreateSpace', 'once', 'I', 'had', 'the', 'final', 'product', 'ready', 'to', 'roll', 'it', 'took', '3', 'days']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoqn8c\\n\\nusing', 'createspac', 'fin', 'produc', 'ready', 'rol', 'took', 'three', 'day'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoqn8c\\n\\nusing', 'createspace', 'final', 'product', 'ready', 'roll', 'take', 'three', 'days'])\n",
      "original document: \n",
      "['Cool.', 'Thanks', 'for', 'the', 'insight.', 'I', 'have', 'a', 'feeling', 'I’ll', 'be', 'buying', 'another', 'lol.', 'Got', 'the', 'zombie', 'green', 'this', 'time', 'around.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cool', 'thank', 'insight', 'feel', 'il', 'buy', 'anoth', 'lol', 'got', 'zomby', 'green', 'tim', 'around'], ['cool', 'thank', 'insight', 'feel', 'ill', 'buy', 'another', 'lol', 'get', 'zombie', 'green', 'time', 'around'])\n",
      "original document: \n",
      "['IN-TENSE.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['intens'], ['intense'])\n",
      "original document: \n",
      "['The', 'googles', 'just', 'told', 'me', 'this:', '', 'Denominational', 'families', 'that', 'practice', 'infant', 'baptism', 'include', 'Catholics,', 'Eastern', 'and', 'Oriental', 'Orthodox,', 'Anglicans,', 'Episcopalians,', 'Lutherans,', 'Presbyterians,', 'Congregationals', 'and', 'other', 'Reformed', 'denominations,', 'Methodists', 'and', 'some', 'Nazarenes,', 'and', 'the', 'Moravian', 'Church.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['googl', 'told', 'denomin', 'famy', 'pract', 'inf', 'bapt', 'includ', 'cathol', 'eastern', 'ory', 'orthodox', 'angl', 'episcop', 'luth', 'presbyt', 'congreg', 'reform', 'denomin', 'method', 'naz', 'morav', 'church'], ['google', 'tell', 'denominational', 'families', 'practice', 'infant', 'baptism', 'include', 'catholics', 'eastern', 'oriental', 'orthodox', 'anglicans', 'episcopalians', 'lutherans', 'presbyterians', 'congregationals', 'reform', 'denominations', 'methodists', 'nazarenes', 'moravian', 'church'])\n",
      "original document: \n",
      "['Agreed.', '', 'PJW', 'is', 'a', 'cunt', 'too.', 'He', 'isnt', 'Pro', 'Trump.', 'He', 'at', 'least', 'is', 'witty', 'but', 'i', 'dont', 'consider', 'him', 'loyal.', 'He', 'can', 'fuck', 'off', 'too.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'pjw', 'cunt', 'isnt', 'pro', 'trump', 'least', 'witty', 'dont', 'consid', 'loy', 'fuck'], ['agree', 'pjw', 'cunt', 'isnt', 'pro', 'trump', 'least', 'witty', 'dont', 'consider', 'loyal', 'fuck'])\n",
      "original document: \n",
      "['I', 'am', 'more', 'sick', 'of', 'the', 'fake', 'mat', '4s', 'ffs..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sick', 'fak', 'mat', '4s', 'ffs'], ['sick', 'fake', 'mat', '4s', 'ffs'])\n",
      "original document: \n",
      "['My', 'method', 'is', 'to', 'draw', 'attention', 'to', 'it', 'afterwards', 'even', 'though', 'I', 'know', \"it's\", 'obnoxious', 'and', 'kills', 'the', 'joke,', 'but', \"I'm\", 'too', 'upset', 'about', 'the', 'joke', 'going', 'unnoticed', 'to', 'not', 'do', 'it.\\n\\nJust', 'end', 'me', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['method', 'draw', 'at', 'afterward', 'ev', 'though', 'know', 'obnoxy', 'kil', 'jok', 'im', 'upset', 'jok', 'going', 'unnot', 'it\\n\\njust', 'end'], ['method', 'draw', 'attention', 'afterwards', 'even', 'though', 'know', 'obnoxious', 'kill', 'joke', 'im', 'upset', 'joke', 'go', 'unnoticed', 'it\\n\\njust', 'end'])\n",
      "original document: \n",
      "['Even', 'bots', 'are', 'trolling', 'us', 'now!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'bot', 'trol', 'us'], ['even', 'bots', 'troll', 'us'])\n",
      "original document: \n",
      "[\"That's\", 'a', 'great', 'pic.', '', 'Looks', 'like', 'a', 'fatherson', 'ish', 'relationship', 'and', 'Brett', 'has', 'known', 'Ben', 'and', 'his', 'family', 'since', 'Ben', 'was', 'a', 'baby']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'gre', 'pic', 'look', 'lik', 'fatherson', 'ish', 'rel', 'bret', 'known', 'ben', 'famy', 'sint', 'ben', 'baby'], ['thats', 'great', 'pic', 'look', 'like', 'fatherson', 'ish', 'relationship', 'brett', 'know', 'ben', 'family', 'since', 'ben', 'baby'])\n",
      "original document: \n",
      "['Yeah,', 'if', \"you're\", 'looking', 'to', 'rewrite', 'and', 'misrepresent', 'recent', 'events,', \"I'm\", 'probably', 'not', 'the', 'right', 'guy', 'to', 'talk', 'to', '-', 'I', \"don't\", 'care', 'about', 'your', 'revisionism.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'yo', 'look', 'rewrit', 'misrepres', 'rec', 'ev', 'im', 'prob', 'right', 'guy', 'talk', 'dont', 'car', 'revid'], ['yeah', 'youre', 'look', 'rewrite', 'misrepresent', 'recent', 'events', 'im', 'probably', 'right', 'guy', 'talk', 'dont', 'care', 'revisionism'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Not', 'a', 'fan', 'of', 'the', 'color', 'yellow,', 'but', 'that', 'is', 'one', 'sexy', 'yellow', 'console.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fan', 'col', 'yellow', 'on', 'sexy', 'yellow', 'consol'], ['fan', 'color', 'yellow', 'one', 'sexy', 'yellow', 'console'])\n",
      "original document: \n",
      "['7', 'rings', '7', 'fmvps', 'is', 'what', 'it’s', 'gonna', 'take,', 'even', 'MJ', 'has', 'basically', 'made', 'that', 'point', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sev', 'ring', 'sev', 'fmvps', 'gonn', 'tak', 'ev', 'mj', 'bas', 'mad', 'point'], ['seven', 'ring', 'seven', 'fmvps', 'gonna', 'take', 'even', 'mj', 'basically', 'make', 'point'])\n",
      "original document: \n",
      "['Haha.', 'No...', 'If', 'I', 'remember', 'a', 'unp', 'wilp', 'be', 'developed', 'in', 'the', 'future', 'at', 'some', 'point', 'maybe.', 'I', 'do', 'remember', 'a', 'comment', 'about', 'that,', 'but', 'it', 'would', '', 'take', 'longer', 'than', 'CBBE.', 'People', 'like', 'UNP.', 'My', 'guess', 'is', 'someone', 'will', 'update', 'it', 'to', 'the', 'new', 'CBBE', '2', 'stander.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'rememb', 'unp', 'wilp', 'develop', 'fut', 'point', 'mayb', 'rememb', 'com', 'would', 'tak', 'long', 'cbbe', 'peopl', 'lik', 'unp', 'guess', 'someon', 'upd', 'new', 'cbbe', 'two', 'stand'], ['haha', 'remember', 'unp', 'wilp', 'develop', 'future', 'point', 'maybe', 'remember', 'comment', 'would', 'take', 'longer', 'cbbe', 'people', 'like', 'unp', 'guess', 'someone', 'update', 'new', 'cbbe', 'two', 'stander'])\n",
      "original document: \n",
      "['It', \"won't\", 'kill', 'you', 'in', 'Animal', 'Crossing!\\n\\nBut', 'it', 'has', 'a', 'chance', 'of', 'falling', 'out', 'of', 'a', 'tree.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wont', 'kil', 'anim', 'crossing\\n\\nbut', 'chant', 'fal', 'tre'], ['wont', 'kill', 'animal', 'crossing\\n\\nbut', 'chance', 'fall', 'tree'])\n",
      "original document: \n",
      "['Yeah', 'maybe,', 'I', 'just', 'feel', 'the', 'rematch', 'coming']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'mayb', 'feel', 'rematch', 'com'], ['yeah', 'maybe', 'feel', 'rematch', 'come'])\n",
      "original document: \n",
      "['thnx']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thnx'], ['thnx'])\n",
      "original document: \n",
      "[\"He's\", 'got', 'a', 'pepper', 'guy', 'and', 'a', 'legit', 'hot', 'sauce', 'business,', \"it's\", 'probably', 'a', '7', 'pot.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'got', 'pep', 'guy', 'legit', 'hot', 'sauc', 'busy', 'prob', 'sev', 'pot'], ['hes', 'get', 'pepper', 'guy', 'legit', 'hot', 'sauce', 'business', 'probably', 'seven', 'pot'])\n",
      "original document: \n",
      "['I', 'think', 'we', 'end', 'up', 'with', 'a', 'soft', 'lineup', 'tonight,', 'hard', 'one', 'tomorrow.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'end', 'soft', 'lineup', 'tonight', 'hard', 'on', 'tomorrow'], ['think', 'end', 'soft', 'lineup', 'tonight', 'hard', 'one', 'tomorrow'])\n",
      "original document: \n",
      "['Oh', 'wow', \"that's\", 'awfully', 'generous', 'and', 'compassionate...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'wow', 'that', 'aw', 'gen', 'compass'], ['oh', 'wow', 'thats', 'awfully', 'generous', 'compassionate'])\n",
      "original document: \n",
      "['Do', 'you', 'not', 'remember', 'how', 'bad', 'WR', 'last', 'year?', 'Texans', 'had', 'a', 'damn', 'good', 'line', 'and', 'still', \"can't\", 'do', 'shit.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'bad', 'wr', 'last', 'year', 'tex', 'damn', 'good', 'lin', 'stil', 'cant', 'shit'], ['remember', 'bad', 'wr', 'last', 'year', 'texans', 'damn', 'good', 'line', 'still', 'cant', 'shit'])\n",
      "original document: \n",
      "['LLS?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lls'], ['lls'])\n",
      "original document: \n",
      "['Gone', 'as', 'quickly', 'as', 'it', 'came', 'to', 'be....']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gon', 'quick', 'cam'], ['go', 'quickly', 'come'])\n",
      "original document: \n",
      "['Welp', 'there', 'it', 'is,', 'The', 'biggest', 'reddest', 'Dildo', 'WaltonChain', 'has', 'ever', 'seen', 'haha!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['welp', 'biggest', 'reddest', 'dildo', 'waltonchain', 'ev', 'seen', 'hah'], ['welp', 'biggest', 'reddest', 'dildo', 'waltonchain', 'ever', 'see', 'haha'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Ludicrous!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ludicr'], ['ludicrous'])\n",
      "original document: \n",
      "['Bruh', 'chill', 'some', 'people', 'don’t', 'have', 'their', 'phone', 'on', 'them', '24/7.', 'And/or', 'she', 'went', 'out', 'without', 'her', 'phone.', '\\n\\nReal', 'worries', 'are', 'when', 'someone', 'opens', 'a', 'snap', 'then', 'never', 'responds']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bruh', 'chil', 'peopl', 'dont', 'phon', 'two hundred and forty-seven', 'and', 'went', 'without', 'phon', '\\n\\nreal', 'worry', 'someon', 'op', 'snap', 'nev', 'respond'], ['bruh', 'chill', 'people', 'dont', 'phone', 'two hundred and forty-seven', 'andor', 'go', 'without', 'phone', '\\n\\nreal', 'worry', 'someone', 'open', 'snap', 'never', 'respond'])\n",
      "original document: \n",
      "['She’s', 'so', 'hot.', 'Her', 'snapchat', 'is', 'great', 'too', '👌🏼']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['she', 'hot', 'snapch', 'gre'], ['shes', 'hot', 'snapchat', 'great'])\n",
      "original document: \n",
      "['Correct,', 'this', 'is', 'a', 'special', 'puke.', 'I', 'think', 'the', 'officiant', 'probably', 'shat', 'and', 'spewed', 'at', 'the', 'same', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['correct', 'spec', 'puk', 'think', 'officy', 'prob', 'shat', 'spew', 'tim'], ['correct', 'special', 'puke', 'think', 'officiant', 'probably', 'shit', 'spew', 'time'])\n",
      "original document: \n",
      "['Aww', 'I', 'feel', 'like', \"she's\", 'been', 'hinting', 'at', 'this', 'for', 'a', 'while', 'on', 'twitter!', 'Happy', 'for', 'her,', 'hope', \"it's\", 'not', 'anyone', 'from', 'bachelor', 'nation', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['aww', 'feel', 'lik', 'she', 'hint', 'twit', 'happy', 'hop', 'anyon', 'bachel', 'nat', 'though'], ['aww', 'feel', 'like', 'shes', 'hint', 'twitter', 'happy', 'hope', 'anyone', 'bachelor', 'nation', 'though'])\n",
      "original document: \n",
      "['I', 'get', 'where', 'hes', 'coming', 'from.', 'In', 'CSGO', '(obviously', 'the', 'most', 'competitive', 'fps', 'game', 'atm)', 'most', 'pro', 'players', 'use', '400', 'dpi.', 'So', 'I', 'assume', 'thats', 'where', 'he', 'gets', 'it', 'from']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'hes', 'com', 'csgo', 'obvy', 'competit', 'fps', 'gam', 'atm', 'pro', 'play', 'us', 'four hundred', 'dpi', 'assum', 'that', 'get'], ['get', 'hes', 'come', 'csgo', 'obviously', 'competitive', 'fps', 'game', 'atm', 'pro', 'players', 'use', 'four hundred', 'dpi', 'assume', 'thats', 'get'])\n",
      "original document: \n",
      "['Not', 'sure', 'if', 'this', 'is', 'normal', 'but', 'I', 'was', 'able', 'to', 'because', 'I', 'broke', 'my', 'Pixel.', 'I', 'got', 'a', 'cheap', 'Moto', 'g4', 'play', 'and', 'called', 'to', 'get', 'it', 'activated', 'and', 'transferred', 'my', 'number', 'to', 'it.', 'That', 'worked', 'when', 'I', 'called', 'the', 'tech', 'support.', 'It', 'was', 'pain', 'but', 'worked', 'and', 'the', 'plan', 'stayed', 'the', 'same.', 'Not', 'sure', 'if', 'the', 'CS', 'tech', 'did', 'some', 'exception', 'but', 'you', 'could', 'try', 'that', 'route.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'norm', 'abl', 'brok', 'pixel', 'got', 'cheap', 'moto', 'g4', 'play', 'cal', 'get', 'act', 'transfer', 'numb', 'work', 'cal', 'tech', 'support', 'pain', 'work', 'plan', 'stay', 'sur', 'cs', 'tech', 'exceiv', 'could', 'try', 'rout'], ['sure', 'normal', 'able', 'break', 'pixel', 'get', 'cheap', 'moto', 'g4', 'play', 'call', 'get', 'activate', 'transfer', 'number', 'work', 'call', 'tech', 'support', 'pain', 'work', 'plan', 'stay', 'sure', 'cs', 'tech', 'exception', 'could', 'try', 'route'])\n",
      "original document: \n",
      "['&gt;', 'Also', 'remember', 'that', 'the', 'Federation', 'has', 'multiple', 'fleets,\\n\\nI', \"can't\", 'remember', 'this', 'because', 'I', \"didn't\", 'know', 'it', 'in', 'the', 'first', 'place!', 'Can', 'you', 'recommend', 'some', 'reading', 'to', 'learn', 'more', 'about', 'this?', 'How', 'are', 'these', 'fleets', 'subdivided?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'also', 'rememb', 'fed', 'multipl', 'fleets\\n\\ni', 'cant', 'rememb', 'didnt', 'know', 'first', 'plac', 'recommend', 'read', 'learn', 'fleet', 'subdivid'], ['gt', 'also', 'remember', 'federation', 'multiple', 'fleets\\n\\ni', 'cant', 'remember', 'didnt', 'know', 'first', 'place', 'recommend', 'read', 'learn', 'fleet', 'subdivide'])\n",
      "original document: \n",
      "['And', 'the', 'Leafs', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['leaf'], ['leaf'])\n",
      "original document: \n",
      "['lol', 'gl']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'gl'], ['lol', 'gl'])\n",
      "original document: \n",
      "['This', 'is', 'awesome', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['awesom'], ['awesome'])\n",
      "original document: \n",
      "['Yes,', 'Sandmeh', 'shows', 'up', 'as', 'a', 'random', 'encounter', 'on', 'Stage', '35', 'with', 'Beetler', 'and', 'Quaken,', 'and', 'replaces', 'Sgt.', 'Burly', 'occasionally,', 'which', 'actually', 'makes', 'the', 'stage', 'easier', 'since', 'Sandmeh', 'has', 'a', 'lower', 'attack', 'stat.\\n\\nIf', \"you're\", 'playing', 'the', 'U.S.', 'version', 'of', 'Wibble', 'Wobble,', 'Sternynan', 'is', 'an', 'S', 'rank', 'from', 'the', 'Tough', 'Tribe,', 'a', 'Jibanyan', 'reskin.', 'He', 'appears', 'in', 'the', 'second', 'hidden', 'stage', 'on', 'the', 'second', 'map.', 'You', 'have', 'to', 'beat', 'the', 'end', 'bosses', 'Tanbo,', 'Payn', 'and', 'Sandmeh', 'with', 'Sheen', 'on', 'your', 'team.', 'Then', 'the', 'stage', 'unlocks.\\n\\nSandmeh', 'is', 'horrible.', 'He', 'had', 'a', 'purpose', 'in', 'the', 'Liberty', 'Way', 'event', 'this', 'past', 'spring.', 'You', 'would', 'catch', 'him', 'and', 'then', 'crank', 'for', 'the', 'Sand', 'Suit', 'to', 'fuse', 'him', 'into', 'Mr.', 'Sandmeh.', 'He', 'would', 'then', 'get', 'an', 'attack', 'damage', 'boost', 'when', 'fighting', 'Libertynyan', 'S.', 'Now', \"he's\", 'only', 'in', 'this', 'event', 'along', 'with', 'other', 'past', 'event', 'Yo-kai', 'just', 'so', 'people', 'who', 'missed', 'out', 'can', 'add', 'to', 'their', 'Medallium.', 'I', \"don't\", 'really', 'need', 'him,', 'I', 'was', 'just', 'trying', 'to', 'earn', 'all', 'of', 'the', 'stars', 'for', 'the', 'sake', 'of', 'it.', 'It', 'means', 'nothing', 'because', \"they're\", 'not', 'offering', 'rewards', 'for', 'earning', 'all', 'stars', 'for', 'this', 'particular', 'event.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'sandmeh', 'show', 'random', 'encount', 'stag', 'thirty-five', 'beetl', 'quak', 'replac', 'sgt', 'bur', 'occas', 'act', 'mak', 'stag', 'easy', 'sint', 'sandmeh', 'low', 'attack', 'stat\\n\\nif', 'yo', 'play', 'us', 'vert', 'wibbl', 'wobbl', 'sternyn', 'rank', 'tough', 'trib', 'jibany', 'reskin', 'appear', 'second', 'hid', 'stag', 'second', 'map', 'beat', 'end', 'boss', 'tanbo', 'payn', 'sandmeh', 'sheen', 'team', 'stag', 'unlocks\\n\\nsandmeh', 'horr', 'purpos', 'liberty', 'way', 'ev', 'past', 'spring', 'would', 'catch', 'crank', 'sand', 'suit', 'fus', 'mr', 'sandmeh', 'would', 'get', 'attack', 'dam', 'boost', 'fight', 'libertyny', 'hes', 'ev', 'along', 'past', 'ev', 'yoka', 'peopl', 'miss', 'ad', 'medall', 'dont', 'real', 'nee', 'try', 'earn', 'star', 'sak', 'mean', 'noth', 'theyr', 'off', 'reward', 'earn', 'star', 'particul', 'event\\n\\n'], ['yes', 'sandmeh', 'show', 'random', 'encounter', 'stage', 'thirty-five', 'beetler', 'quaken', 'replace', 'sgt', 'burly', 'occasionally', 'actually', 'make', 'stage', 'easier', 'since', 'sandmeh', 'lower', 'attack', 'stat\\n\\nif', 'youre', 'play', 'us', 'version', 'wibble', 'wobble', 'sternynan', 'rank', 'tough', 'tribe', 'jibanyan', 'reskin', 'appear', 'second', 'hide', 'stage', 'second', 'map', 'beat', 'end', 'boss', 'tanbo', 'payn', 'sandmeh', 'sheen', 'team', 'stage', 'unlocks\\n\\nsandmeh', 'horrible', 'purpose', 'liberty', 'way', 'event', 'past', 'spring', 'would', 'catch', 'crank', 'sand', 'suit', 'fuse', 'mr', 'sandmeh', 'would', 'get', 'attack', 'damage', 'boost', 'fight', 'libertynyan', 'hes', 'event', 'along', 'past', 'event', 'yokai', 'people', 'miss', 'add', 'medallium', 'dont', 'really', 'need', 'try', 'earn', 'star', 'sake', 'mean', 'nothing', 'theyre', 'offer', 'reward', 'earn', 'star', 'particular', 'event\\n\\n'])\n",
      "original document: \n",
      "['[THE', 'VIDEO', 'ON', 'THE', 'ROCKIES', 'TWITTER!!!!!!!!!!](https://twitter.com/Rockies/status/914272299834007552)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['video', 'rocky', 'twitterhttpstwittercomrockiesstatus914272299834007552'], ['video', 'rockies', 'twitterhttpstwittercomrockiesstatus914272299834007552'])\n",
      "original document: \n",
      "['I', 'totally', \"agree.\\nI've\", 'watched', 'all', 'their', 'videos', 'and', 'I', \"can't\", 'wait', 'for', 'their', 'next', 'one,', 'each', 'time.', '(My', 'favorites', 'are', 'the', '*Song', 'of', 'Durin*', 'and', 'the', '*Fall', 'of', 'Gil', \"Galad*)\\nI'm\", 'not', 'as', 'good', 'in', 'English', 'as', 'I', 'should', 'be', 'to', 'express', 'myself', 'proprely.', 'So', \"I'll\", 'continue', 'in', 'my', 'native', 'language', '(French).\\n\\nSi', 'vous', 'saviez', 'comment', \"j'éprouve\", 'une', 'profonde', 'admiration', 'pour', 'les', 'gens', 'de', 'cette', 'chaîne.', 'Ils', \"m'ont\", 'aidé', 'à', 'travers', 'des', 'moments', 'où', \"j'aurais\", 'pu', 'être', 'fâché,', 'triste,', 'mais', 'leurs', 'chansons', \"m'ont\", 'donné', 'la', 'motivation', 'de', 'continuer,', 'parfois.\\nCes', 'œuvres', \"m'ont\", 'permis', 'de', 'réfléchir', 'plus', 'loin,', 'de', 'mieux', 'comprendre', \"l'immense\", 'univers', 'de', 'Tolkien,', 'au-delà', 'de', 'ce', 'que', \"j'aurais\", 'pu', \"m'imaginer.\", 'Je', \"n'écoute\", 'pas', 'les', 'paroles,', 'je', 'les', 'vis,', 'je', 'vie', 'les', 'personnages', 'dont', 'ils', 'chantent', \"l'histoire.\\nJe\", 'les', 'chantonne', 'également', 'très', 'souvent,', 'peut-être', 'un', 'peu', 'trop', 'pour', 'les', 'gens', 'qui', \"m'entourent\", 'qui,', 'surtout', 'mes', 'amis,', 'semblent', 'à', 'mon', 'grand', 'désespoir', 'avoir', 'une', 'aversion', 'pour', 'ce', 'genre', 'musical.', 'Comme', 'si', 'le', 'rap', 'moderne', 'signifiait', 'quelque', 'chose.', 'La', 'musique', 'classique', 'peut', 'atteindre', 'un', 'point', 'de', 'mon', 'cœur', 'qui', 'se', 'trouve', 'intouchable', 'par', 'le', 'pop', 'et', 'les', 'autres', 'genres.\\nLes', 'membres', 'de', 'Clamavi', 'De', 'Profundis', 'sont', 'divins.\\n\\nSorry', 'for', 'that', 'French', 'text,', 'but', 'I', \"wouldn't\", 'have', 'made', 'it', 'in', 'English...\\nLike', 'you', 'said,', '\"Clamavi', 'De', 'Profundis', 'is', 'an', 'incredible', 'group\".\\nWow', 'that', 'was', 'so', 'long,', 'sorry,\\nMy', 'best', 'regards,\\nArosgan.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tot', 'agree\\niv', 'watch', 'video', 'cant', 'wait', 'next', 'on', 'tim', 'favorit', 'song', 'durin', 'fal', 'gil', 'galad\\nim', 'good', 'engl', 'express', 'propr', 'il', 'continu', 'nat', 'langu', 'french\\n\\nsi', 'vou', 'saviez', 'com', 'jeprouv', 'un', 'profond', 'admir', 'pour', 'les', 'gen', 'de', 'cet', 'chain', 'il', 'mont', 'aid', 'trav', 'des', 'mom', 'ou', 'jaura', 'pu', 'et', 'fach', 'trist', 'mai', 'leur', 'chanson', 'mont', 'don', 'la', 'mot', 'de', 'continu', 'parfois\\nce', 'uvr', 'mont', 'perm', 'de', 'reflechir', 'plu', 'loin', 'de', 'mieux', 'comprendr', 'limmens', 'un', 'de', 'tolky', 'audel', 'de', 'ce', 'que', 'jaura', 'pu', 'mimagin', 'je', 'necout', 'pas', 'les', 'parol', 'je', 'les', 'vis', 'je', 'vie', 'les', 'person', 'dont', 'il', 'chant', 'lhistoire\\nj', 'les', 'chanton', 'eg', 'tre', 'souv', 'peutet', 'un', 'peu', 'trop', 'pour', 'les', 'gen', 'qui', 'mento', 'qui', 'surtout', 'mes', 'am', 'sembl', 'mon', 'grand', 'desespoir', 'avoir', 'un', 'avert', 'pour', 'ce', 'genr', 'mus', 'com', 'si', 'le', 'rap', 'modern', 'signifiait', 'quelqu', 'chos', 'la', 'mus', 'class', 'peut', 'atteindr', 'un', 'point', 'de', 'mon', 'cur', 'qui', 'se', 'trouv', 'intouch', 'par', 'le', 'pop', 'et', 'les', 'aut', 'genres\\nle', 'membr', 'de', 'clamav', 'de', 'profund', 'sont', 'divins\\n\\nsorry', 'french', 'text', 'wouldnt', 'mad', 'english\\nlike', 'said', 'clamav', 'de', 'profund', 'incred', 'group\\nwow', 'long', 'sorry\\nmy', 'best', 'regards\\narosgan'], ['totally', 'agree\\nive', 'watch', 'videos', 'cant', 'wait', 'next', 'one', 'time', 'favorites', 'song', 'durin', 'fall', 'gil', 'galad\\nim', 'good', 'english', 'express', 'proprely', 'ill', 'continue', 'native', 'language', 'french\\n\\nsi', 'vous', 'saviez', 'comment', 'jeprouve', 'une', 'profonde', 'admiration', 'pour', 'les', 'gens', 'de', 'cette', 'chaine', 'ils', 'mont', 'aide', 'travers', 'des', 'moments', 'ou', 'jaurais', 'pu', 'etre', 'fache', 'triste', 'mais', 'leurs', 'chansons', 'mont', 'donne', 'la', 'motivation', 'de', 'continuer', 'parfois\\nces', 'uvres', 'mont', 'permis', 'de', 'reflechir', 'plus', 'loin', 'de', 'mieux', 'comprendre', 'limmense', 'univers', 'de', 'tolkien', 'audela', 'de', 'ce', 'que', 'jaurais', 'pu', 'mimaginer', 'je', 'necoute', 'pas', 'les', 'parole', 'je', 'les', 'vis', 'je', 'vie', 'les', 'personnages', 'dont', 'ils', 'chantent', 'lhistoire\\nje', 'les', 'chantonne', 'egalement', 'tres', 'souvent', 'peutetre', 'un', 'peu', 'trop', 'pour', 'les', 'gens', 'qui', 'mentourent', 'qui', 'surtout', 'mes', 'amis', 'semblent', 'mon', 'grand', 'desespoir', 'avoir', 'une', 'aversion', 'pour', 'ce', 'genre', 'musical', 'comme', 'si', 'le', 'rap', 'moderne', 'signifiait', 'quelque', 'choose', 'la', 'musique', 'classique', 'peut', 'atteindre', 'un', 'point', 'de', 'mon', 'cur', 'qui', 'se', 'trouve', 'intouchable', 'par', 'le', 'pop', 'et', 'les', 'autres', 'genres\\nles', 'membres', 'de', 'clamavi', 'de', 'profundis', 'sont', 'divins\\n\\nsorry', 'french', 'text', 'wouldnt', 'make', 'english\\nlike', 'say', 'clamavi', 'de', 'profundis', 'incredible', 'group\\nwow', 'long', 'sorry\\nmy', 'best', 'regards\\narosgan'])\n",
      "original document: \n",
      "['Mods!', 'If', 'you', 'please!', 'Award', 'this', 'good', 'person', 'a', 'flair']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mod', 'pleas', 'award', 'good', 'person', 'flair'], ['mods', 'please', 'award', 'good', 'person', 'flair'])\n",
      "original document: \n",
      "['Yeah,', 'that', 'appears', 'to', 'be', 'part', 'of', 'the', 'disk.', '', 'This', \"wouldn't\", 'be', 'the', 'first', 'disk', 'to', 'come', 'apart.', '', 'That', 'DC-10', 'that', 'crash', 'landed', 'in', 'Sioux', 'City', 'back', 'in', '89', 'had', 'a', 'fan', 'disk', 'come', 'apart.', '', 'And', 'I', 'know', 'of', 'at', 'least', 'one', 'Pratt', '&amp;', 'Whitney', 'F100', '(F-15/16)', 'engine', 'that', 'had', 'a', 'fan', 'disk', 'fail.', '', 'That', 'caused', 'us', 'in', 'the', 'military,', 'a', 'LOT', 'of', 'work!', '', 'Fortunately,', 'that', 'engine', 'that', 'came', 'apart,', 'was', 'on', 'the', 'test', 'cell', 'at', 'the', 'time,', 'and', 'not', 'in', 'an', 'aircraft.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'appear', 'part', 'disk', 'wouldnt', 'first', 'disk', 'com', 'apart', 'dc10', 'crash', 'land', 'sioux', 'city', 'back', 'eighty-nine', 'fan', 'disk', 'com', 'apart', 'know', 'least', 'on', 'prat', 'amp', 'whitney', 'f100', 'f1516', 'engin', 'fan', 'disk', 'fail', 'caus', 'us', 'milit', 'lot', 'work', 'fortun', 'engin', 'cam', 'apart', 'test', 'cel', 'tim', 'aircraft'], ['yeah', 'appear', 'part', 'disk', 'wouldnt', 'first', 'disk', 'come', 'apart', 'dc10', 'crash', 'land', 'sioux', 'city', 'back', 'eighty-nine', 'fan', 'disk', 'come', 'apart', 'know', 'least', 'one', 'pratt', 'amp', 'whitney', 'f100', 'f1516', 'engine', 'fan', 'disk', 'fail', 'cause', 'us', 'military', 'lot', 'work', 'fortunately', 'engine', 'come', 'apart', 'test', 'cell', 'time', 'aircraft'])\n",
      "original document: \n",
      "['This', 'is', 'some', 'high', 'level', 'shit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['high', 'level', 'shit'], ['high', 'level', 'shit'])\n",
      "original document: \n",
      "['Hi,', 'dis', 'you', 'lost', 'you', 'pvp', 'honor', 'and', 'your', 'rank', 'if', 'you', 'rebirth', 'before', 'the', 'end', 'of', 'the', 'cd?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'dis', 'lost', 'pvp', 'hon', 'rank', 'rebir', 'end', 'cd'], ['hi', 'dis', 'lose', 'pvp', 'honor', 'rank', 'rebirth', 'end', 'cd'])\n",
      "original document: \n",
      "['143413050|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143412979\\ngood\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand and fifty', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143412979\\ngood\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand and fifty', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143412979\\ngood\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Polling', 'companies', 'are', 'in', 'the', 'business', 'of', 'producing', 'polls', 'that', 'are', 'their', 'customers', 'happy,', 'of', 'polls', 'designed', 'to', 'push', 'narratives,', 'and', 'a', 'bunch', 'of', 'other', 'things.\\n\\nThey', 'are', 'not', 'impartial', 'arbiters', 'of', 'sacred', 'statistics', 'and', 'it', 'is', 'naive', 'to', 'think', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pol', 'company', 'busy', 'produc', 'pol', 'custom', 'happy', 'pol', 'design', 'push', 'nar', 'bunch', 'things\\n\\nthey', 'impart', 'arbit', 'sacr', 'stat', 'naiv', 'think'], ['poll', 'company', 'business', 'produce', 'poll', 'customers', 'happy', 'poll', 'design', 'push', 'narratives', 'bunch', 'things\\n\\nthey', 'impartial', 'arbiters', 'sacred', 'statistics', 'naive', 'think'])\n",
      "original document: \n",
      "['Most', 'of', 'us', 'handles', 'the', 'transfers', 'our', 'selves.\\n\\nAs', 'for', 'morale', 'you', 'can', 'praise', 'your', 'players', 'for', 'their', 'recent', 'form,', 'training', 'level', 'and', 'their', 'conduct.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'handl', 'transf', 'selves\\n\\na', 'mor', 'pra', 'play', 'rec', 'form', 'train', 'level', 'conduc'], ['us', 'handle', 'transfer', 'selves\\n\\nas', 'morale', 'praise', 'players', 'recent', 'form', 'train', 'level', 'conduct'])\n",
      "original document: \n",
      "['Are', 'you', 'using', 'a', '60Hz', 'rom?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', '60hz', 'rom'], ['use', '60hz', 'rom'])\n",
      "original document: \n",
      "['Does', 'it', 'have', 'a', 'crew', 'which', 'can', 'respond', 'intelligently?', 'Or', 'are', 'they', 'restricted', 'to', 'staying', 'buttoned', 'up', 'inside', 'the', 'vehicle', 'and', \"won't\", 'intervene', 'if', 'I,', 'say,', 'get', 'on', 'top', 'of', 'it?', 'If', 'the', 'former,', \"I'm\", 'completely', 'dead.', 'If', 'the', 'latter,', 'I', 'might,', 'just', 'might,', 'survive.', 'Unescorted', 'tanks', \"aren't\", 'great', 'at', 'urban', 'environments,', 'and', 'it', 'has', 'to', 'find', 'me', 'to', 'kill', 'me.', '', \"\\n\\n\\nI'm\", 'on', 'a', 'UK-style', 'housing', 'estate', '-', 'lots', 'of', 'two-story', 'brick', 'buildings,', 'hedges,', 'fences.', 'Roads', \"aren't\", 'in', 'a', 'grid,', \"it's\", 'all', 'cul-de-sacs', 'and', 'tight', 'bends.', 'Not', 'much', 'line', 'of', 'sight.', 'Now', 'the', 'Abrams', 'has', 'thermal', 'and', 'night', 'vision', 'kit,', 'but', \"that's\", 'mostly', 'directional', 'and', 'generally', 'trained', 'on', 'the', 'forward', 'arc', '(as', 'are', 'the', 'weapons).', 'It', \"won't\", 'be', 'able', 'to', 'see', 'through,', 'say', 'thick', 'hedgerows,', 'walls,', 'buildings,', 'or', 'fences.', 'Obviously', 'if', 'it', 'actually', 'sees', 'me', \"I'm\", 'dead.', 'But', 'since', 'I', 'know', 'the', 'estate', 'quite', 'well', 'I', 'can', 'probably', 'evade', 'it', 'reasonably', 'easily,', 'for', 'a', 'small', 'time', 'at', 'least.', '', '\\nSo', 'my', 'plan', 'is', 'to', 'collect', 'some', 'firelighters', 'and', 'kindling', 'materials,', 'and', 'a', 'load', 'of', 'ceramic', 'plant', 'pots.', \"I'm\", 'also', 'buying', 'some', 'duct', 'tape,', 'rope,', 'some', 'food', 'and', 'water.', 'And', 'a', 'few', 'of', 'those', 'foil', 'emergency', 'blankets.', \"I'm\", 'going', 'to', 'make', 'a', 'bunch', 'of', 'small', 'fires', 'all', 'over', 'the', 'place,', 'and', 'use', 'them', 'to', 'heat', 'the', 'ceramic', 'pots.', 'These', 'are', 'to', 'serve', 'as', 'false', 'heat', 'signatures', 'all', 'over', 'the', 'place.', '', '\\nMeanwhile,', \"I'll\", 'be', 'in', 'an', 'alley', 'across', 'the', 'street,', 'behind', 'a', 'fence,', 'covered', 'in', 'the', 'foil', 'blankets', 'to', 'conceal', 'body', 'heat.', 'The', 'tank', 'will', 'probably', 'use', 'the', 'road', 'to', 'get', 'to', 'my', 'address', 'because', 'my', 'cul-de-sac', 'is', 'relatively', 'inaccessible', 'without', 'driving', 'through', 'other', 'houses.', 'That', 'puts', 'me', 'right', 'behind', 'it,', 'at', 'which', 'point', 'I', 'run', 'and', 'climb', 'right', 'on', 'top', 'of', 'the', 'turret.', \"That'll\", 'take', 'like', '5', 'seconds,', 'so', 'hopefully', \"I'm\", 'not', 'spotted.', '', 'Once', 'on', 'top', 'I', 'lie', 'spread', 'eagled,', 'and', 'securely', 'rope/tape', 'myself', 'to', 'whatever', 'stanchions', 'and', 'protuberances', 'there', 'are', 'on', 'top,', 'in', 'order', 'to', 'avoid', 'exhausting', 'myself', 'or', 'falling', 'off.', 'Then', 'wait', 'for', '24', 'hours', 'to', 'win!', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['crew', 'respond', 'intellig', 'restrict', 'stay', 'button', 'insid', 'vehic', 'wont', 'interv', 'say', 'get', 'top', 'form', 'im', 'complet', 'dead', 'lat', 'might', 'might', 'surv', 'unescort', 'tank', 'ar', 'gre', 'urb', 'environ', 'find', 'kil', '\\n\\n\\nim', 'ukstyl', 'hous', 'est', 'lot', 'twost', 'brick', 'build', 'hedg', 'fent', 'road', 'ar', 'grid', 'culdesac', 'tight', 'bend', 'much', 'lin', 'sight', 'abram', 'therm', 'night', 'vis', 'kit', 'that', 'most', 'direct', 'gen', 'train', 'forward', 'arc', 'weapon', 'wont', 'abl', 'see', 'say', 'thick', 'hedgerow', 'wal', 'build', 'fent', 'obvy', 'act', 'see', 'im', 'dead', 'sint', 'know', 'est', 'quit', 'wel', 'prob', 'evad', 'reason', 'easy', 'smal', 'tim', 'least', '\\nso', 'plan', 'collect', 'firelight', 'kindl', 'mat', 'load', 'ceram', 'plant', 'pot', 'im', 'also', 'buy', 'duc', 'tap', 'rop', 'food', 'wat', 'foil', 'emerg', 'blanket', 'im', 'going', 'mak', 'bunch', 'smal', 'fir', 'plac', 'us', 'heat', 'ceram', 'pot', 'serv', 'fals', 'heat', 'sign', 'plac', '\\nmeanwhile', 'il', 'alley', 'across', 'street', 'behind', 'fent', 'cov', 'foil', 'blanket', 'cont', 'body', 'heat', 'tank', 'prob', 'us', 'road', 'get', 'address', 'culdesac', 'rel', 'inaccess', 'without', 'driv', 'hous', 'put', 'right', 'behind', 'point', 'run', 'climb', 'right', 'top', 'turret', 'thatl', 'tak', 'lik', 'fiv', 'second', 'hop', 'im', 'spot', 'top', 'lie', 'spread', 'eagl', 'sec', 'ropetap', 'whatev', 'stanch', 'protub', 'top', 'ord', 'avoid', 'exhaust', 'fal', 'wait', 'twenty-four', 'hour', 'win'], ['crew', 'respond', 'intelligently', 'restrict', 'stay', 'button', 'inside', 'vehicle', 'wont', 'intervene', 'say', 'get', 'top', 'former', 'im', 'completely', 'dead', 'latter', 'might', 'might', 'survive', 'unescorted', 'tank', 'arent', 'great', 'urban', 'environments', 'find', 'kill', '\\n\\n\\nim', 'ukstyle', 'house', 'estate', 'lot', 'twostory', 'brick', 'build', 'hedge', 'fence', 'roads', 'arent', 'grid', 'culdesacs', 'tight', 'bend', 'much', 'line', 'sight', 'abrams', 'thermal', 'night', 'vision', 'kit', 'thats', 'mostly', 'directional', 'generally', 'train', 'forward', 'arc', 'weapons', 'wont', 'able', 'see', 'say', 'thick', 'hedgerows', 'wall', 'build', 'fence', 'obviously', 'actually', 'see', 'im', 'dead', 'since', 'know', 'estate', 'quite', 'well', 'probably', 'evade', 'reasonably', 'easily', 'small', 'time', 'least', '\\nso', 'plan', 'collect', 'firelighters', 'kindle', 'materials', 'load', 'ceramic', 'plant', 'pot', 'im', 'also', 'buy', 'duct', 'tape', 'rope', 'food', 'water', 'foil', 'emergency', 'blanket', 'im', 'go', 'make', 'bunch', 'small', 'fire', 'place', 'use', 'heat', 'ceramic', 'pot', 'serve', 'false', 'heat', 'signatures', 'place', '\\nmeanwhile', 'ill', 'alley', 'across', 'street', 'behind', 'fence', 'cover', 'foil', 'blanket', 'conceal', 'body', 'heat', 'tank', 'probably', 'use', 'road', 'get', 'address', 'culdesac', 'relatively', 'inaccessible', 'without', 'drive', 'house', 'put', 'right', 'behind', 'point', 'run', 'climb', 'right', 'top', 'turret', 'thatll', 'take', 'like', 'five', 'second', 'hopefully', 'im', 'spot', 'top', 'lie', 'spread', 'eagle', 'securely', 'ropetape', 'whatever', 'stanchions', 'protuberances', 'top', 'order', 'avoid', 'exhaust', 'fall', 'wait', 'twenty-four', 'hours', 'win'])\n",
      "original document: \n",
      "[\"That's\", 'awesome.', 'I', 'love', 'his', 'videos!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'awesom', 'lov', 'video'], ['thats', 'awesome', 'love', 'videos'])\n",
      "original document: \n",
      "['That', 'IS', 'his', 'thesis', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thes'], ['thesis'])\n",
      "original document: \n",
      "['Odd', 'to', 'think', 'a', 'still-updated', 'game', 'is', 'older', 'than', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['od', 'think', 'stillupd', 'gam', 'old'], ['odd', 'think', 'stillupdated', 'game', 'older'])\n",
      "original document: \n",
      "['One', 'of', 'those', 'comments', 'I', 'wish', 'I', 'could', 'upvote', 'more', 'than', 'once.', 'A', 'lot', 'of', 'people', 'now', 'are', 'giving', 'serious', 'Kinnock', 'in', \"'92\", 'vibes,', 'luckily', 'Corbyn', 'and', 'co', \"aren't\", 'quite', 'there', 'yet.', 'But', 'some', 'of', 'the', 'social', 'media', 'comments', 'are', 'definitely', 'making', 'me', 'cringe', 'as', 'hard', 'as', 'watching', 'the', 'Sheffield', 'Rally', 'does.\\n\\nhttps://www.youtube.com/watch?v=ROKXlvYMKQc\\n\\nWe', 'are', 'not', 'alright', 'yet', 'guys.', 'There', \"isn't\", 'even', 'an', 'election', 'on', 'the', 'cards', 'right', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'com', 'wish', 'could', 'upvot', 'lot', 'peopl', 'giv', 'sery', 'kinnock', 'ninety-two', 'vib', 'lucky', 'corbyn', 'co', 'ar', 'quit', 'yet', 'soc', 'med', 'com', 'definit', 'mak', 'cring', 'hard', 'watch', 'sheffield', 'ral', 'does\\n\\nhttpswwwyoutubecomwatchvrokxlvymkqc\\n\\nwe', 'alright', 'yet', 'guy', 'isnt', 'ev', 'elect', 'card', 'right'], ['one', 'comment', 'wish', 'could', 'upvote', 'lot', 'people', 'give', 'serious', 'kinnock', 'ninety-two', 'vibes', 'luckily', 'corbyn', 'co', 'arent', 'quite', 'yet', 'social', 'media', 'comment', 'definitely', 'make', 'cringe', 'hard', 'watch', 'sheffield', 'rally', 'does\\n\\nhttpswwwyoutubecomwatchvrokxlvymkqc\\n\\nwe', 'alright', 'yet', 'guy', 'isnt', 'even', 'election', 'card', 'right'])\n",
      "original document: \n",
      "[\"That's\", 'actually', 'wet']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'act', 'wet'], ['thats', 'actually', 'wet'])\n",
      "original document: \n",
      "['Lord']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lord'], ['lord'])\n",
      "original document: \n",
      "['TFC', 'should', 'sub', 'out', 'Bradley', 'and', 'Altidore', 'so', \"they're\", 'well', 'rested', 'for', 'the', 'panama', 'game,', 'just', 'saying', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tfc', 'sub', 'bradley', 'altid', 'theyr', 'wel', 'rest', 'panam', 'gam', 'say'], ['tfc', 'sub', 'bradley', 'altidore', 'theyre', 'well', 'rest', 'panama', 'game', 'say'])\n",
      "original document: \n",
      "['Unpaid', 'mods', 'are', 'superior', 'in', 'both', 'quality', 'and', 'ofc,', 'price.', 'Do', 'you', 'think', 'Bethesda', 'will', 'like', 'competition', 'for', 'their', 'new', 'way', 'of', 'forcing', 'people', 'to', 'pay', 'more', 'money?', 'Do', 'you', 'think', 'they', 'like', 'sharing', 'the', 'game', 'with', 'unpaid', 'mods', 'when', 'that', 'money', 'could', 'be', 'theirs?', 'They', 'released', 'this', 'now', 'so', 'people', 'get', 'used', 'to', 'it', 'so', 'when', 'they', 'announce', 'a', 'new', 'game', 'with', 'only', 'creation', 'club', 'the', 'backlash', 'will', 'be', 'less', 'and', 'people', 'will', 'already', 'be', 'used', 'to', 'paid', 'mods.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unpaid', 'mod', 'supery', 'qual', 'ofc', 'pric', 'think', 'bethesd', 'lik', 'competit', 'new', 'way', 'forc', 'peopl', 'pay', 'money', 'think', 'lik', 'shar', 'gam', 'unpaid', 'mod', 'money', 'could', 'releas', 'peopl', 'get', 'us', 'annount', 'new', 'gam', 'cre', 'club', 'backlash', 'less', 'peopl', 'already', 'us', 'paid', 'mod'], ['unpaid', 'mods', 'superior', 'quality', 'ofc', 'price', 'think', 'bethesda', 'like', 'competition', 'new', 'way', 'force', 'people', 'pay', 'money', 'think', 'like', 'share', 'game', 'unpaid', 'mods', 'money', 'could', 'release', 'people', 'get', 'use', 'announce', 'new', 'game', 'creation', 'club', 'backlash', 'less', 'people', 'already', 'use', 'pay', 'mods'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['What', 'tier(s)', 'in', 'arena', 'are', 'you', 'in?', 'You', \"don't\", 'need', 'full', 'horse', 'emblem', 'or', 'armour', 'emblem', 'to', 'stay', 'in', 'T20.', 'Granted', 'I', 'have', 'min-maxed', 'a', 'little', 'so', 'I', \"don't\", 'have', 'to', 'reroll', 'score', 'as', 'often,', \"it's\", 'definitely', 'doable', 'and', 'enjoyable', 'so', 'long', 'as', 'you', 'can', 'counter', 'the', 'common', 'heroes', 'you', 'run', 'into']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tier', 'aren', 'dont', 'nee', 'ful', 'hors', 'emblem', 'armo', 'emblem', 'stay', 't20', 'grant', 'minmax', 'littl', 'dont', 'rerol', 'scor', 'oft', 'definit', 'doabl', 'enjoy', 'long', 'count', 'common', 'hero', 'run'], ['tiers', 'arena', 'dont', 'need', 'full', 'horse', 'emblem', 'armour', 'emblem', 'stay', 't20', 'grant', 'minmaxed', 'little', 'dont', 'reroll', 'score', 'often', 'definitely', 'doable', 'enjoyable', 'long', 'counter', 'common', 'heroes', 'run'])\n",
      "original document: \n",
      "['I', 'too', 'saw', 'this', 'on', 'this', \"week's\", 'New', 'Yorker.', 'Laughed', 'then', 'and', 'had', 'another', 'laugh', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'week', 'new', 'york', 'laugh', 'anoth', 'laugh'], ['saw', 'weeks', 'new', 'yorker', 'laugh', 'another', 'laugh'])\n",
      "original document: \n",
      "['Thanks', 'a', 'ton!', ':D', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'ton'], ['thank', 'ton'])\n",
      "original document: \n",
      "[\"That's\", 'what', \"I'm\", 'thinking', 'too.', 'I', 'think', \"I'll\", 'do', 'that.', 'Thanks', '!', ':D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'im', 'think', 'think', 'il', 'thank'], ['thats', 'im', 'think', 'think', 'ill', 'thank'])\n",
      "original document: \n",
      "['Hey,', 'I', 'have', 'no', 'friends', 'too..', 'But', 'I', \"don't\", 'have', \"anyone's\", 'contact', 'info', 'either.', \"You're\", 'not', 'alone']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'friend', 'dont', 'anyon', 'contact', 'info', 'eith', 'yo', 'alon'], ['hey', 'friends', 'dont', 'anyones', 'contact', 'info', 'either', 'youre', 'alone'])\n",
      "original document: \n",
      "['10', 'team', 'ppr:\\n\\nGiving:', 'Michael', 'Thomas', '&amp;', 'Chris', 'Thompson\\n\\nGetting:', 'Keenan', 'Allen', '&amp;', 'Lesean', 'McCoy\\n\\nMy', 'other', 'WRs:', 'OBJ,', 'Alshon', 'Jeffery,', 'TY', 'Hilton\\n\\nMy', 'other', 'RBs:', 'Ingram,', 'Crowell,', 'Gillislee,', 'Duke', 'Johnson,', 'Riddick,', 'Smallwood', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'team', 'ppr\\n\\ngiving', 'michael', 'thoma', 'amp', 'chris', 'thompson\\n\\ngetting', 'keen', 'al', 'amp', 'les', 'mccoy\\n\\nmy', 'wrs', 'obs', 'alshon', 'jeffery', 'ty', 'hilton\\n\\nmy', 'rbs', 'ingram', 'crowel', 'gillisl', 'duk', 'johnson', 'riddick', 'smallwood'], ['ten', 'team', 'ppr\\n\\ngiving', 'michael', 'thomas', 'amp', 'chris', 'thompson\\n\\ngetting', 'keenan', 'allen', 'amp', 'lesean', 'mccoy\\n\\nmy', 'wrs', 'obj', 'alshon', 'jeffery', 'ty', 'hilton\\n\\nmy', 'rbs', 'ingram', 'crowell', 'gillislee', 'duke', 'johnson', 'riddick', 'smallwood'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[+Zaorish9](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnovlhe/):\\n\\n[Edit:', 'I', 'removed', 'this', 'comment', 'after', 'the', 'original', \"poster's\", 'dishonest', 'marketing', 'ploy', 'was', 'revealed.]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['zaorish9httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnovlhe\\n\\nedit', 'remov', 'com', 'origin', 'post', 'dishonest', 'market', 'ploy', 'rev'], ['zaorish9httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnovlhe\\n\\nedit', 'remove', 'comment', 'original', 'posters', 'dishonest', 'market', 'ploy', 'reveal'])\n",
      "original document: \n",
      "['I', 'want', 'Ice', 'as', 'a', 'friend', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'ic', 'friend'], ['want', 'ice', 'friend'])\n",
      "original document: \n",
      "['If', 'only', 'the', '30%', 'of', 'people', 'giving', 'Trump', 'his', 'abysmal', 'approval', 'rating', 'felt', 'the', 'same,', 'maybe', 'we', 'could', 'end', 'this', 'farce', 'and', 'move', 'forward.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thirty', 'peopl', 'giv', 'trump', 'abysm', 'approv', 'rat', 'felt', 'mayb', 'could', 'end', 'farc', 'mov', 'forward'], ['thirty', 'people', 'give', 'trump', 'abysmal', 'approval', 'rat', 'felt', 'maybe', 'could', 'end', 'farce', 'move', 'forward'])\n",
      "original document: \n",
      "['I', 'beg', 'to', 'differ', ':\\\\^)\\n\\nhttps://i.imgur.com/qNIlEUY.png']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['beg', 'diff', '\\n\\nhttpsiimgurcomqnileuypng'], ['beg', 'differ', '\\n\\nhttpsiimgurcomqnileuypng'])\n",
      "original document: \n",
      "['Since', 'I', 'met', 'my', 'girlfriend,', 'I', 'fantasize', 'about', 'her', 'when', 'I', 'jerk', 'it.', 'Super', 'weird', 'to', 'me.,', 'but', 'yeah.', 'Anytime', 'I', 'go', 'at', 'it', 'with', 'something', 'else', 'on', 'my', 'mind,', 'she', 'pops', 'into', 'my', 'head', 'and', 'stays', 'until', 'the', 'deed', 'is', 'done.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sint', 'met', 'girlfriend', 'fantas', 'jerk', 'sup', 'weird', 'yeah', 'anytim', 'go', 'someth', 'els', 'mind', 'pop', 'head', 'stay', 'dee', 'don'], ['since', 'meet', 'girlfriend', 'fantasize', 'jerk', 'super', 'weird', 'yeah', 'anytime', 'go', 'something', 'else', 'mind', 'pop', 'head', 'stay', 'deed', 'do'])\n",
      "original document: \n",
      "['How', 'does', 'a', 'person', 'get', 'away', 'with', 'using', 'chewing', 'tobacco', 'in', 'school', 'and', 'not', 'ending', 'up', 'in', 'deep', 'shit?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['person', 'get', 'away', 'us', 'chew', 'tobacco', 'school', 'end', 'deep', 'shit'], ['person', 'get', 'away', 'use', 'chew', 'tobacco', 'school', 'end', 'deep', 'shit'])\n",
      "original document: \n",
      "['I', 'often', 'wonder', 'why', 'owners', 'feel', 'the', 'need', 'to', 'bathe', 'their', 'cats....', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oft', 'wond', 'own', 'feel', 'nee', 'bath', 'cat'], ['often', 'wonder', 'owners', 'feel', 'need', 'bathe', 'cat'])\n",
      "original document: \n",
      "['Cars', 'are', 'also', 'stupid...', 'just', 'sprint', 'faster', 'and', 'you', 'go', 'the', 'same', 'speed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['car', 'also', 'stupid', 'sprint', 'fast', 'go', 'spee'], ['cars', 'also', 'stupid', 'sprint', 'faster', 'go', 'speed'])\n",
      "original document: \n",
      "['Obama', 'was', 'one', 'corrupt', 'SOB']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['obam', 'on', 'corrupt', 'sob'], ['obama', 'one', 'corrupt', 'sob'])\n",
      "original document: \n",
      "['agreed']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree'], ['agree'])\n",
      "original document: \n",
      "['Your', 'comment', 'has', 'been', 'removed', 'because:\\n\\nGendered', 'slurs', 'are', 'strictly', 'scrutinized;', 'please', 'see', 'our', '[gendered', 'slurs', 'policy', 'guide.](/r/askwomen/w/genderedslurs)', '', '\\nIf', 'you', 'edit', 'your', 'comment,', 'let', 'us', 'know', 'and', 'it', 'may', 'be', 'reinstated.', '\\n\\n\\n\\n**[Have', 'questions', 'about', 'this', 'moderator', 'action?', 'CLICK', 'HERE!](http://www.reddit.com/message/compose/?to=/r/AskWomen&amp;subject=Why+was+this+removed?&amp;message=\\\\[My+comment\\\\]\\\\(https://www.reddit.com/r/AskWomen/comments/73i316/what_was_the_most_ridiculous_case_of_someone/dnqgo50/\\\\)+was+removed+and+I+do+not+understand+the+reason+given+by+the+mod+who+acted', 'upon+it.)**', '', '', '', '\\n\\n\\n[AskWomen', 'rules](/r/askwomen/w/rules)', '|', '[AskWomen', 'FAQ](/r/askwomen/w/index)', '', '\\n[reddit', 'rules](http://www.reddit.com/rules/)', '|', '[reddiquette](http://www.reddit.com/wiki/reddiquette)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['com', 'remov', 'because\\n\\ngendered', 'slur', 'strictly', 'scrutinized', 'pleas', 'see', 'gend', 'slur', 'policy', 'guideraskwomenwgenderedsl', '\\nif', 'edit', 'com', 'let', 'us', 'know', 'may', 'reinst', '\\n\\n\\n\\nhave', 'quest', 'mod', 'act', 'click', 'herehttpwwwredditcommessagecomposetoraskwomenampsubjectwhywasthisremovedampmessagemycommenthttpswwwredditcomraskwomencomments73i316what_was_the_most_ridiculous_case_of_someonednqgo50wasremovedandidonotunderstandthereasongivenbythemodwhoacted', 'uponit', '\\n\\n\\naskwomen', 'rulesraskwomenwr', 'askwom', 'faqraskwomenwindex', '\\nreddit', 'ruleshttpwwwredditcomr', 'reddiquettehttpwwwredditcomwikireddiquet'], ['comment', 'remove', 'because\\n\\ngendered', 'slur', 'strictly', 'scrutinize', 'please', 'see', 'gendered', 'slur', 'policy', 'guideraskwomenwgenderedslurs', '\\nif', 'edit', 'comment', 'let', 'us', 'know', 'may', 'reinstate', '\\n\\n\\n\\nhave', 'question', 'moderator', 'action', 'click', 'herehttpwwwredditcommessagecomposetoraskwomenampsubjectwhywasthisremovedampmessagemycommenthttpswwwredditcomraskwomencomments73i316what_was_the_most_ridiculous_case_of_someonednqgo50wasremovedandidonotunderstandthereasongivenbythemodwhoacted', 'uponit', '\\n\\n\\naskwomen', 'rulesraskwomenwrules', 'askwomen', 'faqraskwomenwindex', '\\nreddit', 'ruleshttpwwwredditcomrules', 'reddiquettehttpwwwredditcomwikireddiquette'])\n",
      "original document: \n",
      "['Goku', 'always', 'wants', 'to', 'test', 'himself,', 'and', 'also', 'the', 'writers', \"don't\", 'have', 'a', 'whole', 'saga', 'to', 'work', 'with.', '', 'They', 'had', 'a', 'little', 'more', 'than', 'an', 'hour', 'so', 'they', 'had', 'to', 'build', 'some', 'tension,', 'so', 'they', 'saved', 'super', 'saiyan', 'till', 'the', 'end.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goku', 'alway', 'want', 'test', 'also', 'writ', 'dont', 'whol', 'sag', 'work', 'littl', 'hour', 'build', 'tend', 'sav', 'sup', 'saiy', 'til', 'end'], ['goku', 'always', 'want', 'test', 'also', 'writers', 'dont', 'whole', 'saga', 'work', 'little', 'hour', 'build', 'tension', 'save', 'super', 'saiyan', 'till', 'end'])\n",
      "original document: \n",
      "['Meh.', 'First', 'time', \"I've\", 'seen', 'it', 'posted.', \"I'm\", 'sure', 'for', 'many', 'others', 'too.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meh', 'first', 'tim', 'iv', 'seen', 'post', 'im', 'sur', 'many', 'oth'], ['meh', 'first', 'time', 'ive', 'see', 'post', 'im', 'sure', 'many', 'others'])\n",
      "original document: \n",
      "['just', 'get', 'the', 'first', 'down,', 'it', 'was', '2', 'yards']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'first', 'two', 'yard'], ['get', 'first', 'two', 'yards'])\n",
      "original document: \n",
      "['Fucking', 'gross.', 'Have', 'my', 'upvote']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'gross', 'upvot'], ['fuck', 'gross', 'upvote'])\n",
      "original document: \n",
      "[\"Can't\", 'say', \"I'm\", 'not', 'nervous,', 'but', \"I'm\", 'definitely', 'less', 'nervous', 'than', 'in', '2015.', 'I', 'still', \"can't\", 'believe', 'we', 'made', 'it', 'to', 'the', 'GF', 'haha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'say', 'im', 'nerv', 'im', 'definit', 'less', 'nerv', 'two thousand and fifteen', 'stil', 'cant', 'believ', 'mad', 'gf', 'hah'], ['cant', 'say', 'im', 'nervous', 'im', 'definitely', 'less', 'nervous', 'two thousand and fifteen', 'still', 'cant', 'believe', 'make', 'gf', 'haha'])\n",
      "original document: \n",
      "['E']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['e'], ['e'])\n",
      "original document: \n",
      "['No', 'way.', 'He', 'gave', 'up', '2', 'innocent', 'lives', 'to', 'save', 'his', 'own.', 'Stank', 'gum', \"didn't\", 'know', 'about', 'them', 'until', 'he', 'told', 'them.', 'Stank', 'gum', 'tortured', 'him', 'to', 'find', 'max,', 'he', \"didn't\", 'even', 'know', 'about', 'hope', 'and', 'glory', 'until', 'chum', 'brought', 'it', 'up.', 'He', 'should', 'have', 'told', 'him', 'where', 'max', 'was', 'and', 'never', 'even', 'mentioned', 'them.', 'And', 'he', 'stole', 'my', 'Damn', 'car!', ':-(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'gav', 'two', 'innoc', 'liv', 'sav', 'stank', 'gum', 'didnt', 'know', 'told', 'stank', 'gum', 'tort', 'find', 'max', 'didnt', 'ev', 'know', 'hop', 'glory', 'chum', 'brought', 'told', 'max', 'nev', 'ev', 'ment', 'stol', 'damn', 'car'], ['way', 'give', 'two', 'innocent', 'live', 'save', 'stink', 'gum', 'didnt', 'know', 'tell', 'stink', 'gum', 'torture', 'find', 'max', 'didnt', 'even', 'know', 'hope', 'glory', 'chum', 'bring', 'tell', 'max', 'never', 'even', 'mention', 'steal', 'damn', 'car'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['will', 'do', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['This', 'deal', 'is', 'getting', 'worse', 'all', 'the', 'time!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['deal', 'get', 'wors', 'tim'], ['deal', 'get', 'worse', 'time'])\n",
      "original document: \n",
      "[\"It's\", 'not', 'the', 'post-processing.', 'I', 'had', 'that', 'enabled', 'pre-patch,', 'and', 'still', 'experienced', 'similar', 'drops', 'as', 'the', 'above', 'poster.', 'Disabling', 'the', 'post-processing', 'effects', 'via', 'engine.ini', 'also', 'does', 'not', 'help.\\n\\nThe', 'shadows', 'should', 'have', 'been', 'a', 'performance', 'boost,', 'since', 'they', 'were', 'changed', 'to', 'baked-in', 'static', 'ones.\\n\\nMy', 'guess', 'is', 'that', 'they', 'are', 'overusing', 'a', 'sub-component', 'somewhere', 'causing', 'the', 'system', 'to', 'cap', 'out', 'early,', 'though', 'I', 'have', 'little', 'knowledge/experience', 'there.', 'I', 'just', 'rarely', 'see', 'the', 'GPU', 'usage', 'above', '40%,', 'or', 'any', 'single', 'thread', 'on', 'any', 'core', 'of', 'the', 'CPU', 'go', 'above', '60%', 'usage.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['postprocess', 'en', 'prepatch', 'stil', 'expery', 'simil', 'drop', 'post', 'dis', 'postprocess', 'effect', 'via', 'enginein', 'also', 'help\\n\\nthe', 'shadow', 'perform', 'boost', 'sint', 'chang', 'bakedin', 'stat', 'ones\\n\\nmy', 'guess', 'overus', 'subcompon', 'somewh', 'caus', 'system', 'cap', 'ear', 'though', 'littl', 'knowledgeexpery', 'rar', 'see', 'gpu', 'us', 'forty', 'singl', 'thread', 'cor', 'cpu', 'go', 'sixty', 'us'], ['postprocessing', 'enable', 'prepatch', 'still', 'experience', 'similar', 'drop', 'poster', 'disable', 'postprocessing', 'effect', 'via', 'engineini', 'also', 'help\\n\\nthe', 'shadow', 'performance', 'boost', 'since', 'change', 'bakedin', 'static', 'ones\\n\\nmy', 'guess', 'overuse', 'subcomponent', 'somewhere', 'cause', 'system', 'cap', 'early', 'though', 'little', 'knowledgeexperience', 'rarely', 'see', 'gpu', 'usage', 'forty', 'single', 'thread', 'core', 'cpu', 'go', 'sixty', 'usage'])\n",
      "original document: \n",
      "['1463', 'chainz']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one thousand, four hundred and sixty-thr', 'chainz'], ['one thousand, four hundred and sixty-three', 'chainz'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['The', 'flip', 'side', 'is', 'that', 'unlike', 'Chase,', 'which', 'strictly', 'segregates', 'your', 'consumer', 'and', 'biz', 'accounts,', 'Amex', 'seems', 'pretty', 'liberal', 'about', 'intermingling', 'everything.', 'And', 'Amex', 'has', 'a', 'habit', 'of', 'not', 'doing', 'hard', 'pulls', 'for', 'longstanding', 'customers.', 'A', 'foreign', 'Amex', \"won't\", 'count', 'against', '5/24', 'since', \"it's\", 'a', 'separate', 'credit', 'profile', 'but', 'I', \"wouldn't\", 'necessarily', 'want', 'to', 'count', 'on', 'Amex', 'ignoring', 'that', 'they', 'already', 'have', 'their', 'own', 'profile', 'on', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flip', 'sid', 'unlik', 'chas', 'strictly', 'segreg', 'consum', 'biz', 'account', 'amex', 'seem', 'pretty', 'lib', 'intermingl', 'everyth', 'amex', 'habit', 'hard', 'pul', 'longstand', 'custom', 'foreign', 'amex', 'wont', 'count', 'five hundred and twenty-four', 'sint', 'sep', 'credit', 'profil', 'wouldnt', 'necess', 'want', 'count', 'amex', 'ign', 'already', 'profil'], ['flip', 'side', 'unlike', 'chase', 'strictly', 'segregate', 'consumer', 'biz', 'account', 'amex', 'seem', 'pretty', 'liberal', 'intermingle', 'everything', 'amex', 'habit', 'hard', 'pull', 'longstanding', 'customers', 'foreign', 'amex', 'wont', 'count', 'five hundred and twenty-four', 'since', 'separate', 'credit', 'profile', 'wouldnt', 'necessarily', 'want', 'count', 'amex', 'ignore', 'already', 'profile'])\n",
      "original document: \n",
      "['Last', 'time', 'I', 'checked', 'Crawford', \"isn't\", 'licensened', 'by', 'the', 'British', 'Board', 'of', 'Boxing', 'so', 'he', 'would', 'be', 'ineligible.', 'Errol', 'Spence', 'Jr.', 'won', 'foreign', 'fighter', 'of', 'the', 'year', 'tho', 'so', 'make', 'of', 'that', 'what', 'you', 'will.', \"\\n\\nIt's\", 'an', 'award', 'to', 'celebrate', 'British', 'Boxing.', 'They', 'have', 'a', 'separate', 'award', 'for', 'non', 'British', 'fighters,', 'presumably', 'who', 'have', 'fought', 'in', 'the', 'U.K.', 'Or', 'against', 'British', 'opposition', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['last', 'tim', 'check', 'crawford', 'isnt', 'licens', 'brit', 'board', 'box', 'would', 'inelig', 'errol', 'spent', 'jr', 'foreign', 'fight', 'year', 'tho', 'mak', '\\n\\nits', 'award', 'celebr', 'brit', 'box', 'sep', 'award', 'non', 'brit', 'fight', 'presum', 'fought', 'uk', 'brit', 'opposit'], ['last', 'time', 'check', 'crawford', 'isnt', 'licensened', 'british', 'board', 'box', 'would', 'ineligible', 'errol', 'spence', 'jr', 'foreign', 'fighter', 'year', 'tho', 'make', '\\n\\nits', 'award', 'celebrate', 'british', 'box', 'separate', 'award', 'non', 'british', 'fighters', 'presumably', 'fight', 'uk', 'british', 'opposition'])\n",
      "original document: \n",
      "['There', 'is', 'no', 'shortage', 'of', 'Magpul', 'and', 'Hexmag', '10', 'rounders', 'in', '30', 'round', 'bodies', 'in', 'stores.', 'I', \"wouldn't\", 'worry', 'about', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['short', 'magp', 'hexm', 'ten', 'round', 'thirty', 'round', 'body', 'stor', 'wouldnt', 'worry'], ['shortage', 'magpul', 'hexmag', 'ten', 'rounders', 'thirty', 'round', 'body', 'store', 'wouldnt', 'worry'])\n",
      "original document: \n",
      "['Oh', 'yeah.', 'Forgot', 'about', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'yeah', 'forgot'], ['oh', 'yeah', 'forget'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnox5hl/):\\n\\nThank', 'you', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnox5hl\\n\\nthank'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnox5hl\\n\\nthank'])\n",
      "original document: \n",
      "['Amsa', 'vs', 'army', '2-1', 'for', 'amsa', '2', 'stock', 'comeback', 'game', '3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ams', 'vs', 'army', 'twenty-one', 'ams', 'two', 'stock', 'comeback', 'gam', 'three'], ['amsa', 'vs', 'army', 'twenty-one', 'amsa', 'two', 'stock', 'comeback', 'game', 'three'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['https://q39kc.com/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsq39kccom'], ['httpsq39kccom'])\n",
      "original document: \n",
      "[\"I'm\", 'from', 'NJ.', 'And', 'Obama', 'did', 'step', 'up', 'and', 'worked', 'quite', 'well', 'win', 'Christie', 'and', 'gave', 'our', 'state', 'plenty', 'of', 'aid', 'and', 'support', 'after', 'Sandy', 'wrecked', 'us.', '\\n\\nQuick', 'edit:', 'I', 'am', 'not', 'a', 'fan', 'of', 'Christie', 'but', 'he', 'did', 'do', 'a', 'great', 'job', 'standing', 'up', 'for', 'the', 'us', 'and', 'getting', 'us', 'what', 'we', 'needed', 'to', 'recover', 'and', 'help', 'those', 'hit', 'the', 'hardest.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'nj', 'obam', 'step', 'work', 'quit', 'wel', 'win', 'christie', 'gav', 'stat', 'plenty', 'aid', 'support', 'sandy', 'wreck', 'us', '\\n\\nquick', 'edit', 'fan', 'christie', 'gre', 'job', 'stand', 'us', 'get', 'us', 'nee', 'recov', 'help', 'hit', 'hardest'], ['im', 'nj', 'obama', 'step', 'work', 'quite', 'well', 'win', 'christie', 'give', 'state', 'plenty', 'aid', 'support', 'sandy', 'wreck', 'us', '\\n\\nquick', 'edit', 'fan', 'christie', 'great', 'job', 'stand', 'us', 'get', 'us', 'need', 'recover', 'help', 'hit', 'hardest'])\n",
      "original document: \n",
      "['yes', 'you', 'can,', 'but', 'skimming', 'throught', 'a', 'legal', 'thread', 'read', 'that', 'sometimes', 'prosecutors', 'think', 'it', 'is', 'better', 'to', 'play', 'it', 'safe', 'and', 'just', 'convict', 'for', 'one', 'crime', 'instead', 'of', '2', 'or', '3,', 'becuz', 'then', 'you', 'need', 'more', 'evidence,', 'trials,', 'etc.', 'So', 'just', 'to', 'get', 'them', 'to', 'jail', 'they', 'focus', 'on', 'one', 'type', 'of', 'crime.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'skim', 'throught', 'leg', 'thread', 'read', 'sometim', 'prosecut', 'think', 'bet', 'play', 'saf', 'convict', 'on', 'crim', 'instead', 'two', 'three', 'becuz', 'nee', 'evid', 'tri', 'etc', 'get', 'jail', 'foc', 'on', 'typ', 'crim'], ['yes', 'skim', 'throught', 'legal', 'thread', 'read', 'sometimes', 'prosecutors', 'think', 'better', 'play', 'safe', 'convict', 'one', 'crime', 'instead', 'two', 'three', 'becuz', 'need', 'evidence', 'trials', 'etc', 'get', 'jail', 'focus', 'one', 'type', 'crime'])\n",
      "original document: \n",
      "['My', 'delivery', 'person', 'never', 'comes', 'to', 'my', 'door', 'and', 'I', 'live', 'on', 'the', 'first', 'floor.', 'It', 'bothers', 'my', 'soul.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delivery', 'person', 'nev', 'com', 'door', 'liv', 'first', 'flo', 'both', 'soul'], ['delivery', 'person', 'never', 'come', 'door', 'live', 'first', 'floor', 'bother', 'soul'])\n",
      "original document: \n",
      "['I', 'can', 'trade', 'at', 'anytime', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['trad', 'anytim'], ['trade', 'anytime'])\n",
      "original document: \n",
      "['Your', 'submission', 'was', 'automatically', 'flagged.', '[Message', 'the', 'moderators', 'immediately', 'after', 'submitting](https://www.reddit.com/login?dest=https%3A%2F%2Fwww.reddit.com%2Fmessage%2Fcompose%3Fto%3D%252Fr%252Fweb_design)', 'if', 'you', 'feel', 'this', 'should', 'be', 'an', 'exception.', 'Be', 'certain', 'that', 'your', 'submission', 'follows', 'our', 'posting', 'guidelines', 'and', \"reddit's\", 'rules.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/web_design)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'autom', 'flag', 'mess', 'mod', 'immedy', 'submittinghttpswwwredditcomlogindesthttps3a2f2fwwwredditcom2fmessage2fcompose3fto3d252fr252fweb_design', 'feel', 'exceiv', 'certain', 'submit', 'follow', 'post', 'guidelin', 'reddit', 'rules\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorweb_design', 'quest', 'concern'], ['submission', 'automatically', 'flag', 'message', 'moderators', 'immediately', 'submittinghttpswwwredditcomlogindesthttps3a2f2fwwwredditcom2fmessage2fcompose3fto3d252fr252fweb_design', 'feel', 'exception', 'certain', 'submission', 'follow', 'post', 'guidelines', 'reddits', 'rules\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorweb_design', 'question', 'concern'])\n",
      "original document: \n",
      "[\"It's\", 'there.', 'You', \"weren't\", 'paying', 'close', 'enough', 'attention.', 'Detention', 'for', 'you']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wer', 'pay', 'clos', 'enough', 'at', 'det'], ['werent', 'pay', 'close', 'enough', 'attention', 'detention'])\n",
      "original document: \n",
      "['Hey', 'guys,', 'just', 'got', 'a', 'tesarion.', 'What', 'runes', 'should', 'i', 'use', 'on', 'him?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'guy', 'got', 'tes', 'run', 'us'], ['hey', 'guy', 'get', 'tesarion', 'run', 'use'])\n",
      "original document: \n",
      "['I', 'remember', 'reading', 'the', 'list', '(it', 'had', 'been', 'posted', 'on', 'the', 'InRev', 'TS', 'a', 'week', 'or', 'so', 'before', 'that', 'post', 'was', 'made),', 'and', 'it', 'looked', 'like', 'it', 'was', 'internal.', '', \"I'm\", 'not', 'going', 'to', 'lie,', 'at', 'the', 'time', 'I', 'was', 'upset,', 'but', \"it's\", 'still', 'scummy', 'to', 'bring', 'it', 'up', 'now']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'read', 'list', 'post', 'inrev', 'ts', 'week', 'post', 'mad', 'look', 'lik', 'intern', 'im', 'going', 'lie', 'tim', 'upset', 'stil', 'scummy', 'bring'], ['remember', 'read', 'list', 'post', 'inrev', 'ts', 'week', 'post', 'make', 'look', 'like', 'internal', 'im', 'go', 'lie', 'time', 'upset', 'still', 'scummy', 'bring'])\n",
      "original document: \n",
      "['Let', 'me', 'guess,', 'NYC', 'or', 'SF?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'guess', 'nyc', 'sf'], ['let', 'guess', 'nyc', 'sf'])\n",
      "original document: \n",
      "['I', 'think', 'you', 'are', 'missing', 'the', 'Contents', 'model?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'miss', 'cont', 'model'], ['think', 'miss', 'content', 'model'])\n",
      "original document: \n",
      "['Até', 'lá', 'eles', 'vão', 'arrumar', 'mais', 'recibos', 'com', 'data', 'errada', 'e', 'outros', 'sujeitos', 'dizendo', 'que', 'ouviram', 'alguém', 'falar', 'que', 'o', 'Lula', 'é', 'culpado', 'de', 'alguma', 'coisa.\\n\\n\\n*Aliás,', 'dá', 'uma', 'olhada', 'nos', 'tópicos', 'disso', 'na', 'outra', 'comunidade.', 'Dá', 'até', 'pena', 'de', 'tão', 'engraçado...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['at', 'la', 'el', 'vao', 'arrum', 'mai', 'recibo', 'com', 'dat', 'errad', 'e', 'outro', 'sujeito', 'dizendo', 'que', 'ouviram', 'alguem', 'fal', 'que', 'lul', 'e', 'culpado', 'de', 'algum', 'coisa\\n\\n\\nalias', 'da', 'um', 'olhad', 'nos', 'topico', 'disso', 'na', 'outr', 'comunidad', 'da', 'at', 'pen', 'de', 'tao', 'engracado'], ['eat', 'la', 'eles', 'vao', 'arrumar', 'mais', 'recibos', 'com', 'data', 'errada', 'e', 'outros', 'sujeitos', 'dizendo', 'que', 'ouviram', 'alguem', 'falar', 'que', 'lula', 'e', 'culpado', 'de', 'alguma', 'coisa\\n\\n\\nalias', 'da', 'uma', 'olhada', 'nos', 'topicos', 'disso', 'na', 'outra', 'comunidade', 'da', 'eat', 'pena', 'de', 'tao', 'engracado'])\n",
      "original document: \n",
      "[\"*You're*\", 'an', 'iPhone', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'iphon'], ['youre', 'iphone'])\n",
      "original document: \n",
      "['143413876|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'rZGCaIJJ)\\n\\n&gt;&gt;143412250', '(OP)\\nnigger\\n\\t\\t\\t']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, eight hundred and seventy-six', 'gt', 'unit', 'stat', 'anonym', 'id', 'rzgcaijj\\n\\ngtgt143412250', 'op\\nnigger\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, eight hundred and seventy-six', 'gt', 'unite', 'state', 'anonymous', 'id', 'rzgcaijj\\n\\ngtgt143412250', 'op\\nnigger\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[Mirror', 'of', 'Amazing!!', 'Irving', 'Lozano', 'scores', 'two', 'goals', 'with', 'PSV!](https://streamable.com/zo6nb)\\n\\n', '***', '\\nIf', 'the', 'original', 'post', 'is', 'already', 'a', 'Streamable', 'link,', 'and', 'I', 'posted', 'a', 'Streamable', 'version', 'of', 'it,', 'it', 'acts', 'as', 'a', 'mirror', 'in', 'case', 'the', 'original', 'is', 'taken', 'down.', 'If', 'I', 'still', 'offended', 'you,', 'well...', \"can't\", 'please', 'every', 'human.', '\\n\\n', \"I'm\", 'a', 'bot.', 'Leave', 'me', 'some', 'feedback', 'if', 'possible!', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mir', 'amaz', 'irv', 'lozano', 'scor', 'two', 'goal', 'psvhttpsstreamablecomzo6nb\\n\\n', '\\nif', 'origin', 'post', 'already', 'streamable', 'link', 'post', 'streamable', 'vert', 'act', 'mir', 'cas', 'origin', 'tak', 'stil', 'offend', 'wel', 'cant', 'pleas', 'every', 'hum', '\\n\\n', 'im', 'bot', 'leav', 'feedback', 'poss'], ['mirror', 'amaze', 'irving', 'lozano', 'score', 'two', 'goals', 'psvhttpsstreamablecomzo6nb\\n\\n', '\\nif', 'original', 'post', 'already', 'streamable', 'link', 'post', 'streamable', 'version', 'act', 'mirror', 'case', 'original', 'take', 'still', 'offend', 'well', 'cant', 'please', 'every', 'human', '\\n\\n', 'im', 'bot', 'leave', 'feedback', 'possible'])\n",
      "original document: \n",
      "['Jimiller', 'band', 'sucks', 'if', \"you're\", 'an', 'actual', 'dead', 'head', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jimil', 'band', 'suck', 'yo', 'act', 'dead', 'head'], ['jimiller', 'band', 'suck', 'youre', 'actual', 'dead', 'head'])\n",
      "original document: \n",
      "['god', 'damn', 'it,', 'you', 'got', 'me', 'with', 'that', 'ending']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'damn', 'got', 'end'], ['god', 'damn', 'get', 'end'])\n",
      "original document: \n",
      "['Already', 'there', 'bby']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'bby'], ['already', 'bby'])\n",
      "original document: \n",
      "['Fuck', 'BourManPig']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'bourmanpig'], ['fuck', 'bourmanpig'])\n",
      "original document: \n",
      "['Yes.', 'Employment', 'at', 'will', 'means', 'you', 'can', 'leave', 'anytime', 'you', 'want', 'and', 'your', 'employer', 'can', 'ask', 'you', 'to', 'leave', 'anytime', 'they', 'want.\\n\\nPeople', 'have', 'been', 'fired', 'for', 'making', 'dumb', 'social', 'media', 'posts,', 'which', 'is', 'not', 'something', 'I', 'generally', 'agree', 'with.', 'However,', 'if', 'you', 'where', 'to', 'be', 'fired', 'because', 'you', 'identified', 'yourself', 'as', 'a', 'member', 'of', 'a', 'political', 'party', 'that', 'your', 'boss', \"doesn't\", 'like...', 'I', 'def.', 'think', \"that's\", 'bullshit', 'in', 'its', 'highest', 'form.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'employ', 'mean', 'leav', 'anytim', 'want', 'employ', 'ask', 'leav', 'anytim', 'want\\n\\npeople', 'fir', 'mak', 'dumb', 'soc', 'med', 'post', 'someth', 'gen', 'agr', 'howev', 'fir', 'ident', 'memb', 'polit', 'party', 'boss', 'doesnt', 'lik', 'def', 'think', 'that', 'bullshit', 'highest', 'form\\n'], ['yes', 'employment', 'mean', 'leave', 'anytime', 'want', 'employer', 'ask', 'leave', 'anytime', 'want\\n\\npeople', 'fire', 'make', 'dumb', 'social', 'media', 'post', 'something', 'generally', 'agree', 'however', 'fire', 'identify', 'member', 'political', 'party', 'boss', 'doesnt', 'like', 'def', 'think', 'thats', 'bullshit', 'highest', 'form\\n'])\n",
      "original document: \n",
      "['I', 'played', 'a', 'ton', 'of', 'racing', 'games', 'as', 'a', 'kid.', 'my', 'racing', 'skills', 'developed', 'from', 'them', 'have', 'saved', 'me', 'from', 'countless', 'accidents', 'in', 'real', 'life.', 'my', 'town', 'is', 'full', 'or', 'horrible', 'drivers.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'ton', 'rac', 'gam', 'kid', 'rac', 'skil', 'develop', 'sav', 'countless', 'accid', 'real', 'lif', 'town', 'ful', 'horr', 'driv'], ['play', 'ton', 'race', 'game', 'kid', 'race', 'skills', 'develop', 'save', 'countless', 'accidents', 'real', 'life', 'town', 'full', 'horrible', 'drivers'])\n",
      "original document: \n",
      "['Notre', 'Dame', 'and', 'USC?', 'You', 'just', 'seem', 'to', 'like', 'overrated', 'teams']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['not', 'dam', 'usc', 'seem', 'lik', 'over', 'team'], ['notre', 'dame', 'usc', 'seem', 'like', 'overrate', 'team'])\n",
      "original document: \n",
      "['Its', 'almost', 'winter', 'here', 'in', 'Canada,', 'and', 'my', 'university', 'has', 'a', 'shit', 'ton', 'of', 'international', 'students', 'and', 'so', 'far', 'every', 'year', 'i', 'love', 'seeing', 'the', 'face', 'of', 'international', 'students', 'experiencing', 'snow!', '1', 'of', 'my', 'best', 'friends', 'i', 'met', 'in', 'first', 'year', 'was', 'from', 'india', 'and', 'when', 'it', 'snowed', 'she', 'was', 'freaking', 'out', 'and', 'was', 'so', 'happy,', 'it', 'was', 'a', 'great', 'time,', 'we', 'built', 'snowmen', 'and', 'rolled', 'around', 'in', 'it', 'until', 'we', 'got', 'cold.', 'Would', 'love', 'to', 'hear', 'your', 'experiences!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['almost', 'wint', 'canad', 'univers', 'shit', 'ton', 'intern', 'stud', 'far', 'every', 'year', 'lov', 'see', 'fac', 'intern', 'stud', 'expery', 'snow', 'on', 'best', 'friend', 'met', 'first', 'year', 'ind', 'snow', 'freak', 'happy', 'gre', 'tim', 'built', 'snowm', 'rol', 'around', 'got', 'cold', 'would', 'lov', 'hear', 'expery'], ['almost', 'winter', 'canada', 'university', 'shit', 'ton', 'international', 'students', 'far', 'every', 'year', 'love', 'see', 'face', 'international', 'students', 'experience', 'snow', 'one', 'best', 'friends', 'meet', 'first', 'year', 'india', 'snow', 'freak', 'happy', 'great', 'time', 'build', 'snowmen', 'roll', 'around', 'get', 'cold', 'would', 'love', 'hear', 'experience'])\n",
      "original document: \n",
      "['First', 'day', 'in', 'my', 'blind', 'up', 'here', 'in', 'Manitoba.', 'Geese', 'are', 'finally', 'starting', 'to', 'group', 'up', 'so', 'shooting', 'was', 'good.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'day', 'blind', 'manitob', 'gees', 'fin', 'start', 'group', 'shoot', 'good'], ['first', 'day', 'blind', 'manitoba', 'geese', 'finally', 'start', 'group', 'shoot', 'good'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['(in', 'the', 'ship?)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ship'], ['ship'])\n",
      "original document: \n",
      "['Great,', \"I'll\", 'be', 'here', 'if', 'you', 'want', 'to', 'play.', \"I'm\", 'G_Rod12', 'on', 'psn', 'and', 'i', 'think', 'i', 'already', 'added', 'you', ':^)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'il', 'want', 'play', 'im', 'g_rod12', 'psn', 'think', 'already', 'ad'], ['great', 'ill', 'want', 'play', 'im', 'g_rod12', 'psn', 'think', 'already', 'add'])\n",
      "original document: \n",
      "['Honestly', 'I', 'agree', 'with', 'this.', 'He', \"hasn't\", 'proven', 'to', 'be', 'a', 'dirty', 'player', 'in', 'the', 'past,', 'and', 'massive', 'hits', 'generally', 'result', 'in', 'fumbles', 'and', 'huge', 'momentum', 'swings.', '', '\\n', '', '\\nI', \"can't\", 'tell', 'you', 'how', 'many', 'times', \"I've\", 'seen', 'fumbles', 'caused', 'by', 'leading', 'with', 'the', 'helmet', '(towards', 'the', 'ball/arms)', 'on', 'a', 'hard', 'hit.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'agr', 'hasnt', 'prov', 'dirty', 'play', 'past', 'mass', 'hit', 'gen', 'result', 'fumbl', 'hug', 'moment', 'swing', '\\n', '\\ni', 'cant', 'tel', 'many', 'tim', 'iv', 'seen', 'fumbl', 'caus', 'lead', 'helmet', 'toward', 'ballarm', 'hard', 'hit'], ['honestly', 'agree', 'hasnt', 'prove', 'dirty', 'player', 'past', 'massive', 'hit', 'generally', 'result', 'fumble', 'huge', 'momentum', 'swing', '\\n', '\\ni', 'cant', 'tell', 'many', 'time', 'ive', 'see', 'fumble', 'cause', 'lead', 'helmet', 'towards', 'ballarms', 'hard', 'hit'])\n",
      "original document: \n",
      "['Why', 'is', 'it', 'clear', 'that', 'earth', 'is', 'flat?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['clear', 'ear', 'flat'], ['clear', 'earth', 'flat'])\n",
      "original document: \n",
      "['I', 'CAN’T', 'WIIIIIIN\\nWITH', 'OR', 'WITHOUTTT', 'YOU']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'wiiiiiin\\nwith', 'withoutt'], ['cant', 'wiiiiiin\\nwith', 'withouttt'])\n",
      "original document: \n",
      "['CAREFUL', 'NED\\n\\nCAREFUL', 'NOW']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['car', 'ned\\n\\ncareful'], ['careful', 'ned\\n\\ncareful'])\n",
      "original document: \n",
      "['Did', 'you', 'have', 'insurance', 'on', 'it?', 'I', 'have', 'no', 'insurance', 'but', 'my', 'charging', 'port', 'went', 'like', '10', 'months', 'in', 'and', 'they', 'gave', 'me', 'a', 'free', '6p.', 'Now,', 'another', '5', 'months', 'later', 'or', 'so', 'with', 'the', 'replacement', 'and', 'I', 'swear', \"I'm\", 'the', 'only', 'person', 'with', 'no', 'battery', 'issues', 'on', 'the', '6p.', 'I', 'use', 'almost', 'the', 'whole', 'battery', 'and', 'it', 'dies', 'at', 'like', '5%', '...maybe', \"that's\", 'still', 'not', 'acceptable', 'tho']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ins', 'ins', 'charg', 'port', 'went', 'lik', 'ten', 'month', 'gav', 'fre', '6p', 'anoth', 'fiv', 'month', 'lat', 'replac', 'swear', 'im', 'person', 'battery', 'issu', '6p', 'us', 'almost', 'whol', 'battery', 'die', 'lik', 'fiv', 'mayb', 'that', 'stil', 'acceiv', 'tho'], ['insurance', 'insurance', 'charge', 'port', 'go', 'like', 'ten', 'months', 'give', 'free', '6p', 'another', 'five', 'months', 'later', 'replacement', 'swear', 'im', 'person', 'battery', 'issue', '6p', 'use', 'almost', 'whole', 'battery', 'die', 'like', 'five', 'maybe', 'thats', 'still', 'acceptable', 'tho'])\n",
      "original document: \n",
      "[\"I'm\", 'sixteen,', 'and', 'it', 'was', 'last', 'week.', \"I'm\", 'living', 'in', 'a', 'new', 'town', 'and', 'moving', 'into', 'highschool', 'is', 'incredibly', 'hard', 'I', 'find.', 'I', 'feel', 'so', 'out', 'of', 'place', 'and', 'alone,', 'and', 'it', 'was', 'overwhelming.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sixteen', 'last', 'week', 'im', 'liv', 'new', 'town', 'mov', 'highschool', 'incred', 'hard', 'find', 'feel', 'plac', 'alon', 'overwhelm'], ['im', 'sixteen', 'last', 'week', 'im', 'live', 'new', 'town', 'move', 'highschool', 'incredibly', 'hard', 'find', 'feel', 'place', 'alone', 'overwhelm'])\n",
      "original document: \n",
      "['on', 'my', 'way!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way'], ['way'])\n",
      "original document: \n",
      "['Human', 'at', 'sight,', 'monster', 'at', 'heart.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hum', 'sight', 'monst', 'heart'], ['human', 'sight', 'monster', 'heart'])\n",
      "original document: \n",
      "['What', 'is', 'the', 'difference?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['diff'], ['difference'])\n",
      "original document: \n",
      "['And', 'he', 'also', 'said', 'that', 'Puerto', 'Rico', '\"wants', 'everything', 'done', 'for', 'them.\"\\n\\nLike,', 'Donald.', \"They're\", '*dying*.', 'The', 'fuck', 'is', 'wrong', 'with', 'you?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'said', 'puerto', 'rico', 'want', 'everyth', 'don', 'them\\n\\nlike', 'donald', 'theyr', 'dying', 'fuck', 'wrong'], ['also', 'say', 'puerto', 'rico', 'want', 'everything', 'do', 'them\\n\\nlike', 'donald', 'theyre', 'die', 'fuck', 'wrong'])\n",
      "original document: \n",
      "[\"I'm\", 'white,', 'but', 'things', 'like', 'police', 'brutality,', 'black', 'poverty', 'and', 'inner', 'city', 'violence.', '', 'They', \"don't\", 'hit', 'close', 'to', 'home', 'for', 'me.', '', \"I'm\", 'not', 'trying', 'to', 'be', 'the', 'great', 'white', 'hope.', 'I', 'just', 'think', \"it's\", 'stupid', \"we're\", 'paying', 'attention', 'to', 'the', 'kneeling', 'and', 'not', 'the', 'words', 'behind', 'it.', '', 'Then', 'again,', 'this', 'is', 'the', 'population', 'that', 'argued', 'about', 'bathrooms', 'for', 'months.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'whit', 'thing', 'lik', 'pol', 'brut', 'black', 'poverty', 'in', 'city', 'viol', 'dont', 'hit', 'clos', 'hom', 'im', 'try', 'gre', 'whit', 'hop', 'think', 'stupid', 'pay', 'at', 'kneel', 'word', 'behind', 'pop', 'argu', 'bathroom', 'month'], ['im', 'white', 'things', 'like', 'police', 'brutality', 'black', 'poverty', 'inner', 'city', 'violence', 'dont', 'hit', 'close', 'home', 'im', 'try', 'great', 'white', 'hope', 'think', 'stupid', 'pay', 'attention', 'kneel', 'word', 'behind', 'population', 'argue', 'bathrooms', 'months'])\n",
      "original document: \n",
      "['Not', 'at', 'all', 'to', 'be', 'honest.', 'I', 'actually', 'kind', 'of', 'like', 'it.', 'Even', 'if', 'you', 'said', 'it', 'to', 'me', 'in', 'a', 'derogatory', 'way', 'I', 'wouldn’t', 'really', 'care.', 'It’s', 'such', 'an', 'old', 'word', 'that', 'it', 'basically', 'just', 'means', 'American.', 'It’s', 'nothing', 'insulting', 'these', 'days.', '', 'To', 'southerners,', 'they', 'use', 'it', 'to', 'refer', 'to', 'anyone', 'from', 'the', 'north.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'act', 'kind', 'lik', 'ev', 'said', 'derog', 'way', 'wouldnt', 'real', 'car', 'old', 'word', 'bas', 'mean', 'am', 'noth', 'insult', 'day', 'southern', 'us', 'ref', 'anyon', 'nor'], ['honest', 'actually', 'kind', 'like', 'even', 'say', 'derogatory', 'way', 'wouldnt', 'really', 'care', 'old', 'word', 'basically', 'mean', 'american', 'nothing', 'insult', 'days', 'southerners', 'use', 'refer', 'anyone', 'north'])\n",
      "original document: \n",
      "['Yeah.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah'], ['yeah'])\n",
      "original document: \n",
      "['Is', 'it', 'the', 'same', 'size', 'as', 'a', 'real', 'one?', 'And', 'how', 'recently', 'did', 'that', 'become', 'available?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['siz', 'real', 'on', 'rec', 'becom', 'avail'], ['size', 'real', 'one', 'recently', 'become', 'available'])\n",
      "original document: \n",
      "['Just', 'start', 'where', 'goth', 'starts.', 'The', 'music.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'goth', 'start', 'mus'], ['start', 'goth', 'start', 'music'])\n",
      "original document: \n",
      "['Could', 'be', 'fun!:)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'fun'], ['could', 'fun'])\n",
      "original document: \n",
      "['Why', 'tho']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tho'], ['tho'])\n",
      "original document: \n",
      "['Down', 'voted', 'for', 'not', 'killing', 'anything']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vot', 'kil', 'anyth'], ['vote', 'kill', 'anything'])\n",
      "original document: \n",
      "['It’s', 'doing', 'exactly', 'the', 'same', 'dance', 'as', '“cool', 'guy”']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'dant', 'cool', 'guy'], ['exactly', 'dance', 'cool', 'guy'])\n",
      "original document: \n",
      "['As', 'long', 'as', 'you', 'can', 'stand', 'and', 'mumble', 'a', 'few', 'words', \"they'll\", 'serve', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['long', 'stand', 'mumbl', 'word', 'theyl', 'serv'], ['long', 'stand', 'mumble', 'word', 'theyll', 'serve'])\n",
      "original document: \n",
      "['that', 'is', 'a', 'tibetan', 'letter.', '\\n\\nhttps://www.omniglot.com/images/writing/tibetan_cons.gif\\n\\nIt', 'is', 'the', 'last', 'letter', 'listed', 'in', 'the', 'link.', \"I'm\", 'not', 'sure', 'what', \"it's\", 'significance', 'is.', '\\n\\nEdit:', \"I'm\", 'sure', 'it', 'is', 'the', 'Tibetan', 'om.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tibet', 'let', '\\n\\nhttpswwwomniglotcomimageswritingtibetan_consgif\\n\\nit', 'last', 'let', 'list', 'link', 'im', 'sur', 'sign', '\\n\\nedit', 'im', 'sur', 'tibet', 'om'], ['tibetan', 'letter', '\\n\\nhttpswwwomniglotcomimageswritingtibetan_consgif\\n\\nit', 'last', 'letter', 'list', 'link', 'im', 'sure', 'significance', '\\n\\nedit', 'im', 'sure', 'tibetan', 'om'])\n",
      "original document: \n",
      "['A', 'chimney?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chimney'], ['chimney'])\n",
      "original document: \n",
      "['wikileaks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wikileak'], ['wikileaks'])\n",
      "original document: \n",
      "[\"I'm\", 'pretty', 'sure', \"that's\", 'the', 'plan.\\n\\nElon', 'referred', 'to', 'the', 'passenger', 'variant', 'coming', 'in', 'multiple', 'configurations,', 'with', 'the', 'Mars', 'cabins', 'just', 'being', 'one', 'of', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'pretty', 'sur', 'that', 'plan\\n\\nelon', 'refer', 'passeng', 'vary', 'com', 'multipl', 'config', 'mar', 'cabin', 'on'], ['im', 'pretty', 'sure', 'thats', 'plan\\n\\nelon', 'refer', 'passenger', 'variant', 'come', 'multiple', 'configurations', 'mar', 'cabin', 'one'])\n",
      "original document: \n",
      "['I', 'love', 'gen', 'three', 'so', 'much.', 'I', 'can’t', 'wait', 'either.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'gen', 'three', 'much', 'cant', 'wait', 'eith'], ['love', 'gen', 'three', 'much', 'cant', 'wait', 'either'])\n",
      "original document: \n",
      "['A', 'hokujo', 'pot', 'and', 'cup', 'was', '$25', 'to', 'ship.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hokujo', 'pot', 'cup', 'twenty-five', 'ship'], ['hokujo', 'pot', 'cup', 'twenty-five', 'ship'])\n",
      "original document: \n",
      "[\"I'm\", 'using', 'both', '1', '&amp;', '4', 'of', 'your', 'options.', 'Slowly', 'creeping', 'up,', 'early', '80s', 'now']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'us', 'on', 'amp', 'four', 'opt', 'slow', 'creep', 'ear', '80s'], ['im', 'use', 'one', 'amp', 'four', 'options', 'slowly', 'creep', 'early', '80s'])\n",
      "original document: \n",
      "['Is', 'that', 'just', 'a', 'straw', 'cut', 'and', 'put', 'in', 'there', 'or', 'is', 'it', 'split', 'in', 'the', 'middle', 'being', '1', 'half', 'of', 'a', 'straw?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['straw', 'cut', 'put', 'split', 'middl', 'on', 'half', 'straw'], ['straw', 'cut', 'put', 'split', 'middle', 'one', 'half', 'straw'])\n",
      "original document: \n",
      "['[+Laetitian](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoj81y/):\\n\\n&gt;', 'You', 'have', 'to', '[look', 'it](https://www.youtube.com/watch?v=Do5qzzCnjgc&amp;feature=youtu.be&amp;t=50)', 'and', 'see', 'the', 'next', 'step']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['laetitianhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoj81y\\n\\ngt', 'look', 'ithttpswwwyoutubecomwatchvdo5qzzcnjgcampfeatureyoutubeampt50', 'see', 'next', 'step'], ['laetitianhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoj81y\\n\\ngt', 'look', 'ithttpswwwyoutubecomwatchvdo5qzzcnjgcampfeatureyoutubeampt50', 'see', 'next', 'step'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['/u/waterguy12', 'check', 'this', 'out', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['uwaterguy12', 'check'], ['uwaterguy12', 'check'])\n",
      "original document: \n",
      "['Nerf', 'genji', 'pls', 'kthx']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nerf', 'genj', 'pls', 'kthx'], ['nerf', 'genji', 'pls', 'kthx'])\n",
      "original document: \n",
      "['So', 'should', 'I', 'be', 'thanking', 'my', 'Lucky', 'Stars', 'that', 'I', 'pulled', 'both', 'Cloud', 'usb', 'and', 'Zack', 'csb', 'because', 'of', 'anniversary?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'lucky', 'star', 'pul', 'cloud', 'usb', 'zack', 'csb', 'annivers'], ['thank', 'lucky', 'star', 'pull', 'cloud', 'usb', 'zack', 'csb', 'anniversary'])\n",
      "original document: \n",
      "['The', 'shorter', 'the', 'trip,', 'the', 'bigger', 'the', 'tip.\\nIf', \"you're\", 'ordering', 'to', 'an', 'apartment', 'complex', 'or', 'business,', 'give', 'the', 'name.\\nDouble', 'check', 'the', 'address.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['short', 'trip', 'big', 'tip\\nif', 'yo', 'ord', 'apart', 'complex', 'busy', 'giv', 'name\\ndoubl', 'check', 'address'], ['shorter', 'trip', 'bigger', 'tip\\nif', 'youre', 'order', 'apartment', 'complex', 'business', 'give', 'name\\ndouble', 'check', 'address'])\n",
      "original document: \n",
      "['[all', 'of', 'the', 'most', 'recently', 'released', 'japanese', 'language', 'free', 'vns](https://vndb.org/v/all?q=;fil=tagspoil-0.olang-ja;rfil=patch-0.released-1.type-complete.freeware-1;o=d;s=rel)\\n\\n[all', 'of', 'the', 'most', 'recently', 'released', 'free', 'vns', 'that', 'are', 'in', 'english](https://vndb.org/v/all?q=&amp;fil=&amp;rfil=type-complete.patch-0.freeware-1.released-1.lang-en&amp;s=rel&amp;o=d)\\n\\n[all', 'of', 'the', 'most', 'recently', 'released', 'free', 'japanese', 'VNs', 'that', 'are', 'also', 'in', 'english](https://vndb.org/v/all?q=&amp;fil=olang-ja&amp;rfil=type-complete.patch-0.freeware-1.released-1.lang-en&amp;s=rel&amp;o=d)\\n\\nlearn', 'to', 'use', 'filters', 'thanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rec', 'releas', 'japanes', 'langu', 'fre', 'vnshttpsvndborgvallqfiltagspoil0olangjarfilpatch0released1typecompletefreeware1odsrel\\n\\nall', 'rec', 'releas', 'fre', 'vns', 'englishhttpsvndborgvallqampfilamprfiltypecompletepatch0freeware1released1langenampsrelampod\\n\\nall', 'rec', 'releas', 'fre', 'japanes', 'vns', 'also', 'englishhttpsvndborgvallqampfilolangjaamprfiltypecompletepatch0freeware1released1langenampsrelampod\\n\\nlearn', 'us', 'filt', 'thank'], ['recently', 'release', 'japanese', 'language', 'free', 'vnshttpsvndborgvallqfiltagspoil0olangjarfilpatch0released1typecompletefreeware1odsrel\\n\\nall', 'recently', 'release', 'free', 'vns', 'englishhttpsvndborgvallqampfilamprfiltypecompletepatch0freeware1released1langenampsrelampod\\n\\nall', 'recently', 'release', 'free', 'japanese', 'vns', 'also', 'englishhttpsvndborgvallqampfilolangjaamprfiltypecompletepatch0freeware1released1langenampsrelampod\\n\\nlearn', 'use', 'filter', 'thank'])\n",
      "original document: \n",
      "['Hey', \"everyone!\\n\\nI'm\", 'thinking', 'on', 'throwing', 'up', 'an', 'NM', 'tomorrow', 'but', 'wanted', 'to', 'see', 'how', 'people', 'feel', 'about', 'it', 'first.\\n\\nIt', 'is', 'a', 'Kershaw', 'knockout', '(assisted', 'opening)', 'I', \"don't\", 'have', 'the', 'box', 'there', 'are', 'some', 'snail', 'trails', 'on', 'the', 'blade', 'from', 'opening', 'a', 'few', 'cardboard', 'boxes', 'other', 'than', 'that', 'the', 'knife', 'is', 'in', 'great', 'condition.', 'Thinking', 'of', 'doing', '10', '@', '$5\\n\\nWhat', 'does', 'everyone', 'think?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'everyone\\n\\nim', 'think', 'throwing', 'nm', 'tomorrow', 'want', 'see', 'peopl', 'feel', 'first\\n\\nit', 'kershaw', 'knockout', 'assist', 'op', 'dont', 'box', 'snail', 'trail', 'blad', 'op', 'cardboard', 'box', 'knif', 'gre', 'condit', 'think', 'ten', '5\\n\\nwhat', 'everyon', 'think'], ['hey', 'everyone\\n\\nim', 'think', 'throw', 'nm', 'tomorrow', 'want', 'see', 'people', 'feel', 'first\\n\\nit', 'kershaw', 'knockout', 'assist', 'open', 'dont', 'box', 'snail', 'trail', 'blade', 'open', 'cardboard', 'box', 'knife', 'great', 'condition', 'think', 'ten', '5\\n\\nwhat', 'everyone', 'think'])\n",
      "original document: \n",
      "['I', 'loved', 'his', 'reaction:', '\"Holy', 'shit!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'react', 'holy', 'shit'], ['love', 'reaction', 'holy', 'shit'])\n",
      "original document: \n",
      "['Snopes:', 'Mostly', 'false.', 'It', 'was', '1.2', 'billion', 'dollars.', '(if', 'I', 'remember', 'correctly)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['snop', 'most', 'fals', 'twelv', 'bil', 'doll', 'rememb', 'correct'], ['snopes', 'mostly', 'false', 'twelve', 'billion', 'dollars', 'remember', 'correctly'])\n",
      "original document: \n",
      "['[Original', 'post](https://www.reddit.com/r/snackexchange/comments/73ighu/soy_sauce_flavored_ramen_cups_located_in_central/)', 'by', '/u/Shamitro', 'in', '/r/snackexchange\\n\\n&amp;nbsp;\\n\\n---\\n\\n^(This', 'crosspost', 'was', 'performed', 'automatically', 'as', 'a', 'part', 'of', 'the', 'ImagesOfNetwork.)\\n[^(Learn', 'more', 'about', 'the', 'ImagesOfNetwork,', 'how', 'you', 'can', 'help,', 'and', 'other', 'Frequently', 'Asked', 'Questions)](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_what_is_images_of.3F)^,', '^or', '[^(visit', 'us', 'on', 'Discord!)](https://discordapp.com/invite/0tZsejLgM2vzaExR)\\n\\n^([)[^(\"Remove', 'my', 'post!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_remove_my_post)', '^|', '[^(\"The', 'bot', 'got', 'this', 'one', 'wrong!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_the_bot_got_this_one_wrong)', '^|', '[^(\"Stop', 'Crossposting', 'My', 'Stuff!\")](https://www.reddit.com/r/imagesofnetwork/wiki/faqandinstructions#wiki_no_really._don.27t_ever_crosspost_me_or_my_subreddit.)', '', '^(])\\n\\n[](#match', '\"California\")\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'posthttpswwwredditcomrsnackexchangecomments73ighusoy_sauce_flavored_ramen_cups_located_in_central', 'ushamitro', 'rsnackexchange\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'autom', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequ', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremov', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'got', 'on', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crosspost', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nmatch', 'california\\n'], ['original', 'posthttpswwwredditcomrsnackexchangecomments73ighusoy_sauce_flavored_ramen_cups_located_in_central', 'ushamitro', 'rsnackexchange\\n\\nampnbsp\\n\\n\\n\\nthis', 'crosspost', 'perform', 'automatically', 'part', 'imagesofnetwork\\nlearn', 'imagesofnetwork', 'help', 'frequently', 'ask', 'questionshttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_what_is_images_of3f', 'visit', 'us', 'discordhttpsdiscordappcominvite0tzsejlgm2vzaexr\\n\\nremove', 'posthttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_remove_my_post', 'bot', 'get', 'one', 'wronghttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_the_bot_got_this_one_wrong', 'stop', 'crossposting', 'stuffhttpswwwredditcomrimagesofnetworkwikifaqandinstructionswiki_no_really_don27t_ever_crosspost_me_or_my_subreddit', '\\n\\nmatch', 'california\\n'])\n",
      "original document: \n",
      "['Lol', 'I', 'did']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['PM']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pm'], ['pm'])\n",
      "original document: \n",
      "['hello\\nif', 'you', 'still', 'want', 'to', 'play', 'Add', 'me', 'on', 'steam', 'Themasterseb\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hello\\nif', 'stil', 'want', 'play', 'ad', 'steam', 'themasterseb\\n'], ['hello\\nif', 'still', 'want', 'play', 'add', 'steam', 'themasterseb\\n'])\n",
      "original document: \n",
      "[\"That's\", 'not', 'really', 'a', 'great', 'reason', 'to', 'agree', 'with', 'him', 'though', '-', 'he', 'consistently', 'treats', 'his', 'own', 'family', 'as', 'pawns', 'to', 'be', 'disposed', 'of', 'as', 'he', 'sees', 'fit', 'and', 'is', 'incredibly', 'comfortable', 'with', 'sexual', 'violence.', \"He's\", 'also', 'an', 'enormous', 'hypocrite,', 'railing', 'against', \"Tyrion's\", 'whores', 'while', 'both', 'sleeping', 'with', 'Shae', 'and', 'having', 'a', 'tunnel', 'built', 'to', 'his', 'chambers', 'from', 'a', 'whorehouse', 'down', 'the', 'street.', 'Tywin', 'knows', 'his', 'way', 'around', 'a', 'battlefield,', 'but', \"he's\", 'an', 'enormous', 'piece', 'of', 'shit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'real', 'gre', 'reason', 'agr', 'though', 'consist', 'tre', 'famy', 'pawn', 'dispos', 'see', 'fit', 'incred', 'comfort', 'sex', 'viol', 'hes', 'also', 'enorm', 'hypocrit', 'rail', 'tyr', 'whor', 'sleep', 'sha', 'tunnel', 'built', 'chamb', 'whoreh', 'street', 'tywin', 'know', 'way', 'around', 'battlefield', 'hes', 'enorm', 'piec', 'shit'], ['thats', 'really', 'great', 'reason', 'agree', 'though', 'consistently', 'treat', 'family', 'pawn', 'dispose', 'see', 'fit', 'incredibly', 'comfortable', 'sexual', 'violence', 'hes', 'also', 'enormous', 'hypocrite', 'rail', 'tyrions', 'whore', 'sleep', 'shae', 'tunnel', 'build', 'chamber', 'whorehouse', 'street', 'tywin', 'know', 'way', 'around', 'battlefield', 'hes', 'enormous', 'piece', 'shit'])\n",
      "original document: \n",
      "['What', 'does', 'jacob', 'do', 'to', 'benefit', 'the', 'streams?', 'Exactly.', 'Delete', 'this', 'post.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jacob', 'benefit', 'streams', 'exact', 'delet', 'post'], ['jacob', 'benefit', 'stream', 'exactly', 'delete', 'post'])\n",
      "original document: \n",
      "[\"Let's\", 'be', 'real', 'though,', 'Apple', 'is', 'not', 'helping', 'themselves', 'with', 'gouging', 'people', 'for', 'shit', 'like', 'chargers', 'out', 'of', 'the', 'box.', '8+', 'with', 'a', '5W', 'charger?', \"It's\", 'almost', 'insulting.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'real', 'though', 'appl', 'help', 'goug', 'peopl', 'shit', 'lik', 'charg', 'box', 'eight', '5w', 'charg', 'almost', 'insult'], ['let', 'real', 'though', 'apple', 'help', 'gouge', 'people', 'shit', 'like', 'chargers', 'box', 'eight', '5w', 'charger', 'almost', 'insult'])\n",
      "original document: \n",
      "['I', 'couldn’t', 'remember', 'which', 'splitter', 'it', 'was...lol.', 'I', 'guess', 'I’ll', 'just', 'retake', 'the', 'LSAT!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['couldnt', 'rememb', 'splitter', 'waslol', 'guess', 'il', 'retak', 'lsat'], ['couldnt', 'remember', 'splitter', 'waslol', 'guess', 'ill', 'retake', 'lsat'])\n",
      "original document: \n",
      "[\"that's\", 'digital', 'blackface']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'digit', 'blackfac'], ['thats', 'digital', 'blackface'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Countdown', 'is', '[here!](https://www.piliapp.com/timer/countdown/#stop=1506817812294,all=00:30:00)\\n\\nRemember', 'No', 'spoilers!\\n\\nPretend', 'it’s', 'July', '12,', '1996', 'and', '[How', 'do', 'U', 'Want', 'It', 'by', 'Tupac](https://www.youtube.com/watch?v=w9KWYwczHEw)', 'and', '[California', 'Love](https://www.youtube.com/watch?v=5wBTdfAkqGU)', 'are', 'at', 'the', 'top', 'of', 'the', 'charts', '(really', 'it', 'was', 'officially', 'July', '13th', 'but', 'come', 'on)', 'and', '[Independence', 'Day', 'is', '#1', 'at', 'the', 'box', 'office](https://www.youtube.com/watch?v=kA2WzBi2grE)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['countdown', 'herehttpswwwpiliappcomtimercountdownstop1506817812294all003000\\n\\nremember', 'spoilers\\n\\npretend', 'july', 'twelv', 'one thousand, nine hundred and ninety-six', 'u', 'want', 'tupachttpswwwyoutubecomwatchvw9kwywczhew', 'californ', 'lovehttpswwwyoutubecomwatchv5wbtdfakqgu', 'top', 'chart', 'real', 'off', 'july', '13th', 'com', 'independ', 'day', 'on', 'box', 'officehttpswwwyoutubecomwatchvka2wzbi2gre'], ['countdown', 'herehttpswwwpiliappcomtimercountdownstop1506817812294all003000\\n\\nremember', 'spoilers\\n\\npretend', 'july', 'twelve', 'one thousand, nine hundred and ninety-six', 'u', 'want', 'tupachttpswwwyoutubecomwatchvw9kwywczhew', 'california', 'lovehttpswwwyoutubecomwatchv5wbtdfakqgu', 'top', 'chart', 'really', 'officially', 'july', '13th', 'come', 'independence', 'day', 'one', 'box', 'officehttpswwwyoutubecomwatchvka2wzbi2gre'])\n",
      "original document: \n",
      "['Hahahaha', 'so', 'funny', 'video', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hahahah', 'funny', 'video'], ['hahahaha', 'funny', 'video'])\n",
      "original document: \n",
      "['on', 'what', 'exchange?', 'basically', 'no', 'volume', 'on', 'Binance']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exchang', 'bas', 'volum', 'bin'], ['exchange', 'basically', 'volume', 'binance'])\n",
      "original document: \n",
      "['...see,', 'I', 'had', 'never', 'made', 'this', 'connection', 'before,', 'and', 'I', 'am', 'glad', 'you', 'pointed', 'it', 'out', 'to', 'me,', 'I', 'still', \"don't\", 'like', 'it,', 'but', 'that', 'there', 'is', 'a', 'logical', 'explanation', 'for', 'why', 'he', 'dresses', 'like', 'that', 'is', 'actually', 'kinda', 'satisfying', 'to', 'know.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'nev', 'mad', 'connect', 'glad', 'point', 'stil', 'dont', 'lik', 'log', 'expl', 'dress', 'lik', 'act', 'kind', 'satisfy', 'know'], ['see', 'never', 'make', 'connection', 'glad', 'point', 'still', 'dont', 'like', 'logical', 'explanation', 'dress', 'like', 'actually', 'kinda', 'satisfy', 'know'])\n",
      "original document: \n",
      "['The', 'Slenderverse', 'has', 'gotten', 'old', 'to', 'the', 'point', 'where', 'it', 'is', 'frowned', 'upon', 'to', 'create', 'something', 'with', 'Slenderman', 'in', 'it.', 'This', 'reason', 'is', 'why', 'I', 'create', 'my', 'own', 'monsters.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['slendervers', 'got', 'old', 'point', 'frown', 'upon', 'cre', 'someth', 'slenderm', 'reason', 'cre', 'monst'], ['slenderverse', 'get', 'old', 'point', 'frown', 'upon', 'create', 'something', 'slenderman', 'reason', 'create', 'monsters'])\n",
      "original document: \n",
      "['This', 'is', 'the', 'first', \"I've\", 'seen', 'of', 'these.', 'Are', 'there', 'glams?', 'I', 'imagine', \"they're\", 'just', 'solid', 'black?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'iv', 'seen', 'glam', 'imagin', 'theyr', 'solid', 'black'], ['first', 'ive', 'see', 'glams', 'imagine', 'theyre', 'solid', 'black'])\n",
      "original document: \n",
      "['Did', 'Donald', 'tell', 'you', \"it's\", 'ok', 'to', 'believe', 'that?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['donald', 'tel', 'ok', 'believ'], ['donald', 'tell', 'ok', 'believe'])\n",
      "original document: \n",
      "['Sorry', 'I', 'think', 'I', 'dropped', 'this', '/s.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'think', 'drop'], ['sorry', 'think', 'drop'])\n",
      "original document: \n",
      "['In', 'Britain,', 'in', 'the', 'days', 'when', 'people', 'stayed', 'in', 'their', 'towns', 'or', 'villages', 'for', 'their', 'whole', 'lives,', 'the', 'local', 'vicar', 'or', 'priest', 'held', 'all', 'the', 'family', 'records', 'and', 'would', 'have', 'prevented', 'close', 'family', 'marriages.', 'It', 'was', 'another', 'reason', 'for', \"'Banns'\", '-', 'so', 'that', 'locals', 'could', 'give', 'a', 'reason,', 'other', 'than', 'bigamy,', 'for', 'a', 'proposed', 'marriage', 'not', 'taking', 'place.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['britain', 'day', 'peopl', 'stay', 'town', 'vil', 'whol', 'liv', 'loc', 'vic', 'priest', 'held', 'famy', 'record', 'would', 'prev', 'clos', 'famy', 'marry', 'anoth', 'reason', 'ban', 'loc', 'could', 'giv', 'reason', 'bigamy', 'propos', 'marry', 'tak', 'plac'], ['britain', 'days', 'people', 'stay', 'towns', 'villages', 'whole', 'live', 'local', 'vicar', 'priest', 'hold', 'family', 'record', 'would', 'prevent', 'close', 'family', 'marriages', 'another', 'reason', 'banns', 'locals', 'could', 'give', 'reason', 'bigamy', 'propose', 'marriage', 'take', 'place'])\n",
      "original document: \n",
      "['&gt;', 'Everyone', 'hate', 'him', \"here\\n\\nThat's\", 'why', 'he', 'has', 'absolute', 'majority', 'in', 'the', 'parliament.', 'Oh', 'wait...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'everyon', 'hat', 'here\\n\\nthats', 'absolv', 'maj', 'parlia', 'oh', 'wait'], ['gt', 'everyone', 'hate', 'here\\n\\nthats', 'absolute', 'majority', 'parliament', 'oh', 'wait'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Fuck', 'Michael.', '\\n\\nI', 'mean', 'yeah,', 'the', 'Klingon', 'killed', 'the', 'Captian.', 'But', 'capturing', 'him', 'alive', 'was', 'the', 'only', 'way', 'to', 'prevent', 'war,', 'and', 'YOU', 'SET', 'YOUR', 'PHASER', 'TO', 'KILL!!\\n\\nI', 'though', 'someone', 'who', 'was', 'raised', 'Vulcan', 'would', 'be', 'able', 'to', 'control', 'their', 'emotions.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'michael', '\\n\\ni', 'mean', 'yeah', 'klingon', 'kil', 'capt', 'capt', 'al', 'way', 'prev', 'war', 'set', 'phas', 'kill\\n\\ni', 'though', 'someon', 'rais', 'vulc', 'would', 'abl', 'control', 'emot'], ['fuck', 'michael', '\\n\\ni', 'mean', 'yeah', 'klingon', 'kill', 'captian', 'capture', 'alive', 'way', 'prevent', 'war', 'set', 'phaser', 'kill\\n\\ni', 'though', 'someone', 'raise', 'vulcan', 'would', 'able', 'control', 'emotions'])\n",
      "original document: \n",
      "['143414594|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143414467\\nNo.', 'Hurtful', 'is', 'when', 'people', 'look', 'at', 'me', 'with', 'hatred,', 'disgust,', 'and', 'contempt', 'for', 'no', 'other', 'reason', 'than', 'my', 'gender', 'identity.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, five hundred and ninety-four', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143414467\\nno', 'hurt', 'peopl', 'look', 'hat', 'disgust', 'contempt', 'reason', 'gend', 'identity\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, five hundred and ninety-four', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143414467\\nno', 'hurtful', 'people', 'look', 'hatred', 'disgust', 'contempt', 'reason', 'gender', 'identity\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['*Your', 'post', 'has', 'been', 'removed', 'for', 'the', 'following', 'reason(s):*\\n\\n**Off', 'Topic**\\n\\n*', 'Posts', 'or', 'submissions', 'that', 'are', 'not', 'primarily', 'asking', 'or', 'discussing', 'legal', 'questions', '', 'are', 'removed.', '', '\\n\\n\\n\\n*If', 'you', 'feel', 'this', 'was', 'in', 'error,', 'message', 'the', '[moderators.](http://www.reddit.com/message/compose?to=%2Fr%2FLegalAdvice)*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'remov', 'follow', 'reasons\\n\\noff', 'topic\\n\\n', 'post', 'submit', 'prim', 'ask', 'discuss', 'leg', 'quest', 'remov', '\\n\\n\\n\\nif', 'feel', 'er', 'mess', 'moderatorshttpwwwredditcommessagecomposeto2fr2flegaladvice'], ['post', 'remove', 'follow', 'reasons\\n\\noff', 'topic\\n\\n', 'post', 'submissions', 'primarily', 'ask', 'discuss', 'legal', 'question', 'remove', '\\n\\n\\n\\nif', 'feel', 'error', 'message', 'moderatorshttpwwwredditcommessagecomposeto2fr2flegaladvice'])\n",
      "original document: \n",
      "['I', 'wiggled', 'and', 'flashlight', 'spammed', 'a', 'scav', 'player', 'from', 'behind', 'and', \"didn't\", 'shoot,', 'he', 'looked', 'at', 'me,', 'proceeded', 'to', 'shoot', 'and', 'still', 'died.\\n\\nThis', 'is', 'why', 'you', \"don't\", 'trust', 'people.', \"It's\", 'only', 'rarely', 'worth', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wiggl', 'flashlight', 'spam', 'scav', 'play', 'behind', 'didnt', 'shoot', 'look', 'process', 'shoot', 'stil', 'died\\n\\nthis', 'dont', 'trust', 'peopl', 'rar', 'wor'], ['wiggle', 'flashlight', 'spammed', 'scav', 'player', 'behind', 'didnt', 'shoot', 'look', 'proceed', 'shoot', 'still', 'died\\n\\nthis', 'dont', 'trust', 'people', 'rarely', 'worth'])\n",
      "original document: \n",
      "['$25', 'shipped']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['twenty-five', 'ship'], ['twenty-five', 'ship'])\n",
      "original document: \n",
      "['Not', 'on', 'PC.', 'You', 'can', 'play', 'it', 'in', 'private', 'matches', 'but', \"it's\", 'not', 'in', 'public', 'playlists.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pc', 'play', 'priv', 'match', 'publ', 'playl'], ['pc', 'play', 'private', 'match', 'public', 'playlists'])\n",
      "original document: \n",
      "['Yeah', 'man,', 'I', 'stayed', 'watching', 'nick', 'and', 'CN', 'for\\n', 'a', 'long', 'time.', 'Toonami', 'was', 'my', 'jam', 'back', 'in', 'the', 'day.', '\\n\\nThere', 'was', 'one', 'cartoon', 'that', 'aired', 'on', 'toonami,', 'I', 'cant', 'remember', 'the', 'name', 'of', 'it,', 'but', 'it', 'was', 'about', 'this', 'mechanic', 'dude', 'who', 'found', 'a', 'giant', 'Gundam', 'style', 'robot', 'from', 'another', 'planet,', 'and', 'then', 'totally', 'reworked', 'all', 'the', 'parts', 'into', 'this', 'badass', 'mech.', 'Then', 'the', 'alien', 'who', 'originally', 'owned', 'it', 'comes', 'looking', 'for', 'it', '(and', 'she', 'is', 'a', 'hot', 'af', 'redhead)', 'and', 'she', \"can't\", 'pilot', 'it', 'anymore', 'because', 'he', 'totally', 'fucked', 'it', 'up,', 'so', 'they', 'gotta', 'work', 'together,', 'and', 'crazy', 'shit', 'happens.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'man', 'stay', 'watch', 'nick', 'cn', 'for\\n', 'long', 'tim', 'toonam', 'jam', 'back', 'day', '\\n\\nthere', 'on', 'cartoon', 'air', 'toonam', 'cant', 'rememb', 'nam', 'mech', 'dud', 'found', 'giant', 'gundam', 'styl', 'robot', 'anoth', 'planet', 'tot', 'rework', 'part', 'badass', 'mech', 'aly', 'origin', 'own', 'com', 'look', 'hot', 'af', 'redhead', 'cant', 'pilot', 'anym', 'tot', 'fuck', 'gott', 'work', 'togeth', 'crazy', 'shit', 'hap'], ['yeah', 'man', 'stay', 'watch', 'nick', 'cn', 'for\\n', 'long', 'time', 'toonami', 'jam', 'back', 'day', '\\n\\nthere', 'one', 'cartoon', 'air', 'toonami', 'cant', 'remember', 'name', 'mechanic', 'dude', 'find', 'giant', 'gundam', 'style', 'robot', 'another', 'planet', 'totally', 'rework', 'part', 'badass', 'mech', 'alien', 'originally', 'own', 'come', 'look', 'hot', 'af', 'redhead', 'cant', 'pilot', 'anymore', 'totally', 'fuck', 'gotta', 'work', 'together', 'crazy', 'shit', 'happen'])\n",
      "original document: \n",
      "['Use', 'a', 'trusted', 'exchange', 'for', 'escrow.', 'Seems', 'like', 'the', 'only', 'practical', 'way', 'to', 'do', 'this.', 'Many', 'people', 'want', 'to', 'scam', 'or', 'bluff.', 'No', 'vaporware,', 'just', 'old', 'fashioned', 'escrow.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'trust', 'exchang', 'escrow', 'seem', 'lik', 'pract', 'way', 'many', 'peopl', 'want', 'scam', 'bluff', 'vaporw', 'old', 'fash', 'escrow'], ['use', 'trust', 'exchange', 'escrow', 'seem', 'like', 'practical', 'way', 'many', 'people', 'want', 'scam', 'bluff', 'vaporware', 'old', 'fashion', 'escrow'])\n",
      "original document: \n",
      "['I', 'mean', 'there’s', 'sugar', 'in', 'it', 'but', 'I', 'wouldn’t', 'call', 'ketchup', 'sweet', '(at', 'lest', 'Heinz).', 'It’s', 'more', 'complex', 'than', 'that.', 'But', 'I', 'agree', 'on', 'pickles.', 'Dill', '=', 'excellent.', 'Sweet', '=', 'shit.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'ther', 'sug', 'wouldnt', 'cal', 'ketchup', 'sweet', 'lest', 'heinz', 'complex', 'agr', 'pickl', 'dil', 'excel', 'sweet', 'shit'], ['mean', 'theres', 'sugar', 'wouldnt', 'call', 'ketchup', 'sweet', 'lest', 'heinz', 'complex', 'agree', 'pickle', 'dill', 'excellent', 'sweet', 'shit'])\n",
      "original document: \n",
      "['Morals', \"aren't\", 'inherently', 'good.', '', 'He', 'lost', 'the', 'ones', 'he', 'had', 'that', 'made', 'him', 'interesting.', '', 'He', 'was', 'a', 'noble', 'evil', 'that', 'was', 'focused', 'on', 'making', 'his', 'group', 'prosperous,', 'sort', 'of', 'a', 'Robin', 'Hood', 'type', 'character', 'turned', 'worse.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mor', 'ar', 'inh', 'good', 'lost', 'on', 'mad', 'interest', 'nobl', 'evil', 'focus', 'mak', 'group', 'prosp', 'sort', 'robin', 'hood', 'typ', 'charact', 'turn', 'wors'], ['morals', 'arent', 'inherently', 'good', 'lose', 'ones', 'make', 'interest', 'noble', 'evil', 'focus', 'make', 'group', 'prosperous', 'sort', 'robin', 'hood', 'type', 'character', 'turn', 'worse'])\n",
      "original document: \n",
      "['I', 'agree', 'and', 'disagree.', 'The', 'problem', 'is', 'these', 'people', \"don't\", 'see', 'that', 'behaviour', 'as', 'being', 'bigoted.', 'Many', 'probably', 'believe', 'they', 'actually', 'are', 'defending', 'the', 'sanctity', 'of', 'marriage.', 'I', 'think', \"it'd\", 'be', 'important', 'not', 'to', 'call', 'someone', 'a', 'bigot,', 'but', 'to', 'say', 'you', 'believe', 'it', 'is', 'a', 'bigoted', 'view', 'to', 'have', 'and', 'to', 'explain', 'why', 'you', 'think', 'that.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'disagr', 'problem', 'peopl', 'dont', 'see', 'behavio', 'bigot', 'many', 'prob', 'believ', 'act', 'defend', 'sanct', 'marry', 'think', 'itd', 'import', 'cal', 'someon', 'bigot', 'say', 'believ', 'bigot', 'view', 'explain', 'think'], ['agree', 'disagree', 'problem', 'people', 'dont', 'see', 'behaviour', 'bigoted', 'many', 'probably', 'believe', 'actually', 'defend', 'sanctity', 'marriage', 'think', 'itd', 'important', 'call', 'someone', 'bigot', 'say', 'believe', 'bigoted', 'view', 'explain', 'think'])\n",
      "original document: \n",
      "['Pumas', 'cant', 'buy', 'a', 'try']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['puma', 'cant', 'buy', 'try'], ['pumas', 'cant', 'buy', 'try'])\n",
      "original document: \n",
      "[\"I'm\", 'sure', \"Arin's\", 'story', 'about', 'having', 'a', 'horrible', 'trip', 'is', 'less', 'likely', 'than', 'having', 'a', 'regular', 'high,', 'but', 'I', 'still', \"don't\", 'like', 'those', 'odds.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'arin', 'story', 'horr', 'trip', 'less', 'lik', 'regul', 'high', 'stil', 'dont', 'lik', 'od'], ['im', 'sure', 'arins', 'story', 'horrible', 'trip', 'less', 'likely', 'regular', 'high', 'still', 'dont', 'like', 'odds'])\n",
      "original document: \n",
      "['As', 'a', 'side', 'note,', 'if', 'you', 'happen', 'to', 'have', 'a', 'decklist', \"I'd\", 'love', 'to', 'see', 'it\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sid', 'not', 'hap', 'deckl', 'id', 'lov', 'see', 'it\\n'], ['side', 'note', 'happen', 'decklist', 'id', 'love', 'see', 'it\\n'])\n",
      "original document: \n",
      "['Haha,', 'thanks,', 'but', 'I', 'just', 'meant', 'that', 'these', 'colors', 'are', 'notorious', 'for', 'never', 'drying.', '', 'Apache', 'Sunset', 'laid', 'down', 'thick', 'on', 'Tomoe', 'River', 'can', 'smear', 'weeks', 'later.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'thank', 'meant', 'col', 'not', 'nev', 'dry', 'apach', 'sunset', 'laid', 'thick', 'tomo', 'riv', 'smear', 'week', 'lat'], ['haha', 'thank', 'mean', 'color', 'notorious', 'never', 'dry', 'apache', 'sunset', 'lay', 'thick', 'tomoe', 'river', 'smear', 'weeks', 'later'])\n",
      "original document: \n",
      "['https://redditenhancementsuite.com/', 'is', 'as', 'close', 'as', 'I', 'can', 'give', 'ya.', 'My', 'gf', 'set', 'it', 'up', 'and', \"I'm\", 'using', 'her', 'computer.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsredditenhancementsuitecom', 'clos', 'giv', 'ya', 'gf', 'set', 'im', 'us', 'comput'], ['httpsredditenhancementsuitecom', 'close', 'give', 'ya', 'gf', 'set', 'im', 'use', 'computer'])\n",
      "original document: \n",
      "['The', '\"Cat\\'s', 'Eye\"', 'chroma', 'is', 'my', 'favorite', 'one,', 'the', 'one', 'with', 'mostly', 'the', 'original', 'colorscheme', 'rearraged', 'a', 'big.', 'The', 'red/red-orange', 'blades', 'are', 'beautiful.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cat', 'ey', 'chroma', 'favorit', 'on', 'on', 'most', 'origin', 'colorschem', 'rear', 'big', 'redredorang', 'blad', 'beauty'], ['cat', 'eye', 'chroma', 'favorite', 'one', 'one', 'mostly', 'original', 'colorscheme', 'rearraged', 'big', 'redredorange', 'blades', 'beautiful'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnojk6f/):\\n\\nWhat', 'is', 'typing?', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnojk6f\\n\\nwhat', 'typ', 'lol'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnojk6f\\n\\nwhat', 'type', 'lol'])\n",
      "original document: \n",
      "['I', 'took', 'a', 'cab', 'across', 'Puerto', 'Rico', 'on', 'my', 'way', 'to', 'a', 'walled', 'and', 'guarded', 'resort.', 'The', 'homes', 'looked', 'like', 'a', 'hurricane', 'had', 'just', 'blown', 'through', 'and', 'that', 'was', 'before', 'Maria.', 'What', 'a', 'dump!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['took', 'cab', 'across', 'puerto', 'rico', 'way', 'wal', 'guard', 'resort', 'hom', 'look', 'lik', 'hur', 'blown', 'mar', 'dump'], ['take', 'cab', 'across', 'puerto', 'rico', 'way', 'wall', 'guard', 'resort', 'home', 'look', 'like', 'hurricane', 'blow', 'maria', 'dump'])\n",
      "original document: \n",
      "['Nice', 'pic,', 'looks', 'good']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'pic', 'look', 'good'], ['nice', 'pic', 'look', 'good'])\n",
      "original document: \n",
      "['HAHA', 'FUNNY']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'funny'], ['haha', 'funny'])\n",
      "original document: \n",
      "['Heck', 'again.', '\\n\\nI', 'had', 'written', 'it', 'but', 'I', 'guess', 'I', 'accidentally', 'deleted', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['heck', '\\n\\ni', 'writ', 'guess', 'accid', 'delet'], ['heck', '\\n\\ni', 'write', 'guess', 'accidentally', 'delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Has', 'to', 'work', 'better', 'than', 'Shireen.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'bet', 'shireen'], ['work', 'better', 'shireen'])\n",
      "original document: \n",
      "['Calculus', 'is', 'too', 'basic.', 'I', 'make', \"calculus'\", 'mom', 'bite', 'the', 'pillow.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['calcul', 'bas', 'mak', 'calcul', 'mom', 'bit', 'pillow'], ['calculus', 'basic', 'make', 'calculus', 'mom', 'bite', 'pillow'])\n",
      "original document: \n",
      "['Go', 'Briscoe']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'brisco'], ['go', 'briscoe'])\n",
      "original document: \n",
      "['My', 'dad', 'always', 'said', 'it', 'was', 'a', 'mouse', 'on', 'a', 'motorcycle.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dad', 'alway', 'said', 'mous', 'motorcyc'], ['dad', 'always', 'say', 'mouse', 'motorcycle'])\n",
      "original document: \n",
      "[\"Doesn't\", 'take', 'more', 'effort', 'and', \"it's\", 'fine', 'quality.', 'But', 'why', 'you', 'even', 'arguing', 'about', 'this?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'tak', 'effort', 'fin', 'qual', 'ev', 'argu'], ['doesnt', 'take', 'effort', 'fine', 'quality', 'even', 'argue'])\n",
      "original document: \n",
      "['No,', 'they', \"weren't.\", 'Stop', 'trying', 'to', 'pass', 'off', 'less-than-half-truths', 'as', 'supporting', 'your', 'case.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wer', 'stop', 'try', 'pass', 'lessthanhalftruth', 'support', 'cas'], ['werent', 'stop', 'try', 'pass', 'lessthanhalftruths', 'support', 'case'])\n",
      "original document: \n",
      "['Oh', 'another', 'USA', 'only', 'service..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'anoth', 'us', 'serv'], ['oh', 'another', 'usa', 'service'])\n",
      "original document: \n",
      "['Arizona', 'is', 'having', 'trouble', 'keeping', 'the', 'OL', 'healthy.', 'They', \"couldn't\", 'even', 'run', 'well', 'against', 'Dallas.', 'C.johnson', 'my', 'look', 'appealing', 'but', 'Ellington', 'or', 'Williams', 'could', 'take', 'over', 'if', 'they', 'just', 'happen', 'to', 'run', 'more', 'efficiently.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['arizon', 'troubl', 'keep', 'ol', 'healthy', 'couldnt', 'ev', 'run', 'wel', 'dalla', 'cjohnson', 'look', 'ap', 'ellington', 'william', 'could', 'tak', 'hap', 'run', 'efficy'], ['arizona', 'trouble', 'keep', 'ol', 'healthy', 'couldnt', 'even', 'run', 'well', 'dallas', 'cjohnson', 'look', 'appeal', 'ellington', 'williams', 'could', 'take', 'happen', 'run', 'efficiently'])\n",
      "original document: \n",
      "['I', 'remember', 'seeing', 'the', 'pitch,', 'thinking', 'back', 'on', 'my', 'chemistry', 'classes', 'and', 'thinking', '\"there', 'is', 'no', 'way', \"they'd\", 'be', 'needing', 'crowdfunding', 'if', 'this', 'was', 'a', 'real', 'thing.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rememb', 'see', 'pitch', 'think', 'back', 'chem', 'class', 'think', 'way', 'theyd', 'nee', 'crowdfund', 'real', 'thing'], ['remember', 'see', 'pitch', 'think', 'back', 'chemistry', 'class', 'think', 'way', 'theyd', 'need', 'crowdfunding', 'real', 'thing'])\n",
      "original document: \n",
      "['I', 'blame', 'Russia', 'and', 'idiots', 'beliveing', 'all', 'the', 'trolls', 'on', 'Facebook', 'and', 'reddit.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['blam', 'russ', 'idiot', 'bel', 'trol', 'facebook', 'reddit'], ['blame', 'russia', 'idiots', 'beliveing', 'troll', 'facebook', 'reddit'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Corn']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['corn'], ['corn'])\n",
      "original document: \n",
      "['One', 'of', 'them', 'is', 'red', 'u', 'monkey']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'red', 'u', 'monkey'], ['one', 'red', 'u', 'monkey'])\n",
      "original document: \n",
      "['Thanks', 'for', 'posting', 'to', '/r/dirtykikpals,', '/u/SoRandomWow!', 'We', 'encourage', 'all', 'of', 'our', 'users', 'here', 'to', 'verify', 'themselves,', 'to', 'possibly', 'get', 'more/better', 'responses,', 'as', 'well', 'as', 'help', 'us', 'in', 'dealing', 'with', 'sellers,', 'scammers,', 'etc.', 'For', 'information', 'on', 'how', 'to', 'verify,', 'please', 'check', 'our', 'sidebar', 'or', 'message', 'us', 'at', '[modmail](https://www.reddit.com/message/compose?to=%2Fr%2Fdirtykikpals).\\nAlso,', 'be', 'sure', 'to', 'familiarize', 'yourself', 'with', 'the', 'rules', 'of', 'this', 'subreddit,', 'as', 'well', 'as', 'helpful', 'tips', '[here](https://redd.it/6ojo0r).\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/dirtykikpals)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'post', 'rdirtykikp', 'usorandomwow', 'enco', 'us', 'ver', 'poss', 'get', 'morebet', 'respons', 'wel', 'help', 'us', 'deal', 'sel', 'scam', 'etc', 'inform', 'ver', 'pleas', 'check', 'sideb', 'mess', 'us', 'modmailhttpswwwredditcommessagecomposeto2fr2fdirtykikpals\\nalso', 'sur', 'famili', 'rul', 'subreddit', 'wel', 'help', 'tip', 'herehttpsreddit6ojo0r\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetordirtykikp', 'quest', 'concern'], ['thank', 'post', 'rdirtykikpals', 'usorandomwow', 'encourage', 'users', 'verify', 'possibly', 'get', 'morebetter', 'responses', 'well', 'help', 'us', 'deal', 'sellers', 'scammers', 'etc', 'information', 'verify', 'please', 'check', 'sidebar', 'message', 'us', 'modmailhttpswwwredditcommessagecomposeto2fr2fdirtykikpals\\nalso', 'sure', 'familiarize', 'rule', 'subreddit', 'well', 'helpful', 'tip', 'herehttpsreddit6ojo0r\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetordirtykikpals', 'question', 'concern'])\n",
      "original document: \n",
      "['I', 'mixed', 'up', 'their', 'helmets', 'and', 'the', 'fact', 'that', 'Mansell', 'tested', 'for', 'Jordan', 'in', '1996.\\n\\nBut', 'yes,', 'it', 'is', 'Brundle.', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mix', 'helmet', 'fact', 'mansel', 'test', 'jord', '1996\\n\\nbut', 'ye', 'brundl'], ['mix', 'helmets', 'fact', 'mansell', 'test', 'jordan', '1996\\n\\nbut', 'yes', 'brundle'])\n",
      "original document: \n",
      "['But', \"he's\", 'not', 'someone', 'I', 'want', 'to', 'be', 'a', 'close', 'friend', ',', 'I', \"don't\", 'feel', 'comfortable', 'speaking', 'about', 'anything', 'personal', 'with', 'him', ',', 'I', 'find', 'him', 'kind', 'of', 'annoying', 'and', \"he's\", 'quite', 'aggressive.', 'I', 'want', 'to', 'make', 'new', 'friends', 'but', \"he's\", 'making', 'it', 'difficult', 'by', 'always', 'being', 'there.', \"It's\", 'been', '3', 'weeks', 'and', 'I', \"haven't\", 'been', 'able', 'to', 'have', 'a', 'conversation', 'with', 'anyone', 'without', 'him', 'being', 'there.', 'I', 'want', 'to', 'be', 'my', 'own', 'thing', ',', 'separate', 'from', 'him']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'someon', 'want', 'clos', 'friend', 'dont', 'feel', 'comfort', 'speak', 'anyth', 'person', 'find', 'kind', 'annoy', 'hes', 'quit', 'aggress', 'want', 'mak', 'new', 'friend', 'hes', 'mak', 'difficult', 'alway', 'three', 'week', 'hav', 'abl', 'convers', 'anyon', 'without', 'want', 'thing', 'sep'], ['hes', 'someone', 'want', 'close', 'friend', 'dont', 'feel', 'comfortable', 'speak', 'anything', 'personal', 'find', 'kind', 'annoy', 'hes', 'quite', 'aggressive', 'want', 'make', 'new', 'friends', 'hes', 'make', 'difficult', 'always', 'three', 'weeks', 'havent', 'able', 'conversation', 'anyone', 'without', 'want', 'thing', 'separate'])\n",
      "original document: \n",
      "['*can', 'you', 'text', 'it', 'over', 'to', 'my', 'number', 'please?*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['text', 'numb', 'pleas'], ['text', 'number', 'please'])\n",
      "original document: \n",
      "['Just', 'let', 'them', 'read', 'Robert', 'Crumb', 'comics,', 'those', 'are', 'healthy', 'and', 'artistic', 'for', 'the', 'child.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'read', 'robert', 'crumb', 'com', 'healthy', 'art', 'child'], ['let', 'read', 'robert', 'crumb', 'comics', 'healthy', 'artistic', 'child'])\n",
      "original document: \n",
      "['I', 'run', '2', 'in', 'my', 'roach', 'forest', 'along', 'with', '1', 'wotf', 'and', 'i', 'think', 'she', 'is', 'fine.', 'The', 'main', 'reason', 'i', 'like', 'her', 'is', 'that', 'she', 'goes', 'on', 'curve', 'better', 'than', 'wotf,', 'because', 'turn', '', '5', 'is', 'usually', 'time', 'where', 'you', 'want', 'to', 'play', 'Teena/EPM/use', 'DoD', 'or', 'whatever', 'else.', 'Turn', '6', 'is', 'often', 'period', 'where', 'some', 'combo', 'pieces', 'are', 'already', 'gathered,', 'so', 'its', 'time', 'to', 'stall/clear', 'the', 'board', 'while', 'waiting', 'for', 'lethal', 'range', 'and/or', 'more', 'combo', 'pieces.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['run', 'two', 'roach', 'forest', 'along', 'on', 'wotf', 'think', 'fin', 'main', 'reason', 'lik', 'goe', 'curv', 'bet', 'wotf', 'turn', 'fiv', 'us', 'tim', 'want', 'play', 'teenaepmus', 'dod', 'whatev', 'els', 'turn', 'six', 'oft', 'period', 'combo', 'piec', 'already', 'gath', 'tim', 'stallclear', 'board', 'wait', 'leth', 'rang', 'and', 'combo', 'piec'], ['run', 'two', 'roach', 'forest', 'along', 'one', 'wotf', 'think', 'fine', 'main', 'reason', 'like', 'go', 'curve', 'better', 'wotf', 'turn', 'five', 'usually', 'time', 'want', 'play', 'teenaepmuse', 'dod', 'whatever', 'else', 'turn', 'six', 'often', 'period', 'combo', 'piece', 'already', 'gather', 'time', 'stallclear', 'board', 'wait', 'lethal', 'range', 'andor', 'combo', 'piece'])\n",
      "original document: \n",
      "['Nice', 'amp.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'amp'], ['nice', 'amp'])\n",
      "original document: \n",
      "['Hah!', 'I', 'had', 'to', 'hop', 'off', 'the', 'cab', 'in', 'a', 'minute,', 'so', 'I', 'could', 'only', 'write', '\"Retinol', 'and', 'Lactic', 'Acid', 'alternate', 'PM,', 'and', 'sunscreen', 'AM.\"\\n\\n\\nHere\\'s', 'the', 'full', 'routine:\\n\\n\\nAM:\\n\\n\\nWash', 'with', 'water&gt;', 'Timeless', 'CE', 'Ferulic&gt;', 'Hydrating', 'Toner&gt;', 'Jojoba/Grapeseed', 'Oil&gt;', 'CeraVe', 'moisturizing', 'lotion&gt;Sunscreen\\n\\nPM', '1:\\n\\nDIY', 'oil', 'cleanser', 'if', 'I', 'have', 'sunscreen/makeup', 'on&gt;', 'CeraVe', 'hydrating', 'cleanser&gt;', 'Hydrating', 'Toner&gt;', 'Hylamide', 'SubQ', 'serum&gt;', 'TO', 'Lactic', 'Acid', '5%/Alpha', 'Skin', 'Care', '10%', 'Glycolic', 'gel&gt;', 'Rosehip', 'seed/Passion', 'fruit', 'oil&gt;', 'CeraVe', 'PM/Stratia', 'LG&gt;', 'CeraVe', 'healing', 'ointment\\n\\nPM2:\\n\\nSame', 'up', 'to', 'serum,', 'then', 'Oil&gt;', 'TO', 'Advanced', 'Retinoid', '2%&gt;', 'Moisturizer&gt;', 'Occlusive\\n\\n\\nI', 'wait', 'the', 'suggested', 'time', 'between', 'products.', 'With', 'sunscreen', \"it's\", 'usually', 'more', 'than', '30', 'mins+', 'after', 'moisturizer.', 'I', 'work', 'from', 'home,', 'so', 'I', 'have', 'the', 'luxury', 'of', 'stretching', 'out', 'my', 'AM', 'routine.', 'I', 'also', \"don't\", 'have', 'to', 'wear', 'sunscreen', 'or', 'makeup', 'every', 'day.\\n\\n\\nToners', 'I', 'use:', 'Klairs', 'Supple', 'Preparation,', 'Hada', 'Labo', 'Gokujyun', 'Moist,', 'Kiku', 'Masamune', 'Sake', 'High', 'Moist\\n\\n\\nSunscreens:', 'Neogen', 'Day-Light', 'on', 'dry', 'days,', 'Anessa', 'Aqua', 'Booster', 'Mild', 'SPF', '50', 'on', 'humid', 'days\\n\\n\\nOh,', 'and', 'I', 'have', 'combo', 'leaning', 'dry', 'skin,', 'btw.', 'Hope', 'this', 'helps!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'hop', 'cab', 'minut', 'could', 'writ', 'retinol', 'lact', 'acid', 'altern', 'pm', 'sunscreen', 'am\\n\\n\\nheres', 'ful', 'routine\\n\\n\\nam\\n\\n\\nwash', 'watergt', 'timeless', 'ce', 'ferulicgt', 'hydr', 'tonergt', 'jojobagrapesee', 'oilgt', 'cerav', 'moist', 'lotiongtsunscreen\\n\\npm', '1\\n\\ndiy', 'oil', 'cleans', 'sunscreenmakeup', 'ongt', 'cerav', 'hydr', 'cleansergt', 'hydr', 'tonergt', 'hylamid', 'subq', 'serumgt', 'lact', 'acid', '5alpha', 'skin', 'car', 'ten', 'glycol', 'gelgt', 'rosehip', 'seedpass', 'fruit', 'oilgt', 'cerav', 'pmstratia', 'lggt', 'cerav', 'heal', 'ointment\\n\\npm2\\n\\nsame', 'ser', 'oilgt', 'adv', 'retinoid', '2gt', 'moisturizergt', 'occlusive\\n\\n\\ni', 'wait', 'suggest', 'tim', 'produc', 'sunscreen', 'us', 'thirty', 'min', 'moist', 'work', 'hom', 'luxury', 'stretching', 'routin', 'also', 'dont', 'wear', 'sunscreen', 'makeup', 'every', 'day\\n\\n\\ntoners', 'us', 'klair', 'suppl', 'prep', 'had', 'labo', 'gokujyun', 'moist', 'kiku', 'masamun', 'sak', 'high', 'moist\\n\\n\\nsunscreens', 'neog', 'daylight', 'dry', 'day', 'aness', 'aqu', 'boost', 'mild', 'spf', 'fifty', 'humid', 'days\\n\\n\\noh', 'combo', 'lean', 'dry', 'skin', 'btw', 'hop', 'help'], ['hah', 'hop', 'cab', 'minute', 'could', 'write', 'retinol', 'lactic', 'acid', 'alternate', 'pm', 'sunscreen', 'am\\n\\n\\nheres', 'full', 'routine\\n\\n\\nam\\n\\n\\nwash', 'watergt', 'timeless', 'ce', 'ferulicgt', 'hydrate', 'tonergt', 'jojobagrapeseed', 'oilgt', 'cerave', 'moisturize', 'lotiongtsunscreen\\n\\npm', '1\\n\\ndiy', 'oil', 'cleanser', 'sunscreenmakeup', 'ongt', 'cerave', 'hydrate', 'cleansergt', 'hydrate', 'tonergt', 'hylamide', 'subq', 'serumgt', 'lactic', 'acid', '5alpha', 'skin', 'care', 'ten', 'glycolic', 'gelgt', 'rosehip', 'seedpassion', 'fruit', 'oilgt', 'cerave', 'pmstratia', 'lggt', 'cerave', 'heal', 'ointment\\n\\npm2\\n\\nsame', 'serum', 'oilgt', 'advance', 'retinoid', '2gt', 'moisturizergt', 'occlusive\\n\\n\\ni', 'wait', 'suggest', 'time', 'products', 'sunscreen', 'usually', 'thirty', 'mins', 'moisturizer', 'work', 'home', 'luxury', 'stretch', 'routine', 'also', 'dont', 'wear', 'sunscreen', 'makeup', 'every', 'day\\n\\n\\ntoners', 'use', 'klairs', 'supple', 'preparation', 'hada', 'labo', 'gokujyun', 'moist', 'kiku', 'masamune', 'sake', 'high', 'moist\\n\\n\\nsunscreens', 'neogen', 'daylight', 'dry', 'days', 'anessa', 'aqua', 'booster', 'mild', 'spf', 'fifty', 'humid', 'days\\n\\n\\noh', 'combo', 'lean', 'dry', 'skin', 'btw', 'hope', 'help'])\n",
      "original document: \n",
      "[\"Didn't\", 'know', 'that', 'was', 'a', 'thing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'know', 'thing'], ['didnt', 'know', 'thing'])\n",
      "original document: \n",
      "['It', 'may', 'be', 'a', 'different', 'DIL', 'completely.', '', 'This', 'story', 'sadly', 'is', 'all', 'too', 'familiar.', '', \"She's\", 'getting', 'hammered', 'with', 'the', 'responses', 'though.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'diff', 'dil', 'complet', 'story', 'sad', 'famili', 'she', 'get', 'ham', 'respons', 'though'], ['may', 'different', 'dil', 'completely', 'story', 'sadly', 'familiar', 'shes', 'get', 'hammer', 'responses', 'though'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'would', 'greatly', 'appreciate', 'a', 'run.', '290+', 'Hunter.', '\\n\\nGT:', 'chugachninja', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'gre', 'apprecy', 'run', 'two hundred and ninety', 'hunt', '\\n\\ngt', 'chugachninj'], ['would', 'greatly', 'appreciate', 'run', 'two hundred and ninety', 'hunter', '\\n\\ngt', 'chugachninja'])\n",
      "original document: \n",
      "['No,', 'and', 'you', 'should', 'really', 'just', 'read', 'the', 'all-ages', 'version.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'read', 'al', 'vert'], ['really', 'read', 'allages', 'version'])\n",
      "original document: \n",
      "['If', 'we', 'see', 'an', 'escalation', 'of', 'the', 'kind', 'of', 'attacks', 'we', 'saw', 'against', 'individual', 'node', 'operators', 'of', 'XT,', 'Classic,', 'Unlimited,', 'etc.,', 'then', 'I', \"wouldn't\", 'be', 'surprised', 'if', 'we', 'start', 'to', 'see', 'rumblings', 'of', 'residential', 'ISPs', 'prohibiting', 'running', 'of', 'Bitcoin', 'nodes', 'in', 'their', 'terms', 'of', 'service,', 'because', 'why', 'deal', 'with', 'this', 'childish', 'nonsense', 'in', 'the', 'first', 'place', 'if', \"you're\", 'trying', 'to', 'run', 'a', 'business?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'esc', 'kind', 'attack', 'saw', 'individ', 'nod', 'op', 'xt', 'class', 'unlimit', 'etc', 'wouldnt', 'surpr', 'start', 'see', 'rumbl', 'resid', 'isp', 'prohibit', 'run', 'bitcoin', 'nod', 'term', 'serv', 'deal', 'child', 'nonsens', 'first', 'plac', 'yo', 'try', 'run', 'busy'], ['see', 'escalation', 'kind', 'attack', 'saw', 'individual', 'node', 'operators', 'xt', 'classic', 'unlimited', 'etc', 'wouldnt', 'surprise', 'start', 'see', 'rumble', 'residential', 'isps', 'prohibit', 'run', 'bitcoin', 'nod', 'term', 'service', 'deal', 'childish', 'nonsense', 'first', 'place', 'youre', 'try', 'run', 'business'])\n",
      "original document: \n",
      "['Ahh.', 'Glad', 'you', 'looked', 'into', 'it', 'more.', '\\nMaybe', 'adding', 'a', 'silicone', 'tube', 'to', 'the', 'end', 'of', 'a', 'vaporizer', '', 'or', 'use', 'it', 'with', 'a', 'water', 'tool', 'will', 'give', 'you', 'cooler', 'vapor', 'if', 'needed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ahh', 'glad', 'look', '\\nmaybe', 'ad', 'silicon', 'tub', 'end', 'vap', 'us', 'wat', 'tool', 'giv', 'cool', 'vap', 'nee'], ['ahh', 'glad', 'look', '\\nmaybe', 'add', 'silicone', 'tube', 'end', 'vaporizer', 'use', 'water', 'tool', 'give', 'cooler', 'vapor', 'need'])\n",
      "original document: \n",
      "['I', \"don't\", 'see', 'the', 'confusion.', '\\n\\nr/btc', 'says', 's2x', 'will', 'cripple', 'and', 'nullify', 'core.', '\\nr/bitcoin', 'says', 's2x', 'will', 'die', 'out', 'like', 'bcash', \"did\\n\\nyou're\", 'just', 'misreading', 'what', 'I', 'typed.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'see', 'confus', '\\n\\nrbtc', 'say', 's2x', 'crippl', 'null', 'cor', '\\nrbitcoin', 'say', 's2x', 'die', 'lik', 'bcash', 'did\\n\\nyoure', 'misread', 'typ'], ['dont', 'see', 'confusion', '\\n\\nrbtc', 'say', 's2x', 'cripple', 'nullify', 'core', '\\nrbitcoin', 'say', 's2x', 'die', 'like', 'bcash', 'did\\n\\nyoure', 'misread', 'type'])\n",
      "original document: \n",
      "['Probably', 'going', 'straight', 'to', 'Iran.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prob', 'going', 'straight', 'ir'], ['probably', 'go', 'straight', 'iran'])\n",
      "original document: \n",
      "['Hooked', 'up', 'with', 'a', 'guy', 'and', 'it', 'was', 'basically', 'doomed', 'from', 'the', 'start.\\n\\nHe', 'showed', 'up', '3', 'hours', 'late', 'to', 'my', 'place', 'cause', 'he', 'decided', 'it', 'was', 'a', 'good', 'idea', 'to', 'sell', 'one', 'of', 'his', 'old', 'game', 'consoles', 'before', 'he', 'came', 'over', 'and', 'the', 'person', 'took', 'forever', 'to', 'pick', 'it', 'up.', 'Once', 'we', 'started', 'fooling', 'around', 'I', 'had', 'to', 'tell', 'him', 'to', 'take', 'his', 'clothes', 'off,', 'he', 'refused', 'to', 'take', 'off', 'his', 'tshirt.', 'He', \"didn't\", 'know', 'what', 'foreplay', 'was', 'and', 'I', 'had', 'to', 'tell', 'him', 'to', 'do', 'it.\\n\\nSurprise', 'surprise', 'I', \"couldn't\", 'finish', 'cause', 'I', 'just', \"wasn't\", 'feeling', 'into', 'it', 'at', 'that', 'point', 'and', 'I', \"don't\", 'think', 'he', 'did', 'either.', 'This', 'is', 'why', 'I', \"don't\", 'do', 'hookups', 'anymore.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hook', 'guy', 'bas', 'doom', 'start\\n\\nhe', 'show', 'three', 'hour', 'lat', 'plac', 'caus', 'decid', 'good', 'ide', 'sel', 'on', 'old', 'gam', 'consol', 'cam', 'person', 'took', 'forev', 'pick', 'start', 'fool', 'around', 'tel', 'tak', 'cloth', 'refus', 'tak', 'tshirt', 'didnt', 'know', 'foreplay', 'tel', 'it\\n\\nsurprise', 'surpr', 'couldnt', 'fin', 'caus', 'wasnt', 'feel', 'point', 'dont', 'think', 'eith', 'dont', 'hookup', 'anym'], ['hook', 'guy', 'basically', 'doom', 'start\\n\\nhe', 'show', 'three', 'hours', 'late', 'place', 'cause', 'decide', 'good', 'idea', 'sell', 'one', 'old', 'game', 'console', 'come', 'person', 'take', 'forever', 'pick', 'start', 'fool', 'around', 'tell', 'take', 'clothe', 'refuse', 'take', 'tshirt', 'didnt', 'know', 'foreplay', 'tell', 'it\\n\\nsurprise', 'surprise', 'couldnt', 'finish', 'cause', 'wasnt', 'feel', 'point', 'dont', 'think', 'either', 'dont', 'hookups', 'anymore'])\n",
      "original document: \n",
      "['Looks', 'like', 'a', 'goddamn', 'fire', 'hazard', 'to', 'me']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'goddamn', 'fir', 'hazard'], ['look', 'like', 'goddamn', 'fire', 'hazard'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'understand', '*why*', 'they’re', 'doing', 'it,', 'but', 'PSUs', 'blatantly', 'obvious', 'Heisman', 'stat', 'padding', 'still', 'rubs', 'me', 'the', 'wrong', 'way.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['understand', 'theyr', 'psu', 'blat', 'obvy', 'heism', 'stat', 'pad', 'stil', 'rub', 'wrong', 'way'], ['understand', 'theyre', 'psus', 'blatantly', 'obvious', 'heisman', 'stat', 'pad', 'still', 'rub', 'wrong', 'way'])\n",
      "original document: \n",
      "['Comrade!', 'I', 'am', 'real', 'puerto', 'rican/asian/canadian', 'man.\\n\\nI', 'am', 'not', 'Ruski.', 'Cyka!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['comrad', 'real', 'puerto', 'ricanasiancanad', 'man\\n\\ni', 'rusk', 'cyk'], ['comrade', 'real', 'puerto', 'ricanasiancanadian', 'man\\n\\ni', 'ruski', 'cyka'])\n",
      "original document: \n",
      "['this', 'happened', 'to', 'me', 'expect', 'my', 'game', 'just', 'crashed', 'and', 'reset', 'my', 'player']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hap', 'expect', 'gam', 'crash', 'reset', 'play'], ['happen', 'expect', 'game', 'crash', 'reset', 'player'])\n",
      "original document: \n",
      "['#####&amp;#009;\\n\\n######&amp;#009;\\n\\n####&amp;#009;\\n', '', \"\\n[Kinjalli's\", 'Caller](https://img.scryfall.com/cards/normal/en/xln/18.jpg?1505309002)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Kinjalli%27s%20Caller)', '[(SF)](https://scryfall.com/card/xln/18?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!Kinjalli%27s%20Caller)', '', '\\n[Optec', 'Huntmaster](http://mythicspoiler.com/ixa/cards/otepechuntmaster.jpg)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Otepec%20Huntmaster)', '[(SF)](https://scryfall.com/card/xln/153?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!Otepec%20Huntmaster)', '', '\\n[Ripjaw', 'Raptor](http://mythicspoiler.com/ixa/cards/ripjawraptor.jpg)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Ripjaw%20Raptor)', '[(SF)](https://scryfall.com/card/xln/203?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!Ripjaw%20Raptor)', '', '\\n[Carnage', 'Tyrant](http://mythicspoiler.com/ixa/cards/carnagetyrant.jpg)', '-', '[(G)](http://gatherer.wizards.com/Pages/Card/Details.aspx?name=Carnage%20Tyrant)', '[(SF)](https://scryfall.com/card/xln/179?utm_source=mtgcardfetcher)', '[(MC)](http://magiccards.info/query?q=!Carnage%20Tyrant)', '', '\\n^^^[[cardname]]', '^^^or', '^^^[[cardname|SET]]', '^^^to', '^^^call']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amp009\\n\\namp009\\n\\namp009\\n', '\\nkinjallis', 'callerhttpsimgscryfallcomcardsnormalenxln18jpg1505309002', 'ghttpgathererwizardscompagescarddetailsaspxnamekinjalli27s20caller', 'sfhttpsscryfallcomcardxln18utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqkinjalli27s20caller', '\\noptec', 'huntmasterhttpmythicspoilercomixacardsotepechuntmasterjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameotepec20huntmaster', 'sfhttpsscryfallcomcardxln153utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqotepec20huntmaster', '\\nripjaw', 'raptorhttpmythicspoilercomixacardsripjawraptorjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameripjaw20raptor', 'sfhttpsscryfallcomcardxln203utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqripjaw20raptor', '\\ncarnage', 'tyranthttpmythicspoilercomixacardscarnagetyrantjpg', 'ghttpgathererwizardscompagescarddetailsaspxnamecarnage20tyrant', 'sfhttpsscryfallcomcardxln179utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqcarnage20tyrant', '\\ncardname', 'cardnameset', 'cal'], ['amp009\\n\\namp009\\n\\namp009\\n', '\\nkinjallis', 'callerhttpsimgscryfallcomcardsnormalenxln18jpg1505309002', 'ghttpgathererwizardscompagescarddetailsaspxnamekinjalli27s20caller', 'sfhttpsscryfallcomcardxln18utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqkinjalli27s20caller', '\\noptec', 'huntmasterhttpmythicspoilercomixacardsotepechuntmasterjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameotepec20huntmaster', 'sfhttpsscryfallcomcardxln153utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqotepec20huntmaster', '\\nripjaw', 'raptorhttpmythicspoilercomixacardsripjawraptorjpg', 'ghttpgathererwizardscompagescarddetailsaspxnameripjaw20raptor', 'sfhttpsscryfallcomcardxln203utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqripjaw20raptor', '\\ncarnage', 'tyranthttpmythicspoilercomixacardscarnagetyrantjpg', 'ghttpgathererwizardscompagescarddetailsaspxnamecarnage20tyrant', 'sfhttpsscryfallcomcardxln179utm_sourcemtgcardfetcher', 'mchttpmagiccardsinfoqueryqcarnage20tyrant', '\\ncardname', 'cardnameset', 'call'])\n",
      "original document: \n",
      "['Ok', 'Eagerjewbear-\\n\\n\\nWhat', 'should', 'who', 'have', 'done', 'better?', 'Trump?\\n\\nPre-stage', 'the', 'USNS', 'Comfort', 'prior', 'to', 'the', 'hurricane,', 'just', 'like', 'what', 'was', 'done', 'for', 'hurricane', 'Matthew.', '\\n\\nDispatch', 'another', 'two', 'LHDs', 'to', 'accompany', 'the', 'Wasp.', '\\n\\nOpen', 'transport', 'for', 'power', 'companies', 'and', 'personnel', 'on', 'the', 'SE', 'Atlantic', 'coast', 'as', 'soon', 'as', 'the', 'SJ', 'airport', 'was', 'cleared.', '\\n\\nIssue', 'National', 'Guard', 'mobilization', 'to', 'support', 'the', 'Puerto', 'Rico', 'units', '(they', 'have', 'a', 'few', 'of', 'their', 'own', 'NG', 'units', 'there,', 'but', 'not', 'enough)\\n\\nYeah', 'helicopters', 'would', 'perform', 'supply', 'airdrops', 'of', 'food', 'and', 'water', 'while', 'the', 'USCG', 'performed', 'air', 'medevac', 'to', 'SJ', 'to', 'await', 'the', 'Comfort,', 'however', 'would', 'be', 'in', 'a', 'central', 'triage', 'location', 'with', 'access', 'to', 'supplies', 'as', 'needed.', '\\n\\nIn', 'summary,', 'what', 'they', 'are', 'saying', 'they', 'are', 'going', 'to', 'do', 'really', 'soon,', 'but', 'about', '5', 'days', 'ago.', '\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'eagerjewbear\\n\\n\\nwhat', 'don', 'bet', 'trump\\n\\nprestage', 'usn', 'comfort', 'pri', 'hur', 'lik', 'don', 'hur', 'matthew', '\\n\\ndispatch', 'anoth', 'two', 'lhds', 'accompany', 'wasp', '\\n\\nopen', 'transport', 'pow', 'company', 'personnel', 'se', 'atl', 'coast', 'soon', 'sj', 'airport', 'clear', '\\n\\nissue', 'nat', 'guard', 'mobl', 'support', 'puerto', 'rico', 'unit', 'ng', 'unit', 'enough\\n\\nyeah', 'helicopt', 'would', 'perform', 'supply', 'airdrop', 'food', 'wat', 'uscg', 'perform', 'air', 'medevac', 'sj', 'await', 'comfort', 'howev', 'would', 'cent', 'tri', 'loc', 'access', 'supply', 'nee', '\\n\\nin', 'sum', 'say', 'going', 'real', 'soon', 'fiv', 'day', 'ago', '\\n\\n\\n'], ['ok', 'eagerjewbear\\n\\n\\nwhat', 'do', 'better', 'trump\\n\\nprestage', 'usns', 'comfort', 'prior', 'hurricane', 'like', 'do', 'hurricane', 'matthew', '\\n\\ndispatch', 'another', 'two', 'lhds', 'accompany', 'wasp', '\\n\\nopen', 'transport', 'power', 'company', 'personnel', 'se', 'atlantic', 'coast', 'soon', 'sj', 'airport', 'clear', '\\n\\nissue', 'national', 'guard', 'mobilization', 'support', 'puerto', 'rico', 'units', 'ng', 'units', 'enough\\n\\nyeah', 'helicopters', 'would', 'perform', 'supply', 'airdrops', 'food', 'water', 'uscg', 'perform', 'air', 'medevac', 'sj', 'await', 'comfort', 'however', 'would', 'central', 'triage', 'location', 'access', 'supply', 'need', '\\n\\nin', 'summary', 'say', 'go', 'really', 'soon', 'five', 'days', 'ago', '\\n\\n\\n'])\n",
      "original document: \n",
      "['This', 'may', 'be', 'related', 'to', 'your', 'system,', 'as', 'you', \"aren't\", 'getting', 'any', 'connections.', 'Did', 'you', 'allow', 'both', 'incoming', 'and', 'outgoing', 'connections', 'for', '`monerod`?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['may', 'rel', 'system', 'ar', 'get', 'connect', 'allow', 'incom', 'outgo', 'connect', 'monerod'], ['may', 'relate', 'system', 'arent', 'get', 'connections', 'allow', 'incoming', 'outgo', 'connections', 'monerod'])\n",
      "original document: \n",
      "['What', 'I', 'am', 'suggesting', 'is', 'raising', 'the', 'healing', 'rate', 'of', 'healers', 'other', 'than', 'mercy', 'to', 'help', 'them', 'with', 'the', 'challenge', 'that', 'is', 'keeping', 'teammates', 'up.', 'While', 'some', 'kits', 'have', 'alternate', 'perks,', \"I'd\", 'say', 'that', 'they', 'still', 'leave', 'you', 'in', 'a', 'spot', 'where', 'you', 'are', 'unable', 'to', 'capitalize', 'because', 'you', 'lose', 'teammates', 'that', 'would', 'help', 'you', 'capitalize', 'too', 'quickly.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suggest', 'rais', 'heal', 'rat', 'heal', 'mercy', 'help', 'challeng', 'keep', 'team', 'kit', 'altern', 'perk', 'id', 'say', 'stil', 'leav', 'spot', 'un', 'capit', 'los', 'team', 'would', 'help', 'capit', 'quick'], ['suggest', 'raise', 'heal', 'rate', 'healers', 'mercy', 'help', 'challenge', 'keep', 'teammates', 'kit', 'alternate', 'perk', 'id', 'say', 'still', 'leave', 'spot', 'unable', 'capitalize', 'lose', 'teammates', 'would', 'help', 'capitalize', 'quickly'])\n",
      "original document: \n",
      "['And', 'soundtrack!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['soundtrack'], ['soundtrack'])\n",
      "original document: \n",
      "['Anything', 'you', 'saw', 'in', 'a', 'porn.', 'Stick', 'to', 'the', 'basics', 'for', 'the', 'first', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyth', 'saw', 'porn', 'stick', 'bas', 'first', 'tim'], ['anything', 'saw', 'porn', 'stick', 'basics', 'first', 'time'])\n",
      "original document: \n",
      "['What', 'ever', 'happened', 'to', 'Hugh', 'Jaynuss?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'hap', 'hugh', 'jaynuss'], ['ever', 'happen', 'hugh', 'jaynuss'])\n",
      "original document: \n",
      "['R']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['r'], ['r'])\n",
      "original document: \n",
      "['Boooooooss']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['boooooooss'], ['boooooooss'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['and', 'Buster', \"Posey's\", 'mask', 'was', 'never', 'seen', 'again']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bust', 'posey', 'mask', 'nev', 'seen'], ['buster', 'poseys', 'mask', 'never', 'see'])\n",
      "original document: \n",
      "['143415951|', '&gt;', 'Australia', 'Anonymous', '(ID:', \"CSDg7ZEw)\\n\\n&gt;&gt;143415817\\nHow'd\", 'you', 'go', 'from', 'a', 'lolcuck', 'to', 'an', 'establishment', 'cuck?\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fifteen thousand, nine hundred and fifty-on', 'gt', 'austral', 'anonym', 'id', 'csdg7zew\\n\\ngtgt143415817\\nhowd', 'go', 'lolcuck', 'est', 'cuck\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fifteen thousand, nine hundred and fifty-one', 'gt', 'australia', 'anonymous', 'id', 'csdg7zew\\n\\ngtgt143415817\\nhowd', 'go', 'lolcuck', 'establishment', 'cuck\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['**SD**', '|', '[Los', 'Angeles', 'Angels', 'vs', 'Seattle', 'Mariners](http://plr.livestreamsonline.net/embed/24)', '|', 'Ad', 'Overlays:', '3', '|', 'Mobile:', 'No', '\\n\\n\\n', '1.', 'If', 'black', 'screen', 'make', 'sure', 'flash', 'player', 'is', 'enabled.', '\\n\\n\\n', '2.', 'Stream', 'Live', '5', '-', '10', 'Before']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sd', 'los', 'angel', 'angel', 'vs', 'seattl', 'marinershttpplrlivestreamsonlinenetembed24', 'ad', 'overlay', 'three', 'mobl', '\\n\\n\\n', 'on', 'black', 'screen', 'mak', 'sur', 'flash', 'play', 'en', '\\n\\n\\n', 'two', 'stream', 'liv', 'fiv', 'ten'], ['sd', 'los', 'angeles', 'angels', 'vs', 'seattle', 'marinershttpplrlivestreamsonlinenetembed24', 'ad', 'overlay', 'three', 'mobile', '\\n\\n\\n', 'one', 'black', 'screen', 'make', 'sure', 'flash', 'player', 'enable', '\\n\\n\\n', 'two', 'stream', 'live', 'five', 'ten'])\n",
      "original document: \n",
      "['Someone', 'will', 'take', 'his', 'seat', 'talk', 'to', 'the', 'bride.', '', 'Make', 'sure', 'you', 'go', 'though']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['someon', 'tak', 'seat', 'talk', 'brid', 'mak', 'sur', 'go', 'though'], ['someone', 'take', 'seat', 'talk', 'bride', 'make', 'sure', 'go', 'though'])\n",
      "original document: \n",
      "['while', 'channel', 'surfing', 'I', 'came', 'across', 'the', '\"speech\"', 'part', 'of', 'this', 'protest', 'on', 'c-span..', '\\n\\nthis', 'particular', 'speaker', 'was', 'some', 'short,', 'overweight', 'and', 'unattractive', 'black', 'woman', 'bitching', 'about', 'something', '-', 'watched', 'for', 'about', '5', 'minutes..', '', 'she', 'made', 'sure', 'to', 'include', '\"white\"', 'and', '\"racist\"', '/', '\"racists\"', 'in', 'every', 'sentence.\\n\\nwhatever', 'her', 'message', 'was,', 'if', 'her', 'message', 'was', 'about', 'anything', 'else', 'other', 'than', 'all', 'white', 'people', 'are', 'racists,', 'it', 'was', 'lost', 'on', 'me.\\n\\nI', 'guess', 'if', 'she', 'believes', 'all', 'whites', 'are', 'racists,', 'then', 'I', 'can', 'believe', 'all', 'blacks', 'are', 'crybaby', 'niggers.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['channel', 'surf', 'cam', 'across', 'speech', 'part', 'protest', 'cspan', '\\n\\nthis', 'particul', 'speak', 'short', 'overweight', 'unattract', 'black', 'wom', 'bitch', 'someth', 'watch', 'fiv', 'minut', 'mad', 'sur', 'includ', 'whit', 'rac', 'rac', 'every', 'sentence\\n\\nwhatever', 'mess', 'mess', 'anyth', 'els', 'whit', 'peopl', 'rac', 'lost', 'me\\n\\ni', 'guess', 'believ', 'whit', 'rac', 'believ', 'black', 'crybaby', 'niggers\\n'], ['channel', 'surf', 'come', 'across', 'speech', 'part', 'protest', 'cspan', '\\n\\nthis', 'particular', 'speaker', 'short', 'overweight', 'unattractive', 'black', 'woman', 'bitch', 'something', 'watch', 'five', 'minutes', 'make', 'sure', 'include', 'white', 'racist', 'racists', 'every', 'sentence\\n\\nwhatever', 'message', 'message', 'anything', 'else', 'white', 'people', 'racists', 'lose', 'me\\n\\ni', 'guess', 'believe', 'white', 'racists', 'believe', 'black', 'crybaby', 'niggers\\n'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"I'm\", 'sure', 'Hillary', \"doesn't\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'hil', 'doesnt'], ['im', 'sure', 'hillary', 'doesnt'])\n",
      "original document: \n",
      "['[+Lukavian](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoo383/):\\n\\nThis', 'is', 'not', 'a', 'comforting', 'thing', 'to', 'hear', 'from', 'a', 'writer...', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lukavianhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo383\\n\\nth', 'comfort', 'thing', 'hear', 'writ'], ['lukavianhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoo383\\n\\nthis', 'comfort', 'thing', 'hear', 'writer'])\n",
      "original document: \n",
      "['You', 'see', 'it', 'when', 'you', 'read', 'old', 'stuff.', 'It', 'means', 'the', 'flu', 'and', 'flu-like', 'symptoms.', \"I've\", 'always', 'pronounced', 'it', '\"ayg\".', \"It's\", 'actually', 'pronounced', '\"ay-gyoo.\"', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'read', 'old', 'stuff', 'mean', 'flu', 'flulik', 'symptom', 'iv', 'alway', 'pronount', 'ayg', 'act', 'pronount', 'aygyoo'], ['see', 'read', 'old', 'stuff', 'mean', 'flu', 'flulike', 'symptoms', 'ive', 'always', 'pronounce', 'ayg', 'actually', 'pronounce', 'aygyoo'])\n",
      "original document: \n",
      "['at', '70,', 'all', 'of', 'it', 'can', 'be', 'burned', 'in', 'a', 'party', 'of', '4']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seventy', 'burn', 'party', 'four'], ['seventy', 'burn', 'party', 'four'])\n",
      "original document: \n",
      "['The', 'Warring', 'States', 'period', 'is', 'perfect', 'for', 'a', 'a', 'total', 'War', 'game.', \"It's\", 'a', 'super', 'interesting', 'period', 'because', 'it', 'sits', 'right', 'at', 'two', 'important', 'events', 'in', 'chinese', 'technological', 'history:', 'The', 'shift', 'from', 'bronze', 'to', 'iron,', 'and', 'the', 'introduction', 'of', 'cavalry', 'into', 'chinese', 'military', 'tactics.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['war', 'stat', 'period', 'perfect', 'tot', 'war', 'gam', 'sup', 'interest', 'period', 'sit', 'right', 'two', 'import', 'ev', 'chines', 'technolog', 'hist', 'shift', 'bronz', 'iron', 'introduc', 'cavalry', 'chines', 'milit', 'tact'], ['war', 'state', 'period', 'perfect', 'total', 'war', 'game', 'super', 'interest', 'period', 'sit', 'right', 'two', 'important', 'events', 'chinese', 'technological', 'history', 'shift', 'bronze', 'iron', 'introduction', 'cavalry', 'chinese', 'military', 'tactics'])\n",
      "original document: \n",
      "['Man', 'is', 'he', 'ever', 'lucky,', 'to', 'be', 'surrounded', 'by', 'all', 'those', 'nuns,', 'unf.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['man', 'ev', 'lucky', 'surround', 'nun', 'unf'], ['man', 'ever', 'lucky', 'surround', 'nuns', 'unf'])\n",
      "original document: \n",
      "['Fun', 'Fact:', 'If', 'you', 'stopped', 'drinking', 'water', 'the', 'day', 'after', 'Rhys', \"Hoskins'\", 'last', 'home', 'run', 'you', 'would', 'probably', 'be', 'dead', 'right', 'now.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fun', 'fact', 'stop', 'drink', 'wat', 'day', 'rhy', 'hoskin', 'last', 'hom', 'run', 'would', 'prob', 'dead', 'right'], ['fun', 'fact', 'stop', 'drink', 'water', 'day', 'rhys', 'hoskins', 'last', 'home', 'run', 'would', 'probably', 'dead', 'right'])\n",
      "original document: \n",
      "['I', 'wasn’t', 'comparing', 'the', 'US', 'system', 'to', 'the', 'Iranian', 'system.', 'I', 'was', 'simply', 'saying', 'how', 'the', 'US', 'system', 'works', 'and', 'came', 'to', 'the', 'conclusion', 'that', 'the', 'US', 'system', 'is', 'indeed', 'and', 'in', 'fact', 'broken.', 'It', 'does', 'not', 'serve', 'the', 'people.', 'It', 'serves', 'the', 'wealthy', 'and', 'influential.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wasnt', 'comp', 'us', 'system', 'ir', 'system', 'simply', 'say', 'us', 'system', 'work', 'cam', 'conclud', 'us', 'system', 'indee', 'fact', 'brok', 'serv', 'peopl', 'serv', 'wealthy', 'influ'], ['wasnt', 'compare', 'us', 'system', 'iranian', 'system', 'simply', 'say', 'us', 'system', 'work', 'come', 'conclusion', 'us', 'system', 'indeed', 'fact', 'break', 'serve', 'people', 'serve', 'wealthy', 'influential'])\n",
      "original document: \n",
      "[\"I'm\", 'using', 'a', 'large', 'step', 'stool', 'as', 'we', 'speak.', 'Laptop', 'shelf', 'on', 'top,', 'foot', 'shelf', 'below.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'us', 'larg', 'step', 'stool', 'speak', 'laptop', 'shelf', 'top', 'foot', 'shelf'], ['im', 'use', 'large', 'step', 'stool', 'speak', 'laptop', 'shelf', 'top', 'foot', 'shelf'])\n",
      "original document: \n",
      "['No', 'just', 'check', 'it', 'once', \"it's\", 'empty', 'there', 'will', 'be', 'a', 'funny', 'message', 'for', 'each', 'chest']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['check', 'empty', 'funny', 'mess', 'chest'], ['check', 'empty', 'funny', 'message', 'chest'])\n",
      "original document: \n",
      "[\"It's\", 'on', 'your', 'computer.', \"It's\", 'up', 'to', 'you', 'to', 'fix', 'it\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['comput', 'fix', 'it\\n'], ['computer', 'fix', 'it\\n'])\n",
      "original document: \n",
      "['34']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thirty-four'], ['thirty-four'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'forget', 'who', 'posted', 'it,', 'but', 'a', 'while', 'back,', 'someone', 'asked', 'a', 'similar', 'question,', 'requesting', \"people's\", 'top', '5', 'podcasts,', 'and', 'in', 'one', \"person's\", 'response,', 'they', 'put', 'The', 'Bright', 'Sessions.', '', 'I', \"hadn't\", 'heard', 'of', 'it', 'before,', 'but', 'I', 'fucking', 'loved', 'it.\\n\\n', '', \"It's\", 'basically', 'the', 'audio', 'logs', 'of', 'a', 'therapist', 'who', 'works', 'with', 'people', 'with', 'superpowers,', 'and', 'it', 'takes', 'itself', 'way', 'more', 'seriously', 'than', \"you'd\", 'expect,', 'and', \"it's\", 'so', 'much', 'better', 'for', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['forget', 'post', 'back', 'someon', 'ask', 'simil', 'quest', 'request', 'peopl', 'top', 'fiv', 'podcast', 'on', 'person', 'respons', 'put', 'bright', 'sess', 'hadnt', 'heard', 'fuck', 'lov', 'it\\n\\n', 'bas', 'audio', 'log', 'therap', 'work', 'peopl', 'superpow', 'tak', 'way', 'sery', 'youd', 'expect', 'much', 'bet'], ['forget', 'post', 'back', 'someone', 'ask', 'similar', 'question', 'request', 'people', 'top', 'five', 'podcast', 'one', 'persons', 'response', 'put', 'bright', 'sessions', 'hadnt', 'hear', 'fuck', 'love', 'it\\n\\n', 'basically', 'audio', 'log', 'therapist', 'work', 'people', 'superpowers', 'take', 'way', 'seriously', 'youd', 'expect', 'much', 'better'])\n",
      "original document: \n",
      "[\"You're\", 'right', 'it', 'is.', '\\n\\nAnd', 'I', 'messed', 'up.', 'The', \"guy's\", 'name', 'is', 'Winston.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'right', '\\n\\nand', 'mess', 'guy', 'nam', 'winston'], ['youre', 'right', '\\n\\nand', 'mess', 'guy', 'name', 'winston'])\n",
      "original document: \n",
      "['Dear', 'Pandarth_Omega,\\n\\nLuckily', 'for', 'you', '-', 'or', 'maybe', 'not', 'so', 'much', '-', 'I', 'read', 'your', 'entire', 'reply.', 'In', 'this', 'comment', 'I', 'will', 'be', 'clearing', 'some', 'things', 'up,', 'setting', 'the', 'records', 'straight,', 'while', 'replying', 'to', 'some', 'of', 'your', 'remarks', 'directly.', 'Therefore', 'I', 'will', 'quote', 'your', 'previous', 'post', 'in', 'order', 'to', 'achieve', 'full', 'transparency.', 'I', 'feel', 'we', 'need', 'this', 'in', 'both', 'of', 'our', 'lives', 'right', 'now.', 'Correct', 'me', 'if', \"I'm\", 'being', 'to', 'presumptuous.', '\\n\\n**Honest**\\n&gt;I', 'thank', 'you', 'for', 'your', 'honest', 'reply\\n\\nI', 'must', 'confess.', 'I', \"wasn't\", 'honest', 'at', 'all,', 'for', 'I', 'have', 'never', 'read', 'the', '4th', 'issue', 'of', 'The', 'Thanos', 'Imperative.', 'So', 'I', 'merely', 'guessed', 'that', 'Thanos', 'said', '\"No\"', 'in', 'this', 'issue.', 'My', 'conclusion', '-', 'as', 'you', 'have', 'correctly', 'pointed', 'out', '-', 'is', 'flawed', 'and', 'playfull', 'mockery.', '\\n\\n**Fun**\\n&gt;As', 'it', 'seems', 'you', 'have', 'not', 'quite', 'understood', 'the', 'latter', 'point,', 'let', 'me', 'put', 'it', 'to', 'you', 'as', 'plainly', 'as', 'I', 'can.', 'This', 'post', 'was', 'made', 'for', 'fun.\\n\\nWhile', 'I', 'was', 'teasing', 'you', 'with', 'my', 'clearly', 'false', 'theory', '-', 'on', 'which', 'I', 'will', 'elaborate', 'later', '-', 'it', \"wasn't\", 'my', 'intention', 'to', 'give', 'you', 'the', 'impression', 'that', 'I', 'took', 'your', 'theory', 'fully', 'serious', 'or', 'that', 'I', \"didn't\", 'enjoy', 'myself.', 'Although', 'this', 'section', \"isn't\", 'titled', '**honest**,', 'I', 'still', 'believe', 'I', 'am', 'when', 'I', 'state', 'that', 'my', 'opening', 'sentence', 'was', 'fully', 'sincere.', 'I', 'shall', 'quote', 'myself', 'here', '-', 'one', 'thing', 'I', 'always', 'enjoy', 'doing', '-', 'to', 'show', 'you', 'to', 'what', 'sentence', \"I'm\", 'referring:', '\"I', 'intensely', 'enjoyed', 'reading', 'your', '-', 'how', 'shall', 'I', 'call', 'it?', '-', 'Theory.\"', 'Your', 'writing', 'style,', 'wit', 'and', 'the', 'theory', 'itself', 'are', 'entertaining.', 'I', 'was', 'merely', 'inspired', 'by', 'your', 'post', 'to', 'reply', 'with', 'another', 'theory.', 'And', 'since', 'I', 'do', 'not', 'believe', 'in', 'the', 'meaning', 'you', 'get', 'from', 'those', 'numbers', '(D23),', 'I', 'decided', 'to', 'exaggerate', 'in', 'an', 'even', 'more', 'far-fetched', 'theory.', '\\n\\n**I', 'told', 'you', 'so**\\n&gt;your', 'telling', 'me', '\"I', 'told', 'you', 'so\"', 'has', 'no', 'meaning\\n\\nIn', 'my', 'modest', 'opinion.', 'I', 'told', 'you', 'so', 'always', 'has', 'meaning.', 'If', 'nothing', 'else,', 'it', 'may', 'hint', 'at', 'a', 'character', 'flaw', 'of', 'the', 'one', 'posing', 'this', 'statement.', '\\n\\n**My', 'clearly', 'false', 'theory**\\n&gt;I', 'can', 'only', 'imagine', 'how', 'long', 'it', 'took', 'you', 'to', 'accomplish', 'this.\\n\\nIt', 'took', 'me', '10', 'minutes.', 'I', 'think.', '\\n\\n&gt;', 'Your', 'theory', 'was', 'built', 'on', 'three', 'numbers', 'which', 'were', 'never', 'shown', 'in', 'the', 'film;', 'to', 'discover', 'them', 'one', 'would', 'have', 'to', 'pay', 'very', 'close', 'attention', 'to', 'the', 'film', 'and,', 'likely', 'as', 'not,', 'watch', 'it', 'multiple', 'times.', '\\n\\nI', 'googled', '\"Captain', 'America', 'Winter', 'Soldier', 'Numbers\"', 'in', 'order', 'to', 'get', 'some', 'data', 'quickly.', 'I', 'quickly', 'found', 'this', 'picture:', 'https://i.pinimg.com/originals/a9/8f/9b/a98f9bce6365417da8056ae3339d8e7c.jpg\\nI', 'choose', 'three', 'numbers', '-', 'which', 'only', 'had', 'in', 'common', 'that', 'Captain', 'America', 'himself', 'was', 'involved', '-', 'and', 'tried', 'googling', 'some', 'dates.', 'Luckily', 'I', 'found', 'that', 'The', 'Thanos', 'Imperative', 'was', 'released', 'on', 'one', 'of', 'the', 'possible', 'dates.', 'As', 'you', 'can', 'see,', 'this', 'is', 'no', 'achievement', 'at', 'all.', 'Of', 'course,', 'while', 'having', 'fun,', 'I', 'was', 'also', 'trying', 'to', 'point', 'out', 'that', 'giving', 'meaning', 'to', 'numbers', \"isn't\", 'that', 'difficult.', 'And', 'while', 'I', 'will', 'admit,', 'the', 'numbers', 'in', 'your', 'theory', 'have', 'a', 'clearer', 'connection', 'to', 'each', 'other,', 'I', 'still', 'think', 'it', \"isn't\", 'a', 'meaningful', 'one.', 'I', 'cannot', 'disprove', 'this', '-', 'and', 'since', \"you're\", 'not', 'that', 'serious', 'about', 'it,', 'I', 'probably', \"won't\", 'have', 'to', '-', 'but', 'I', 'simply', 'highly,', 'highly', 'doubt', 'a', 'director', 'or', 'marketing', 'team', 'would', 'put', 'in', 'coded', 'messages', 'about', 'trailer', 'drops', 'for', 'a', 'movie', 'that', 'far', 'ahead', 'of', 'time.', 'It', 'is', 'way', 'more', 'likely', 'that', 'this', 'is', 'just', 'coincidence', '(if', 'the', 'trailer', 'does', 'drop', 'at', 'October', '14th).', '\\n\\n**The', 'Trailer**\\n&gt;As', 'I', 'mentioned', 'in', 'my', 'original', 'post,', 'a', 'quick', 'look', 'through', 'recent', 'activity', 'will', 'reveal', 'that', 'the', 'most', 'popular', 'answer', 'is', '\"sometime', 'in', 'October\"\\n\\nThis', 'is', 'likely.', 'I', 'think', 'the', 'trailer', 'will', 'drop', 'around', 'Thor:', 'Ragnarok.', 'This', 'strategy', 'will', 'remind', 'moviegoers', 'that', 'the', 'Thor', 'film', 'might', 'be', 'worth', 'checking', 'out', 'because', \"he's\", 'in', 'Infinity', 'War.', 'And', 'we', 'all', 'want', 'to', 'know', 'which', 'events', 'will', 'lead-up', 'to', 'that', 'movie.', 'But', 'this', 'is', 'merely', 'a', 'prediction', 'of', 'Marvels', 'overall', 'strategy.', 'You', 'probably', \"wouldn't\", 'be', 'surprised', 'at', 'this', 'point', 'that', 'I', 'too', 'think', 'the', 'Infinity', 'War', 'trailer', 'will', 'be', 'dropped', 'at', 'the', 'end', 'of', 'October', 'or', 'the', 'beginning', 'of', 'November.', '\"Around', 'October\"', 'sounds', 'accurate', 'to', 'me.', \"\\n\\nu/AdmiralSnackBar69's\", 'analysis', 'is', 'therefore', 'much', 'more', 'reliable', 'then', 'your', 'fun-to-read-theory.', 'He', \"doesn't\", 'look', 'at', 'a', 'coded', 'message', '-', 'which', 'we', 'just', 'entertainingly', 'assume', 'it', 'might', 'be', 'a', 'coded', 'message', '-', 'but', 'he', 'looks', 'at', 'production', 'trends.', '\\n\\n**Closing**\\n\\nIt', 'was', 'never', 'my', 'intend', 'to', 'be', 'hurtful', 'or', 'mock', 'you', 'as', 'an', 'author.', 'I', 'really', 'enjoyed', 'reading', 'your', 'text.', 'So', 'maybe', 'I', \"didn't\", 'formulate', 'my', 'reply', 'as', 'carefully', 'as', 'I', 'could', 'have.', 'English', \"isn't\", 'my', 'first', 'language,', 'so', 'communicating', 'can', 'be', 'difficult', 'sometimes.', 'That', 'being', 'said,', 'I', \"don't\", 'feel', 'you', 'took', 'it', 'the', 'wrong', 'way.', 'Your', 'reply', 'was', 'equally', 'entertaining.', 'This', 'exchange', 'of', 'simple', 'words', \"we've\", 'had', 'today', 'was', 'so', 'fun', 'and', 'inspiring,', 'that', 'I', 'think', 'we', 'should', 'become', 'electronic', 'pen', 'pals.', '\\n\\nTo', 'summarize:', '\\n\\n-', 'I', 'had', 'fun', 'reading', 'your', 'original', 'post', 'and', 'your', 'reply.', '\\n\\n-', 'I', 'was', 'fully', 'aware', 'you', 'were', 'not', 'entirely', 'serious.', '\\n\\n-', 'My', 'theory', 'was', 'very', 'much', 'incorrect.', '\\n\\n-', 'I', 'think', 'a', 'trailer', 'will', 'drop', 'in', 'October/November.\\n\\n-', 'While', 'I', 'was', 'mocking,', 'I', 'hope', 'you', 'know', 'by', 'now', 'it', \"wasn't\", 'meant', 'to', 'be', 'hurtful,', 'personal', 'or', 'hateful.', '\\n\\n\\nHopefully,', 'I', 'cleared', 'some', 'things', 'up.', 'If', 'not,', 'if', 'everything', 'I', 'said', 'you', 'already', 'knew', 'or', 'assumed,', 'then', 'consider', 'my', 'post', 'simply', 'as', 'meaningless', 'rambling,', 'attention', 'seeking', 'or', 'a', 'waste', 'of', 'your', 'valuable', 'time.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dear', 'pandarth_omega\\n\\nluckily', 'mayb', 'much', 'read', 'entir', 'reply', 'com', 'clear', 'thing', 'set', 'record', 'straight', 'reply', 'remark', 'direct', 'theref', 'quot', 'prevy', 'post', 'ord', 'achiev', 'ful', 'transp', 'feel', 'nee', 'liv', 'right', 'correct', 'im', 'presumptu', '\\n\\nhonest\\ngti', 'thank', 'honest', 'reply\\n\\ni', 'must', 'confess', 'wasnt', 'honest', 'nev', 'read', '4th', 'issu', 'thano', 'imp', 'mer', 'guess', 'thano', 'said', 'issu', 'conclud', 'correct', 'point', 'flaw', 'playful', 'mockery', '\\n\\nfun\\ngtas', 'seem', 'quit', 'understood', 'lat', 'point', 'let', 'put', 'plain', 'post', 'mad', 'fun\\n\\nwhile', 'teas', 'clear', 'fals', 'the', 'elab', 'lat', 'wasnt', 'int', 'giv', 'impress', 'took', 'the', 'ful', 'sery', 'didnt', 'enjoy', 'although', 'sect', 'isnt', 'titl', 'honest', 'stil', 'believ', 'stat', 'op', 'sent', 'ful', 'sint', 'shal', 'quot', 'on', 'thing', 'alway', 'enjoy', 'show', 'sent', 'im', 'refer', 'intens', 'enjoy', 'read', 'shal', 'cal', 'the', 'writ', 'styl', 'wit', 'the', 'entertain', 'mer', 'inspir', 'post', 'reply', 'anoth', 'the', 'sint', 'believ', 'mean', 'get', 'numb', 'd23', 'decid', 'exag', 'ev', 'farfetch', 'the', '\\n\\ni', 'told', 'so\\ngtyour', 'tel', 'told', 'meaning\\n\\nin', 'modest', 'opin', 'told', 'alway', 'mean', 'noth', 'els', 'may', 'hint', 'charact', 'flaw', 'on', 'pos', 'stat', '\\n\\nmy', 'clear', 'fals', 'theory\\ngti', 'imagin', 'long', 'took', 'accompl', 'this\\n\\nit', 'took', 'ten', 'minut', 'think', '\\n\\ngt', 'the', 'built', 'three', 'numb', 'nev', 'shown', 'film', 'discov', 'on', 'would', 'pay', 'clos', 'at', 'film', 'lik', 'watch', 'multipl', 'tim', '\\n\\ni', 'googl', 'captain', 'americ', 'wint', 'soldy', 'numb', 'ord', 'get', 'dat', 'quick', 'quick', 'found', 'pict', 'httpsipinimgcomoriginalsa98f9ba98f9bce6365417da8056ae3339d8e7cjpg\\ni', 'choos', 'three', 'numb', 'common', 'captain', 'americ', 'involv', 'tri', 'googl', 'dat', 'lucky', 'found', 'thano', 'imp', 'releas', 'on', 'poss', 'dat', 'see', 'achiev', 'cours', 'fun', 'also', 'try', 'point', 'giv', 'mean', 'numb', 'isnt', 'difficult', 'admit', 'numb', 'the', 'clear', 'connect', 'stil', 'think', 'isnt', 'mean', 'on', 'cannot', 'disprov', 'sint', 'yo', 'sery', 'prob', 'wont', 'simply', 'high', 'high', 'doubt', 'direct', 'market', 'team', 'would', 'put', 'cod', 'mess', 'trail', 'drop', 'movy', 'far', 'ahead', 'tim', 'way', 'lik', 'coincid', 'trail', 'drop', 'octob', '14th', '\\n\\nthe', 'trailer\\ngtas', 'ment', 'origin', 'post', 'quick', 'look', 'rec', 'act', 'rev', 'popul', 'answ', 'sometim', 'october\\n\\nthis', 'lik', 'think', 'trail', 'drop', 'around', 'thor', 'ragnarok', 'strategy', 'remind', 'moviego', 'thor', 'film', 'might', 'wor', 'check', 'hes', 'infin', 'war', 'want', 'know', 'ev', 'leadup', 'movy', 'mer', 'predict', 'marvel', 'overal', 'strategy', 'prob', 'wouldnt', 'surpr', 'point', 'think', 'infin', 'war', 'trail', 'drop', 'end', 'octob', 'begin', 'novemb', 'around', 'octob', 'sound', 'acc', '\\n\\nuadmiralsnackbar69s', 'analys', 'theref', 'much', 'rely', 'funtoreadth', 'doesnt', 'look', 'cod', 'mess', 'entertain', 'assum', 'might', 'cod', 'mess', 'look', 'produc', 'trend', '\\n\\nclosing\\n\\nit', 'nev', 'intend', 'hurt', 'mock', 'auth', 'real', 'enjoy', 'read', 'text', 'mayb', 'didnt', 'form', 'reply', 'car', 'could', 'engl', 'isnt', 'first', 'langu', 'commun', 'difficult', 'sometim', 'said', 'dont', 'feel', 'took', 'wrong', 'way', 'reply', 'eq', 'entertain', 'exchang', 'simpl', 'word', 'wev', 'today', 'fun', 'inspir', 'think', 'becom', 'electron', 'pen', 'pal', '\\n\\nto', 'summ', '\\n\\n', 'fun', 'read', 'origin', 'post', 'reply', '\\n\\n', 'ful', 'aw', 'entir', 'sery', '\\n\\n', 'the', 'much', 'incorrect', '\\n\\n', 'think', 'trail', 'drop', 'octobernovember\\n\\n', 'mock', 'hop', 'know', 'wasnt', 'meant', 'hurt', 'person', 'hat', '\\n\\n\\nhopefully', 'clear', 'thing', 'everyth', 'said', 'already', 'knew', 'assum', 'consid', 'post', 'simply', 'meaningless', 'rambl', 'at', 'seek', 'wast', 'valu', 'tim'], ['dear', 'pandarth_omega\\n\\nluckily', 'maybe', 'much', 'read', 'entire', 'reply', 'comment', 'clear', 'things', 'set', 'record', 'straight', 'reply', 'remark', 'directly', 'therefore', 'quote', 'previous', 'post', 'order', 'achieve', 'full', 'transparency', 'feel', 'need', 'live', 'right', 'correct', 'im', 'presumptuous', '\\n\\nhonest\\ngti', 'thank', 'honest', 'reply\\n\\ni', 'must', 'confess', 'wasnt', 'honest', 'never', 'read', '4th', 'issue', 'thanos', 'imperative', 'merely', 'guess', 'thanos', 'say', 'issue', 'conclusion', 'correctly', 'point', 'flaw', 'playfull', 'mockery', '\\n\\nfun\\ngtas', 'seem', 'quite', 'understand', 'latter', 'point', 'let', 'put', 'plainly', 'post', 'make', 'fun\\n\\nwhile', 'tease', 'clearly', 'false', 'theory', 'elaborate', 'later', 'wasnt', 'intention', 'give', 'impression', 'take', 'theory', 'fully', 'serious', 'didnt', 'enjoy', 'although', 'section', 'isnt', 'title', 'honest', 'still', 'believe', 'state', 'open', 'sentence', 'fully', 'sincere', 'shall', 'quote', 'one', 'thing', 'always', 'enjoy', 'show', 'sentence', 'im', 'refer', 'intensely', 'enjoy', 'read', 'shall', 'call', 'theory', 'write', 'style', 'wit', 'theory', 'entertain', 'merely', 'inspire', 'post', 'reply', 'another', 'theory', 'since', 'believe', 'mean', 'get', 'number', 'd23', 'decide', 'exaggerate', 'even', 'farfetched', 'theory', '\\n\\ni', 'tell', 'so\\ngtyour', 'tell', 'tell', 'meaning\\n\\nin', 'modest', 'opinion', 'tell', 'always', 'mean', 'nothing', 'else', 'may', 'hint', 'character', 'flaw', 'one', 'pose', 'statement', '\\n\\nmy', 'clearly', 'false', 'theory\\ngti', 'imagine', 'long', 'take', 'accomplish', 'this\\n\\nit', 'take', 'ten', 'minutes', 'think', '\\n\\ngt', 'theory', 'build', 'three', 'number', 'never', 'show', 'film', 'discover', 'one', 'would', 'pay', 'close', 'attention', 'film', 'likely', 'watch', 'multiple', 'time', '\\n\\ni', 'google', 'captain', 'america', 'winter', 'soldier', 'number', 'order', 'get', 'data', 'quickly', 'quickly', 'find', 'picture', 'httpsipinimgcomoriginalsa98f9ba98f9bce6365417da8056ae3339d8e7cjpg\\ni', 'choose', 'three', 'number', 'common', 'captain', 'america', 'involve', 'try', 'google', 'date', 'luckily', 'find', 'thanos', 'imperative', 'release', 'one', 'possible', 'date', 'see', 'achievement', 'course', 'fun', 'also', 'try', 'point', 'give', 'mean', 'number', 'isnt', 'difficult', 'admit', 'number', 'theory', 'clearer', 'connection', 'still', 'think', 'isnt', 'meaningful', 'one', 'cannot', 'disprove', 'since', 'youre', 'serious', 'probably', 'wont', 'simply', 'highly', 'highly', 'doubt', 'director', 'market', 'team', 'would', 'put', 'cod', 'message', 'trailer', 'drop', 'movie', 'far', 'ahead', 'time', 'way', 'likely', 'coincidence', 'trailer', 'drop', 'october', '14th', '\\n\\nthe', 'trailer\\ngtas', 'mention', 'original', 'post', 'quick', 'look', 'recent', 'activity', 'reveal', 'popular', 'answer', 'sometime', 'october\\n\\nthis', 'likely', 'think', 'trailer', 'drop', 'around', 'thor', 'ragnarok', 'strategy', 'remind', 'moviegoers', 'thor', 'film', 'might', 'worth', 'check', 'hes', 'infinity', 'war', 'want', 'know', 'events', 'leadup', 'movie', 'merely', 'prediction', 'marvel', 'overall', 'strategy', 'probably', 'wouldnt', 'surprise', 'point', 'think', 'infinity', 'war', 'trailer', 'drop', 'end', 'october', 'begin', 'november', 'around', 'october', 'sound', 'accurate', '\\n\\nuadmiralsnackbar69s', 'analysis', 'therefore', 'much', 'reliable', 'funtoreadtheory', 'doesnt', 'look', 'cod', 'message', 'entertainingly', 'assume', 'might', 'cod', 'message', 'look', 'production', 'trend', '\\n\\nclosing\\n\\nit', 'never', 'intend', 'hurtful', 'mock', 'author', 'really', 'enjoy', 'read', 'text', 'maybe', 'didnt', 'formulate', 'reply', 'carefully', 'could', 'english', 'isnt', 'first', 'language', 'communicate', 'difficult', 'sometimes', 'say', 'dont', 'feel', 'take', 'wrong', 'way', 'reply', 'equally', 'entertain', 'exchange', 'simple', 'word', 'weve', 'today', 'fun', 'inspire', 'think', 'become', 'electronic', 'pen', 'pal', '\\n\\nto', 'summarize', '\\n\\n', 'fun', 'read', 'original', 'post', 'reply', '\\n\\n', 'fully', 'aware', 'entirely', 'serious', '\\n\\n', 'theory', 'much', 'incorrect', '\\n\\n', 'think', 'trailer', 'drop', 'octobernovember\\n\\n', 'mock', 'hope', 'know', 'wasnt', 'mean', 'hurtful', 'personal', 'hateful', '\\n\\n\\nhopefully', 'clear', 'things', 'everything', 'say', 'already', 'know', 'assume', 'consider', 'post', 'simply', 'meaningless', 'ramble', 'attention', 'seek', 'waste', 'valuable', 'time'])\n",
      "original document: \n",
      "['Will', 'Butch', 'Jones', 'be', 'fired', 'by', 'the', 'end', 'of', 'the', 'Bama', 'game?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['butch', 'jon', 'fir', 'end', 'bam', 'gam'], ['butch', 'jones', 'fire', 'end', 'bama', 'game'])\n",
      "original document: \n",
      "['Who', 'said', 'you', 'had', 'to', 'put', 'up', 'with', 'it?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'put'], ['say', 'put'])\n",
      "original document: \n",
      "['plenty', 'of', 'hall', 'of', 'famers', 'in', 'action', 'tonight,', 'count', 'me', 'in']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['plenty', 'hal', 'fam', 'act', 'tonight', 'count'], ['plenty', 'hall', 'famers', 'action', 'tonight', 'count'])\n",
      "original document: \n",
      "['Your', 'post', '(probably)', \"hasn't\", 'broken', 'any', 'rules,', 'but', 'we', 'see', 'these', 'kinds', 'of', 'things', 'a', 'lot.', 'Look', 'at', 'our', '[most', 'overdone', 'items', 'here](https://www.reddit.com/r/mildlyinteresting/search?q=flair%3A%28overdone%29&amp;sort=new&amp;restrict_sr=on&amp;t=all)\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/mildlyinteresting)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'prob', 'hasnt', 'brok', 'rul', 'see', 'kind', 'thing', 'lot', 'look', 'overdon', 'item', 'herehttpswwwredditcomrmildlyinterestingsearchqflair3a28overdone29ampsortnewamprestrict_sronamptall\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetormildlyinterest', 'quest', 'concern'], ['post', 'probably', 'hasnt', 'break', 'rule', 'see', 'kinds', 'things', 'lot', 'look', 'overdo', 'items', 'herehttpswwwredditcomrmildlyinterestingsearchqflair3a28overdone29ampsortnewamprestrict_sronamptall\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetormildlyinteresting', 'question', 'concern'])\n",
      "original document: \n",
      "['agree', 'with', 'this.', 'i', 'mean', 'right', 'now', 'hes', 'on', 'pace', 'for', '2,800', 'all', 'purpose', 'yards,', 'about', '50', 'receptions,', 'and', 'over', '30', 'tds.', 'This', 'is', 'ridiculous', 'and', 'impossible', 'basically.', 'I', 'think', 'Hunt', 'owners', 'realize', 'that,', 'and', 'are', 'expecting', 'big', 'games', 'from', 'him,', 'but', 'tempering', 'those', 'expectations', 'a', 'little.', '\\n\\nThink', 'i', 'read', 'somewhere', 'that', 'they', 'estimated', 'regression', 'to', 'maybe', '1200', 'yards', 'the', 'rest', 'of', 'the', 'season', '(which', 'still', 'leaves', 'him', 'with', 'over', '1700),', 'and', 'something', 'like', '14', \"td's.\", 'those', 'are', 'STILL', 'ridiculous', 'numbers,', 'so', 'even', 'if', 'he', 'comes', 'close', 'to', 'that', 'he', 'is', 'worth', 'holding', 'onto,', 'or', 'trading', 'for', 'someone', 'great.\\n\\nI', 'may', 'regret', 'it,', 'but', 'im', 'gonna', 'hold', 'onto', 'him.', 'People', 'were', 'saying', 'to', 'trade', 'DJ', 'while', 'his', 'value', 'was', 'up', 'so', 'high', 'after', 'the', 'first', 'few', 'weeks', 'he', 'took', 'over,', 'and', 'now', 'those', 'people', 'look', 'silly.', 'he', 'could', 'fall', 'off,', 'sure.', 'but', 'he', 'could', 'take', 'you', 'to', 'the', 'ship', 'as', 'well.', 'Im', 'all', 'for', 'the', 'solid,', 'respectable', 'numbers']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'mean', 'right', 'hes', 'pac', 'two thousand, eight hundred', 'purpos', 'yard', 'fifty', 'receiv', 'thirty', 'tds', 'ridic', 'imposs', 'bas', 'think', 'hunt', 'own', 'real', 'expect', 'big', 'gam', 'temp', 'expect', 'littl', '\\n\\nthink', 'read', 'somewh', 'estim', 'regress', 'mayb', 'one thousand, two hundred', 'yard', 'rest', 'season', 'stil', 'leav', 'one thousand, seven hundred', 'someth', 'lik', 'fourteen', 'tds', 'stil', 'ridic', 'numb', 'ev', 'com', 'clos', 'wor', 'hold', 'onto', 'trad', 'someon', 'great\\n\\ni', 'may', 'regret', 'im', 'gonn', 'hold', 'onto', 'peopl', 'say', 'trad', 'dj', 'valu', 'high', 'first', 'week', 'took', 'peopl', 'look', 'sil', 'could', 'fal', 'sur', 'could', 'tak', 'ship', 'wel', 'im', 'solid', 'respect', 'numb'], ['agree', 'mean', 'right', 'hes', 'pace', 'two thousand, eight hundred', 'purpose', 'yards', 'fifty', 'receptions', 'thirty', 'tds', 'ridiculous', 'impossible', 'basically', 'think', 'hunt', 'owners', 'realize', 'expect', 'big', 'game', 'temper', 'expectations', 'little', '\\n\\nthink', 'read', 'somewhere', 'estimate', 'regression', 'maybe', 'one thousand, two hundred', 'yards', 'rest', 'season', 'still', 'leave', 'one thousand, seven hundred', 'something', 'like', 'fourteen', 'tds', 'still', 'ridiculous', 'number', 'even', 'come', 'close', 'worth', 'hold', 'onto', 'trade', 'someone', 'great\\n\\ni', 'may', 'regret', 'im', 'gonna', 'hold', 'onto', 'people', 'say', 'trade', 'dj', 'value', 'high', 'first', 'weeks', 'take', 'people', 'look', 'silly', 'could', 'fall', 'sure', 'could', 'take', 'ship', 'well', 'im', 'solid', 'respectable', 'number'])\n",
      "original document: \n",
      "['\"We', 'chose', 'to', 'paint', 'the', 'guest', 'room', \"Mama's\", 'Cameo', 'with', 'an', 'accent', 'of', \"Madeline's\", 'Muff.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chos', 'paint', 'guest', 'room', 'mama', 'cameo', 'acc', 'madelin', 'muff'], ['choose', 'paint', 'guest', 'room', 'mamas', 'cameo', 'accent', 'madelines', 'muff'])\n",
      "original document: \n",
      "['All', 'you', 'have', 'to', 'do', 'to', 'be', 'eligible', 'for', 'the', 'rate', 'stats', 'are', 'average', '3ABs', 'over', '162', 'games.', 'Which', 'is', 'reasonable.', 'One', 'could', 'argue', 'that', 'if', 'Trout', \"wasn't\", 'hurt', 'he', 'would', 'have', 'a', 'larger', 'lead', 'based', 'on', 'his', 'previous', 'seasons....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['elig', 'rat', 'stat', 'av', '3abs', 'one hundred and sixty-two', 'gam', 'reason', 'on', 'could', 'argu', 'trout', 'wasnt', 'hurt', 'would', 'larg', 'lead', 'bas', 'prevy', 'season'], ['eligible', 'rate', 'stats', 'average', '3abs', 'one hundred and sixty-two', 'game', 'reasonable', 'one', 'could', 'argue', 'trout', 'wasnt', 'hurt', 'would', 'larger', 'lead', 'base', 'previous', 'season'])\n",
      "original document: \n",
      "['Yours', \"don't\", 'look', 'like', 'this?', 'http://i.imgur.com/dOupf.jpg']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'look', 'lik', 'httpiimgurcomdoupfjpg'], ['dont', 'look', 'like', 'httpiimgurcomdoupfjpg'])\n",
      "original document: \n",
      "['I', 'domt', 'entirely', 'trust', 'it.', 'Thats', 'an', 'extremely', 'gated', 'video.', 'You', 'dont', 'get', 'to', 'see', 'it', 'start', 'up', 'cpu', 'intensive', 'programs,', 'you', 'dont', 'get', 'to', 'see', 'them', 'do', 'anything', 'intensive', 'either.', 'It', 'reminds', 'me', 'of', 'the', 'demos', 'apple', 'used', 'to', 'pull,', 'when', 'things', 'didnt', 'have', 'real', 'full', 'functionality', 'yet.', 'Gimme', 'more', 'information', 'about', \"what's\", 'running', 'that,', 'let', 'me', 'see', 'it', 'in', 'action', 'entirely.', 'How', 'hot', 'is', 'it', 'getting,', \"what's\", 'the', 'cooling', 'system,', 'and', 'how', 'is', 'the', 'battery', 'affected?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['domt', 'entir', 'trust', 'that', 'extrem', 'gat', 'video', 'dont', 'get', 'see', 'start', 'cpu', 'intend', 'program', 'dont', 'get', 'see', 'anyth', 'intend', 'eith', 'remind', 'demo', 'appl', 'us', 'pul', 'thing', 'didnt', 'real', 'ful', 'funct', 'yet', 'gim', 'inform', 'what', 'run', 'let', 'see', 'act', 'entir', 'hot', 'get', 'what', 'cool', 'system', 'battery', 'affect'], ['domt', 'entirely', 'trust', 'thats', 'extremely', 'gate', 'video', 'dont', 'get', 'see', 'start', 'cpu', 'intensive', 'program', 'dont', 'get', 'see', 'anything', 'intensive', 'either', 'remind', 'demo', 'apple', 'use', 'pull', 'things', 'didnt', 'real', 'full', 'functionality', 'yet', 'gimme', 'information', 'whats', 'run', 'let', 'see', 'action', 'entirely', 'hot', 'get', 'whats', 'cool', 'system', 'battery', 'affect'])\n",
      "original document: \n",
      "['He', 'was', 'about', 'to', 'OBJ', 'that', 'shit', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['obs', 'shit'], ['obj', 'shit'])\n",
      "original document: \n",
      "['Props', 'to', 'the', 'fort', 'tonight,', 'chants', 'have', 'been', '🔥']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['prop', 'fort', 'tonight', 'chant'], ['prop', 'fort', 'tonight', 'chant'])\n",
      "original document: \n",
      "['[I', 'think', 'Shiggy', 'is', 'the', 'most', 'relate-able', 'though](https://i.imgur.com/6gMD049.png)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'shiggy', 'rel', 'thoughhttpsiimgurcom6gmd049png'], ['think', 'shiggy', 'relateable', 'thoughhttpsiimgurcom6gmd049png'])\n",
      "original document: \n",
      "['IDK', 'what', 'press', 'you', 'are', 'using', 'but', 'the', 'one', 'time', 'I', 'did', 'this', 'I', 'left', 'the', 'die', 'in', 'place,', 'raised', 'the', 'ram', 'all', 'the', 'way', 'up', 'and', 'then', 'put', 'the', 'proper', 'shell', 'plate', 'over', 'the', 'shell', 'while', 'simultaneously', 'attaching', 'it', 'to', 'the', 'ram.', '', 'Lowered', 'ram', 'and', 'yanked', 'the', 'shell', 'out.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['idk', 'press', 'us', 'on', 'tim', 'left', 'die', 'plac', 'rais', 'ram', 'way', 'put', 'prop', 'shel', 'plat', 'shel', 'simult', 'attach', 'ram', 'low', 'ram', 'yank', 'shel'], ['idk', 'press', 'use', 'one', 'time', 'leave', 'die', 'place', 'raise', 'ram', 'way', 'put', 'proper', 'shell', 'plate', 'shell', 'simultaneously', 'attach', 'ram', 'lower', 'ram', 'yank', 'shell'])\n",
      "original document: \n",
      "['How', 'can', 'you', 'determine', 'if', 'the', 'internet', 'is', 'worth', 'a', 'damn', 'before', 'you', 'move', 'in', 'somewhere?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['determin', 'internet', 'wor', 'damn', 'mov', 'somewh'], ['determine', 'internet', 'worth', 'damn', 'move', 'somewhere'])\n",
      "original document: \n",
      "['But,', \"it's\", 'so', 'weird', 'to', 'see', 'him', 'at', \"Sam's\", 'Club', 'in', 'Logan.', 'I', 'feel', 'like', 'I', 'need', 'to', 'hide', 'until', 'I', 'can', 'get', 'some', 'fig', 'leaves.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weird', 'see', 'sam', 'club', 'log', 'feel', 'lik', 'nee', 'hid', 'get', 'fig', 'leav'], ['weird', 'see', 'sams', 'club', 'logan', 'feel', 'like', 'need', 'hide', 'get', 'fig', 'leave'])\n",
      "original document: \n",
      "['Anti', 'bethesda', 'AND', 'pro', 'CDPR', 'circlejerk', 'all', 'in', 'one', 'comment!\\n\\nDing', 'ding', 'ding!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ant', 'bethesd', 'pro', 'cdpr', 'circlejerk', 'on', 'comment\\n\\nding', 'ding', 'ding'], ['anti', 'bethesda', 'pro', 'cdpr', 'circlejerk', 'one', 'comment\\n\\nding', 'ding', 'ding'])\n",
      "original document: \n",
      "['Mixing', 'fighting', 'tired', 'and', 'fighting', 'fresh', 'is', 'suboptimal.', 'Huh.', 'I', 'hope', 'for', 'your', 'sake', 'your', 'opponents', 'think', 'the', 'same', 'thing', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mix', 'fight', 'tir', 'fight', 'fresh', 'suboptim', 'huh', 'hop', 'sak', 'oppon', 'think', 'thing'], ['mix', 'fight', 'tire', 'fight', 'fresh', 'suboptimal', 'huh', 'hope', 'sake', 'opponents', 'think', 'thing'])\n",
      "original document: \n",
      "['Okay', 'thanks.', \"I'm\", 'in', 'phx', 'so', \"won't\", 'get', 'my', 'hopes', 'up', 'but', 'hopefully', 'will', 'be', 'pleasantly', 'surprised!', '\\n\\nHas', 'anyone', 'figured', 'out', 'if', 'these', 'are', 'just', 'repackaged', 'Gardein', 'or', 'other', 'brand?', \"They've\", 'been', 'out', 'of', 'stock', 'for', 'sooo', 'long', 'I', \"can't\", 'recall.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'thank', 'im', 'phx', 'wont', 'get', 'hop', 'hop', 'pleas', 'surpr', '\\n\\nhas', 'anyon', 'fig', 'repack', 'gardein', 'brand', 'theyv', 'stock', 'sooo', 'long', 'cant', 'recal'], ['okay', 'thank', 'im', 'phx', 'wont', 'get', 'hop', 'hopefully', 'pleasantly', 'surprise', '\\n\\nhas', 'anyone', 'figure', 'repackaged', 'gardein', 'brand', 'theyve', 'stock', 'sooo', 'long', 'cant', 'recall'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop508/):\\n\\nRight?', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop508\\n\\nright', 'lol'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop508\\n\\nright', 'lol'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['907']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nine hundred and seven'], ['nine hundred and seven'])\n",
      "original document: \n",
      "['Even', 'better,', 'involve', 'her,', 'let', 'HER', 'send', 'out', 'the', 'invitations', 'to', 'those', 'people.', 'Make', 'her', 'think', 'its', 'about', 'making', 'amends', 'to', 'HER,', 'that', \"it's\", 'about', 'HER.', '\\n\\nThat', 'way', 'when', 'DH', 'does', 'the', 'mic', 'drop', 'moment', 'with', 'a', 'speech', 'ending', '\"And', 'of', 'course', \"I'd\", 'like', 'to', 'thank', 'my', 'mother', 'for', 'making', 'all', 'of', 'this', 'possible,', 'after', 'all', 'if', 'it', \"wasn't\", 'for', 'her', 'lies', 'about', 'us', 'having', 'a', 'shotgun', 'wedding', 'and', 'a', 'miscarriage', 'none', 'of', 'us', 'would', 'be', 'here', 'tonight\"', 'it', \"hit's\", 'her', 'fully.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'bet', 'involv', 'let', 'send', 'invit', 'peopl', 'mak', 'think', 'mak', 'amend', '\\n\\nthat', 'way', 'dh', 'mic', 'drop', 'mom', 'speech', 'end', 'cours', 'id', 'lik', 'thank', 'moth', 'mak', 'poss', 'wasnt', 'lie', 'us', 'shotgun', 'wed', 'miscarry', 'non', 'us', 'would', 'tonight', 'hit', 'ful'], ['even', 'better', 'involve', 'let', 'send', 'invitations', 'people', 'make', 'think', 'make', 'amend', '\\n\\nthat', 'way', 'dh', 'mic', 'drop', 'moment', 'speech', 'end', 'course', 'id', 'like', 'thank', 'mother', 'make', 'possible', 'wasnt', 'lie', 'us', 'shotgun', 'wed', 'miscarriage', 'none', 'us', 'would', 'tonight', 'hit', 'fully'])\n",
      "original document: \n",
      "['Glorious', 'rack', 'too']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['glory', 'rack'], ['glorious', 'rack'])\n",
      "original document: \n",
      "['I', 'think', 'Kentaro', 'will', 'continue', 'to', 'get', 'better', 'and', 'challenge', 'Brandon', 'at', 'the', 'end.', '', 'None', 'of', 'the', 'others', 'are', 'consistently', 'good', 'enough', 'or', 'have', 'a', 'newer', 'style', 'that', 'pleases', 'the', 'judges.', '', 'Margarita', 'may', 'be', 'a', 'surprise', 'if', 'they', 'are', 'considering', 'JC', 'Penney', 'styling.', '', 'Kenya', 'is', 'a', 'little', 'too', 'old', 'fashioned.', '', 'I', 'am', 'old', 'and', '', 'I', 'like', 'many', 'of', 'her', 'designs', 'but', 'again,', 'I', 'am', 'old', 'and', 'designers', \"don't\", 'design', 'for', 'baby', 'boomers.', '', 'Same', 'reasoning', 'applies', 'for', 'Amy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'kentaro', 'continu', 'get', 'bet', 'challeng', 'brandon', 'end', 'non', 'oth', 'consist', 'good', 'enough', 'new', 'styl', 'pleas', 'judg', 'margarit', 'may', 'surpr', 'consid', 'jc', 'penney', 'styl', 'keny', 'littl', 'old', 'fash', 'old', 'lik', 'many', 'design', 'old', 'design', 'dont', 'design', 'baby', 'boom', 'reason', 'apply', 'amy'], ['think', 'kentaro', 'continue', 'get', 'better', 'challenge', 'brandon', 'end', 'none', 'others', 'consistently', 'good', 'enough', 'newer', 'style', 'please', 'judge', 'margarita', 'may', 'surprise', 'consider', 'jc', 'penney', 'style', 'kenya', 'little', 'old', 'fashion', 'old', 'like', 'many', 'design', 'old', 'designers', 'dont', 'design', 'baby', 'boomers', 'reason', 'apply', 'amy'])\n",
      "original document: \n",
      "['Lol', 'haha', 'XD', 'le', 'pickle', 'Rick', 'amirite?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'hah', 'xd', 'le', 'pickl', 'rick', 'amirit'], ['lol', 'haha', 'xd', 'le', 'pickle', 'rick', 'amirite'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Thanks', 'bud.', 'I’m', 'gonna', 'look', 'into', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'bud', 'im', 'gonn', 'look'], ['thank', 'bud', 'im', 'gonna', 'look'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['It', \"doesn't\", 'even', 'say', 'that', 'he', 'kills', 'them', 'in', 'the', 'book.', 'He', 'just', 'gets', 'rid', 'of', 'them', 'somehow.', 'Maybe', 'he', 'kills', 'them.', 'Maybe', 'he', 'banishes', 'them', 'and', 'they', 'become', 'pirates', 'and', 'Indians.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'ev', 'say', 'kil', 'book', 'get', 'rid', 'somehow', 'mayb', 'kil', 'mayb', 'ban', 'becom', 'pir', 'ind'], ['doesnt', 'even', 'say', 'kill', 'book', 'get', 'rid', 'somehow', 'maybe', 'kill', 'maybe', 'banish', 'become', 'pirate', 'indians'])\n",
      "original document: \n",
      "['I', 'think', 'it', 'really', 'depends', 'on', 'how', 'the', 'season', 'plays', 'out', 'tbh.', 'If', 'either', 'team', 'is', 'an', 'absolute', 'lock', 'in', 'the', 'playoffs', 'it’s', 'possible.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'real', 'depend', 'season', 'play', 'tbh', 'eith', 'team', 'absolv', 'lock', 'playoff', 'poss'], ['think', 'really', 'depend', 'season', 'play', 'tbh', 'either', 'team', 'absolute', 'lock', 'playoffs', 'possible'])\n",
      "original document: \n",
      "['The', 'weather', 'matches', 'your', 'mood']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weath', 'match', 'mood'], ['weather', 'match', 'mood'])\n",
      "original document: \n",
      "['Oh', 'yeah', 'totally', 'agree.', 'I', 'think', 'she', 'had', 'a', 'boob', 'job', 'done', 'for', 'sure.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'yeah', 'tot', 'agr', 'think', 'boob', 'job', 'don', 'sur'], ['oh', 'yeah', 'totally', 'agree', 'think', 'boob', 'job', 'do', 'sure'])\n",
      "original document: \n",
      "['Wait', 'I', 'honestly', 'don’t', 'know', 'what', 'happened', 'to', 'Cross.', 'Did', 'he', 'just', 'leave', 'or', 'what?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wait', 'honest', 'dont', 'know', 'hap', 'cross', 'leav'], ['wait', 'honestly', 'dont', 'know', 'happen', 'cross', 'leave'])\n",
      "original document: \n",
      "['I', 'am', 'the', 'same,', 'no', 'animosity,', 'but', 'I', 'have', 'noticed', 'institutional', '(?)', 'discrimination.', 'Many', '\"multicultural\"', 'savings', 'accounts', 'are', 'only', 'eligible', 'for', 'foreign', 'women,', 'and', 'if', 'you', 'look', 'at', 'government', 'welfare', 'systems', \"they're\", 'made', 'for', 'foreign', 'women.', 'I', 'guess', \"that's\", 'more', 'just', 'sexism', 'than', 'racial', 'discrimination', '-', 'anything', 'related', 'to', 'the', 'family', 'will', 'be', 'written', 'for', '\"mom\".\\n\\nJust', 'look', 'up', 'something', 'about', 'Chuseok', 'for', 'foreign', 'spouses,', 'it', 'will', 'tell', 'you', 'how', 'to', 'be', 'a', 'good', 'little', 'wife:)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['animos', 'not', 'institut', 'discrimin', 'many', 'multicult', 'sav', 'account', 'elig', 'foreign', 'wom', 'look', 'govern', 'welf', 'system', 'theyr', 'mad', 'foreign', 'wom', 'guess', 'that', 'sex', 'rac', 'discrimin', 'anyth', 'rel', 'famy', 'writ', 'mom\\n\\njust', 'look', 'someth', 'chuseok', 'foreign', 'spous', 'tel', 'good', 'littl', 'wif'], ['animosity', 'notice', 'institutional', 'discrimination', 'many', 'multicultural', 'save', 'account', 'eligible', 'foreign', 'women', 'look', 'government', 'welfare', 'systems', 'theyre', 'make', 'foreign', 'women', 'guess', 'thats', 'sexism', 'racial', 'discrimination', 'anything', 'relate', 'family', 'write', 'mom\\n\\njust', 'look', 'something', 'chuseok', 'foreign', 'spouses', 'tell', 'good', 'little', 'wife'])\n",
      "original document: \n",
      "['Oh,', 'so', 'you', 'want', 'to', 'see', 'my', 'historical', 'financials', 'to', 'prove', 'my', 'healthcare', 'costs', 'went', 'up?\\n\\nYawn.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'want', 'see', 'hist', 'fin', 'prov', 'healthc', 'cost', 'went', 'up\\n\\nyawn'], ['oh', 'want', 'see', 'historical', 'financials', 'prove', 'healthcare', 'cost', 'go', 'up\\n\\nyawn'])\n",
      "original document: \n",
      "['Yea', 'because', 'the', 'cartel', 'are', 'based', 'off', 'of', 'U.S.', 'state', 'parks', 'and', 'not', 'off', 'Mexico.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'cartel', 'bas', 'us', 'stat', 'park', 'mexico'], ['yea', 'cartel', 'base', 'us', 'state', 'park', 'mexico'])\n",
      "original document: \n",
      "['I', 'had', 'to', 'stop', 'taking', 'my', 'Saint', 'Bernard', 'to', 'the', 'dog', 'park', 'because', 'he', 'started', 'getting', 'aggressive', 'with', 'unneutered', 'males.', 'Will', 'training', 'help', 'or', 'that’s', 'just', 'him', 'becoming', 'an', 'adjunct', 'jerk?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'tak', 'saint', 'bernard', 'dog', 'park', 'start', 'get', 'aggress', 'unneut', 'mal', 'train', 'help', 'that', 'becom', 'adjunct', 'jerk'], ['stop', 'take', 'saint', 'bernard', 'dog', 'park', 'start', 'get', 'aggressive', 'unneutered', 'males', 'train', 'help', 'thats', 'become', 'adjunct', 'jerk'])\n",
      "original document: \n",
      "['Also', 'removal', 'of', 'life', 'steal', 'and', '%damage']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'remov', 'lif', 'ste', 'dam'], ['also', 'removal', 'life', 'steal', 'damage'])\n",
      "original document: \n",
      "['We', 'are', 'still', 'the', 'Titans']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'tit'], ['still', 'titans'])\n",
      "original document: \n",
      "['I', 'like', 'it,', \"it's\", 'a', 'way', 'to', 'use', 'speedy', 'cam/cf', 'at', 'a', 'wider', 'formation,', 'almost', 'acting', 'like', 'wingers', 'but', 'without', 'getting', 'isolated', 'in', 'the', 'wings.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'way', 'us', 'speedy', 'camcf', 'wid', 'form', 'almost', 'act', 'lik', 'wing', 'without', 'get', 'isol', 'wing'], ['like', 'way', 'use', 'speedy', 'camcf', 'wider', 'formation', 'almost', 'act', 'like', 'wingers', 'without', 'get', 'isolate', 'wing'])\n",
      "original document: \n",
      "[\"There's\", 'more', 'chance', 'of', 'him', 'learning', 'Irish', 'than', 'ever', 'meeting', 'anyone', 'on', 'this', 'sub', 'in', 'person.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'chant', 'learn', 'ir', 'ev', 'meet', 'anyon', 'sub', 'person'], ['theres', 'chance', 'learn', 'irish', 'ever', 'meet', 'anyone', 'sub', 'person'])\n",
      "original document: \n",
      "['ye', 'fam']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'fam'], ['ye', 'fam'])\n",
      "original document: \n",
      "[\"I'm\", 'guessing', 'because', 'if', 'the', 'send', 'her', 'to', 'the', '\"fat', 'farm\"', 'then', 'Jane', 'can', 'take', 'her', 'leave', 'from', 'work', 'for', 'a', 'bit.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'guess', 'send', 'fat', 'farm', 'jan', 'tak', 'leav', 'work', 'bit', '\\n'], ['im', 'guess', 'send', 'fat', 'farm', 'jane', 'take', 'leave', 'work', 'bite', '\\n'])\n",
      "original document: \n",
      "['Embiid', 'is', 'also', 'like', '28']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['embiid', 'also', 'lik', 'twenty-eight'], ['embiid', 'also', 'like', 'twenty-eight'])\n",
      "original document: \n",
      "['Thanks!', '😁', 'do', 'you', 'think', 'aguero', 'will', 'be', 'back', 'in', 'two', 'weeks', 'tho?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'think', 'aguero', 'back', 'two', 'week', 'tho'], ['thank', 'think', 'aguero', 'back', 'two', 'weeks', 'tho'])\n",
      "original document: \n",
      "['DMac', 'with', 'another', 'try-saving', 'tackle']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dmac', 'anoth', 'trysav', 'tackl'], ['dmac', 'another', 'trysaving', 'tackle'])\n",
      "original document: \n",
      "['Agreed,', 'this', 'Shen', 'is', 'better', 'than', 'the', 'old,', 'and', 'I', \"don't\", 'dislike', 'the', 'new', 'Q,', 'I', 'think', 'it', 'is', 'a', 'nice', 'ability', 'but', 'feels', 'like', 'it', 'could', 'be', 'a', 'bit', 'better', 'still.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'shen', 'bet', 'old', 'dont', 'dislik', 'new', 'q', 'think', 'nic', 'abl', 'feel', 'lik', 'could', 'bit', 'bet', 'stil'], ['agree', 'shen', 'better', 'old', 'dont', 'dislike', 'new', 'q', 'think', 'nice', 'ability', 'feel', 'like', 'could', 'bite', 'better', 'still'])\n",
      "original document: \n",
      "['Fleshlight?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fleshlight'], ['fleshlight'])\n",
      "original document: \n",
      "['Just', 'go', 'to', 'mcpedl.com', '.', 'You', 'can', 'find', 'lots', 'of', 'free', 'maps,', 'textures', 'and', 'addons.', 'The', 'marketplace', 'is', 'completely', 'optional,', \"don't\", 'get', 'why', 'people', 'think', \"it's\", 'the', 'only', 'way', 'to', 'get', 'stuff']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'mcpedlcom', 'find', 'lot', 'fre', 'map', 'text', 'addon', 'marketplac', 'complet', 'opt', 'dont', 'get', 'peopl', 'think', 'way', 'get', 'stuff'], ['go', 'mcpedlcom', 'find', 'lot', 'free', 'map', 'textures', 'addons', 'marketplace', 'completely', 'optional', 'dont', 'get', 'people', 'think', 'way', 'get', 'stuff'])\n",
      "original document: \n",
      "['143414681|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'zR7qrd9W)\\n\\n2012:', 'Aleppo\\n2016:', 'Trump', 'and', 'Trump\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, six hundred and eighty-on', 'gt', 'unit', 'stat', 'anonym', 'id', 'zr7qrd9w\\n\\n2012', 'aleppo\\n2016', 'trump', 'trump\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, six hundred and eighty-one', 'gt', 'unite', 'state', 'anonymous', 'id', 'zr7qrd9w\\n\\n2012', 'aleppo\\n2016', 'trump', 'trump\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['He', 'and', 'most', 'of', 'the', 'population', 'who', 'actually', 'think', 'not', 'supporting', 'certain', 'animal', 'abuses', 'is', 'terrible', 'because', '\"If', 'they', \"can't\", 'be', 'abused', 'they', \"wouldn't\", 'exist', 'in', 'the', 'first', 'place', 'and', \"that's\", 'worse!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pop', 'act', 'think', 'support', 'certain', 'anim', 'abus', 'terr', 'cant', 'abus', 'wouldnt', 'ex', 'first', 'plac', 'that', 'wors'], ['population', 'actually', 'think', 'support', 'certain', 'animal', 'abuse', 'terrible', 'cant', 'abuse', 'wouldnt', 'exist', 'first', 'place', 'thats', 'worse'])\n",
      "original document: \n",
      "['Lucky!', 'About', '20', 'days', 'shy', 'of', 'a', 'year', 'here!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lucky', 'twenty', 'day', 'shy', 'year'], ['lucky', 'twenty', 'days', 'shy', 'year'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Of', 'all', 'the', 'possibilities...', 'Cumnog', 'and', 'Thotness', 'wtf']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poss', 'cumnog', 'thot', 'wtf'], ['possibilities', 'cumnog', 'thotness', 'wtf'])\n",
      "original document: \n",
      "['Wiggins', 'is', 'a', 'bit', \"terrifying.\\n\\nHe's\", 'getting', 'that', 'max', 'deal,', 'and', 'I', 'know', 'that', 'I', \"shouldn't\", 'be', 'concerned', 'about', 'him', 'being', 'worth', 'it,', 'but', 'I', 'am', 'a', 'bit', 'concerned', 'about', 'him', 'for', 'a', 'few', 'different', 'reasons.\\n\\nAlso', 'interesting', 'to', 'note', 'the', 'average', 'age', 'of', 'the', 'guys', 'above', 'the', 'line', 'versus', 'the', 'average', 'age', 'of', 'the', 'guys', 'below', 'it', 'in', 'general.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wiggin', 'bit', 'terrifying\\n\\nhes', 'get', 'max', 'deal', 'know', 'shouldnt', 'concern', 'wor', 'bit', 'concern', 'diff', 'reasons\\n\\nalso', 'interest', 'not', 'av', 'ag', 'guy', 'lin', 'vers', 'av', 'ag', 'guy', 'gen'], ['wiggins', 'bite', 'terrifying\\n\\nhes', 'get', 'max', 'deal', 'know', 'shouldnt', 'concern', 'worth', 'bite', 'concern', 'different', 'reasons\\n\\nalso', 'interest', 'note', 'average', 'age', 'guy', 'line', 'versus', 'average', 'age', 'guy', 'general'])\n",
      "original document: \n",
      "['Ward', 'almost', 'had', 'it!!!!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ward', 'almost'], ['ward', 'almost'])\n",
      "original document: \n",
      "['[Imgur](https://i.imgur.com/jxB6GQW.jpg)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['imgurhttpsiimgurcomjxb6gqwjpg'], ['imgurhttpsiimgurcomjxb6gqwjpg'])\n",
      "original document: \n",
      "['why']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Our', 'D-linemen', 'are', 'making', 'them', 'jump', 'by', 'just', 'being', 'there.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dlinem', 'mak', 'jump'], ['dlinemen', 'make', 'jump'])\n",
      "original document: \n",
      "['\"Oh', \"you're\", 'in', 'for', 'quite', 'a', 'journey', 'boy.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'yo', 'quit', 'journey', 'boy'], ['oh', 'youre', 'quite', 'journey', 'boy'])\n",
      "original document: \n",
      "['Lauren', 'back', 'at', 'it', 'with', 'her', 'assault', 'and', 'battery', 'charges.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['laur', 'back', 'assault', 'battery', 'charg'], ['lauren', 'back', 'assault', 'battery', 'charge'])\n",
      "original document: \n",
      "['Anarchy,', 'as', 'a', 'political', 'concept,', 'is', 'a', 'naive', 'floating', 'abstraction:', '.\\xa0.\\xa0.', 'a', 'society', 'without', 'an', 'organized', 'government', 'would', 'be', 'at', 'the', 'mercy', 'of', 'the', 'first', 'criminal', 'who', 'came', 'along', 'and', 'who', 'would', 'precipitate', 'it', 'into', 'the', 'chaos', 'of', 'gang', 'warfare.', 'But', 'the', 'possibility', 'of', 'human', 'immorality', 'is', 'not', 'the', 'only', 'objection', 'to', 'anarchy:', 'even', 'a', 'society', 'whose', 'every', 'member', 'were', 'fully', 'rational', 'and', 'faultlessly', 'moral,', 'could', 'not', 'function', 'in', 'a', 'state', 'of', 'anarchy;', 'it', 'is', 'the', 'need', 'of\\xa0objectivelaws', 'and', 'of', 'an', 'arbiter', 'for', 'honest', 'disagreements', 'among', 'men', 'that', 'necessitates', 'the', 'establishment', 'of', 'a', 'government.\\n\\n“The', 'Nature', 'of', 'Government,”\\nThe', 'Virtue', 'of', 'Selfishness,', '112\\n\\n¶\\n\\nIf', 'a', 'society', 'provided', 'no', 'organized', 'protection', 'against', 'force,', 'it', 'would', 'compel', 'every', 'citizen', 'to', 'go', 'about', 'armed,', 'to', 'turn', 'his', 'home', 'into', 'a', 'fortress,', 'to', 'shoot', 'any', 'strangers', 'approaching', 'his', 'door—or', 'to', 'join', 'a', 'protective', 'gang', 'of', 'citizens', 'who', 'would', 'fight', 'other', 'gangs,', 'formed', 'for', 'the', 'same', 'purpose,', 'and', 'thus', 'bring', 'about', 'the', 'degeneration', 'of', 'that', 'society', 'into', 'the', 'chaos', 'of', 'gang-rule,', 'i.e.,', 'rule', 'by', 'brute', 'force,', 'into', 'perpetual', 'tribal', 'warfare', 'of', 'prehistorical', 'savages.\\n\\nThe', 'use', 'of', 'physical', 'force—even', 'its', 'retaliatory', 'use—cannot', 'be', 'left', 'at', 'the', 'discretion', 'of', 'individual', 'citizens.', 'Peaceful', 'coexistence', 'is', 'impossible', 'if', 'a', 'man', 'has', 'to', 'live', 'under', 'the', 'constant', 'threat', 'of', 'force', 'to', 'be', 'unleashed', 'against', 'him', 'by', 'any', 'of', 'his', 'neighbors', 'at', 'any', 'moment.', 'Whether', 'his', 'neighbors’', 'intentions', 'are', 'good', 'or', 'bad,', 'whether', 'their', 'judgment', 'is', 'rational', 'or', 'irrational,', 'whether', 'they', 'are', 'motivated', 'by', 'a', 'sense', 'of', 'justice', 'or', 'by', 'ignorance', 'or', 'by', 'prejudice', 'or', 'by', 'malice—the', 'use', 'of', 'force', 'against', 'one', 'man', 'cannot', 'be', 'left', 'to', 'the', 'arbitrary', 'decision', 'of', 'another.\\n\\n“The', 'Nature', 'of', 'Government,”\\nThe', 'Virtue', 'of', 'Selfishness,', '108\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anarchy', 'polit', 'conceiv', 'naiv', 'flo', 'abstract', '  ', 'socy', 'without', 'org', 'govern', 'would', 'mercy', 'first', 'crimin', 'cam', 'along', 'would', 'precipit', 'chao', 'gang', 'warf', 'poss', 'hum', 'im', 'object', 'anarchy', 'ev', 'socy', 'whos', 'every', 'memb', 'ful', 'rat', 'faultless', 'mor', 'could', 'funct', 'stat', 'anarchy', 'nee', 'of objectivelaws', 'arbit', 'honest', 'disagr', 'among', 'men', 'necessit', 'est', 'government\\n\\nthe', 'nat', 'government\\nthe', 'virtu', 'self', '112\\n\\n\\n\\nif', 'socy', 'provid', 'org', 'protect', 'forc', 'would', 'compel', 'every', 'cit', 'go', 'arm', 'turn', 'hom', 'fortress', 'shoot', 'strangers', 'approach', 'door', 'join', 'protect', 'gang', 'cit', 'would', 'fight', 'gang', 'form', 'purpos', 'thu', 'bring', 'deg', 'socy', 'chao', 'gangr', 'ie', 'rul', 'brut', 'forc', 'perpet', 'trib', 'warf', 'preh', 'savages\\n\\nthe', 'us', 'phys', 'forceev', 'reta', 'usecannot', 'left', 'discret', 'individ', 'cit', 'peac', 'coex', 'imposs', 'man', 'liv', 'const', 'threat', 'forc', 'unleash', 'neighb', 'mom', 'wheth', 'neighb', 'int', 'good', 'bad', 'wheth', 'judg', 'rat', 'ir', 'wheth', 'mot', 'sens', 'just', 'ign', 'prejud', 'maliceth', 'us', 'forc', 'on', 'man', 'cannot', 'left', 'arbit', 'decid', 'another\\n\\nthe', 'nat', 'government\\nthe', 'virtu', 'self', '108\\n\\n'], ['anarchy', 'political', 'concept', 'naive', 'float', 'abstraction', '  ', 'society', 'without', 'organize', 'government', 'would', 'mercy', 'first', 'criminal', 'come', 'along', 'would', 'precipitate', 'chaos', 'gang', 'warfare', 'possibility', 'human', 'immorality', 'objection', 'anarchy', 'even', 'society', 'whose', 'every', 'member', 'fully', 'rational', 'faultlessly', 'moral', 'could', 'function', 'state', 'anarchy', 'need', 'of objectivelaws', 'arbiter', 'honest', 'disagreements', 'among', 'men', 'necessitate', 'establishment', 'government\\n\\nthe', 'nature', 'government\\nthe', 'virtue', 'selfishness', '112\\n\\n\\n\\nif', 'society', 'provide', 'organize', 'protection', 'force', 'would', 'compel', 'every', 'citizen', 'go', 'arm', 'turn', 'home', 'fortress', 'shoot', 'strangers', 'approach', 'dooror', 'join', 'protective', 'gang', 'citizens', 'would', 'fight', 'gang', 'form', 'purpose', 'thus', 'bring', 'degeneration', 'society', 'chaos', 'gangrule', 'ie', 'rule', 'brute', 'force', 'perpetual', 'tribal', 'warfare', 'prehistorical', 'savages\\n\\nthe', 'use', 'physical', 'forceeven', 'retaliatory', 'usecannot', 'leave', 'discretion', 'individual', 'citizens', 'peaceful', 'coexistence', 'impossible', 'man', 'live', 'constant', 'threat', 'force', 'unleash', 'neighbor', 'moment', 'whether', 'neighbor', 'intentions', 'good', 'bad', 'whether', 'judgment', 'rational', 'irrational', 'whether', 'motivate', 'sense', 'justice', 'ignorance', 'prejudice', 'malicethe', 'use', 'force', 'one', 'man', 'cannot', 'leave', 'arbitrary', 'decision', 'another\\n\\nthe', 'nature', 'government\\nthe', 'virtue', 'selfishness', '108\\n\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['In', 'Surah', 'Al-Ahzab', 'verse', '33.', \"Shi'as\", 'use', 'this', 'verse', 'to', 'claim', 'that', 'the', 'ahl', 'al-bayt', 'cannot', 'make', 'any', 'mistakes', 'and', 'are', 'completely', 'infallible.', '\\n\\nedit:', 'By', 'the', 'way,', \"I'm\", 'not', 'claiming', 'that', 'they', 'are', 'infallible', 'nor', 'do', 'I', 'believe', \"that's\", 'what', 'the', 'verse', 'is', 'implying.', 'I', 'was', 'just', 'trying', 'to', 'figure', 'out', 'what', 'exactly', \"shi'as\", 'mean', 'when', 'they', 'say', 'infallible.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['surah', 'alahzab', 'vers', 'thirty-three', 'shia', 'us', 'vers', 'claim', 'ahl', 'albayt', 'cannot', 'mak', 'mistak', 'complet', 'infall', '\\n\\nedit', 'way', 'im', 'claim', 'infall', 'believ', 'that', 'vers', 'imply', 'try', 'fig', 'exact', 'shia', 'mean', 'say', 'infall'], ['surah', 'alahzab', 'verse', 'thirty-three', 'shias', 'use', 'verse', 'claim', 'ahl', 'albayt', 'cannot', 'make', 'mistake', 'completely', 'infallible', '\\n\\nedit', 'way', 'im', 'claim', 'infallible', 'believe', 'thats', 'verse', 'imply', 'try', 'figure', 'exactly', 'shias', 'mean', 'say', 'infallible'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['I', 'passionately', '', 'wait', 'for', 'the', 'moment', 'that', 'graves', 'is', 'meta', 'again', ':[', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pass', 'wait', 'mom', 'grav', 'met'], ['passionately', 'wait', 'moment', 'grave', 'meta'])\n",
      "original document: \n",
      "['Not', 'rating', 'a', 'guy', 'highly', '(or', 'not', 'talking', 'about', 'him)', 'because', 'he', 'gets', 'injured', 'a', 'lot', 'is', 'not', 'overlooking', 'him.', 'Availability', 'matters.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rat', 'guy', 'high', 'talk', 'get', 'ind', 'lot', 'overlook', 'avail', 'mat'], ['rat', 'guy', 'highly', 'talk', 'get', 'injure', 'lot', 'overlook', 'availability', 'matter'])\n",
      "original document: \n",
      "['Ignoring', 'the', 'fact', 'that', 'Kaepernick', 'literally', 'chose', 'to', 'kneel', 'as', 'his', 'form', 'of', 'protesting', 'police', 'brutality', 'because', 'he', 'talked', 'to', 'vets', 'that', 'said', 'that', 'would', 'be', 'the', 'most', 'respectful', 'way', 'to', 'do', 'it.', \"\\n\\nI'm\", 'curious', 'as', 'to', 'why', \"you'd\", 'think', 'kneeling', 'is', 'an', 'act', 'of', 'disrespect', 'and', 'what', 'situation', \"you've\", 'seen', 'someone', 'kneel', 'before', 'this', 'that', 'you', 'thought', 'was', 'disrespectful.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ign', 'fact', 'kaepernick', 'lit', 'chos', 'kneel', 'form', 'protest', 'pol', 'brut', 'talk', 'vet', 'said', 'would', 'respect', 'way', '\\n\\nim', 'cury', 'youd', 'think', 'kneel', 'act', 'disrespect', 'situ', 'youv', 'seen', 'someon', 'kneel', 'thought', 'disrespect'], ['ignore', 'fact', 'kaepernick', 'literally', 'choose', 'kneel', 'form', 'protest', 'police', 'brutality', 'talk', 'vet', 'say', 'would', 'respectful', 'way', '\\n\\nim', 'curious', 'youd', 'think', 'kneel', 'act', 'disrespect', 'situation', 'youve', 'see', 'someone', 'kneel', 'think', 'disrespectful'])\n",
      "original document: \n",
      "['I', \"didn't\", 'release', 'her,', \"that's\", 'bullshit,', 'I', 'did', 'NOT...oh', 'hi', 'Eclipsa.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'releas', 'that', 'bullshit', 'notoh', 'hi', 'eclips'], ['didnt', 'release', 'thats', 'bullshit', 'notoh', 'hi', 'eclipsa'])\n",
      "original document: \n",
      "['Man,', 'all', 'respect,', 'I', 'was', 'very', 'lucky,', 'had', 'no', 'real', 'problems', 'stopping', 'working', 'for', 'someone', 'else', 'and', 'moving', 'to', 'working', 'for', 'myself,', 'I', 'was', 'already', 'somewhat', 'in', 'the', 'industry,', 'the', 'transition', 'was', 'very', 'smooth', 'for', 'me.', 'Your', 'a', 'hustler,', 'I', 'respect', 'that', '100%.', 'People', \"don't\", 'realize', 'that', 'hard', 'work', 'really', 'does', 'pay', 'off,', 'you', 'might', 'fall', 'down', 'sometimes', 'but', 'the', 'real', 'winners', 'are', 'those', 'who', \"don't\", 'quit', 'and', 'pick', 'themselves', 'up!', \"I'll\", 'pop', 'a', 'beer', 'for', 'you', 'bro,', 'wish', 'you', 'all', 'the', 'best!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['man', 'respect', 'lucky', 'real', 'problem', 'stop', 'work', 'someon', 'els', 'mov', 'work', 'already', 'somewh', 'industry', 'transit', 'smoo', 'hustl', 'respect', 'one hundred', 'peopl', 'dont', 'real', 'hard', 'work', 'real', 'pay', 'might', 'fal', 'sometim', 'real', 'win', 'dont', 'quit', 'pick', 'il', 'pop', 'beer', 'bro', 'wish', 'best'], ['man', 'respect', 'lucky', 'real', 'problems', 'stop', 'work', 'someone', 'else', 'move', 'work', 'already', 'somewhat', 'industry', 'transition', 'smooth', 'hustler', 'respect', 'one hundred', 'people', 'dont', 'realize', 'hard', 'work', 'really', 'pay', 'might', 'fall', 'sometimes', 'real', 'winners', 'dont', 'quit', 'pick', 'ill', 'pop', 'beer', 'bro', 'wish', 'best'])\n",
      "original document: \n",
      "['That', \"doesn't\", 'sound', 'right,', 'but', 'I', \"don't\", 'know', 'enough', 'about', 'irregular', 'verbs', 'to', 'dispute', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'sound', 'right', 'dont', 'know', 'enough', 'irregul', 'verb', 'disput'], ['doesnt', 'sound', 'right', 'dont', 'know', 'enough', 'irregular', 'verbs', 'dispute'])\n",
      "original document: \n",
      "['Sux.', 'So', 'sorry', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sux', 'sorry'], ['sux', 'sorry'])\n",
      "original document: \n",
      "['Is', 'it', 'really', 'that', 'much', 'wow']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'much', 'wow'], ['really', 'much', 'wow'])\n",
      "original document: \n",
      "['I', 'figured,', 'thanks', 'though', 'and', 'good', 'luck']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fig', 'thank', 'though', 'good', 'luck'], ['figure', 'thank', 'though', 'good', 'luck'])\n",
      "original document: \n",
      "['Cures', 'alcoholism...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cur', 'alcohol'], ['cure', 'alcoholism'])\n",
      "original document: \n",
      "['DC', 'VegFest', 'is', 'always', 'a', 'great', 'time!!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dc', 'vegfest', 'alway', 'gre', 'tim'], ['dc', 'vegfest', 'always', 'great', 'time'])\n",
      "original document: \n",
      "['Nothing', 'wrong', 'with', 'respecting', 'your', 'dad.', 'Even', 'if', 'it', 'means', 'hanging', 'out', 'somewhere', 'you’d', 'rather', 'not', 'be.', 'I', 'mean,', 'he’s', 'your', 'dad.', 'Mine', 'died', 'over', '10', 'years', 'ago', 'and', 'I’d', 'love', 'to', 'hang', 'out', 'with', 'him,', 'even', 'if', 'it', 'was', 'at', 'the', 'opera,', 'ballet,', 'rock', 'concert,', 'poetic', 'reading,', 'or', 'a', 'couple', 'hours', 'sitting', 'in', 'the', 'dark', 'sleeping', 'with', 'my', 'eyes', 'open.', 'He', 'would', 'take', 'me', 'to', 'Wendy’s', 'afterwards.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['noth', 'wrong', 'respect', 'dad', 'ev', 'mean', 'hang', 'somewh', 'youd', 'rath', 'mean', 'hes', 'dad', 'min', 'died', 'ten', 'year', 'ago', 'id', 'lov', 'hang', 'ev', 'oper', 'ballet', 'rock', 'concert', 'poet', 'read', 'coupl', 'hour', 'sit', 'dark', 'sleep', 'ey', 'op', 'would', 'tak', 'wendy', 'afterward'], ['nothing', 'wrong', 'respect', 'dad', 'even', 'mean', 'hang', 'somewhere', 'youd', 'rather', 'mean', 'hes', 'dad', 'mine', 'die', 'ten', 'years', 'ago', 'id', 'love', 'hang', 'even', 'opera', 'ballet', 'rock', 'concert', 'poetic', 'read', 'couple', 'hours', 'sit', 'dark', 'sleep', 'eye', 'open', 'would', 'take', 'wendys', 'afterwards'])\n",
      "original document: \n",
      "['incels:', 'the', 'most', 'fragile', 'of', 'all', 'manchildren']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['incel', 'fragil', 'manchildr'], ['incels', 'fragile', 'manchildren'])\n",
      "original document: \n",
      "['Yeah', 'if', 'you', 'have', 'man', 'city,', 'there', 'probably', 'won’t', 'be', 'any', 'chants\\nAnd', 'if', 'you', 'are', 'losing', 'the', 'stadium', 'will', 'magically', 'empty', 'itself']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'man', 'city', 'prob', 'wont', 'chants\\nand', 'los', 'stad', 'mag', 'empty'], ['yeah', 'man', 'city', 'probably', 'wont', 'chants\\nand', 'lose', 'stadium', 'magically', 'empty'])\n",
      "original document: \n",
      "['The', 'poor', 'bloke', \"isn't\", 'even', 'to', 'blame', 'for', 'this', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poor', 'blok', 'isnt', 'ev', 'blam', 'on'], ['poor', 'bloke', 'isnt', 'even', 'blame', 'one'])\n",
      "original document: \n",
      "['What', 'is', 'that', 'link?', 'It', 'wants', 'me', 'to', 'download', 'something??', '\\n\\nSketch...', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['link', 'want', 'download', 'someth', '\\n\\nsketch', 'lol'], ['link', 'want', 'download', 'something', '\\n\\nsketch', 'lol'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['spitting', 'in', 'public.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['spit', 'publ'], ['spit', 'public'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['**HD**', 'Stream', ':', '[NCAAF', 'Ole', 'Miss', 'vs', 'Alabama', '*ESPN*', '**HD**', 'Stream](http://nowwatchtvlive.cc/espn-live-stream-watch-espn-sportscenter-online-free/)\\n\\nSD', 'TV', 'Streams', ':', '[NCAAF', 'Ole', 'Miss', 'vs', 'Alabama', '*ESPN*', '', 'English', 'Stream](http://nowwatchtvlive.cc/live-sports/stream-1-13.php)\\n', '\\nAd', 'Overlays:', '4', '|', 'Mobile:', 'Yes', '|', 'NSFW:', 'Yes', '|', 'Chat']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hd', 'stream', 'ncaaf', 'ol', 'miss', 'vs', 'alabam', 'espn', 'hd', 'streamhttpnowwatchtvliveccespnlivestreamwatchespnsportscenteronlinefree\\n\\nsd', 'tv', 'streams', 'ncaaf', 'ol', 'miss', 'vs', 'alabam', 'espn', 'engl', 'streamhttpnowwatchtvlivecclivesportsstream113php\\n', '\\nad', 'overlay', 'four', 'mobl', 'ye', 'nsfw', 'ye', 'chat'], ['hd', 'stream', 'ncaaf', 'ole', 'miss', 'vs', 'alabama', 'espn', 'hd', 'streamhttpnowwatchtvliveccespnlivestreamwatchespnsportscenteronlinefree\\n\\nsd', 'tv', 'stream', 'ncaaf', 'ole', 'miss', 'vs', 'alabama', 'espn', 'english', 'streamhttpnowwatchtvlivecclivesportsstream113php\\n', '\\nad', 'overlay', 'four', 'mobile', 'yes', 'nsfw', 'yes', 'chat'])\n",
      "original document: \n",
      "['I', 'can', 'see', 'it', 'now', '\"Hillary', 'Clinton', 'says', 'her', 'movie', 'bombed', 'because', 'America', 'is', 'sexist.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'hil', 'clinton', 'say', 'movy', 'bomb', 'americ', 'sex'], ['see', 'hillary', 'clinton', 'say', 'movie', 'bomb', 'america', 'sexist'])\n",
      "original document: \n",
      "['I', 'meant', '\"command\"', 'the', 'printer.', '\\n\\nI', 'mean,', 'those', 'are', 'OK', 'specs,', 'but', 'for', 'a', '\"gaming\"', 'setup', \"it's\", 'a', 'bit', 'low.', 'The', 'CPU', 'is', 'OK', 'but', 'the', '940M', 'should', 'struggle', 'in', 'recent', 'games,', 'right?', '\\n\\nI', 'meant', 'no', 'offense.', 'I', 'just', 'found', 'it', 'weird', 'that', 'it', 'would', 'be', 'a', 'gaming', 'pc', 'with', 'those', 'specs,', 'when', 'the', 'same', 'specs', 'are', 'perfectly', 'on', 'par', 'with', 'what', \"I'd\", 'expect', 'from', 'the', 'companion', 'PC', 'of', 'a', '3d', 'printer,', 'since', 'you', 'still', 'need', 'some', 'GPU', 'power', 'to', 'design', 'the', 'pieces', 'and', 'manipulate', 'them', 'in', 'software.', '', '\\nThat', 'just', 'seemed', 'logical.\\n\\nHence', 'my', 'question.', 'Sorry', 'if', 'I', 'offended', 'you', 'in', 'any', 'way.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['meant', 'command', 'print', '\\n\\ni', 'mean', 'ok', 'spec', 'gam', 'setup', 'bit', 'low', 'cpu', 'ok', '940m', 'struggle', 'rec', 'gam', 'right', '\\n\\ni', 'meant', 'offens', 'found', 'weird', 'would', 'gam', 'pc', 'spec', 'spec', 'perfect', 'par', 'id', 'expect', 'comp', 'pc', '3d', 'print', 'sint', 'stil', 'nee', 'gpu', 'pow', 'design', 'piec', 'manip', 'softw', '\\nthat', 'seem', 'logical\\n\\nhence', 'quest', 'sorry', 'offend', 'way'], ['mean', 'command', 'printer', '\\n\\ni', 'mean', 'ok', 'specs', 'game', 'setup', 'bite', 'low', 'cpu', 'ok', '940m', 'struggle', 'recent', 'game', 'right', '\\n\\ni', 'mean', 'offense', 'find', 'weird', 'would', 'game', 'pc', 'specs', 'specs', 'perfectly', 'par', 'id', 'expect', 'companion', 'pc', '3d', 'printer', 'since', 'still', 'need', 'gpu', 'power', 'design', 'piece', 'manipulate', 'software', '\\nthat', 'seem', 'logical\\n\\nhence', 'question', 'sorry', 'offend', 'way'])\n",
      "original document: \n",
      "['Archived', 'for', 'your', 'convenience\\n\\nSnapshots:\\n\\n1.', '*This', 'Post*', '-', '[archive.org](https://web.archive.org/20171001000150/https://i.redd.it/5qcnbrtc04pz.jpg),', '[_megalodon.jp\\\\*_](http://megalodon.jp/pc/get_simple/decide?url=https://i.redd.it/5qcnbrtc04pz.jpg', '\"could', 'not', 'auto-archive;', 'click', 'to', 'resubmit', 'it!\"),', '[archive.is](https://archive.is/oLpqJ)\\n\\n*^(I', 'am', 'a', 'bot.)', '^\\\\([*Info*](/r/SnapshillBot)', '^/', '^[*Contact*](/message/compose?to=\\\\/r\\\\/SnapshillBot))*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['arch', 'convenience\\n\\nsnapshots\\n\\n1', 'post', 'archiveorghttpswebarchiveorg20171001000150httpsireddit5qcnbrtc04pzjpg', '_megalodonjp_httpmegalodonjppcget_simpledecideurlhttpsireddit5qcnbrtc04pzjpg', 'could', 'autoarch', 'click', 'resubmit', 'archiveishttpsarchiveisolpqj\\n\\ni', 'bot', 'inforsnapshillbot', 'contactmessagecomposetorsnapshillbot'], ['archive', 'convenience\\n\\nsnapshots\\n\\n1', 'post', 'archiveorghttpswebarchiveorg20171001000150httpsireddit5qcnbrtc04pzjpg', '_megalodonjp_httpmegalodonjppcget_simpledecideurlhttpsireddit5qcnbrtc04pzjpg', 'could', 'autoarchive', 'click', 'resubmit', 'archiveishttpsarchiveisolpqj\\n\\ni', 'bot', 'inforsnapshillbot', 'contactmessagecomposetorsnapshillbot'])\n",
      "original document: \n",
      "['salt']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['salt'], ['salt'])\n",
      "original document: \n",
      "['200', 'for', 'a', 'fucking', 'cd', 'player?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two hundred', 'fuck', 'cd', 'play'], ['two hundred', 'fuck', 'cd', 'player'])\n",
      "original document: \n",
      "['I', 'will', 'once', 'I', 'get', 'more', 'lucky', 'occurrences', 'like', 'this', 'lol', ':D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'lucky', 'occur', 'lik', 'lol'], ['get', 'lucky', 'occurrences', 'like', 'lol'])\n",
      "original document: \n",
      "[\"I've\", 'been', 'in', 'one', 'USF', 'game', 'thread', 'so', 'i', \"haven't\", 'seen', 'this,', 'but', 'yeah', 'i', \"wouldn't\", 'be', 'surprised', 'if', 'we', 'had', 'our', 'fair', 'shares', 'of', 'jerks', 'on', 'our', 'side', 'as', 'well', 'and', 'the', 'rest', 'of', 'us', 'apologize', 'for', 'it']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'on', 'usf', 'gam', 'thread', 'hav', 'seen', 'yeah', 'wouldnt', 'surpr', 'fair', 'shar', 'jerk', 'sid', 'wel', 'rest', 'us', 'apolog'], ['ive', 'one', 'usf', 'game', 'thread', 'havent', 'see', 'yeah', 'wouldnt', 'surprise', 'fair', 'share', 'jerk', 'side', 'well', 'rest', 'us', 'apologize'])\n",
      "original document: \n",
      "['You', 'should', 'definitely', 'talk', 'to', 'your', 'doctor', 'about', 'this.', \"Don't\", 'hurt', 'yourself!', 'Hugs', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'talk', 'doct', 'dont', 'hurt', 'hug'], ['definitely', 'talk', 'doctor', 'dont', 'hurt', 'hug'])\n",
      "original document: \n",
      "['Lol,', 'not', 'sure', 'if', 'you', 'are', 'kidding', 'or', 'not']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'sur', 'kid'], ['lol', 'sure', 'kid'])\n",
      "original document: \n",
      "['Exactly.', 'If', 'it', 'weren’t', 'for', 'posts', 'talking', 'about', 'female', 'nature,', 'we’d', 'likely', 'stop', 'gaining', 'many', 'new', 'members,', 'and', 'we’d', 'lose', 'quite', 'a', 'few', 'as', 'well.', 'Take', 'away', 'the', 'reminder', 'of', 'why', 'we', 'left,', 'and', 'sooner', 'or', 'later', 'biology', 'will', 'take', 'control', 'and', 'we’ll', 'reenter', 'the', 'game', '(no', 'matter', 'how', 'stupid', 'it', 'would', 'be)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'wer', 'post', 'talk', 'fem', 'nat', 'wed', 'lik', 'stop', 'gain', 'many', 'new', 'memb', 'wed', 'los', 'quit', 'wel', 'tak', 'away', 'remind', 'left', 'soon', 'lat', 'biolog', 'tak', 'control', 'wel', 'reent', 'gam', 'mat', 'stupid', 'would'], ['exactly', 'werent', 'post', 'talk', 'female', 'nature', 'wed', 'likely', 'stop', 'gain', 'many', 'new', 'members', 'wed', 'lose', 'quite', 'well', 'take', 'away', 'reminder', 'leave', 'sooner', 'later', 'biology', 'take', 'control', 'well', 'reenter', 'game', 'matter', 'stupid', 'would'])\n",
      "original document: \n",
      "['[+dmwe225](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnos1l4/):\\n\\nHighly', 'recommend', 'she', 'read', 'Stephen', \"King's\", '\"On', 'Writing\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dmwe225httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnos1l4\\n\\nhighly', 'recommend', 'read', 'steph', 'king', 'writ'], ['dmwe225httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnos1l4\\n\\nhighly', 'recommend', 'read', 'stephen', 'kings', 'write'])\n",
      "original document: \n",
      "['Am', 'I', 'am', 'right', 'in', 'saying', 'there', 'was', 'similar', 'shortages', 'of', 'food', 'in', '1846/7', 'in', 'most', 'of', 'Europe?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'say', 'simil', 'short', 'food', 'eighteen thousand, four hundred and sixty-sev', 'europ'], ['right', 'say', 'similar', 'shortages', 'food', 'eighteen thousand, four hundred and sixty-seven', 'europe'])\n",
      "original document: \n",
      "['Also', 'Jules', 'Verne', 'did', 'a', 'good', 'job', 'of', 'describing', 'how', 'to', 'put', 'a', 'man', 'on', 'the', 'moon', 'using', 'a', 'giant', 'cannon', 'and', '', 'HG', 'Wells', 'provides', 'details', 'on', 'Cavorite', 'and', 'the', 'sphere', 'built', 'using', 'it', 'to', 'get', 'to', 'the', 'moon.\\n\\nThe', 'point', 'is', 'ancient', 'Hebrews', 'knew', 'how', 'to', 'build', 'a', 'ship.', 'So', 'in', 'telling', 'the', 'story', 'of', 'Noah', 'they', 'just', 'expanded', 'the', 'dimensions.', '', 'On', 'the', 'key', 'issues,', 'how', 'food', 'was', 'stored,', 'how', 'Noah', 'got', 'enough', 'gopher', 'wood', 'to', 'build', 'the', 'Ark,', 'and', 'a', 'a', 'lot', 'of', 'other', 'details', 'the', 'Bible', 'is', 'silent.\\n\\nPerhaps', 'the', 'story', 'of', \"Noah's\", 'Ark', 'is', 'the', 'ancient', 'version', 'of', 'BattleStar', 'Galatica', '?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'jul', 'vern', 'good', 'job', 'describ', 'put', 'man', 'moon', 'us', 'giant', 'cannon', 'hg', 'wel', 'provid', 'detail', 'cavorit', 'sphere', 'built', 'us', 'get', 'moon\\n\\nthe', 'point', 'ant', 'hebrew', 'knew', 'build', 'ship', 'tel', 'story', 'noah', 'expand', 'dimend', 'key', 'issu', 'food', 'stor', 'noah', 'got', 'enough', 'goph', 'wood', 'build', 'ark', 'lot', 'detail', 'bibl', 'silent\\n\\nperhaps', 'story', 'noah', 'ark', 'ant', 'vert', 'battlest', 'galatic'], ['also', 'jules', 'verne', 'good', 'job', 'describe', 'put', 'man', 'moon', 'use', 'giant', 'cannon', 'hg', 'well', 'provide', 'detail', 'cavorite', 'sphere', 'build', 'use', 'get', 'moon\\n\\nthe', 'point', 'ancient', 'hebrews', 'know', 'build', 'ship', 'tell', 'story', 'noah', 'expand', 'dimension', 'key', 'issue', 'food', 'store', 'noah', 'get', 'enough', 'gopher', 'wood', 'build', 'ark', 'lot', 'detail', 'bible', 'silent\\n\\nperhaps', 'story', 'noahs', 'ark', 'ancient', 'version', 'battlestar', 'galatica'])\n",
      "original document: \n",
      "['They', 'rent', 'too', 'as', 'do', 'other', 'tool', 'rental', 'places', 'like', 'Aurora', 'Rents.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rent', 'tool', 'rent', 'plac', 'lik', 'auror', 'rent'], ['rent', 'tool', 'rental', 'place', 'like', 'aurora', 'rent'])\n",
      "original document: \n",
      "['No', 'people', 'just', 'like', 'using', 'new', 'ways', 'to', 'insult', 'people.', 'Pretending', 'otherwise', 'would', 'be', 'a', 'lie.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'lik', 'us', 'new', 'way', 'insult', 'peopl', 'pretend', 'otherw', 'would', 'lie'], ['people', 'like', 'use', 'new', 'ways', 'insult', 'people', 'pretend', 'otherwise', 'would', 'lie'])\n",
      "original document: \n",
      "['Night', 'bud.', 'Glad', 'you', 'appreciate', 'my', 'over-sharing!', '&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['night', 'bud', 'glad', 'apprecy', 'oversh', 'lt3'], ['night', 'bud', 'glad', 'appreciate', 'oversharing', 'lt3'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['A', 'decentralized', 'grid', 'is', 'probably', 'the', 'way', 'of', 'the', 'future.', 'There', 'remains', 'much', 'to', 'be', 'worked', 'out,', 'but', \"that's\", 'the', 'fun', 'part.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dec', 'grid', 'prob', 'way', 'fut', 'remain', 'much', 'work', 'that', 'fun', 'part'], ['decentralize', 'grid', 'probably', 'way', 'future', 'remain', 'much', 'work', 'thats', 'fun', 'part'])\n",
      "original document: \n",
      "[\"That's\", 'only', 'cause', 'everybody', 'else', 'sucked', 'though', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'caus', 'everybody', 'els', 'suck', 'though'], ['thats', 'cause', 'everybody', 'else', 'suck', 'though'])\n",
      "original document: \n",
      "['I', 'know', 'its', 'crap', 'quality', 'and', 'everything', 'but', 'i', 'always', 'like', 'the', 'aesthetic', 'of', 'it.', 'Plus', 'being', 'an', 'introvert', 'myself', 'adds', 'to', 'that\\n\\nEdit:', 'its', 'also', 'the', 'brand', 'that', 'first', 'got', 'me', 'into', 'streetwear', 'in', 'general']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'crap', 'qual', 'everyth', 'alway', 'lik', 'aesthet', 'plu', 'introvert', 'ad', 'that\\n\\nedit', 'also', 'brand', 'first', 'got', 'streetwear', 'gen'], ['know', 'crap', 'quality', 'everything', 'always', 'like', 'aesthetic', 'plus', 'introvert', 'add', 'that\\n\\nedit', 'also', 'brand', 'first', 'get', 'streetwear', 'general'])\n",
      "original document: \n",
      "['Taking', 'a', 'personal', 'stance', 'here:', 'I', 'know', 'a', 'few', 'good', 'people', 'in', 'the', 'community', 'who', 'have', 'succeeded', 'and', 'are', 'happy', 'today', 'after', 'starting', 'tulpamancy', 'with', 'possession.', 'Compared', 'to', 'them', 'Koomer', 'and', 'Ougigi', 'are', 'not', 'only', 'an', 'exception,', 'but', 'as', 'many', 'others', 'have', 'pointed', 'out', 'today', 'and', 'in', 'the', 'past,', 'they', 'had', 'several', 'other', 'underlying', 'problems', 'that', 'are', 'probably', 'more', 'connected', 'to', 'their', 'issues', 'involving', 'tulpamancy', 'than', 'the', 'skills', 'they', 'practiced,', 'which', 'further', 'distances', 'the', 'problems', 'they', 'had', 'from', 'the', 'actual', 'practice.\\n\\nAlso,', 'you', 'call', 'your', 'post', 'an', 'opinion', 'here,', '[but', 'in', 'your', 'blog', 'its', 'title', \"doesn't\", 'reflect', 'the', 'same', 'stance](https://i.imgur.com/y15xA2D.png),', 'although', 'you', 'do', 'mention', 'its', 'something', 'along', 'that', 'line', 'in', 'the', 'very', 'last', 'line', 'of', 'your', 'article.\\n\\nHonestly,', 'as', 'much', 'as', 'it', 'is', 'indeed', 'your', 'opinion,', 'I', \"don't\", 'think', 'its', 'helpful', 'in', 'any', 'way', 'to', 'newcomers.', 'In', 'fact,', 'I', 'think', \"you're\", 'just', 'painting', 'the', 'skills', 'related', 'to', 'tulpamancy', 'as', 'the', 'villains', 'of', 'their', 'story.\\n\\n**Edit:**', 'reapproved', 'the', 'post.', 'Sorry', 'for', 'the', 'disruption.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'person', 'stant', 'know', 'good', 'peopl', 'commun', 'success', 'happy', 'today', 'start', 'tulpam', 'possess', 'comp', 'koom', 'ougig', 'exceiv', 'many', 'oth', 'point', 'today', 'past', 'sev', 'und', 'problem', 'prob', 'connect', 'issu', 'involv', 'tulpam', 'skil', 'pract', 'dist', 'problem', 'act', 'practice\\n\\nalso', 'cal', 'post', 'opin', 'blog', 'titl', 'doesnt', 'reflect', 'stancehttpsiimgurcomy15xa2dpng', 'although', 'ment', 'someth', 'along', 'lin', 'last', 'lin', 'article\\n\\nhonestly', 'much', 'indee', 'opin', 'dont', 'think', 'help', 'way', 'newcom', 'fact', 'think', 'yo', 'paint', 'skil', 'rel', 'tulpam', 'villain', 'story\\n\\nedit', 'reapprov', 'post', 'sorry', 'disrupt'], ['take', 'personal', 'stance', 'know', 'good', 'people', 'community', 'succeed', 'happy', 'today', 'start', 'tulpamancy', 'possession', 'compare', 'koomer', 'ougigi', 'exception', 'many', 'others', 'point', 'today', 'past', 'several', 'underlie', 'problems', 'probably', 'connect', 'issue', 'involve', 'tulpamancy', 'skills', 'practice', 'distance', 'problems', 'actual', 'practice\\n\\nalso', 'call', 'post', 'opinion', 'blog', 'title', 'doesnt', 'reflect', 'stancehttpsiimgurcomy15xa2dpng', 'although', 'mention', 'something', 'along', 'line', 'last', 'line', 'article\\n\\nhonestly', 'much', 'indeed', 'opinion', 'dont', 'think', 'helpful', 'way', 'newcomers', 'fact', 'think', 'youre', 'paint', 'skills', 'relate', 'tulpamancy', 'villains', 'story\\n\\nedit', 'reapproved', 'post', 'sorry', 'disruption'])\n",
      "original document: \n",
      "['check', 'your', 'spelling', 'bud']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['check', 'spel', 'bud'], ['check', 'spell', 'bud'])\n",
      "original document: \n",
      "['she', 'works', 'in', 'NYC,', 'of', 'course', \"she's\", 'going', 'to', 'try', 'to', 'get', 'discovered', 'as', 'an', 'actress', 'at', 'every', 'opportunity']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'nyc', 'cours', 'she', 'going', 'try', 'get', 'discov', 'actress', 'every', 'opportun'], ['work', 'nyc', 'course', 'shes', 'go', 'try', 'get', 'discover', 'actress', 'every', 'opportunity'])\n",
      "original document: \n",
      "['Thaaanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thaaank'], ['thaaanks'])\n",
      "original document: \n",
      "['Lol', \"Orel's\", 'annoyed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'orel', 'annoy'], ['lol', 'orels', 'annoy'])\n",
      "original document: \n",
      "['not', 'really', 'a', 'reference,', 'just', 'look', 'at', 'her', 'painting', 'of', 'berkut', 'and', 'rinea']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'ref', 'look', 'paint', 'berkut', 'rine'], ['really', 'reference', 'look', 'paint', 'berkut', 'rinea'])\n",
      "original document: \n",
      "['lol', 'its', 'ok', 'just', 'scared', 'the', 'hell', 'out', 'of', 'me!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'ok', 'scar', 'hel'], ['lol', 'ok', 'scar', 'hell'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['/r/ShittyMapPorn?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rshittymapporn'], ['rshittymapporn'])\n",
      "original document: \n",
      "[\"https://www.rei.com/product/828505/rei-co-op-camp-folding-cot\\n\\nIt's\", 'a', 'super', 'basic', 'platform', 'for', 'sleeping', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwreicomproduct828505reicoopcampfoldingcot\\n\\nits', 'sup', 'bas', 'platform', 'sleep'], ['httpswwwreicomproduct828505reicoopcampfoldingcot\\n\\nits', 'super', 'basic', 'platform', 'sleep'])\n",
      "original document: \n",
      "['What', 'the', 'fuck', 'is', 'that', 'supposed', 'to', 'mean?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'suppos', 'mean'], ['fuck', 'suppose', 'mean'])\n",
      "original document: \n",
      "[\"It's\", 'actually', 'a', 'percentage', 'slider.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'perc', 'slid'], ['actually', 'percentage', 'slider'])\n",
      "original document: \n",
      "['Shitty', 'doctors']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shitty', 'doct'], ['shitty', 'doctor'])\n",
      "original document: \n",
      "['great', 'drawing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'draw'], ['great', 'draw'])\n",
      "original document: \n",
      "['No']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Jeremy', 'and', 'I', 'had', 'already', 'left', 'the', 'band', 'by', 'then.', 'That', 'sucked', 'to', 'hear', 'about', 'those', 'thieves.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jeremy', 'already', 'left', 'band', 'suck', 'hear', 'thiev'], ['jeremy', 'already', 'leave', 'band', 'suck', 'hear', 'thieve'])\n",
      "original document: \n",
      "['just', 'shave', 'your', 'face', 'period.', 'Why', 'does', 'every', 'second', 'urbanite', 'today', 'look', 'like', 'a', 'cast', 'member', 'of', 'chapo', 'trap', 'house']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shav', 'fac', 'period', 'every', 'second', 'urbanit', 'today', 'look', 'lik', 'cast', 'memb', 'chapo', 'trap', 'hous'], ['shave', 'face', 'period', 'every', 'second', 'urbanite', 'today', 'look', 'like', 'cast', 'member', 'chapo', 'trap', 'house'])\n",
      "original document: \n",
      "['IE:', 'That', 'the', 'planets', 'Luke', 'visited', 'in-story', 'before', 'landing', 'on', 'Ahch-To', '-', 'as', 'seen', 'briefly', 'in', 'TFA', 'with', 'the', 'whole', 'map', 'subplot', '-', 'were', 'important', 'for', 'his', 'journey', 'to', 'said', 'planet', 'where', 'the', 'First', 'Jedi', 'Temple', 'resides.', \"(Couldn't\", 'really', 'explain', 'that', 'in', 'a', 'headline.)\\n\\nAlso,', \"it's\", 'been', 'ages', 'since', 'we', 'had', 'a', 'thread', 'on', 'one', 'of', 'his', 'tweets,', 'so', 'I', 'figured', 'that', \"it's\", 'worth', 'making', 'one', '-', 'especially', 'considering', 'that', \"there's\", 'not', 'a', 'lot', 'on', 'r/TheSupremePablo', 'right', 'now.', \"I'd\", 'really', 'like', 'to', 'figure', 'out', 'the', 'specifics', 'of', \"Luke's\", 'journey.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ie', 'planet', 'luk', 'visit', 'inst', 'land', 'ahchto', 'seen', 'brief', 'tfa', 'whol', 'map', 'subplot', 'import', 'journey', 'said', 'planet', 'first', 'jed', 'templ', 'resid', 'couldnt', 'real', 'explain', 'headline\\n\\nalso', 'ag', 'sint', 'thread', 'on', 'tweet', 'fig', 'wor', 'mak', 'on', 'espec', 'consid', 'ther', 'lot', 'rthesupremepablo', 'right', 'id', 'real', 'lik', 'fig', 'spec', 'luk', 'journey'], ['ie', 'planets', 'luke', 'visit', 'instory', 'land', 'ahchto', 'see', 'briefly', 'tfa', 'whole', 'map', 'subplot', 'important', 'journey', 'say', 'planet', 'first', 'jedi', 'temple', 'reside', 'couldnt', 'really', 'explain', 'headline\\n\\nalso', 'age', 'since', 'thread', 'one', 'tweet', 'figure', 'worth', 'make', 'one', 'especially', 'consider', 'theres', 'lot', 'rthesupremepablo', 'right', 'id', 'really', 'like', 'figure', 'specifics', 'lukes', 'journey'])\n",
      "original document: \n",
      "['Not', 'all', \"hero's\", 'wear', 'capes.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hero', 'wear', 'cap'], ['heros', 'wear', 'cap'])\n",
      "original document: \n",
      "['Yeah', 'they', 'an', 'be', 'very', 'strict.', 'I', 'love', 'Columbus', 'because', 'of', 'all', 'the', 'brick', 'buildings', 'and', 'my', 'family', 'is', 'here', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'strict', 'lov', 'columb', 'brick', 'build', 'famy'], ['yeah', 'strict', 'love', 'columbus', 'brick', 'build', 'family'])\n",
      "original document: \n",
      "['Very', 'hot', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hot'], ['hot'])\n",
      "original document: \n",
      "['Is', 'it', 'on', 'youtube', 'yet?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youtub', 'yet'], ['youtube', 'yet'])\n",
      "original document: \n",
      "['Not', 'surprised', 'you', \"can't\", 'remember.', 'Adrenaline', 'fucks', 'with', 'the', 'memory.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['surpr', 'cant', 'rememb', 'adrenalin', 'fuck', 'mem'], ['surprise', 'cant', 'remember', 'adrenaline', 'fuck', 'memory'])\n",
      "original document: \n",
      "['It', 'should', 'be', 'noted', 'that', 'it', 'probably', 'only', 'works', 'if', 'most', 'of', 'your', 'pelvis', 'isnt', 'covered', 'by', 'your', 'belly.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['not', 'prob', 'work', 'pelv', 'isnt', 'cov', 'bel'], ['note', 'probably', 'work', 'pelvis', 'isnt', 'cover', 'belly'])\n",
      "original document: \n",
      "['I', 'suggest', 'glassesshop.com.', 'You', 'can', 'get', 'your', 'first', 'pair', 'for', 'free', 'and', 'shipping', 'is', 'only', '$5.', 'You', 'can', 'add', 'anti', 'reflection', 'for', '$5', 'more.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['suggest', 'glassesshopcom', 'get', 'first', 'pair', 'fre', 'ship', 'fiv', 'ad', 'ant', 'reflect', 'fiv'], ['suggest', 'glassesshopcom', 'get', 'first', 'pair', 'free', 'ship', 'five', 'add', 'anti', 'reflection', 'five'])\n",
      "original document: \n",
      "['RT', 'I', 'think', 'is', 'the', 'same', 'as', 'Mouse', '1', 'in', 'the', 'control', 'scheme.', 'So', \"he's\", 'saying', \"don't\", 'spam', 'beam', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rt', 'think', 'mous', 'on', 'control', 'scheme', 'hes', 'say', 'dont', 'spam', 'beam'], ['rt', 'think', 'mouse', 'one', 'control', 'scheme', 'hes', 'say', 'dont', 'spam', 'beam'])\n",
      "original document: \n",
      "['You', 'forgot', 'to', 'post', 'your', 'IGSRep', 'page.', 'I', 'have', 'tried', 'to', 'find', 'it', 'for', 'you', '[Click', 'Here](http://www.reddit.com/r/IGSRep/search?q=DogsOnWeed%27s+IGS+Rep+Page&amp;restrict_sr=on).\\n\\nI', 'have', 'removed', 'this', 'thread.', 'You', 'are', 'required', 'to', 'have', 'a', 'IGSRep', 'page', 'and', 'post', 'it', 'in', 'your', 'thread', 'to', 'trade', 'on', 'this', 'subreddit.', 'Feel', 'free', 'to', 'repost', 'this', 'trade', 'and', 'add', 'your', 'IGSRep', 'page.', 'If', 'you', 'cannot', 'repost', 'this,', 'please', 'message', 'the', 'mods', 'and', 'we', 'will', 'see', 'about', 'reactivating', 'this', 'thread.', 'It', 'is', 'always', 'required', 'to', 'post', 'your', 'IGSrep', 'page,', 'but', 'once', 'you', 'upgrade', 'to', '\"Trader\"', 'flair', 'Automod', 'will', 'become', 'a', 'little', 'more', 'forgiving.\\n\\n**[Click', 'here', 'to', 'create', 'a', 'IGSRep!](https://www.reddit.com/r/IGSRep/wiki/index)**\\n\\n\\n*Tip:', 'Do', 'not', 'use', 'URL', 'shortners*\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/indiegameswap)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['forgot', 'post', 'igsrep', 'pag', 'tri', 'find', 'click', 'herehttpwwwredditcomrigsrepsearchqdogsonweed27sigsreppageamprestrict_sron\\n\\ni', 'remov', 'thread', 'requir', 'igsrep', 'pag', 'post', 'thread', 'trad', 'subreddit', 'feel', 'fre', 'repost', 'trad', 'ad', 'igsrep', 'pag', 'cannot', 'repost', 'pleas', 'mess', 'mod', 'see', 'react', 'thread', 'alway', 'requir', 'post', 'igsrep', 'pag', 'upgrad', 'trad', 'flair', 'automod', 'becom', 'littl', 'forgiving\\n\\nclick', 'cre', 'igsrephttpswwwredditcomrigsrepwikiindex\\n\\n\\ntip', 'us', 'url', 'shortners\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorindiegameswap', 'quest', 'concern'], ['forget', 'post', 'igsrep', 'page', 'try', 'find', 'click', 'herehttpwwwredditcomrigsrepsearchqdogsonweed27sigsreppageamprestrict_sron\\n\\ni', 'remove', 'thread', 'require', 'igsrep', 'page', 'post', 'thread', 'trade', 'subreddit', 'feel', 'free', 'repost', 'trade', 'add', 'igsrep', 'page', 'cannot', 'repost', 'please', 'message', 'mods', 'see', 'reactivate', 'thread', 'always', 'require', 'post', 'igsrep', 'page', 'upgrade', 'trader', 'flair', 'automod', 'become', 'little', 'forgiving\\n\\nclick', 'create', 'igsrephttpswwwredditcomrigsrepwikiindex\\n\\n\\ntip', 'use', 'url', 'shortners\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorindiegameswap', 'question', 'concern'])\n",
      "original document: \n",
      "['Yep.', 'The', 'auction', 'house', 'will', 'be', 'in', 'this', 'game', 'just', 'says', 'coming', 'soon', 'at', 'the', 'moment.', \"They're\", 'probably', 'waiting', 'til', 'Tuesday', 'to', 'activate', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'auct', 'hous', 'gam', 'say', 'com', 'soon', 'mom', 'theyr', 'prob', 'wait', 'til', 'tuesday', 'act'], ['yep', 'auction', 'house', 'game', 'say', 'come', 'soon', 'moment', 'theyre', 'probably', 'wait', 'til', 'tuesday', 'activate'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnotht2/):\\n\\nEXCELLENT', 'BOOK']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnotht2\\n\\nexcellent', 'book'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnotht2\\n\\nexcellent', 'book'])\n",
      "original document: \n",
      "[\"It's\", 'like', 'Marshall', 'is', 'infecting', 'them', 'all']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'marshal', 'infect'], ['like', 'marshall', 'infect'])\n",
      "original document: \n",
      "['More', 'importantly,', \"where's\", 'the', 'turkey?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['import', 'wher', 'turkey'], ['importantly', 'wheres', 'turkey'])\n",
      "original document: \n",
      "['Lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['Scathatch', 'or', 'whatever', 'the', 'crazy', 'sex', 'witch', 'was', 'called', 'in', 'Roanoke.\\n\\nLady', 'Gaga', 'played', 'her', 'during', 'the', 'first', 'part', 'of', 'the', 'season,', 'which', 'was', 'the', 'dramatic', 'reenactment', 'show.', 'So', \"they're\", 'saying', 'what', 'if', 'Gaga', 'herself', 'was', 'an', 'in', 'universe', 'actor', 'playing', 'that', 'part', 'on', 'the', '\"show\".', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['scathatch', 'whatev', 'crazy', 'sex', 'witch', 'cal', 'roanoke\\n\\nlady', 'gag', 'play', 'first', 'part', 'season', 'dram', 'reenact', 'show', 'theyr', 'say', 'gag', 'univers', 'act', 'play', 'part', 'show'], ['scathatch', 'whatever', 'crazy', 'sex', 'witch', 'call', 'roanoke\\n\\nlady', 'gaga', 'play', 'first', 'part', 'season', 'dramatic', 'reenactment', 'show', 'theyre', 'say', 'gaga', 'universe', 'actor', 'play', 'part', 'show'])\n",
      "original document: \n",
      "['All', 'the', 'feels.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feel'], ['feel'])\n",
      "original document: \n",
      "['#FuuUuuUUuUUuuUUuUUuuuuck', 'off!\\n\\nI', '*like*', 'midichlorians!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuuuuuuuuuuuuuuuuuuuuuck', 'off\\n\\ni', 'lik', 'midichl'], ['fuuuuuuuuuuuuuuuuuuuuuck', 'off\\n\\ni', 'like', 'midichlorians'])\n",
      "original document: \n",
      "['143413598|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'eFvRVltr)\\n\\n&gt;&gt;143413258\\nYour', 'gender', 'hurt', 'my', 'feelings\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, five hundred and ninety-eight', 'gt', 'unit', 'stat', 'anonym', 'id', 'efvrvltr\\n\\ngtgt143413258\\nyo', 'gend', 'hurt', 'feelings\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, five hundred and ninety-eight', 'gt', 'unite', 'state', 'anonymous', 'id', 'efvrvltr\\n\\ngtgt143413258\\nyour', 'gender', 'hurt', 'feelings\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['/r/politics', 'basically', 'blames', 'everything', 'on', 'white', 'males.', 'I', 'wish', 'I', 'could', 'find', 'somewhere', 'to', 'discuss', 'politics', 'without', 'all', 'that', 'bullshit.', 'Maybe', 'you', 'and', 'me', 'could', 'start', 'our', 'own', 'identity', 'politics', 'free', 'sub', 'reddit', 'together.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rpolit', 'bas', 'blam', 'everyth', 'whit', 'mal', 'wish', 'could', 'find', 'somewh', 'discuss', 'polit', 'without', 'bullshit', 'mayb', 'could', 'start', 'id', 'polit', 'fre', 'sub', 'reddit', 'togeth'], ['rpolitics', 'basically', 'blame', 'everything', 'white', 'males', 'wish', 'could', 'find', 'somewhere', 'discuss', 'politics', 'without', 'bullshit', 'maybe', 'could', 'start', 'identity', 'politics', 'free', 'sub', 'reddit', 'together'])\n",
      "original document: \n",
      "['https://www.youtube.com/watch?v=nLsCC0LZxkY', 'in', 'case', 'you', 'wanna', 'see', 'the', 'video.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwyoutubecomwatchvnlscc0lzxky', 'cas', 'wann', 'see', 'video'], ['httpswwwyoutubecomwatchvnlscc0lzxky', 'case', 'wanna', 'see', 'video'])\n",
      "original document: \n",
      "['The', 'pure', 'joy', 'on', 'the', \"guy's\", 'face', 'lmao']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pur', 'joy', 'guy', 'fac', 'lmao'], ['pure', 'joy', 'guy', 'face', 'lmao'])\n",
      "original document: \n",
      "['No,', \"I'm\", 'saying', 'many', 'of', 'them', 'are', 'repeat', 'offenders,', \"that's\", 'why', 'they', 'had', 'to', 'highlight', 'two', 'specific', 'people', 'and', 'say', 'they', 'had', 'equal', 'backgrounds']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'say', 'many', 'rep', 'offend', 'that', 'highlight', 'two', 'spec', 'peopl', 'say', 'eq', 'background'], ['im', 'say', 'many', 'repeat', 'offenders', 'thats', 'highlight', 'two', 'specific', 'people', 'say', 'equal', 'background'])\n",
      "original document: \n",
      "['😂😂😂😂😂']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Hmmm', 'ok', 'thanks,', '', 'one', 'more', 'thing,', 'my', 'mav', 'lacks', '4', 'skillups', '3', 'on', 'his', 's1', 'and', '1', 'on', 'his', 's2,', 'do', 'you', 'think', 'that', 'will', 'be', 'a', 'problem?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmmm', 'ok', 'thank', 'on', 'thing', 'mav', 'lack', 'four', 'skillup', 'three', 's1', 'on', 's2', 'think', 'problem'], ['hmmm', 'ok', 'thank', 'one', 'thing', 'mav', 'lack', 'four', 'skillups', 'three', 's1', 'one', 's2', 'think', 'problem'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"I'm\", 'glad', 'that', 'you', 'lived', 'to', 'tell', 'the', 'tale.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'glad', 'liv', 'tel', 'tal'], ['im', 'glad', 'live', 'tell', 'tale'])\n",
      "original document: \n",
      "[\"Where's\", 'that', 'Daily', 'Dose', 'photoshop']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wher', 'dai', 'dos', 'photoshop'], ['wheres', 'daily', 'dose', 'photoshop'])\n",
      "original document: \n",
      "['Thank', 'you', 'for', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank'], ['thank'])\n",
      "original document: \n",
      "['Congrats', 'on', 'a', 'year!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congr', 'year'], ['congrats', 'year'])\n",
      "original document: \n",
      "[\"He's\", 'right,', 'at', 'some', 'point', 'it', 'was', 'patched', 'and', 'defenders', 'got', 'a', 'bit', 'weaker.', 'They', 'were', 'still', 'very', 'OP,', 'but', 'they', 'used', 'to', 'be', 'absolutely', 'insane', 'on', 'the', 'first', 'days/weeks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'right', 'point', 'patch', 'defend', 'got', 'bit', 'weak', 'stil', 'op', 'us', 'absolv', 'ins', 'first', 'daysweek'], ['hes', 'right', 'point', 'patch', 'defenders', 'get', 'bite', 'weaker', 'still', 'op', 'use', 'absolutely', 'insane', 'first', 'daysweeks'])\n",
      "original document: \n",
      "['Do', 'you', 'know', 'why', 'they', 'removed', 'it', 'from', 'rotation?', 'Was', 'there', 'glitches', 'or', 'anything', 'on', 'that', 'map', '?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'remov', 'rot', 'glitch', 'anyth', 'map'], ['know', 'remove', 'rotation', 'glitches', 'anything', 'map'])\n",
      "original document: \n",
      "['Seattle?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seattl'], ['seattle'])\n",
      "original document: \n",
      "['FUCK', 'OFF', 'AND', 'DIE']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'die'], ['fuck', 'die'])\n",
      "original document: \n",
      "['That’s', 'what', 'I', 'got', 'on', 'my', 'first', 'win...', 'absolutely', 'wounded.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'got', 'first', 'win', 'absolv', 'wound'], ['thats', 'get', 'first', 'win', 'absolutely', 'wound'])\n",
      "original document: \n",
      "['I', 'wish', 'there', \"weren't\", 'any', 'armed', 'vehicles', 'at', 'all.', 'This', 'is', 'Grand', 'Theft', 'Auto,', 'not', 'Twisted', 'Metal.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wish', 'wer', 'arm', 'vehic', 'grand', 'theft', 'auto', 'twist', 'met'], ['wish', 'werent', 'arm', 'vehicles', 'grand', 'theft', 'auto', 'twist', 'metal'])\n",
      "original document: \n",
      "['Few', 'will', 'try', 'to', 'paint', 'a', 'picture', 'of', 'perfect', 'innocince.', 'Most', 'will', 'present', 'a', 'facade', 'of', 'honesty', 'by', 'telling', 'you', 'about', 'half', 'the', 'story,', 'if', 'the', 'topic', 'of', 'their', 'sexual', 'history', 'ever', 'comes', 'up.', 'And', 'at', 'the', 'opposite', 'end', 'of', 'the', 'spectrum', 'are', 'the', 'few', 'girls', 'who', 'will', 'look', 'you', 'in', 'the', 'eye', 'and', 'tell', 'you', 'the', '(supposedly)', 'entire', 'nauseating', 'truth,', 'in', 'which', 'case', \"you'll\", 'almost', 'always', 'wish', 'they', \"haden't.\"]\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'paint', 'pict', 'perfect', 'innocint', 'pres', 'facad', 'honesty', 'tel', 'half', 'story', 'top', 'sex', 'hist', 'ev', 'com', 'opposit', 'end', 'spectr', 'girl', 'look', 'ey', 'tel', 'suppos', 'entir', 'naus', 'tru', 'cas', 'youl', 'almost', 'alway', 'wish', 'had'], ['try', 'paint', 'picture', 'perfect', 'innocince', 'present', 'facade', 'honesty', 'tell', 'half', 'story', 'topic', 'sexual', 'history', 'ever', 'come', 'opposite', 'end', 'spectrum', 'girls', 'look', 'eye', 'tell', 'supposedly', 'entire', 'nauseate', 'truth', 'case', 'youll', 'almost', 'always', 'wish', 'hadent'])\n",
      "original document: \n",
      "['Kirk', '', 'bulking', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kirk', 'bulk'], ['kirk', 'bulk'])\n",
      "original document: \n",
      "['Easily', 'doable', 'if', 'I', 'sit', 'and', 'fix', 'it', 'in', 'Lightroom.', 'I', 'was', 'more', 'looking', 'at', 'the', 'colors', 'and', 'what', 'the', 'photo', 'consists', 'of', 'to', 'even', 'worry', 'about', 'the', 'horizon.', 'Thanks!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['easy', 'doabl', 'sit', 'fix', 'lightroom', 'look', 'col', 'photo', 'consist', 'ev', 'worry', 'horizon', 'thank'], ['easily', 'doable', 'sit', 'fix', 'lightroom', 'look', 'color', 'photo', 'consist', 'even', 'worry', 'horizon', 'thank'])\n",
      "original document: \n",
      "['Ah,', 'good', 'to', 'know.', 'So', 'many', 'weird', 'events', 'with', 'carousing.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'good', 'know', 'many', 'weird', 'ev', 'car'], ['ah', 'good', 'know', 'many', 'weird', 'events', 'carouse'])\n",
      "original document: \n",
      "['To', 'be', 'fair,', 'Troy', 'gave', 'Clemson', 'a', 'scare', 'last', 'year.', 'They', 'are', 'competent.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fair', 'troy', 'gav', 'clemson', 'scar', 'last', 'year', 'compet'], ['fair', 'troy', 'give', 'clemson', 'scare', 'last', 'year', 'competent'])\n",
      "original document: \n",
      "['Haha', 'try', 'again.', 'Last', 'time', 'we', 'were', 'shut', 'out', 'AT', 'HOME', 'was', '1923.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'try', 'last', 'tim', 'shut', 'hom', 'one thousand, nine hundred and twenty-thr'], ['haha', 'try', 'last', 'time', 'shut', 'home', 'one thousand, nine hundred and twenty-three'])\n",
      "original document: \n",
      "['It', 'was', 'her', 'idea', 'but', 'then', 'they', 'collaborated', 'and', 'we', \"don't\", 'see', 'the', 'whole', 'process', 'we', 'see', 'a', 'snippet', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ide', 'collab', 'dont', 'see', 'whol', 'process', 'see', 'snippet'], ['idea', 'collaborate', 'dont', 'see', 'whole', 'process', 'see', 'snippet'])\n",
      "original document: \n",
      "['Your', 'desperation', 'to', 'seem', 'edgy', 'and', 'relevant', 'is', 'only', 'outdone', 'with', 'how', 'pathetically', 'sad', 'this', 'comment', 'is.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['desp', 'seem', 'edgy', 'relev', 'outdon', 'pathet', 'sad', 'com'], ['desperation', 'seem', 'edgy', 'relevant', 'outdo', 'pathetically', 'sad', 'comment'])\n",
      "original document: \n",
      "['Because', \"they're\", 'heavies,', 'same', 'as', 'her', 'dodge', 'attacks.', 'Most', 'characters', 'sprints', 'counts', 'as', 'lights.', 'But', 'as', 'for', 'why...', 'I', 'guess', 'since', \"she's\", 'an', 'assassins', 'and', 'supposed', 'to', 'hunt', 'down', 'targets.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['theyr', 'heavy', 'dodg', 'attack', 'charact', 'sprints', 'count', 'light', 'guess', 'sint', 'she', 'assassin', 'suppos', 'hunt', 'target'], ['theyre', 'heavies', 'dodge', 'attack', 'character', 'sprint', 'count', 'light', 'guess', 'since', 'shes', 'assassins', 'suppose', 'hunt', 'target'])\n",
      "original document: \n",
      "['$10', 'to', 'mail', 'a', 'single', 'sheet', 'of', 'paper?', 'Those', 'were', 'for', 'everyone', 'you', 'rat', 'fuck.', \"\\n\\nYou're\", 'lying', 'to', 'yourself', 'if', 'you', 'think', 'a', 'few', 'dollars', 'going', 'towards', 'holocaust', 'awareness', 'compensates', 'for', 'such', 'a', 'selfish', 'act.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ten', 'mail', 'singl', 'sheet', 'pap', 'everyon', 'rat', 'fuck', '\\n\\nyoure', 'lying', 'think', 'doll', 'going', 'toward', 'holocaust', 'aw', 'compens', 'self', 'act'], ['ten', 'mail', 'single', 'sheet', 'paper', 'everyone', 'rat', 'fuck', '\\n\\nyoure', 'lie', 'think', 'dollars', 'go', 'towards', 'holocaust', 'awareness', 'compensate', 'selfish', 'act'])\n",
      "original document: \n",
      "['Dragging', 'small', 'scenes', 'for', 'too', 'long', 'as', 'episode', 'fillers']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['drag', 'smal', 'scen', 'long', 'episod', 'fil'], ['drag', 'small', 'scenes', 'long', 'episode', 'fillers'])\n",
      "original document: \n",
      "['well', 'yes', 'but', \"I'm\", 'talking', 'about', 'the', 'retards', 'here', 'that', 'are', 'like', '17', 'and', 'claim', 'to', 'be', 'incel,', 'fucking', 'lol', 'at', 'these', 'literal', 'children']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'ye', 'im', 'talk', 'retard', 'lik', 'seventeen', 'claim', 'incel', 'fuck', 'lol', 'lit', 'childr'], ['well', 'yes', 'im', 'talk', 'retard', 'like', 'seventeen', 'claim', 'incel', 'fuck', 'lol', 'literal', 'children'])\n",
      "original document: \n",
      "['\"Ya!\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya'], ['ya'])\n",
      "original document: \n",
      "['Top', '5', 'athlete?', 'What', 'are', 'your', 'standards', 'to', 'get', 'into', 'the', 'top', '5?', '\\n\\nHe', 'might', 'not', 'even', 'be', 'the', 'greatest', 'athlete', 'in', 'America', 'right', 'now.', 'I', 'would', 'argue', 'that', 'hes', 'not', 'even', 'the', 'best', 'basketball', 'player', 'right', 'now', 'and', 'then', 'reluctantly', 'agree', 'that', 'he', 'still', 'is,', 'as', 'of', 'the', 'last', 'series', 'he', 'played.', 'That', 'says', 'something', 'but', 'top', '5', 'is', 'rediculous.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['top', 'fiv', 'athlet', 'standard', 'get', 'top', 'fiv', '\\n\\nhe', 'might', 'ev', 'greatest', 'athlet', 'americ', 'right', 'would', 'argu', 'hes', 'ev', 'best', 'basketbal', 'play', 'right', 'reluct', 'agr', 'stil', 'last', 'sery', 'play', 'say', 'someth', 'top', 'fiv', 'redic'], ['top', 'five', 'athlete', 'standards', 'get', 'top', 'five', '\\n\\nhe', 'might', 'even', 'greatest', 'athlete', 'america', 'right', 'would', 'argue', 'hes', 'even', 'best', 'basketball', 'player', 'right', 'reluctantly', 'agree', 'still', 'last', 'series', 'play', 'say', 'something', 'top', 'five', 'rediculous'])\n",
      "original document: \n",
      "['I', 'never', 'once', 'got', 'the', 'impression', 'that', 'Alphamon', 'was', 'trying', 'to', 'protect', 'Meicoomon.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nev', 'got', 'impress', 'alphamon', 'try', 'protect', 'meicoomon'], ['never', 'get', 'impression', 'alphamon', 'try', 'protect', 'meicoomon'])\n",
      "original document: \n",
      "['Hmmm...', \"I've\", 'had', 'it', 'all', 'wrong', 'this', 'whole', 'time.', 'I', 'swear', 'they', 'taught', 'me', 'in', 'our', 'public', 'education', 'system', 'that', 'we,', 'as', 'Americans,', 'have', 'the', 'right', 'to', 'peaceful', 'protests.\\n\\nI', 'guess', 'Kyrie', 'is', 'right.', 'We', 'really', 'need', 'to', 'question/research', 'everything', 'that', \"we're\", 'told.', '', '', '\\n\\n*', 'Edit:', '', 'Wait', 'a', 'second,', 'after', 'research', \"I've\", 'confirmed', 'that', 'we', 'do', 'have', 'the', 'right', 'to', 'protest.', 'So,', '', 'WTF', '', 'is', 'traitorous', 'about', 'protesting?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hmmm', 'iv', 'wrong', 'whol', 'tim', 'swear', 'taught', 'publ', 'educ', 'system', 'am', 'right', 'peac', 'protests\\n\\ni', 'guess', 'kyry', 'right', 'real', 'nee', 'questionresearch', 'everyth', 'told', '\\n\\n', 'edit', 'wait', 'second', 'research', 'iv', 'confirm', 'right', 'protest', 'wtf', 'trait', 'protest'], ['hmmm', 'ive', 'wrong', 'whole', 'time', 'swear', 'teach', 'public', 'education', 'system', 'americans', 'right', 'peaceful', 'protests\\n\\ni', 'guess', 'kyrie', 'right', 'really', 'need', 'questionresearch', 'everything', 'tell', '\\n\\n', 'edit', 'wait', 'second', 'research', 'ive', 'confirm', 'right', 'protest', 'wtf', 'traitorous', 'protest'])\n",
      "original document: \n",
      "['~$80-$100']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eighty thousand, one hundred'], ['eighty thousand, one hundred'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['..how?\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['how\\n'], ['how\\n'])\n",
      "original document: \n",
      "['can', 'anyone', 'estimate', 'whats', 'the', 'most', 'likely', 'way', 'forward', 'to', 'fix', 'that', 'plane?', 'they', 'will', 'try', 'to', 'replace', 'engine', 'on', 'that', 'remote', 'location,', 'or', 'send', 'it', 'back', 'on', '3', 'engines', 'to', 'an', 'AF/Airbus', 'base?', 'and', 'how', 'long', 'will', 'it', 'take', 'for', 'it', 'to', 'be', 'added', 'back', 'to', 'service?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'estim', 'what', 'lik', 'way', 'forward', 'fix', 'plan', 'try', 'replac', 'engin', 'remot', 'loc', 'send', 'back', 'three', 'engin', 'afairb', 'bas', 'long', 'tak', 'ad', 'back', 'serv'], ['anyone', 'estimate', 'whats', 'likely', 'way', 'forward', 'fix', 'plane', 'try', 'replace', 'engine', 'remote', 'location', 'send', 'back', 'three', 'engines', 'afairbus', 'base', 'long', 'take', 'add', 'back', 'service'])\n",
      "original document: \n",
      "['Whoosh!', 'Nice', 'job,', 'fellow', 'comet.', \"I'm\", 'starting', 'my', 'own', 'beard', 'journey.', 'I', 'might', 'just', 'copy', 'this', 'way', 'of', 'showing', 'progress']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['whoosh', 'nic', 'job', 'fellow', 'comet', 'im', 'start', 'beard', 'journey', 'might', 'cop', 'way', 'show', 'progress'], ['whoosh', 'nice', 'job', 'fellow', 'comet', 'im', 'start', 'beard', 'journey', 'might', 'copy', 'way', 'show', 'progress'])\n",
      "original document: \n",
      "['I', 'guess', 'the', 'beauty', 'and', 'the', 'beast', 'rt', 'score', 'is', 'close', 'enough?', '\\n\\nWay', 'off', 'on', 'spiderman', 'though...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['guess', 'beauty', 'beast', 'rt', 'scor', 'clos', 'enough', '\\n\\nway', 'spiderm', 'though'], ['guess', 'beauty', 'beast', 'rt', 'score', 'close', 'enough', '\\n\\nway', 'spiderman', 'though'])\n",
      "original document: \n",
      "[\"Doesn't\", 'the', 'study', 'have', 'to', 'be', 'done', 'post-mortem?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['doesnt', 'study', 'don', 'postmortem'], ['doesnt', 'study', 'do', 'postmortem'])\n",
      "original document: \n",
      "['SoDak', 'baby!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sodak', 'baby'], ['sodak', 'baby'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Donate', 'clan?', 'Someone', 'pulled', 'the', 'same', 'thing', 'in', 'there.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['don', 'clan', 'someon', 'pul', 'thing'], ['donate', 'clan', 'someone', 'pull', 'thing'])\n",
      "original document: \n",
      "['That', \"he's\", 'perfect', 'and', 'deserves', 'a', 'perfect', 'person', 'and', 'i', 'am', 'far', 'from', 'perfect']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'perfect', 'deserv', 'perfect', 'person', 'far', 'perfect'], ['hes', 'perfect', 'deserve', 'perfect', 'person', 'far', 'perfect'])\n",
      "original document: \n",
      "[\"You're\", 'the', 'real', 'hero', 'u/Archaeo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'real', 'hero', 'uarchaeo'], ['youre', 'real', 'hero', 'uarchaeo'])\n",
      "original document: \n",
      "['https://i.imgur.com/0HfzqNp.jpg\\n\\nIt', 'finally', 'came', 'in', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsiimgurcom0hfzqnpjpg\\n\\nit', 'fin', 'cam'], ['httpsiimgurcom0hfzqnpjpg\\n\\nit', 'finally', 'come'])\n",
      "original document: \n",
      "['Yep.', 'As', 'always', 'its', 'a', 'case', 'of', '*beware', 'the', 'simple', 'narrative.*\\n\\n\"Evil', 'breeders', 'are', 'churning', 'out', 'puppies', 'with', 'cleft', 'pallets', 'because', 'people', 'find', 'them', 'cute\"', '/gasp', 'from', 'audience.\\n\\nIf', 'its', 'simple', 'and', 'unnuanced', 'then', 'its', 'probably', 'bullshit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'alway', 'cas', 'bew', 'simpl', 'narrative\\n\\nevil', 'bree', 'churn', 'puppy', 'cleft', 'pallet', 'peopl', 'find', 'cut', 'gasp', 'audience\\n\\nif', 'simpl', 'unnu', 'prob', 'bullshit'], ['yep', 'always', 'case', 'beware', 'simple', 'narrative\\n\\nevil', 'breeders', 'churn', 'puppies', 'cleave', 'pallets', 'people', 'find', 'cute', 'gasp', 'audience\\n\\nif', 'simple', 'unnuanced', 'probably', 'bullshit'])\n",
      "original document: \n",
      "['&gt;', '(I', 'like', 'Obama,', 'but', 'that', 'Peace', 'prize', 'was', 'a', 'joke).', '\\n\\nTo', 'be', 'fair,', \"I'm\", 'pretty', 'sure', 'Obama', 'thinks', 'the', 'same', 'thing']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'lik', 'obam', 'peac', 'priz', 'jok', '\\n\\nto', 'fair', 'im', 'pretty', 'sur', 'obam', 'think', 'thing'], ['gt', 'like', 'obama', 'peace', 'prize', 'joke', '\\n\\nto', 'fair', 'im', 'pretty', 'sure', 'obama', 'think', 'thing'])\n",
      "original document: \n",
      "['https://i.imgur.com/caMm6N8.jpg']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsiimgurcomcamm6n8jpg'], ['httpsiimgurcomcamm6n8jpg'])\n",
      "original document: \n",
      "['Damn,', 'just', 'paid', 'for', 'him', 'and', \"he's\", 'a', 'beast.', 'Enjoy!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'paid', 'hes', 'beast', 'enjoy'], ['damn', 'pay', 'hes', 'beast', 'enjoy'])\n",
      "original document: \n",
      "['This', 'submission', 'has', 'been', 'automatically', 'removed', 'because', 'you', 'did', 'not', 'include', 'a', 'question', 'mark', '(\"?\")', 'in', 'your', 'title.', 'Reddit', 'does', 'not', 'allow', 'post', 'titles', 'to', 'be', 'edited,', 'so', 'if', 'you', 'would', 'like,', 'you', 'can', 'post', 'the', 'question', 'again.', '', 'Please', 'write', 'your', 'title', 'in', 'proper', 'question', 'format,', 'and', 'include', 'a', 'question', 'mark,', 'thank', 'you.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/AskReddit)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'autom', 'remov', 'includ', 'quest', 'mark', 'titl', 'reddit', 'allow', 'post', 'titl', 'edit', 'would', 'lik', 'post', 'quest', 'pleas', 'writ', 'titl', 'prop', 'quest', 'form', 'includ', 'quest', 'mark', 'thank', 'you\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoraskreddit', 'quest', 'concern'], ['submission', 'automatically', 'remove', 'include', 'question', 'mark', 'title', 'reddit', 'allow', 'post', 'title', 'edit', 'would', 'like', 'post', 'question', 'please', 'write', 'title', 'proper', 'question', 'format', 'include', 'question', 'mark', 'thank', 'you\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoraskreddit', 'question', 'concern'])\n",
      "original document: \n",
      "['[+LostVaultDweller](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnokq7s/):\\n\\nYou', 'can', 'try', 'to', 'find', 'an', 'agent', 'who', 'pushes', 'your', 'novel', 'to', 'publishers.', 'Look', 'into', 'that', '(obviously', \"I'm\", 'expecting', 'you', 'to', 'do', 'a', 'ton', 'of', 'research', 'on', 'your', 'own.', 'Just', 'presenting', 'an', 'option', 'you', 'may', 'have', 'not', 'known', 'about)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lostvaultdwellerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokq7s\\n\\nyou', 'try', 'find', 'ag', 'push', 'novel', 'publ', 'look', 'obvy', 'im', 'expect', 'ton', 'research', 'pres', 'opt', 'may', 'known'], ['lostvaultdwellerhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnokq7s\\n\\nyou', 'try', 'find', 'agent', 'push', 'novel', 'publishers', 'look', 'obviously', 'im', 'expect', 'ton', 'research', 'present', 'option', 'may', 'know'])\n",
      "original document: \n",
      "['When', 'Porsche', 'was', 'added', 'in', 'FM6', 'this', 'was', 'one', 'of', 'the', 'first', 'things', 'I', 'noticed', 'about', 'every', '911.', 'They', 'handle', 'differently', 'than', 'any', 'other', 'car', 'in', 'the', 'game', 'because', 'of', 'the', 'rear', 'engine', 'layout.', 'Once', 'you', 'figure', 'out', 'how', 'to', 'modulate', 'the', 'throttle', 'and', 'brake', 'through', 'corners', 'in', 'order', 'to', 'get', 'your', 'suspension', 'leveled', 'correctly', 'they', 'are', 'ridiculously', 'fast', 'cars.', '\\n\\nI', 'also', 'had', 'a', 'ton', 'of', 'trouble', 'driving', 'the', 'GT2', 'RS', 'in', 'the', 'demo', 'but', 'once', 'I', 'figured', 'out', 'that', 'you', 'drive', 'this', 'car', 'with', 'the', 'throttle', 'it', 'became', 'incredibly', 'fast.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['porsch', 'ad', 'fm6', 'on', 'first', 'thing', 'not', 'every', 'nine hundred and eleven', 'handl', 'diff', 'car', 'gam', 'rear', 'engin', 'layout', 'fig', 'mod', 'throttle', 'brak', 'corn', 'ord', 'get', 'suspend', 'level', 'correct', 'ridic', 'fast', 'car', '\\n\\ni', 'also', 'ton', 'troubl', 'driv', 'gt2', 'rs', 'demo', 'fig', 'driv', 'car', 'throttle', 'becam', 'incred', 'fast'], ['porsche', 'add', 'fm6', 'one', 'first', 'things', 'notice', 'every', 'nine hundred and eleven', 'handle', 'differently', 'car', 'game', 'rear', 'engine', 'layout', 'figure', 'modulate', 'throttle', 'brake', 'corner', 'order', 'get', 'suspension', 'level', 'correctly', 'ridiculously', 'fast', 'cars', '\\n\\ni', 'also', 'ton', 'trouble', 'drive', 'gt2', 'rs', 'demo', 'figure', 'drive', 'car', 'throttle', 'become', 'incredibly', 'fast'])\n",
      "original document: \n",
      "['Hey', 'this', 'is', 'an', 'absolutely', 'fantastic', 'prompt,', 'with', 'the', 'exception', 'of', 'the', 'unnecessary', 'apostrophe', 'in', 'the', 'title.\\n\\nTwo', \"won't\", 'play', '**food', 'chain', 'magnate**.', 'They', 'got', 'absolutely', 'demolished', 'in', 'their', 'first', 'game,', 'to', 'the', 'point', 'where', 'they', 'were', '\"irrelevant', 'bystanders\"', 'after', 'turn', '3.', 'Never', 'again,', 'they', 'said.\\n\\nOne', \"won't\", 'play', '**alchemists**', 'because', 'it', 'reminds', 'him', 'of', 'being', 'in', 'college', 'and', 'he', 'goes', 'to', 'game', 'night', 'to', 'forget', 'school', 'and', 'work.\\n\\nOne', \"won't\", 'play', '**resistance', 'or', 'secret', 'Hitler**', 'because', \"he's\", 'a', '(young)', 'retired', 'Afghanistan', 'war', 'vet', 'with', 'ptsd', 'and', 'those', 'games', 'trigger', 'him', 'too', 'much.', 'He', 'gets', 'way', 'to', 'stressed', 'out.\\n\\nTwo', 'friends', \"won't\", 'play', '**arboretum**', ',', 'and', 'these', 'are', 'two', 'really', 'good', 'gamers,', 'not', 'snowflakes.', 'But', 'the', 'hand', 'restriction', 'is', 'so', 'stressful', 'that', 'they', \"can't\", 'have', 'fun', 'with', 'it.', 'Of', 'those', 'two,', 'one', \"won't\", 'play', '**bazaar**', 'because', 'the', 'thought', 'process', 'required', 'to', 'play', 'optimally', 'is', 'so', 'brain', 'burny', 'that', 'he', \"doesn't\", 'think', \"it's\", 'worth', 'it.', 'That', 'being', 'said,', \"he'll\", 'play', 'fcm', 'and', 'battlecon', 'with', 'me', 'all', 'day,', 'ironically.\\n\\nAnother', 'friend', 'has', 'kids', '10,', '8,', '6,', 'who', 'all', 'like', 'game,', 'but', 'they', \"won't\", 'play', '**escape', 'curse', 'of', 'the', 'temple**', 'again', 'because', 'the', 'last', 'game', 'ended', 'in', 'tears,', 'shouting,', 'and', 'rage.', 'The', 'mom', 'and', '10', 'year', 'old', 'did', 'fine,', 'but', 'the', '8&amp;6', 'struggled,', 'and', 'I', 'split', 'the', 'group', 'up', 'poorly.', 'So', 'idiotic,', 'I', 'should', 'have', 'been', 'with', 'the', '8&amp;6,', 'not', 'with', 'the', '10.\\n\\nI', 'almost', \"won't\", 'play', '**splendor**', 'anymore', 'because', 'it', 'removes', 'all', 'socialization', 'and', 'leads', 'to', 'some', 'outburst', 'when', 'someone', 'grabs', 'the', 'card', 'someone', 'else', 'wanted.\\n\\nOne', 'guy', \"won't\", 'play', '**acquire**', 'because', 'he', 'ran', 'out', 'of', 'money', 'real', 'fast,', \"couldn't\", 'buy', 'stocks', 'for', 'like', '6', 'turns,', \"didn't\", 'have', 'the', 'tiles', 'to', 'force', 'a', 'merger,', 'and', 'walked', 'away', 'feeling', 'like', 'it', 'was', 'a', 'luck', 'fest', 'that', 'punished', 'you', 'too', 'hard', 'for', 'being', 'careless', 'with', 'your', 'money.', 'Dunno,', 'me', 'neither..?\\n\\nOne', \"won't\", 'play', '**Chinatown**', 'because', 'the', 'endgame', 'is', 'too', 'mathy', 'and', 'predictable', '(easy', 'to', 'solve).\\n\\nAnd', 'another', \"won't\", 'play', '**dominion**', 'anymore', 'because', \"it's\", 'too', 'themeless', 'and', 'too', 'pure,', 'ie', 'he', 'prefers', 'Clank,', 'where', 'deck', 'building', 'is', 'a', 'means', 'to', 'an', 'end,', 'not', 'an', 'end.', 'By', 'the', 'way,', \"he's\", 'never', 'played', 'a', 'dominion', 'expansion.\\n\\nI', \"won't\", 'play', 'any', 'of', 'the', '**legendary**', 'series', 'games,', '**monopoly,', 'munchkin,', 'galaxy', 'trucker,', 'mice', 'n', 'mystics,', 'or', 'Robinson', 'Crusoe**', 'because', 'I', \"don't\", 'feel', 'I', 'have', 'enough', 'player', 'agency.', 'I', 'feel', 'like', 'the', 'game', 'is', 'playing', 'ME']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'absolv', 'fantast', 'prompt', 'exceiv', 'unnecess', 'apostroph', 'title\\n\\ntwo', 'wont', 'play', 'food', 'chain', 'magn', 'got', 'absolv', 'demol', 'first', 'gam', 'point', 'irrelev', 'bystand', 'turn', 'three', 'nev', 'said\\n\\none', 'wont', 'play', 'alchem', 'remind', 'colleg', 'goe', 'gam', 'night', 'forget', 'school', 'work\\n\\none', 'wont', 'play', 'resist', 'secret', 'hitl', 'hes', 'young', 'retir', 'afgh', 'war', 'vet', 'ptsd', 'gam', 'trig', 'much', 'get', 'way', 'stressed', 'out\\n\\ntwo', 'friend', 'wont', 'play', 'arboret', 'two', 'real', 'good', 'gam', 'snowflak', 'hand', 'restrict', 'stressful', 'cant', 'fun', 'two', 'on', 'wont', 'play', 'baza', 'thought', 'process', 'requir', 'play', 'optim', 'brain', 'burny', 'doesnt', 'think', 'wor', 'said', 'hel', 'play', 'fcm', 'battlecon', 'day', 'ironically\\n\\nanother', 'friend', 'kid', 'ten', 'eight', 'six', 'lik', 'gam', 'wont', 'play', 'escap', 'curs', 'templ', 'last', 'gam', 'end', 'tear', 'shout', 'rag', 'mom', 'ten', 'year', 'old', 'fin', '8amp6', 'struggled', 'split', 'group', 'poor', 'idiot', '8amp6', '10\\n\\ni', 'almost', 'wont', 'play', 'splendor', 'anym', 'remov', 'soc', 'lead', 'outburst', 'someon', 'grab', 'card', 'someon', 'els', 'wanted\\n\\none', 'guy', 'wont', 'play', 'acquir', 'ran', 'money', 'real', 'fast', 'couldnt', 'buy', 'stock', 'lik', 'six', 'turn', 'didnt', 'til', 'forc', 'merg', 'walk', 'away', 'feel', 'lik', 'luck', 'fest', 'pun', 'hard', 'careless', 'money', 'dunno', 'neither\\n\\none', 'wont', 'play', 'chinatown', 'endgam', 'mathy', 'predict', 'easy', 'solve\\n\\nand', 'anoth', 'wont', 'play', 'domin', 'anym', 'themeless', 'pur', 'ie', 'pref', 'clank', 'deck', 'build', 'mean', 'end', 'end', 'way', 'hes', 'nev', 'play', 'domin', 'expansion\\n\\ni', 'wont', 'play', 'legend', 'sery', 'gam', 'monopo', 'munchkin', 'galaxy', 'truck', 'mic', 'n', 'myst', 'robinson', 'cruso', 'dont', 'feel', 'enough', 'play', 'ag', 'feel', 'lik', 'gam', 'play'], ['hey', 'absolutely', 'fantastic', 'prompt', 'exception', 'unnecessary', 'apostrophe', 'title\\n\\ntwo', 'wont', 'play', 'food', 'chain', 'magnate', 'get', 'absolutely', 'demolish', 'first', 'game', 'point', 'irrelevant', 'bystanders', 'turn', 'three', 'never', 'said\\n\\none', 'wont', 'play', 'alchemists', 'remind', 'college', 'go', 'game', 'night', 'forget', 'school', 'work\\n\\none', 'wont', 'play', 'resistance', 'secret', 'hitler', 'hes', 'young', 'retire', 'afghanistan', 'war', 'vet', 'ptsd', 'game', 'trigger', 'much', 'get', 'way', 'stress', 'out\\n\\ntwo', 'friends', 'wont', 'play', 'arboretum', 'two', 'really', 'good', 'gamers', 'snowflakes', 'hand', 'restriction', 'stressful', 'cant', 'fun', 'two', 'one', 'wont', 'play', 'bazaar', 'think', 'process', 'require', 'play', 'optimally', 'brain', 'burny', 'doesnt', 'think', 'worth', 'say', 'hell', 'play', 'fcm', 'battlecon', 'day', 'ironically\\n\\nanother', 'friend', 'kid', 'ten', 'eight', 'six', 'like', 'game', 'wont', 'play', 'escape', 'curse', 'temple', 'last', 'game', 'end', 'tear', 'shout', 'rage', 'mom', 'ten', 'year', 'old', 'fine', '8amp6', 'struggle', 'split', 'group', 'poorly', 'idiotic', '8amp6', '10\\n\\ni', 'almost', 'wont', 'play', 'splendor', 'anymore', 'remove', 'socialization', 'lead', 'outburst', 'someone', 'grab', 'card', 'someone', 'else', 'wanted\\n\\none', 'guy', 'wont', 'play', 'acquire', 'run', 'money', 'real', 'fast', 'couldnt', 'buy', 'stock', 'like', 'six', 'turn', 'didnt', 'tile', 'force', 'merger', 'walk', 'away', 'feel', 'like', 'luck', 'fest', 'punish', 'hard', 'careless', 'money', 'dunno', 'neither\\n\\none', 'wont', 'play', 'chinatown', 'endgame', 'mathy', 'predictable', 'easy', 'solve\\n\\nand', 'another', 'wont', 'play', 'dominion', 'anymore', 'themeless', 'pure', 'ie', 'prefer', 'clank', 'deck', 'build', 'mean', 'end', 'end', 'way', 'hes', 'never', 'play', 'dominion', 'expansion\\n\\ni', 'wont', 'play', 'legendary', 'series', 'game', 'monopoly', 'munchkin', 'galaxy', 'trucker', 'mice', 'n', 'mystics', 'robinson', 'crusoe', 'dont', 'feel', 'enough', 'player', 'agency', 'feel', 'like', 'game', 'play'])\n",
      "original document: \n",
      "['Bet', 'anything', 'they', 'did', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bet', 'anyth'], ['bet', 'anything'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', 'saw', 'John', 'Cena', 'get', 'hit', 'by', 'a', 'chair', 'with', 'my', 'own', 'eyes,', '', 'and', 'dumbasses', 'still', 'say', \"it's\", 'fake']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'john', 'cen', 'get', 'hit', 'chair', 'ey', 'dumbass', 'stil', 'say', 'fak'], ['saw', 'john', 'cena', 'get', 'hit', 'chair', 'eye', 'dumbasses', 'still', 'say', 'fake'])\n",
      "original document: \n",
      "['Power', \"Jump's\", 'number', 'calculation', 'is', 'total', 'malarkey,', \"it's\", 'supposed', 'to', 'deal', 'one', 'Normal', \"Jump's\", 'damage', 'x2', '+2', 'but', 'it', 'instead', 'deals', 'around', '80-90%', 'a', 'Normal', \"Jump's\", 'damage,', 'past', '10', 'damage', 'it', 'becomes', 'apparent', 'that', \"it's\", 'terribly', 'broken.\\n\\nSpeaking', 'of', 'dumb', 'active', 'Badges,', 'why', 'on', 'earth', 'do', 'you', 'get', 'Hammer', 'Throw', 'in', 'Chapter', '4?', 'Vivian', 'and', 'Flurrie', 'have', 'already', 'replaced', 'it,', 'and', 'Spike', 'Shield', 'replaces', 'it', 'for', 'Mario', 'after', 'Chapter', '4', 'is', 'over,', 'so', \"what's\", 'the', 'point?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pow', 'jump', 'numb', 'calc', 'tot', 'malarkey', 'suppos', 'deal', 'on', 'norm', 'jump', 'dam', 'x2', 'two', 'instead', 'deal', 'around', 'eight thousand and ninety', 'norm', 'jump', 'dam', 'past', 'ten', 'dam', 'becom', 'app', 'terr', 'broken\\n\\nspeaking', 'dumb', 'act', 'badg', 'ear', 'get', 'ham', 'throw', 'chapt', 'four', 'viv', 'flurry', 'already', 'replac', 'spik', 'shield', 'replac', 'mario', 'chapt', 'four', 'what', 'point'], ['power', 'jump', 'number', 'calculation', 'total', 'malarkey', 'suppose', 'deal', 'one', 'normal', 'jump', 'damage', 'x2', 'two', 'instead', 'deal', 'around', 'eight thousand and ninety', 'normal', 'jump', 'damage', 'past', 'ten', 'damage', 'become', 'apparent', 'terribly', 'broken\\n\\nspeaking', 'dumb', 'active', 'badge', 'earth', 'get', 'hammer', 'throw', 'chapter', 'four', 'vivian', 'flurrie', 'already', 'replace', 'spike', 'shield', 'replace', 'mario', 'chapter', 'four', 'whats', 'point'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['As', 'a', 'capybara', 'main,', 'this...', 'terrifies', 'me', 'on', 'so', 'many', 'levels.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['capybar', 'main', 'terr', 'many', 'level'], ['capybara', 'main', 'terrify', 'many', 'level'])\n",
      "original document: \n",
      "['Your', 'reality', 'TV', 'garbage', 'is', 'pure', 'cultural', 'marxist', 'bullshit', 'as', 'well.', 'A', 'couple', 'dozen', 'people', 'fighting', 'over', 'someone', 'who', \"isn't\", 'even', 'a', 'prize?', 'Way', 'to', 'cheapen', 'and', 'destroy', 'what', 'relationships', 'are', 'supposed', 'to', 'be', 'about.', 'Pushing', 'degeneracy', 'is', 'what', 'the', 'left', 'is', 'all', 'about', '-', 'and', 'cheap', 'meaningless', 'relationships', 'are', 'part', 'of', 'destroying', 'marriage', 'and', 'family', 'as', 'well.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['real', 'tv', 'garb', 'pur', 'cult', 'marx', 'bullshit', 'wel', 'coupl', 'doz', 'peopl', 'fight', 'someon', 'isnt', 'ev', 'priz', 'way', 'cheap', 'destroy', 'rel', 'suppos', 'push', 'deg', 'left', 'cheap', 'meaningless', 'rel', 'part', 'destroy', 'marry', 'famy', 'wel'], ['reality', 'tv', 'garbage', 'pure', 'cultural', 'marxist', 'bullshit', 'well', 'couple', 'dozen', 'people', 'fight', 'someone', 'isnt', 'even', 'prize', 'way', 'cheapen', 'destroy', 'relationships', 'suppose', 'push', 'degeneracy', 'leave', 'cheap', 'meaningless', 'relationships', 'part', 'destroy', 'marriage', 'family', 'well'])\n",
      "original document: \n",
      "['What', 'does', 'jacob', 'do', 'to', 'benefit', 'the', 'streams?', 'Exactly.', 'Delete', 'this', 'post.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jacob', 'benefit', 'streams', 'exact', 'delet', 'post'], ['jacob', 'benefit', 'stream', 'exactly', 'delete', 'post'])\n",
      "original document: \n",
      "['Please', 'tell', 'how?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'tel'], ['please', 'tell'])\n",
      "original document: \n",
      "['DMac', 'another', 'try', 'saver']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dmac', 'anoth', 'try', 'sav'], ['dmac', 'another', 'try', 'saver'])\n",
      "original document: \n",
      "['I', 'would', '*love*', 'to', 'see', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lov', 'see'], ['would', 'love', 'see'])\n",
      "original document: \n",
      "['Yes', 'they', 'both', 'were', 'at', 'one', 'point.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'on', 'point'], ['yes', 'one', 'point'])\n",
      "original document: \n",
      "['exactly', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact'], ['exactly'])\n",
      "original document: \n",
      "['Fantastic,', 'well', 'done!\\n\\nYou', 'are', 'probably', 'far', 'more', 'critical', 'of', 'your', 'playing', 'than', 'anyone', 'else', 'is,', 'at', 'least', 'if', 'they', 'are', 'not', 'professional', 'musicians.', 'Chances', 'are', 'it', 'sounded', 'better', 'to', 'them', 'than', 'you', \"think.\\n\\nI'm\", 'all', 'scientist,', 'but', 'would', 'exchange', 'it', 'all', 'to', 'be', 'able', 'to', 'play', 'piano', 'really', 'well.', \"I've\", 'tried', 'to', 'learn', '(seriously)', 'but', \"it's\", 'simply', 'not', 'in', 'me', 'to', 'be', 'able', 'to', 'create', 'music', 'beyond', 'the', 'most', 'basic', 'level.', \"I'm\", 'resigned', 'to', 'listening.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fantast', 'wel', 'done\\n\\nyou', 'prob', 'far', 'crit', 'play', 'anyon', 'els', 'least', 'profess', 'mus', 'chant', 'sound', 'bet', 'think\\n\\nim', 'sci', 'would', 'exchang', 'abl', 'play', 'piano', 'real', 'wel', 'iv', 'tri', 'learn', 'sery', 'simply', 'abl', 'cre', 'mus', 'beyond', 'bas', 'level', 'im', 'resign', 'list'], ['fantastic', 'well', 'done\\n\\nyou', 'probably', 'far', 'critical', 'play', 'anyone', 'else', 'least', 'professional', 'musicians', 'chance', 'sound', 'better', 'think\\n\\nim', 'scientist', 'would', 'exchange', 'able', 'play', 'piano', 'really', 'well', 'ive', 'try', 'learn', 'seriously', 'simply', 'able', 'create', 'music', 'beyond', 'basic', 'level', 'im', 'resign', 'listen'])\n",
      "original document: \n",
      "['143417319|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'Jx077B7U)\\n\\n&gt;&gt;143417265\\nLMAO\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and seventeen thousand, three hundred and nineteen', 'gt', 'unit', 'stat', 'anonym', 'id', 'jx077b7u\\n\\ngtgt143417265\\nlmao\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and seventeen thousand, three hundred and nineteen', 'gt', 'unite', 'state', 'anonymous', 'id', 'jx077b7u\\n\\ngtgt143417265\\nlmao\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['I', \"can't\", 'minimize', 'comments', 'unless', \"it's\", 'a', 'top', 'level', 'comment.', 'This', 'app', 'is', 'terrible', 'I', 'wish', 'I', 'kept', 'alien', 'blue']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'minim', 'com', 'unless', 'top', 'level', 'com', 'ap', 'terr', 'wish', 'kept', 'aly', 'blu'], ['cant', 'minimize', 'comment', 'unless', 'top', 'level', 'comment', 'app', 'terrible', 'wish', 'keep', 'alien', 'blue'])\n",
      "original document: \n",
      "['I', 'get', 'the', 'feeling', \"we're\", 'already', 'seeing', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'feel', 'already', 'see'], ['get', 'feel', 'already', 'see'])\n",
      "original document: \n",
      "['Wow', 'this', 'sounds', 'a', 'lot', 'like', 'me', 'and', 'my', 'very', 'recent', 'now', 'ex.', 'We', 'broke', 'up', 'a', 'lot', 'too', 'and', 'always', 'got', 'back', 'together', 'and', 'had', 'exactly', 'the', 'same', 'problems', 'you', 'did', 'and', 'I', 'said', 'the', 'exact', 'same', 'thing', 'she', 'did.', 'The', 'hardest', 'thing', 'is', 'you', 'two', 'were', 'best', 'friends', 'and', 'now', 'coping', 'without', 'that', 'is', 'so', 'difficult.', 'I', \"don't\", 'really', 'think', 'there', 'is', 'a', 'best', 'way', 'to', 'move', 'on', 'other', 'than', 'time.', 'Time', 'is', 'key', 'to', 'help', 'yourself', 'get', 'over', 'this.', '\\n\\nI', 'just', 'sent', 'an', 'email', 'to', 'my', 'ex', 'explaining', 'that', 'what', 'we', 'had', 'was', 'more', 'of', 'a', 'friendship', 'than', 'a', 'relationship.', 'We', 'were', 'together', 'since', 'we', 'were', '14', 'and', 'had', 'been', 'together', '5', 'years.', 'And', 'we', 'did', 'love', 'eachother', 'but', 'sometimes', 'love', 'is', 'not', 'enough.', 'Have', 'some', 'you', 'time', 'and', 'figure', 'out', 'who', 'you', 'are', 'again', 'before', 'you', 'even', 'consider', 'getting', 'back', 'with', 'her', 'because', \"you'll\", 'hurt', 'eachother', 'again.', 'Easier', 'said', 'than', 'done,', 'believe', 'me', 'I', 'know!!\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'sound', 'lot', 'lik', 'rec', 'ex', 'brok', 'lot', 'alway', 'got', 'back', 'togeth', 'exact', 'problem', 'said', 'exact', 'thing', 'hardest', 'thing', 'two', 'best', 'friend', 'cop', 'without', 'difficult', 'dont', 'real', 'think', 'best', 'way', 'mov', 'tim', 'tim', 'key', 'help', 'get', '\\n\\ni', 'sent', 'email', 'ex', 'explain', 'friend', 'rel', 'togeth', 'sint', 'fourteen', 'togeth', 'fiv', 'year', 'lov', 'eachoth', 'sometim', 'lov', 'enough', 'tim', 'fig', 'ev', 'consid', 'get', 'back', 'youl', 'hurt', 'eachoth', 'easy', 'said', 'don', 'believ', 'know\\n'], ['wow', 'sound', 'lot', 'like', 'recent', 'ex', 'break', 'lot', 'always', 'get', 'back', 'together', 'exactly', 'problems', 'say', 'exact', 'thing', 'hardest', 'thing', 'two', 'best', 'friends', 'cop', 'without', 'difficult', 'dont', 'really', 'think', 'best', 'way', 'move', 'time', 'time', 'key', 'help', 'get', '\\n\\ni', 'send', 'email', 'ex', 'explain', 'friendship', 'relationship', 'together', 'since', 'fourteen', 'together', 'five', 'years', 'love', 'eachother', 'sometimes', 'love', 'enough', 'time', 'figure', 'even', 'consider', 'get', 'back', 'youll', 'hurt', 'eachother', 'easier', 'say', 'do', 'believe', 'know\\n'])\n",
      "original document: \n",
      "['You', 'are', 'completely', 'off', 'base.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['complet', 'bas'], ['completely', 'base'])\n",
      "original document: \n",
      "['😉']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Yeah,', 'that', 'certainly', 'removes', 'some', 'of', 'the', 'extra', 'work.', \"I'll\", 'have', 'to', 'start', 'thinking', 'up', 'Foci', 'for', 'my', 'elves,', 'dwarves,', 'and', 'porcine-humanoids!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'certain', 'remov', 'extr', 'work', 'il', 'start', 'think', 'foc', 'elv', 'dwarv', 'porcinehumanoid'], ['yeah', 'certainly', 'remove', 'extra', 'work', 'ill', 'start', 'think', 'foci', 'elves', 'dwarves', 'porcinehumanoids'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Beck', 'was', 'here', 'from', '2010-2014.', 'Get', 'your', 'facts', 'straight', 'before', 'mouthing', 'off.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['beck', 'twenty million, one hundred and two thousand and fourteen', 'get', 'fact', 'straight', 'mouth'], ['beck', 'twenty million, one hundred and two thousand and fourteen', 'get', 'facts', 'straight', 'mouth'])\n",
      "original document: \n",
      "['Maybe', 'you', 'should', 'post', 'a', 'screen', 'shot', 'where', 'you', 'have', 'gotten', 'to', 'round', '50', 'with', 'no', 'power', 'in', 'an', 'hour.', 'I', 'can', 'assure', 'you,', 'you', 'have', 'not.\\n\\n\\n\\nI', 'have', 'an', 'excel', 'sheet', 'I', 'tracked', 'round', 'times', 'and', 'cryptids', 'spawned', 'every', 'round.', 'My', 'numbers', 'are', 'solid', 'and', 'correct.', '\\n\\n\\n\\nLike', 'I', 'said,', 'for', 'high', 'rounds,', 'past', 'round', '80,', 'then', 'yes,', 'use', 'no', 'power', 'because', 'rounds', 'will', 'stay', 'at', '2:20', 'seconds', 'after', 'round', '21', 'to', '1000.', '\\n\\n\\n\\nIf', \"you're\", 'grinding', 'for', 'keys,', \"it's\", 'the', 'other', 'maps.', 'Literally', 'impossible', 'to', 'get', 'to', 'round', '50', 'in', '1', 'hour', 'on', 'Beast.', '41', 'all', 'day', 'on', 'other', 'maps.', '32-35', 'on', 'Beast', 'no', 'power.', '\\n\\n\\n\\nhttps://i.imgur.com/4zjUgRP.jpg']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'post', 'screen', 'shot', 'got', 'round', 'fifty', 'pow', 'hour', 'ass', 'not\\n\\n\\n\\ni', 'excel', 'sheet', 'track', 'round', 'tim', 'cryptid', 'spawn', 'every', 'round', 'numb', 'solid', 'correct', '\\n\\n\\n\\nlike', 'said', 'high', 'round', 'past', 'round', 'eighty', 'ye', 'us', 'pow', 'round', 'stay', 'two hundred and twenty', 'second', 'round', 'twenty-one', 'one thousand', '\\n\\n\\n\\nif', 'yo', 'grind', 'key', 'map', 'lit', 'imposs', 'get', 'round', 'fifty', 'on', 'hour', 'beast', 'forty-one', 'day', 'map', 'three thousand, two hundred and thirty-five', 'beast', 'pow', '\\n\\n\\n\\nhttpsiimgurcom4zjugrpjpg'], ['maybe', 'post', 'screen', 'shoot', 'get', 'round', 'fifty', 'power', 'hour', 'assure', 'not\\n\\n\\n\\ni', 'excel', 'sheet', 'track', 'round', 'time', 'cryptids', 'spawn', 'every', 'round', 'number', 'solid', 'correct', '\\n\\n\\n\\nlike', 'say', 'high', 'round', 'past', 'round', 'eighty', 'yes', 'use', 'power', 'round', 'stay', 'two hundred and twenty', 'second', 'round', 'twenty-one', 'one thousand', '\\n\\n\\n\\nif', 'youre', 'grind', 'key', 'map', 'literally', 'impossible', 'get', 'round', 'fifty', 'one', 'hour', 'beast', 'forty-one', 'day', 'map', 'three thousand, two hundred and thirty-five', 'beast', 'power', '\\n\\n\\n\\nhttpsiimgurcom4zjugrpjpg'])\n",
      "original document: \n",
      "['Working', 'as', 'of', '8', 'EST']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['work', 'eight', 'est'], ['work', 'eight', 'est'])\n",
      "original document: \n",
      "['Holy', 'fuck', 'Mississippi', 'State.', \"You're\", 'going', 'to', 'give', 'Michigan', 'a', 'run', 'for', 'its', 'money', 'with', 'that', '100+', 'yards', 'in', 'penalties']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['holy', 'fuck', 'mississipp', 'stat', 'yo', 'going', 'giv', 'michig', 'run', 'money', 'one hundred', 'yard', 'penal'], ['holy', 'fuck', 'mississippi', 'state', 'youre', 'go', 'give', 'michigan', 'run', 'money', 'one hundred', 'yards', 'penalties'])\n",
      "original document: \n",
      "['Pretty', 'sure', 'that’s', 'BYU', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'sur', 'that', 'byu'], ['pretty', 'sure', 'thats', 'byu'])\n",
      "original document: \n",
      "[\"That's\", 'what', 'I', 'was', 'saying', 'before...', 'the', 'curve', 'seemed', 'off.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'say', 'curv', 'seem'], ['thats', 'say', 'curve', 'seem'])\n",
      "original document: \n",
      "['I', 'mean,', 'g2', 'and', 'faze', 'are', 'both', '5', 'star', 'player', 'teams', 'and', \"they're\", 'both', 'top', '4.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'g2', 'faz', 'fiv', 'star', 'play', 'team', 'theyr', 'top', 'four'], ['mean', 'g2', 'faze', 'five', 'star', 'player', 'team', 'theyre', 'top', 'four'])\n",
      "original document: \n",
      "['Y', 'tho?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tho'], ['tho'])\n",
      "original document: \n",
      "['President', 'Fujimori', 'of', 'PERÚ.', 'He', 'ended', 'hyper', 'inflation,', 'during', 'his', 'government', 'terrorism', 'ended', '(almost', 'completely),', 'and', 'Peru', 'started', 'becoming', 'one', 'of', 'the', 'best', 'places', 'to', 'be', 'in', 'South', 'America.', '\\nThen', 'he', 'became', 'corrupt,', 'stole', 'millions', 'of', 'dollars,', 'changed', 'the', 'constitution', 'to', 'remain', 'in', 'power', 'for', '3', 'terms', '(only', '2', 'allowed', 'legally),', 'among', 'other', 'things.\\nHe', 'flew', 'the', 'country', 'to', 'Japan', '(he', 'had', 'double', 'citizenship).', 'Then', 'he', 'was', 'asked', 'to', 'leave', 'Japan,', 'went', 'to', 'Chile,', 'and', 'then', 'was', 'deported', 'to', 'Peru.', 'He', 'was', 'condemned', 'to', '25', 'years', 'in', 'prison', 'for', 'corruption', 'and', 'human', 'rights', 'violations.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['presid', 'fujimor', 'peru', 'end', 'hyp', 'infl', 'govern', 'ter', 'end', 'almost', 'complet', 'peru', 'start', 'becom', 'on', 'best', 'plac', 'sou', 'americ', '\\nthen', 'becam', 'corrupt', 'stol', 'mil', 'doll', 'chang', 'constitut', 'remain', 'pow', 'three', 'term', 'two', 'allow', 'leg', 'among', 'things\\nhe', 'flew', 'country', 'jap', 'doubl', 'cit', 'ask', 'leav', 'jap', 'went', 'chil', 'deport', 'peru', 'condemn', 'twenty-five', 'year', 'prison', 'corrupt', 'hum', 'right', 'viol'], ['president', 'fujimori', 'peru', 'end', 'hyper', 'inflation', 'government', 'terrorism', 'end', 'almost', 'completely', 'peru', 'start', 'become', 'one', 'best', 'place', 'south', 'america', '\\nthen', 'become', 'corrupt', 'steal', 'millions', 'dollars', 'change', 'constitution', 'remain', 'power', 'three', 'term', 'two', 'allow', 'legally', 'among', 'things\\nhe', 'fly', 'country', 'japan', 'double', 'citizenship', 'ask', 'leave', 'japan', 'go', 'chile', 'deport', 'peru', 'condemn', 'twenty-five', 'years', 'prison', 'corruption', 'human', 'right', 'violations'])\n",
      "original document: \n",
      "['That', 'was', 'kind', 'of', 'my', 'thought,', 'as', 'well.', 'Social', 'interaction', 'with', 'like-minded', 'individuals', 'as', 'opposed', 'to', 'the', 'non-empathetic', 'masses.', 'I', 'imagine', 'having', 'a', 'place', 'like', 'that', 'would', 'help', 'a', 'lot', 'of', 'people.', '\\n\\nDefinitely', 'have', 'some', 'sort', 'of', 'filter', 'system', 'at', 'the', 'door,', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kind', 'thought', 'wel', 'soc', 'interact', 'likemind', 'individ', 'oppos', 'nonempathet', 'mass', 'imagin', 'plac', 'lik', 'would', 'help', 'lot', 'peopl', '\\n\\ndefinitely', 'sort', 'filt', 'system', 'door'], ['kind', 'think', 'well', 'social', 'interaction', 'likeminded', 'individuals', 'oppose', 'nonempathetic', 'mass', 'imagine', 'place', 'like', 'would', 'help', 'lot', 'people', '\\n\\ndefinitely', 'sort', 'filter', 'system', 'door'])\n",
      "original document: \n",
      "[\"It's\", 'been', 'broken', 'for', 'a', 'long', 'time', 'now.', 'But', 'Obama', \"wasn't\", 'the', 'result', 'of', 'the', 'broken', 'system;', 'he', 'won', 'in', 'spite', 'of', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brok', 'long', 'tim', 'obam', 'wasnt', 'result', 'brok', 'system', 'spit'], ['break', 'long', 'time', 'obama', 'wasnt', 'result', 'break', 'system', 'spite'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnol9ox/):\\n\\nAgents,', 'traditionally,', 'are', 'the', 'first', 'step', 'to', 'pushing', 'to', 'publishers.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol9ox\\n\\nagent', 'tradit', 'first', 'step', 'push', 'publ'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnol9ox\\n\\nagents', 'traditionally', 'first', 'step', 'push', 'publishers'])\n",
      "original document: \n",
      "['Damn', 'it.', 'My', 'friend', 'got', 'them', 'Y', 'E', 'E', 'Z', 'Y', 'Boost', '350', 'V2.', 'I', 'need', 'them', 'Y', 'E', 'E', 'Z', 'Y', 'S', 'man', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damn', 'friend', 'got', 'e', 'e', 'z', 'boost', 'three hundred and fifty', 'v2', 'nee', 'e', 'e', 'z', 'man'], ['damn', 'friend', 'get', 'e', 'e', 'z', 'boost', 'three hundred and fifty', 'v2', 'need', 'e', 'e', 'z', 'man'])\n",
      "original document: \n",
      "['Thanks', 'that', 'worked.', 'I', 'just', 'talked', 'to', 'everyone', 'eventually', 'found', 'out', 'I', 'had', 'to', 'talk', 'to', 'the', 'groundskeeper', 'to', 'open', 'a', 'quest', 'to', 'the', 'guy', 'in', 'the', 'top', 'middle.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'work', 'talk', 'everyon', 'ev', 'found', 'talk', 'groundskeep', 'op', 'quest', 'guy', 'top', 'middl'], ['thank', 'work', 'talk', 'everyone', 'eventually', 'find', 'talk', 'groundskeeper', 'open', 'quest', 'guy', 'top', 'middle'])\n",
      "original document: \n",
      "['Lmao', 'at', 'that', 'patchy', 'beard,', 'it', 'looks', 'like', 'middle', 'schoolers', 'growing', 'out', 'their', 'peach', 'fuzz.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lmao', 'patchy', 'beard', 'look', 'lik', 'middl', 'schoolers', 'grow', 'peach', 'fuzz'], ['lmao', 'patchy', 'beard', 'look', 'like', 'middle', 'schoolers', 'grow', 'peach', 'fuzz'])\n",
      "original document: \n",
      "['Legitimately', 'see', 'people', 'completely', 'missing', 'the', 'point', 'on', 'my', 'FB', 'feed', 'saying', 'that', '“Tebow', 'did', 'it', 'before', 'it', 'was', 'cool', 'and', 'was', 'mocked,', 'how', 'is', 'this', 'any', 'different?”.', '\\n\\nThe', 'bible', 'belt', 'is', 'a', 'weird', 'place', 'to', 'be.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['legitim', 'see', 'peopl', 'complet', 'miss', 'point', 'fb', 'fee', 'say', 'tebow', 'cool', 'mock', 'diff', '\\n\\nthe', 'bibl', 'belt', 'weird', 'plac'], ['legitimately', 'see', 'people', 'completely', 'miss', 'point', 'fb', 'fee', 'say', 'tebow', 'cool', 'mock', 'different', '\\n\\nthe', 'bible', 'belt', 'weird', 'place'])\n",
      "original document: \n",
      "['Placebo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['placebo'], ['placebo'])\n",
      "original document: \n",
      "['You', 'betcha']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['betch'], ['betcha'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"Gabbadiggio's\", 'Fresca', 'House', 'of', 'Muzzerel', 'Primero', 'Italiano.', '\\n\\nDunno', 'if', 'the', 'Valley', 'is', 'too', 'far', 'away', 'from', 'you', 'or', 'not,', 'just', 'stay', 'on', 'the', '5', 'until', 'Castaic', 'Junction.', 'Free', 'garlic', 'bread!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gabbadiggio', 'fresc', 'hous', 'muzzerel', 'primero', 'italiano', '\\n\\ndunno', 'valley', 'far', 'away', 'stay', 'fiv', 'casta', 'junct', 'fre', 'garl', 'bread'], ['gabbadiggios', 'fresca', 'house', 'muzzerel', 'primero', 'italiano', '\\n\\ndunno', 'valley', 'far', 'away', 'stay', 'five', 'castaic', 'junction', 'free', 'garlic', 'bread'])\n",
      "original document: \n",
      "['Samsung', 'galaxy', 'S5,', 'A', 'few', 'dings', 'and', 'wear', 'and', 'tear', 'along', 'the', 'bezel', 'but', 'otherwise', 'in', 'pristine', 'condition.', 'One', 'very', 'faint', 'scratch', 'on', 'the', 'screen', 'from', 'rubbing', 'against', 'something', 'in', 'my', 'bag...', 'more', 'of', 'a', 'scuff', 'than', 'anything.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['samsung', 'galaxy', 's5', 'ding', 'wear', 'tear', 'along', 'bezel', 'otherw', 'pristin', 'condit', 'on', 'faint', 'scratch', 'screen', 'rub', 'someth', 'bag', 'scuff', 'anyth'], ['samsung', 'galaxy', 's5', 'ding', 'wear', 'tear', 'along', 'bezel', 'otherwise', 'pristine', 'condition', 'one', 'faint', 'scratch', 'screen', 'rub', 'something', 'bag', 'scuff', 'anything'])\n",
      "original document: \n",
      "['Good', 'lord,', 'like', 'two', 'thirds', 'of', 'these', 'comments', 'are', 'in', 'favor', 'of', 'what', 'he', 'did.', 'Most', 'socially', 'inept,', 'Reddity', 'thread', 'ever.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'lord', 'lik', 'two', 'third', 'com', 'fav', 'soc', 'inept', 'red', 'thread', 'ev'], ['good', 'lord', 'like', 'two', 'thirds', 'comment', 'favor', 'socially', 'inept', 'reddity', 'thread', 'ever'])\n",
      "original document: \n",
      "['The', 'bear', 'had', 'it', 'coming.', 'Just', 'ask', 'anyone.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bear', 'com', 'ask', 'anyon'], ['bear', 'come', 'ask', 'anyone'])\n",
      "original document: \n",
      "['Ohhhh', 'i', 'see', 'haha', 'what', 'a', 'funny', 'coincidence', 'lmao,', 'but', 'hey!', \"it's\", 'the', 'thought', 'that', 'counts!&lt;3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ohhhh', 'see', 'hah', 'funny', 'coincid', 'lmao', 'hey', 'thought', 'countslt3'], ['ohhhh', 'see', 'haha', 'funny', 'coincidence', 'lmao', 'hey', 'think', 'countslt3'])\n",
      "original document: \n",
      "['Mansplaining?', 'Sorry.', 'Not', 'a', 'native', 'English', 'speaker.', \"What's\", 'that', 'mean?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mansplain', 'sorry', 'nat', 'engl', 'speak', 'what', 'mean'], ['mansplaining', 'sorry', 'native', 'english', 'speaker', 'whats', 'mean'])\n",
      "original document: \n",
      "['How', 'long', 'does', 'that', 'free', 'supercharging', 'last?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['long', 'fre', 'supercharg', 'last'], ['long', 'free', 'supercharge', 'last'])\n",
      "original document: \n",
      "['I', 'assume', \"it's\", 'going', 'to', 'be', 'a', 'while,', 'but', 'considering', 'the', 'changes', 'to', 'headphones,', 'I', 'really', 'do', 'not', 'want', 'to', 'be', \"'wasting'\", 'my', 'RP.', 'I', 'know', 'I', \"'could'\", 'buy', 'headphones,', 'but', 'I', 'only', 'have', 'about', '800', 'diamonds', 'that', 'I', 'may', 'need', 'to', 'spend', 'for', 'the', 'LE', 'theme,', 'so', 'buying', 'headphones', 'is', 'out', 'of', 'the', 'question', ':(', 'Even', 'then,', 'buying', 'headphones', 'is', 'barely', 'sufficient', 'compared', 'to', 'how', 'much', 'I', 'used', 'to', 'farm', '(easily', '100+', 'headphones', 'used', 'per', 'day', 'on', 'relaxing', 'days)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['assum', 'going', 'consid', 'chang', 'headphon', 'real', 'want', 'wast', 'rp', 'know', 'could', 'buy', 'headphon', 'eight hundred', 'diamond', 'may', 'nee', 'spend', 'le', 'them', 'buy', 'headphon', 'quest', 'ev', 'buy', 'headphon', 'bar', 'sufficy', 'comp', 'much', 'us', 'farm', 'easy', 'one hundred', 'headphon', 'us', 'per', 'day', 'relax', 'day'], ['assume', 'go', 'consider', 'change', 'headphones', 'really', 'want', 'waste', 'rp', 'know', 'could', 'buy', 'headphones', 'eight hundred', 'diamonds', 'may', 'need', 'spend', 'le', 'theme', 'buy', 'headphones', 'question', 'even', 'buy', 'headphones', 'barely', 'sufficient', 'compare', 'much', 'use', 'farm', 'easily', 'one hundred', 'headphones', 'use', 'per', 'day', 'relax', 'days'])\n",
      "original document: \n",
      "['hes', 'still', 'got', '30', 'upvotes', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'stil', 'got', 'thirty', 'upvot'], ['hes', 'still', 'get', 'thirty', 'upvotes'])\n",
      "original document: \n",
      "['Melee', 'Llama']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mel', 'llam'], ['melee', 'llama'])\n",
      "original document: \n",
      "['Triad', 'and', 'a', 'dump', 'pouch']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['triad', 'dump', 'pouch'], ['triad', 'dump', 'pouch'])\n",
      "original document: \n",
      "['Wtf', 'it', 'looks', 'like', 'they', 'put', 'him', 'at', '2x', 'normal', 'speed', 'and', 'left', 'the', 'other', 'guy', 'the', 'same']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wtf', 'look', 'lik', 'put', '2x', 'norm', 'spee', 'left', 'guy'], ['wtf', 'look', 'like', 'put', '2x', 'normal', 'speed', 'leave', 'guy'])\n",
      "original document: \n",
      "['Wow,', 'just', 'wasted', '20', 'minutes', 'of', 'my', 'life', 'poking', 'around', 'this', 'blog.', 'He', 'never', 'directly', 'approaches', 'questions.', 'He', 'just', 'bitches', 'about', 'the', 'wording', 'they', 'chose,', 'and', 'cries', 'about', 'how', 'Jeremy', 'called', 'the', 'Maxwell', 'institute', 'unofficial', 'apologists.', '\\n\\nI', 'love', 'how', 'he', 'gets', 'upset', 'that', 'Jeremy', \"won't\", 'accept', 'the', 'standard', 'apologist', 'lines,', 'and', 'says', '\"well', \"I've\", 'got', 'nothing', 'new', 'to', 'tell', 'you,', 'fairmormon', 'and', 'farms', 'already', 'gave', 'the', 'answers.\"', 'Then', 'why', 'does', 'your', 'blog', 'exist', 'Mr', 'Cornell?', 'Is', 'it,', 'perhaps,', 'priestcraft?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'wast', 'twenty', 'minut', 'lif', 'pok', 'around', 'blog', 'nev', 'direct', 'approach', 'quest', 'bitch', 'word', 'chos', 'cri', 'jeremy', 'cal', 'maxwel', 'institut', 'unoff', 'apolog', '\\n\\ni', 'lov', 'get', 'upset', 'jeremy', 'wont', 'acceiv', 'standard', 'apolog', 'lin', 'say', 'wel', 'iv', 'got', 'noth', 'new', 'tel', 'fairmormon', 'farm', 'already', 'gav', 'answ', 'blog', 'ex', 'mr', 'cornel', 'perhap', 'priestcraft'], ['wow', 'waste', 'twenty', 'minutes', 'life', 'poke', 'around', 'blog', 'never', 'directly', 'approach', 'question', 'bitch', 'word', 'choose', 'cry', 'jeremy', 'call', 'maxwell', 'institute', 'unofficial', 'apologists', '\\n\\ni', 'love', 'get', 'upset', 'jeremy', 'wont', 'accept', 'standard', 'apologist', 'line', 'say', 'well', 'ive', 'get', 'nothing', 'new', 'tell', 'fairmormon', 'farm', 'already', 'give', 'answer', 'blog', 'exist', 'mr', 'cornell', 'perhaps', 'priestcraft'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "[\"I'm\", 'sorry,', 'but', 'that', 'analysis', 'misses', 'the', 'fact', 'that', 'Trump', \"wasn't\", 'the', 'obstacle', 'to', 'ObamaCare', 'repeal.', 'Trump', 'went', 'along', 'with', 'whatever', 'Ryan', 'or', 'McConnell', 'proposed,', 'despite', 'that', 'being', 'contrary', 'to', 'his', 'campaign', 'promises.', 'Same', 'will', 'be', 'true', 'with', 'tax', '\"reform\"', '(ie.', 'tax', 'cuts).', 'Trump', 'will', 'promote', 'and', 'support', 'anything', 'Ryan', 'puts', 'forth,', 'as', 'long', 'as', 'he', 'can', 'claim', 'victory', 'if', 'it', 'passes.', 'He', 'has', 'no', 'agenda', 'except', 'his', 'image.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sorry', 'analys', 'miss', 'fact', 'trump', 'wasnt', 'obstac', 'obamac', 'rep', 'trump', 'went', 'along', 'whatev', 'ryan', 'mcconnell', 'propos', 'despit', 'cont', 'campaign', 'prom', 'tru', 'tax', 'reform', 'ie', 'tax', 'cut', 'trump', 'promot', 'support', 'anyth', 'ryan', 'put', 'for', 'long', 'claim', 'vict', 'pass', 'agend', 'exceiv', 'im'], ['im', 'sorry', 'analysis', 'miss', 'fact', 'trump', 'wasnt', 'obstacle', 'obamacare', 'repeal', 'trump', 'go', 'along', 'whatever', 'ryan', 'mcconnell', 'propose', 'despite', 'contrary', 'campaign', 'promise', 'true', 'tax', 'reform', 'ie', 'tax', 'cut', 'trump', 'promote', 'support', 'anything', 'ryan', 'put', 'forth', 'long', 'claim', 'victory', 'pass', 'agenda', 'except', 'image'])\n",
      "original document: \n",
      "['Yeah,', 'it', 'seems', 'like', 'you', 'already', 'got', 'a', 'lot', 'of', 'solid', 'advice', 'on', 'what', 'tests', 'and', 'such', 'are', 'next', 'to', 'try.', 'I', 'hope', 'you', 'guys', 'have', 'some', 'better', 'luck', 'soon.', 'I', 'know', 'how', 'not', 'fun', 'the', '\"trying\"', 'part', 'can', 'be', 'after', 'those', 'first', '6', 'months!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'seem', 'lik', 'already', 'got', 'lot', 'solid', 'adv', 'test', 'next', 'try', 'hop', 'guy', 'bet', 'luck', 'soon', 'know', 'fun', 'try', 'part', 'first', 'six', 'month'], ['yeah', 'seem', 'like', 'already', 'get', 'lot', 'solid', 'advice', 'test', 'next', 'try', 'hope', 'guy', 'better', 'luck', 'soon', 'know', 'fun', 'try', 'part', 'first', 'six', 'months'])\n",
      "original document: \n",
      "['Congrats', 'on', 'the', 'pull!', \"That's\", 'some', 'decent', 'coin', 'right', 'there!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congr', 'pul', 'that', 'dec', 'coin', 'right'], ['congrats', 'pull', 'thats', 'decent', 'coin', 'right'])\n",
      "original document: \n",
      "['And', 'you', 'see', 'i', 'have', 'the', 'REM', 'wire', 'in', 'the', 'back', 'of', 'the', 'radio', 'and', 'the', 'REM', 'wire', 'from', 'the', 'amp', 'twisted', 'together', 'but', 'there', 'is', 'no', 'transfer', 'I', 'guess.', 'I', 'got', 'some', 'electrical', 'tape', 'gunk', 'on', 'it.', 'Could', 'it', 'be', 'That', 'or', 'just', 'a', 'faulty', 'wire?', 'I', 'could', 'restrip', 'it', 'I', 'guess.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['see', 'rem', 'wir', 'back', 'radio', 'rem', 'wir', 'amp', 'twist', 'togeth', 'transf', 'guess', 'got', 'elect', 'tap', 'gunk', 'could', 'faul', 'wir', 'could', 'restrip', 'guess'], ['see', 'rem', 'wire', 'back', 'radio', 'rem', 'wire', 'amp', 'twist', 'together', 'transfer', 'guess', 'get', 'electrical', 'tape', 'gunk', 'could', 'faulty', 'wire', 'could', 'restrip', 'guess'])\n",
      "original document: \n",
      "['The', 'system', 'is', 'broken', 'because', 'Americans', 'are', 'constantly', 'fooled', 'into', 'thinking', 'that', 'the', 'Democrat', 'and', 'Republican', 'politicians', 'have', 'their', 'best', 'interests', 'at', 'heart', 'despite', 'constant', 'proof', 'to', 'the', 'contrary,', 'and', 'yet', 'still', 'heatedly', 'will', 'support', 'them', 'because...', 'well', 'I', \"don't\", 'even', 'really', 'know', 'why', 'to', 'tell', 'you', 'the', 'truth.', '', 'Political', 'parties', 'should', 'be', 'abolished', 'in', 'favor', 'of', 'electing', 'people.', '', 'People', 'who', 'actually', 'care', 'about', 'the', 'population', 'and', 'not', 'just', 'about', 'filling', 'their', 'coffers.', '', 'Trump', 'just', 'took', 'advantage', 'of', 'the', 'broken', 'political', 'system', 'we', 'already', 'have.\\n\\nEdit:', 'A', 'word', 'poorly', 'spellchecked.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['system', 'brok', 'am', 'const', 'fool', 'think', 'democr', 'republ', 'polit', 'best', 'interest', 'heart', 'despit', 'const', 'proof', 'cont', 'yet', 'stil', 'heat', 'support', 'wel', 'dont', 'ev', 'real', 'know', 'tel', 'tru', 'polit', 'party', 'abol', 'fav', 'elect', 'peopl', 'peopl', 'act', 'car', 'pop', 'fil', 'coff', 'trump', 'took', 'adv', 'brok', 'polit', 'system', 'already', 'have\\n\\nedit', 'word', 'poor', 'spellcheck'], ['system', 'break', 'americans', 'constantly', 'fool', 'think', 'democrat', 'republican', 'politicians', 'best', 'interest', 'heart', 'despite', 'constant', 'proof', 'contrary', 'yet', 'still', 'heatedly', 'support', 'well', 'dont', 'even', 'really', 'know', 'tell', 'truth', 'political', 'party', 'abolish', 'favor', 'elect', 'people', 'people', 'actually', 'care', 'population', 'fill', 'coffers', 'trump', 'take', 'advantage', 'break', 'political', 'system', 'already', 'have\\n\\nedit', 'word', 'poorly', 'spellchecked'])\n",
      "original document: \n",
      "['Veruna', 'forced', 'a', 'woeful', 'smile.', 'It', 'quickly', 'faded', 'away', 'as', 'he', 'creeped', 'up', 'to', 'Jalvere', 'and', 'whispered', 'into', 'his', 'ear.\\n\\n\"Nute', 'Gunray', 'made', 'me', 'kill', 'him.', 'It', 'was', 'the', 'only', 'way.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['verun', 'forc', 'woe', 'smil', 'quick', 'fad', 'away', 'creep', 'jalv', 'whisp', 'ear\\n\\nnute', 'gunray', 'mad', 'kil', 'way'], ['veruna', 'force', 'woeful', 'smile', 'quickly', 'fade', 'away', 'creep', 'jalvere', 'whisper', 'ear\\n\\nnute', 'gunray', 'make', 'kill', 'way'])\n",
      "original document: \n",
      "['please', 'include', 'links', 'to', 'your', 'TSV', 'threads', '-', 'I', 'need', 'to', 'see', 'that', 'you', 'are', 'an', 'active', 'hatcher', 'in', 'the', 'community', '-', 'and', 'are', 'these', 'all', 'for', 'FFA?', 'or', 'do', 'any', 'of', 'them', 'match', 'your', 'TSV?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pleas', 'includ', 'link', 'tsv', 'threads', 'nee', 'see', 'act', 'hatch', 'commun', 'ffa', 'match', 'tsv'], ['please', 'include', 'link', 'tsv', 'thread', 'need', 'see', 'active', 'hatcher', 'community', 'ffa', 'match', 'tsv'])\n",
      "original document: \n",
      "['Title--Battle', 'of', 'Naguabo\\n\\n\\nGenre--Military', 'Sci-Fi\\n\\n\\nWord', 'Count--5233\\n\\n\\nType', 'of', 'Feedback--General', 'feedback', 'is', 'appreciated,', 'but', 'very', 'interested', 'in', 'thought', 'about', 'the', 'action', 'sequences.', '', 'This', 'is', 'a', 'middle', 'chapter', 'from', 'a', 'novel-length', 'work,', 'so', \"you're\", 'getting', 'things', 'bit', 'out', 'of', 'context.', '', \"There's\", 'also', 'profanity', 'and', 'graphic', 'violence,', 'so', 'if', \"that's\", 'not', 'your', 'thing,', 'you', 'may', 'want', 'to', 'stay', 'clear.\\n\\n[Link', 'to', 'Document](https://docs.google.com/document/d/1X_PQf-j3XKB1IFaJVAFpNX2UkmUJml2lVtsxuedCaIQ/edit?usp=sharing)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['titlebattl', 'naguabo\\n\\n\\ngenremilitary', 'scifi\\n\\n\\nword', 'count5233\\n\\n\\ntype', 'feedbackg', 'feedback', 'apprecy', 'interest', 'thought', 'act', 'sequ', 'middl', 'chapt', 'novelleng', 'work', 'yo', 'get', 'thing', 'bit', 'context', 'ther', 'also', 'prof', 'graph', 'viol', 'that', 'thing', 'may', 'want', 'stay', 'clear\\n\\nlink', 'documenthttpsdocsgooglecomdocumentd1x_pqfj3xkb1ifajvafpnx2ukmujml2lvtsxuedcaiqedituspsharing'], ['titlebattle', 'naguabo\\n\\n\\ngenremilitary', 'scifi\\n\\n\\nword', 'count5233\\n\\n\\ntype', 'feedbackgeneral', 'feedback', 'appreciate', 'interest', 'think', 'action', 'sequence', 'middle', 'chapter', 'novellength', 'work', 'youre', 'get', 'things', 'bite', 'context', 'theres', 'also', 'profanity', 'graphic', 'violence', 'thats', 'thing', 'may', 'want', 'stay', 'clear\\n\\nlink', 'documenthttpsdocsgooglecomdocumentd1x_pqfj3xkb1ifajvafpnx2ukmujml2lvtsxuedcaiqedituspsharing'])\n",
      "original document: \n",
      "['Agreed.', 'In', 'one', 'of', 'my', 'current', 'fics,', 'the', 'age', 'difference', 'between', 'the', 'primary', 'ship', 'is', 'approximately', 'eighteen', 'years', '(early', '20s', 'and', 'mid-40s),', 'so...', \"it's\", 'a', 'thing,', 'lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agree', 'on', 'cur', 'fic', 'ag', 'diff', 'prim', 'ship', 'approxim', 'eighteen', 'year', 'ear', '20s', 'mid40s', 'thing', 'lol'], ['agree', 'one', 'current', 'fics', 'age', 'difference', 'primary', 'ship', 'approximately', 'eighteen', 'years', 'early', '20s', 'mid40s', 'thing', 'lol'])\n",
      "original document: \n",
      "['I', 'recently', 'found', 'out', 'I', \"can't\", 'donate', 'blood', 'anymore,', 'and', 'was', 'pretty', 'gutted', 'as', \"it's\", 'an', 'amazing', 'literally', 'life', 'saving', 'thing', 'to', 'do.', 'So', 'despite', 'a', 'lifelong', 'fear', 'of', 'needles', 'and', 'all', 'things', 'related,', 'my', 'incredible', 'boyfriend', 'is', 'donating', 'for', 'the', 'first', 'time', 'tomorrow,', 'to', '\"make', 'up\"', 'for', 'the', 'blood', 'I', 'can', 'no', 'longer', 'donate.\\n\\nMy', 'advice', 'would', 'be', 'drink', 'plenty', 'of', 'water', 'and', 'have', 'a', 'filling', 'meal,', 'and', 'try', 'and', 'eat', 'something', 'sugary', 'like', 'a', 'chocolate', 'bar', 'an', 'hour', 'or', 'so', 'before', '(also', 'helps', 'with', 'my', 'nerves).', 'The', 'most', 'painful', 'bit', 'is', 'ripping', 'off', 'the', 'plaster', 'after', 'wards', '-', 'I', 'promise!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rec', 'found', 'cant', 'don', 'blood', 'anym', 'pretty', 'gut', 'amaz', 'lit', 'lif', 'sav', 'thing', 'despit', 'lifelong', 'fear', 'needl', 'thing', 'rel', 'incred', 'boyfriend', 'don', 'first', 'tim', 'tomorrow', 'mak', 'blood', 'long', 'donate\\n\\nmy', 'adv', 'would', 'drink', 'plenty', 'wat', 'fil', 'meal', 'try', 'eat', 'someth', 'sug', 'lik', 'chocol', 'bar', 'hour', 'also', 'help', 'nerv', 'pain', 'bit', 'rip', 'plast', 'ward', 'prom'], ['recently', 'find', 'cant', 'donate', 'blood', 'anymore', 'pretty', 'gutted', 'amaze', 'literally', 'life', 'save', 'thing', 'despite', 'lifelong', 'fear', 'needle', 'things', 'relate', 'incredible', 'boyfriend', 'donate', 'first', 'time', 'tomorrow', 'make', 'blood', 'longer', 'donate\\n\\nmy', 'advice', 'would', 'drink', 'plenty', 'water', 'fill', 'meal', 'try', 'eat', 'something', 'sugary', 'like', 'chocolate', 'bar', 'hour', 'also', 'help', 'nerve', 'painful', 'bite', 'rip', 'plaster', 'ward', 'promise'])\n",
      "original document: \n",
      "['In', 'Flames', 'and', 'Mastodon.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['flam', 'mastodon'], ['flame', 'mastodon'])\n",
      "original document: \n",
      "['Hello', 'dog!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hello', 'dog'], ['hello', 'dog'])\n",
      "original document: \n",
      "['one', 'random', 'please']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'random', 'pleas'], ['one', 'random', 'please'])\n",
      "original document: \n",
      "['No', 'problem!', 'Thank', 'you', 'for', 'the', 'feedback!', 'Working', 'on', 'being', 'more', 'descriptive,', 'very', 'wide', 'variety', 'of', 'guests', 'for', 'the', 'clean', 'radio', 'to', 'the', 'comedian', 'unfiltered', 'radio.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['problem', 'thank', 'feedback', 'work', 'describ', 'wid', 'vary', 'guest', 'cle', 'radio', 'com', 'unfilt', 'radio'], ['problem', 'thank', 'feedback', 'work', 'descriptive', 'wide', 'variety', 'guests', 'clean', 'radio', 'comedian', 'unfiltered', 'radio'])\n",
      "original document: \n",
      "['No', 'tire', 'culpa', 'para', 'afuera?', 'dije', 'que', 'aca', 'son', 'muy', 'forros', 'apra', 'arriesgarse', 'asi', 'y', 'que', 'no', 'nos', 'darian', 'mucha', 'bola.', 'L', 'ocual', 'es', 'verdad', 'porque', 'aunque', 'seamos', 'del', 'G20', 'honestamente', 'nuestra', '\"influencia\"', 'no', 'se', 'nota.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tir', 'culp', 'par', 'afuer', 'dij', 'que', 'ac', 'son', 'muy', 'forro', 'apr', 'arriesgars', 'as', 'que', 'nos', 'dar', 'much', 'bol', 'l', 'oc', 'es', 'verdad', 'porqu', 'aunqu', 'seamo', 'del', 'g20', 'honesta', 'nuestr', 'influenc', 'se', 'not'], ['tire', 'culpa', 'para', 'afuera', 'dije', 'que', 'aca', 'son', 'muy', 'forros', 'apra', 'arriesgarse', 'asi', 'que', 'nos', 'darian', 'mucha', 'bola', 'l', 'ocual', 'es', 'verdad', 'porque', 'aunque', 'seamos', 'del', 'g20', 'honestamente', 'nuestra', 'influencia', 'se', 'nota'])\n",
      "original document: \n",
      "['SO', 'SORRY', 'sold', 'but', 'forgot', 'to', 'remove', 'post']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'sold', 'forgot', 'remov', 'post'], ['sorry', 'sell', 'forget', 'remove', 'post'])\n",
      "original document: \n",
      "['If', 'you', 'accidentally', 'go', 'the', 'wrong', 'way', 'trying', 'to', 'leave', 'the', 'taint,', 'you', 'might', 'run', 'into', 'a', 'starfish.', 'For', 'some', 'of', 'us,', 'that', 'might', 'actually', 'be', 'the', 'correct', 'way']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['accid', 'go', 'wrong', 'way', 'try', 'leav', 'taint', 'might', 'run', 'starf', 'us', 'might', 'act', 'correct', 'way'], ['accidentally', 'go', 'wrong', 'way', 'try', 'leave', 'taint', 'might', 'run', 'starfish', 'us', 'might', 'actually', 'correct', 'way'])\n",
      "original document: \n",
      "[\"I'm\", 'into', 'it', 'and', 'you', 'bring', 'up', 'great', 'points', 'there.', '', 'But', 'try', 'convincing', 'an', 'open', 'borders', 'or', 'La', 'Rasa', 'advocate', 'that', 'English', 'be', 'a', 'requisite', 'and', 'the', 'official', 'language,', 'heads', 'will', 'explode.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'bring', 'gre', 'point', 'try', 'convint', 'op', 'bord', 'la', 'ras', 'advoc', 'engl', 'requisit', 'off', 'langu', 'head', 'explod'], ['im', 'bring', 'great', 'point', 'try', 'convince', 'open', 'border', 'la', 'rasa', 'advocate', 'english', 'requisite', 'official', 'language', 'head', 'explode'])\n",
      "original document: \n",
      "[\"I've\", 'heard', 'good', 'things', 'about', 'that', 'one', 'too.', \"I've\", 'actually', 'owned', 'the', 'Intel', '7620', '(I', 'think)', 'years', 'ago.', 'I', 'even', 'got', 'use', 'out', 'of', 'the', 'bluetooth.\\n\\nThe', 'gigabyte', 'one', 'seems', 'to', 'be', 'the', 'newer', 'version', 'of', 'that', 'one.\\n\\nJust', 'curious', 'if', \"I'd\", 'see', 'any', 'benefit', 'in', 'spending', 'more.\\n\\n------\\n\\nAnd', 'I', 'do', 'use', 'pcpartpicker...', 'Links', 'on', 'amazon', 'were', 'just', 'quicker', 'here.', 'Thanks.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'heard', 'good', 'thing', 'on', 'iv', 'act', 'own', 'intel', 'seven thousand, six hundred and twenty', 'think', 'year', 'ago', 'ev', 'got', 'us', 'bluetooth\\n\\nthe', 'gigabyt', 'on', 'seem', 'new', 'vert', 'one\\n\\njust', 'cury', 'id', 'see', 'benefit', 'spend', 'more\\n\\n\\n\\nand', 'us', 'pcpartpicker', 'link', 'amazon', 'quick', 'thank'], ['ive', 'hear', 'good', 'things', 'one', 'ive', 'actually', 'own', 'intel', 'seven thousand, six hundred and twenty', 'think', 'years', 'ago', 'even', 'get', 'use', 'bluetooth\\n\\nthe', 'gigabyte', 'one', 'seem', 'newer', 'version', 'one\\n\\njust', 'curious', 'id', 'see', 'benefit', 'spend', 'more\\n\\n\\n\\nand', 'use', 'pcpartpicker', 'link', 'amazon', 'quicker', 'thank'])\n",
      "original document: \n",
      "['HesaJew', 'would', 'get', 'thrown', 'off', 'reddit', 'tho']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hesajew', 'would', 'get', 'thrown', 'reddit', 'tho'], ['hesajew', 'would', 'get', 'throw', 'reddit', 'tho'])\n",
      "original document: \n",
      "['Lonegunman5....304', 'hunter']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lonegunman5304', 'hunt'], ['lonegunman5304', 'hunter'])\n",
      "original document: \n",
      "['Rip', 'off', 'my', 'nut', 'sack', 'and', 'call', 'me', 'Lord', 'Varys', 'can', 'we', 'get', 'on', 'with', 'the', 'game', 'at', 'the', 'scheduled', 'time', 'please!', 'FUCK', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rip', 'nut', 'sack', 'cal', 'lord', 'vary', 'get', 'gam', 'scheduled', 'tim', 'pleas', 'fuck'], ['rip', 'nut', 'sack', 'call', 'lord', 'vary', 'get', 'game', 'schedule', 'time', 'please', 'fuck'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Great', 'extension', 'there', 'by', 'Ward']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gre', 'extend', 'ward'], ['great', 'extension', 'ward'])\n",
      "original document: \n",
      "['Brandon', 'Jennings', 'belongs', 'pretty', 'high', 'on', 'that', 'last', 'like', 'top', '5', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brandon', 'jen', 'belong', 'pretty', 'high', 'last', 'lik', 'top', 'fiv'], ['brandon', 'jennings', 'belong', 'pretty', 'high', 'last', 'like', 'top', 'five'])\n",
      "original document: \n",
      "[\"I'm\", 'just', 'blown', 'that', 'as', 'soon', 'as', 'the', 'left', 'stops', 'telling', 'us', 'to', 'boycott', 'the', 'team', 'suddenly', 'the', 'right,', 'who', 'loves', 'to', 'pretend', 'they', \"aren't\", 'just', 'as', 'prone', 'to', 'flying', 'off', 'the', 'handle', 'over', 'every', 'little', 'thing', 'in', 'their', 'newsfeeds,', 'is', 'suddenly', 'telling', 'me', 'to', 'boycott', 'the', 'team', 'over', 'this', 'flavor', 'of', 'the', 'month', 'outrage.', '\\n\\nIf', 'this', \"doesn't\", 'prove', 'both', 'sides', 'are', 'equally', 'retarded', 'I', \"don't\", 'know', 'what', 'will.', 'Racial', 'inequality', 'is', 'a', 'massive', 'problem', 'that', 'nobody', 'wants', 'to', 'address,', 'and', 'these', 'NFL', 'protests', 'are', 'doing', 'a', 'good', 'job', 'of', 'highlighting', 'that.', 'Unfortunately,', \"it's\", 'going', 'to', 'take', 'a', 'few', 'more', 'decades', 'of', '\"liberal', 'propaganda\"', 'for', 'people', 'to', 'realize', 'that', 'most', 'racism', \"isn't\", 'in-your-face', 'klansman', 'shit,', 'and', 'this', 'protest', 'is', 'doing', 'nothing', 'to', 'address', 'that', 'divide.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'blown', 'soon', 'left', 'stop', 'tel', 'us', 'boycot', 'team', 'sud', 'right', 'lov', 'pretend', 'ar', 'pron', 'fly', 'handl', 'every', 'littl', 'thing', 'newsfee', 'sud', 'tel', 'boycot', 'team', 'flav', 'mon', 'out', '\\n\\nif', 'doesnt', 'prov', 'sid', 'eq', 'retard', 'dont', 'know', 'rac', 'ineq', 'mass', 'problem', 'nobody', 'want', 'address', 'nfl', 'protest', 'good', 'job', 'highlight', 'unfortun', 'going', 'tak', 'decad', 'lib', 'propagand', 'peopl', 'real', 'rac', 'isnt', 'inyourfac', 'klansm', 'shit', 'protest', 'noth', 'address', 'divid'], ['im', 'blow', 'soon', 'leave', 'stop', 'tell', 'us', 'boycott', 'team', 'suddenly', 'right', 'love', 'pretend', 'arent', 'prone', 'fly', 'handle', 'every', 'little', 'thing', 'newsfeeds', 'suddenly', 'tell', 'boycott', 'team', 'flavor', 'month', 'outrage', '\\n\\nif', 'doesnt', 'prove', 'side', 'equally', 'retard', 'dont', 'know', 'racial', 'inequality', 'massive', 'problem', 'nobody', 'want', 'address', 'nfl', 'protest', 'good', 'job', 'highlight', 'unfortunately', 'go', 'take', 'decades', 'liberal', 'propaganda', 'people', 'realize', 'racism', 'isnt', 'inyourface', 'klansman', 'shit', 'protest', 'nothing', 'address', 'divide'])\n",
      "original document: \n",
      "['I', 'always', 'cringe', 'when', 'announcers', 'talk', 'about', 'how', 'good', 'Davis', 'is', 'or', 'how', 'great', 'his', 'career', 'has', 'been.', 'Not', 'putting', 'the', 'blocked', 'kicks', 'on', 'him', 'but', 'close', 'games', 'scare', 'me', 'because', 'of', 'the', 'mix', 'of', 'our', 'iffy', 'red', 'zone', 'offense', 'and', 'terrible', 'kicking', 'game.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'cring', 'annount', 'talk', 'good', 'dav', 'gre', 'car', 'put', 'block', 'kick', 'clos', 'gam', 'scar', 'mix', 'iffy', 'red', 'zon', 'offens', 'terr', 'kick', 'gam'], ['always', 'cringe', 'announcers', 'talk', 'good', 'davis', 'great', 'career', 'put', 'block', 'kick', 'close', 'game', 'scare', 'mix', 'iffy', 'red', 'zone', 'offense', 'terrible', 'kick', 'game'])\n",
      "original document: \n",
      "['I', 'know', 'what', 'you', 'mean.', 'I', 'dont', 'like', 'to', 'do', 'the', 'math:(', 'Cant', 'believe', 'I', 'have', 'been', 'poisioning', 'my', 'body', 'for', 'this', 'long.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'mean', 'dont', 'lik', 'math', 'cant', 'believ', 'poid', 'body', 'long'], ['know', 'mean', 'dont', 'like', 'math', 'cant', 'believe', 'poisioning', 'body', 'long'])\n",
      "original document: \n",
      "['FYI', '-', 'those', 'are', 'for', 'the', 'little', 'cups', 'that', 'are', 'used', 'during', 'communion']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fyi', 'littl', 'cup', 'us', 'commun'], ['fyi', 'little', 'cup', 'use', 'communion'])\n",
      "original document: \n",
      "['143413518|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'qrXQpZdq)\\n\\nKerry\\nObama\\nObama\\nTrump\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, five hundred and eighteen', 'gt', 'unit', 'stat', 'anonym', 'id', 'qrxqpzdq\\n\\nkerry\\nobama\\nobama\\ntrump\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, five hundred and eighteen', 'gt', 'unite', 'state', 'anonymous', 'id', 'qrxqpzdq\\n\\nkerry\\nobama\\nobama\\ntrump\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Yeah', 'okay', 'go', 'do', 'all', 'of', 'vHoF', 'without', 'a', 'food', 'buff', 'and', 'let', 'me', 'know', 'how', 'you', 'feel', 'about', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'okay', 'go', 'vhof', 'without', 'food', 'buff', 'let', 'know', 'feel'], ['yeah', 'okay', 'go', 'vhof', 'without', 'food', 'buff', 'let', 'know', 'feel'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['And', \"what's\", 'that', 'reason', 'if', 'you', 'want', 'to', 'answer?\\n\\nI', 'mean', 'Kickstarter', 'is', 'not', 'really', 'preordering,', \"it's\", 'mostly', 'pledging', 'money', 'for', 'a', 'thing', 'you', 'want', 'to', 'support.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['what', 'reason', 'want', 'answer\\n\\ni', 'mean', 'kickstart', 'real', 'preord', 'most', 'pledg', 'money', 'thing', 'want', 'support'], ['whats', 'reason', 'want', 'answer\\n\\ni', 'mean', 'kickstarter', 'really', 'preordering', 'mostly', 'pledge', 'money', 'thing', 'want', 'support'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'a', 'troll.', 'This', 'is', 'very', 'real', 'and', 'Monday', \"we'll\", 'see', 'how', 'and', 'if', 'this', 'employment', 'situation', \"continues.\\n\\nShe's\", 'not', 'cleaning', 'any', 'bottles.', 'My', 'daughter', \"doesn't\", 'spit', 'up', 'exorcist', 'style.', \"She'll\", 'dribble', 'some', 'extra', 'milk', 'down', 'her', 'chin', 'while', 'eating', 'so', \"that's\", 'a', 'quick', 'wipe', 'and', \"it's\", 'done.', 'There', 'have', 'been', 'no', 'diaper', 'blowouts.', 'Dirty', 'onesies', 'get', 'tossed', 'into', 'a', 'laundry', 'basket', 'and', 'new', 'ones', 'are', 'always', 'available', 'because', \"I'm\", 'the', 'one', 'who', 'has', 'washed', 'them', 'and', 'folded', 'them.', 'There', 'is', 'no', 'diaper', 'laundry', 'because', 'we', 'use', 'disposable', 'ones', 'and', 'the', 'disposable', 'ones', 'are', \"what's\", 'taking', 'put', 'most', 'of', 'the', 'space', 'int', 'he', 'garbage', 'that', \"she's\", 'not', 'taking', 'out.\\n\\nWith', 'how', 'much', 'time', 'she', 'spends', 'texting', 'on', 'her', 'phone', 'and', 'then', 'streaming', 'TV', 'shows,', 'oh', 'she', 'gets', 'more', 'than', '15', 'mine', 'very', '2', 'hours', 'and', \"she's\", 'free', 'to', 'eat', 'lunch', 'whenever', 'the', \"baby's\", 'down', 'for', 'a', 'nap', 'which', 'means', 'hours', 'worth', 'of', 'lunch', \"time.\\n\\nI'm\", 'far', 'from', 'over', 'working', 'this', 'girl.', \"I'm\", 'trying', 'to', 'get', 'her', 'to', 'actually', 'do', 'some', 'work.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'trol', 'real', 'monday', 'wel', 'see', 'employ', 'situ', 'continues\\n\\nshe', 'cle', 'bottl', 'daught', 'doesnt', 'spit', 'exorc', 'styl', 'shel', 'dribbl', 'extr', 'milk', 'chin', 'eat', 'that', 'quick', 'wip', 'don', 'diap', 'blowout', 'dirty', 'onesy', 'get', 'toss', 'laundry', 'basket', 'new', 'on', 'alway', 'avail', 'im', 'on', 'wash', 'fold', 'diap', 'laundry', 'us', 'dispos', 'on', 'dispos', 'on', 'what', 'tak', 'put', 'spac', 'int', 'garb', 'she', 'tak', 'out\\n\\nwith', 'much', 'tim', 'spend', 'text', 'phon', 'streaming', 'tv', 'show', 'oh', 'get', 'fifteen', 'min', 'two', 'hour', 'she', 'fre', 'eat', 'lunch', 'whenev', 'baby', 'nap', 'mean', 'hour', 'wor', 'lunch', 'time\\n\\nim', 'far', 'work', 'girl', 'im', 'try', 'get', 'act', 'work'], ['im', 'troll', 'real', 'monday', 'well', 'see', 'employment', 'situation', 'continues\\n\\nshes', 'clean', 'bottle', 'daughter', 'doesnt', 'spit', 'exorcist', 'style', 'shell', 'dribble', 'extra', 'milk', 'chin', 'eat', 'thats', 'quick', 'wipe', 'do', 'diaper', 'blowouts', 'dirty', 'onesies', 'get', 'toss', 'laundry', 'basket', 'new', 'ones', 'always', 'available', 'im', 'one', 'wash', 'fold', 'diaper', 'laundry', 'use', 'disposable', 'ones', 'disposable', 'ones', 'whats', 'take', 'put', 'space', 'int', 'garbage', 'shes', 'take', 'out\\n\\nwith', 'much', 'time', 'spend', 'texting', 'phone', 'stream', 'tv', 'show', 'oh', 'get', 'fifteen', 'mine', 'two', 'hours', 'shes', 'free', 'eat', 'lunch', 'whenever', 'baby', 'nap', 'mean', 'hours', 'worth', 'lunch', 'time\\n\\nim', 'far', 'work', 'girl', 'im', 'try', 'get', 'actually', 'work'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Love', 'is,', 'uh,', 'hard', 'to', 'explain.', '', 'I', 'guess.\\n\\n[spoiler](/s', '\"Most', 'of', 'that', 'route', \"doesn't\", 'even', 'feel', 'like', \"they're\", 'romantically', 'attracted,', 'like', 'in', \"Hanako's.\", '', 'In', 'that', 'route', \"it's\", 'all', 'in', 'act', '4', 'and', 'hinges', 'one', 'of', 'three', 'ways', 'based', 'on', 'two', 'choices.', '', 'I', 'liked', 'how', 'that', 'was', 'handled,', 'felt', 'very', 'real.', 'In', \"Rin's,\", 'Hisao', 'monologues', 'about', 'how', \"he's\", 'not', 'sure', 'how', 'or', 'why', 'he', 'fell', 'for', 'Rin,', 'it', 'just', 'happened.\")', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'uh', 'hard', 'explain', 'guess\\n\\nspoiler', 'rout', 'doesnt', 'ev', 'feel', 'lik', 'theyr', 'rom', 'attract', 'lik', 'hanako', 'rout', 'act', 'four', 'hing', 'on', 'three', 'way', 'bas', 'two', 'cho', 'lik', 'handl', 'felt', 'real', 'rin', 'hisao', 'monolog', 'hes', 'sur', 'fel', 'rin', 'hap'], ['love', 'uh', 'hard', 'explain', 'guess\\n\\nspoilers', 'route', 'doesnt', 'even', 'feel', 'like', 'theyre', 'romantically', 'attract', 'like', 'hanakos', 'route', 'act', 'four', 'hinge', 'one', 'three', 'ways', 'base', 'two', 'choices', 'like', 'handle', 'felt', 'real', 'rins', 'hisao', 'monologues', 'hes', 'sure', 'fell', 'rin', 'happen'])\n",
      "original document: \n",
      "['\"street', 'food', 'in', 'Akihabara\"\\n\\nthat\\'s', 'when', 'I', 'knew', 'this', 'person', 'has', 'no', 'clue', 'as', 'to', 'what', 'the', 'fuck', 'is', 'anywhere', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['street', 'food', 'akihabara\\n\\nthats', 'knew', 'person', 'clu', 'fuck', 'anywh'], ['street', 'food', 'akihabara\\n\\nthats', 'know', 'person', 'clue', 'fuck', 'anywhere'])\n",
      "original document: \n",
      "['Well', 'ya', 'thats', 'why', 'I', 'dont', 'understand,', 'you', 'can', 'get', 'the', 'same', 'chain', 'going', 'with', 'OK', 'innate', 'skills,', '', 'dont', 'see', 'the', 'reason', 'why', 'you', 'would', 'put', 'a', 'pod', 'on', 'him']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'ya', 'that', 'dont', 'understand', 'get', 'chain', 'going', 'ok', 'in', 'skil', 'dont', 'see', 'reason', 'would', 'put', 'pod'], ['well', 'ya', 'thats', 'dont', 'understand', 'get', 'chain', 'go', 'ok', 'innate', 'skills', 'dont', 'see', 'reason', 'would', 'put', 'pod'])\n",
      "original document: \n",
      "['Wow,', 'I', 'love', 'these!', 'Great', 'job,', 'your', 'execution', 'is', 'awesome!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wow', 'lov', 'gre', 'job', 'execut', 'awesom'], ['wow', 'love', 'great', 'job', 'execution', 'awesome'])\n",
      "original document: \n",
      "['[+eqleriq](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoj457/):\\n\\n&gt;', 'My', 'g.f.', 'is', 'an', 'extremely', 'good', 'writer', 'but', 'the', 'distance', 'between', \"'typing\", 'in', 'a', 'word', \"document'\", 'and', \"'being\", \"published'\", 'seems', 'intimidatingly', 'vast', 'to', 'her.\\n&gt;', '\\n&gt;', 'What', 'would', 'you', 'say', 'where', 'the', 'steps', '(big', 'or', 'small)', 'from', 'aspiring', 'to', 'published?\\n\\nHuh?', 'You', 'can', 'upload', 'a', 'PDF', 'to', 'amazon', 'and', 'be', '\"published\"\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eqleriqhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoj457\\n\\ngt', 'gf', 'extrem', 'good', 'writ', 'dist', 'typ', 'word', 'docu', 'publ', 'seem', 'intimid', 'vast', 'her\\ngt', '\\ngt', 'would', 'say', 'step', 'big', 'smal', 'aspir', 'published\\n\\nhuh', 'upload', 'pdf', 'amazon', 'published\\n'], ['eqleriqhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoj457\\n\\ngt', 'gf', 'extremely', 'good', 'writer', 'distance', 'type', 'word', 'document', 'publish', 'seem', 'intimidatingly', 'vast', 'her\\ngt', '\\ngt', 'would', 'say', 'step', 'big', 'small', 'aspire', 'published\\n\\nhuh', 'upload', 'pdf', 'amazon', 'published\\n'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Eh,', 'maybe', 'during', 'the', 'regular', 'season,', 'but', 'they', 'were', 'playing', 'pretty', 'well', 'with', 'Rondo', 'in', 'the', 'first', 'games', 'of', 'the', 'playoffs', 'until', 'they', 'lost', 'him']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eh', 'mayb', 'regul', 'season', 'play', 'pretty', 'wel', 'rondo', 'first', 'gam', 'playoff', 'lost'], ['eh', 'maybe', 'regular', 'season', 'play', 'pretty', 'well', 'rondo', 'first', 'game', 'playoffs', 'lose'])\n",
      "original document: \n",
      "[\"You'll\", 'be', 'back.', 'They', 'all', 'come', 'back.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['youl', 'back', 'com', 'back\\n'], ['youll', 'back', 'come', 'back\\n'])\n",
      "original document: \n",
      "['That', 'escalated', 'at', 'the', 'end.', '', \"I'll\", 'allow', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['esc', 'end', 'il', 'allow'], ['escalate', 'end', 'ill', 'allow'])\n",
      "original document: \n",
      "['I', 'could', 'see', 'that', 'but', 'then', 'you', 'have', 'to', 'take', 'into', 'account', 'that', 'the', 'computer', 'would', 'have', 'to', 'have', 'anticipated', 'him', 'sitting', 'on', 'the', 'desk', 'ahead', 'of', 'time', 'to', 'ensure', 'that', 'his', 'orientation', 'from', 'the', 'outset', 'was', 'such', 'that', 'both', 'his', 'desk', 'and', 'her', 'desk', 'would', 'align.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'see', 'tak', 'account', 'comput', 'would', 'anticip', 'sit', 'desk', 'ahead', 'tim', 'ens', 'ory', 'outset', 'desk', 'desk', 'would', 'align'], ['could', 'see', 'take', 'account', 'computer', 'would', 'anticipate', 'sit', 'desk', 'ahead', 'time', 'ensure', 'orientation', 'outset', 'desk', 'desk', 'would', 'align'])\n",
      "original document: \n",
      "['Jay', '305', '-', 'Thuggin\\n\\nGame', '-', 'My', 'Flag\\n\\nMeek', 'Mill', '-', 'Body', 'Count\\n\\n7', 'Mile', 'Clee', '-', 'Famous', 'Where', \"I'm\", 'From\\n\\nWilly', 'Northpole', '-', 'Straight', 'Jacket']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jay', 'three hundred and five', 'thuggin\\n\\ngame', 'flag\\n\\nmeek', 'mil', 'body', 'count\\n\\n7', 'mil', 'cle', 'fam', 'im', 'from\\n\\nwilly', 'northpol', 'straight', 'jacket'], ['jay', 'three hundred and five', 'thuggin\\n\\ngame', 'flag\\n\\nmeek', 'mill', 'body', 'count\\n\\n7', 'mile', 'clee', 'famous', 'im', 'from\\n\\nwilly', 'northpole', 'straight', 'jacket'])\n",
      "original document: \n",
      "['Sorry,', 'I', 'was', 'out', 'today!', 'Let', 'me', 'know', 'when', \"you're\", 'next', 'available!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'today', 'let', 'know', 'yo', 'next', 'avail'], ['sorry', 'today', 'let', 'know', 'youre', 'next', 'available'])\n",
      "original document: \n",
      "['It', 'sounds', 'like', 'a', 'therapist', 'would', 'be', 'something', 'to', 'consider.', 'Mine', 'helped', 'me', 'a', 'great', 'deal.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'lik', 'therap', 'would', 'someth', 'consid', 'min', 'help', 'gre', 'deal'], ['sound', 'like', 'therapist', 'would', 'something', 'consider', 'mine', 'help', 'great', 'deal'])\n",
      "original document: \n",
      "['Apparently', 'yes,', 'she', 'was', 'happy', 'that', 'i', \"didn't\", 'learn', 'anything', 'worse', 'at', 'school!', 'Jokes', 'on', 'her', 'I', 'never', 'learn', 'anything', 'there', 'anyway!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['app', 'ye', 'happy', 'didnt', 'learn', 'anyth', 'wors', 'school', 'jok', 'nev', 'learn', 'anyth', 'anyway'], ['apparently', 'yes', 'happy', 'didnt', 'learn', 'anything', 'worse', 'school', 'joke', 'never', 'learn', 'anything', 'anyway'])\n",
      "original document: \n",
      "['Yes.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye'], ['yes'])\n",
      "original document: \n",
      "['Because', 'they', 'are', 'there', 'and', 'weaken', 'the', 'glass.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['weak', 'glass'], ['weaken', 'glass'])\n",
      "original document: \n",
      "['I', 'will,', 'bc', 'we', 'were', 'just', 'approved', 'to', 'pick', 'him', 'up', 'tomorrow.', 'Already', 'in', 'love.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bc', 'approv', 'pick', 'tomorrow', 'already', 'lov'], ['bc', 'approve', 'pick', 'tomorrow', 'already', 'love'])\n",
      "original document: \n",
      "['Au', 'final', 'très', 'peu', \"d'info\", 'actuellement.', 'On', 'verra', 'bien', 'une', 'fois', 'que', \"l'enquête\", 'aura', 'avancée.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['au', 'fin', 'tre', 'peu', 'dinfo', 'actuel', 'verr', 'bien', 'un', 'foi', 'que', 'lenquet', 'aur', 'av'], ['au', 'final', 'tres', 'peu', 'dinfo', 'actuellement', 'verra', 'bien', 'une', 'fois', 'que', 'lenquete', 'aura', 'avancee'])\n",
      "original document: \n",
      "['If', 'you', 'tag', 'more', 'than', '3', 'people,', 'they', 'wont', 'get', 'notified', 'btw.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tag', 'three', 'peopl', 'wont', 'get', 'not', 'btw'], ['tag', 'three', 'people', 'wont', 'get', 'notify', 'btw'])\n",
      "original document: \n",
      "['Ah', 'must', 'be', 'thinking', 'of', 'walls']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ah', 'must', 'think', 'wal'], ['ah', 'must', 'think', 'wall'])\n",
      "original document: \n",
      "['If', \"you're\", 'doing', 'your', 'schoolwork,', \"it's\", 'not', 'procrastinating', 'anymore.', 'If', \"you're\", 'not', 'procrastinating,', 'you', \"can't\", 'be', 'doing', 'your', 'schoolwork.', 'So', 'I', 'guess', 'you', 'are', 'procrastinating', 'after', 'all.', 'Wait', 'a', 'minute...', ';-)\\n\\nI', 'actually', 'did', 'do', 'my', 'test', 'before', 'the', 'FDA', 'crackdown,', 'but', 'after', 'that', 'the', '23andme', 'web', 'interface', 'changed', 'for', 'me', 'along', 'with', 'all', 'new', 'customers.', 'However,', 'I', 'believe', 'all', 'users', 'can', 'still', '[download', 'their', 'raw', 'data](https://customercare.23andme.com/hc/en-us/articles/212196868-Accessing-and-Downloading-Your-Raw-Data),', 'and', 'then', 'there', 'are', 'is', 'some', 'other', 'software', 'that', 'could', 'be', 'used', 'to', 'analyze', 'that', 'data', '([example](http://medicalfuturist.com/analyse-your-dna-in-your-living-room/)),', 'which', 'maybe', 'skirts', 'the', 'FDA', 'rules', 'by', 'virtue', 'of', 'being', 'more', 'DIY.', \"I've\", 'always', 'thought', 'it', 'would', 'be', 'interesting', 'to', 'explore,', 'but', \"haven't\", 'gotten', 'around', 'to', 'it', 'amongst', 'everything', 'else.\\n\\nThanks', 'for', 'pointing', 'out', 'the', 'genes', 'in', 'the', 'sidebar,', 'somehow', 'I', 'just', 'skimmed', 'the', 'lefthand', 'text', 'and', 'totally', 'missed', 'those', 'despite', 'them', 'being', 'RED!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'schoolwork', 'procrastin', 'anym', 'yo', 'procrastin', 'cant', 'schoolwork', 'guess', 'procrastin', 'wait', 'minut', '\\n\\ni', 'act', 'test', 'fda', 'crackdown', '23andme', 'web', 'interfac', 'chang', 'along', 'new', 'custom', 'howev', 'believ', 'us', 'stil', 'download', 'raw', 'datahttpscustomercare23andmecomhcenusarticles212196868accessinganddownloadingyourrawdata', 'softw', 'could', 'us', 'analys', 'dat', 'examplehttpmedicalfuturistcomanalyseyourdnainyourlivingroom', 'mayb', 'skirt', 'fda', 'rul', 'virtu', 'diy', 'iv', 'alway', 'thought', 'would', 'interest', 'expl', 'hav', 'got', 'around', 'amongst', 'everyth', 'else\\n\\nthanks', 'point', 'gen', 'sideb', 'somehow', 'skim', 'lefthand', 'text', 'tot', 'miss', 'despit', 'red'], ['youre', 'schoolwork', 'procrastinate', 'anymore', 'youre', 'procrastinate', 'cant', 'schoolwork', 'guess', 'procrastinate', 'wait', 'minute', '\\n\\ni', 'actually', 'test', 'fda', 'crackdown', '23andme', 'web', 'interface', 'change', 'along', 'new', 'customers', 'however', 'believe', 'users', 'still', 'download', 'raw', 'datahttpscustomercare23andmecomhcenusarticles212196868accessinganddownloadingyourrawdata', 'software', 'could', 'use', 'analyze', 'data', 'examplehttpmedicalfuturistcomanalyseyourdnainyourlivingroom', 'maybe', 'skirt', 'fda', 'rule', 'virtue', 'diy', 'ive', 'always', 'think', 'would', 'interest', 'explore', 'havent', 'get', 'around', 'amongst', 'everything', 'else\\n\\nthanks', 'point', 'genes', 'sidebar', 'somehow', 'skim', 'lefthand', 'text', 'totally', 'miss', 'despite', 'red'])\n",
      "original document: \n",
      "[\"I'm\", 'surprised', 'they', \"don't\", 'run', 'more', 'with', 'Fitzgerald.', 'Would', 'keep', 'the', 'Defense', 'more', 'honest', 'down', 'to', 'down.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'surpr', 'dont', 'run', 'fitzgerald', 'would', 'keep', 'defens', 'honest'], ['im', 'surprise', 'dont', 'run', 'fitzgerald', 'would', 'keep', 'defense', 'honest'])\n",
      "original document: \n",
      "['Not', 'gonna', 'lie,', 'this', 'is', 'probably', 'the', 'weakest', 'korea', 'has', 'been', 'or', 'is', 'going', 'to', 'be', 'in', 'a', 'while', 'with', 'the', 'implosion', 'of', 'KT;', 'SSG', 'and', 'SKT', 'are', 'experiencing', 'slumps', 'and', 'are', 'wildly', 'inconsistent', 'and', 'have', 'crown', 'and', 'huni', 'as', 'liabilities,', 'as', 'well', 'as', 'the', 'fact', 'that', 'LZ', 'is', 'a', 'very', 'inexperienced', 'roster', 'that', 'may', 'choke', 'under', 'pressure']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gonn', 'lie', 'prob', 'weakest', 'kore', 'going', 'implod', 'kt', 'ssg', 'skt', 'expery', 'slump', 'wild', 'inconsist', 'crown', 'hun', 'liabl', 'wel', 'fact', 'lz', 'inexpery', 'rost', 'may', 'chok', 'press'], ['gonna', 'lie', 'probably', 'weakest', 'korea', 'go', 'implosion', 'kt', 'ssg', 'skt', 'experience', 'slump', 'wildly', 'inconsistent', 'crown', 'huni', 'liabilities', 'well', 'fact', 'lz', 'inexperienced', 'roster', 'may', 'choke', 'pressure'])\n",
      "original document: \n",
      "['All', 'sakki', 'is,', 'is', 'believing', 'your', 'own', 'feints.', 'Believing', 'your', 'own', 'feints', 'makes', 'your', 'body', 'move', 'in', 'such', 'a', 'way', 'that', 'the', 'opponent', 'TRULY', 'believes', 'that', 'a', 'punch', 'not', 'only', 'is', 'about', 'to', 'be', 'thrown,', 'but', 'is', '*actually*', 'thrown,', 'and', 'moves', \"accordingly.\\n\\nIt's\", 'just', 'a', 'higher', 'level', 'version', 'of', 'a', 'deceptive', 'technique.', 'Actual', 'feinting', 'does', 'work', 'like', 'this.', 'The', 'more', 'convincing', 'you', 'can', 'make', 'it,', 'the', 'better,', 'and', 'the', 'lies', 'that', 'are', 'the', 'most', 'convincing', 'are', 'the', 'ones', 'that', 'you', 'can', 'get', 'yourself', 'to', 'believe,', 'even', 'if', 'just', 'for', 'a', 'moment.', 'Long', 'enough', 'for', 'the', 'opponent', 'to', 'react', 'in', 'the', 'wrong', 'way.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sakk', 'believ', 'feint', 'believ', 'feint', 'mak', 'body', 'mov', 'way', 'oppon', 'tru', 'believ', 'punch', 'thrown', 'act', 'thrown', 'mov', 'accordingly\\n\\nits', 'high', 'level', 'vert', 'deceiv', 'techn', 'act', 'feint', 'work', 'lik', 'convint', 'mak', 'bet', 'lie', 'convint', 'on', 'get', 'believ', 'ev', 'mom', 'long', 'enough', 'oppon', 'react', 'wrong', 'way'], ['sakki', 'believe', 'feint', 'believe', 'feint', 'make', 'body', 'move', 'way', 'opponent', 'truly', 'believe', 'punch', 'throw', 'actually', 'throw', 'move', 'accordingly\\n\\nits', 'higher', 'level', 'version', 'deceptive', 'technique', 'actual', 'feint', 'work', 'like', 'convince', 'make', 'better', 'lie', 'convince', 'ones', 'get', 'believe', 'even', 'moment', 'long', 'enough', 'opponent', 'react', 'wrong', 'way'])\n",
      "original document: \n",
      "['how', 'bad', 'is', 'wvu', 'engineering?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bad', 'wvu', 'engin'], ['bad', 'wvu', 'engineer'])\n",
      "original document: \n",
      "['\\nAs', 'a', 'gay', 'man,', \"I've\", 'sat', 'through', '30+', 'years', 'of', 'watching', 'heteros', 'get', 'lovey-dovey', 'in', 'Star', 'Trek,', 'but', 'if', 'you', \"can't\", 'sit', 'through', 'a', 'bit', 'of', 'man-on-man', 'affection,', 'it', 'is', 'totally', 'your', 'prerogative', 'to', 'stop', 'watching', 'the', 'show.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\nas', 'gay', 'man', 'iv', 'sat', 'thirty', 'year', 'watch', 'hetero', 'get', 'loveydovey', 'star', 'trek', 'cant', 'sit', 'bit', 'manonm', 'affect', 'tot', 'prerog', 'stop', 'watch', 'show', '\\n'], ['\\nas', 'gay', 'man', 'ive', 'sit', 'thirty', 'years', 'watch', 'heteros', 'get', 'loveydovey', 'star', 'trek', 'cant', 'sit', 'bite', 'manonman', 'affection', 'totally', 'prerogative', 'stop', 'watch', 'show', '\\n'])\n",
      "original document: \n",
      "['Thinking', 'of', 'getting', 'my', 'wife', 'the', 'Lelo', 'wand', 'but', 'anytime', 'I', 'go', 'in', 'to', 'have', 'a', 'look', 'at', 'one,', 'the', 'display', 'model', 'is', 'always', 'out', 'of', 'damn', 'battery.\\n\\n&amp;nbsp;\\n\\nIs', 'it', 'a', 'rotary', 'vibration', 'or', 'is', 'it', 'magnetic?', 'I', 'want', 'to', 'get', 'her', 'one', 'of', 'the', 'ones', 'that', 'leave', 'that', 'super', 'weird', 'tingle,', 'they', 'are', 'almost', 'unbearable', 'to', 'touch', 'with', 'your', 'hand', 'for', 'any', 'length', 'of', 'time', 'due', 'to', 'the', 'sensitivity', 'of', 'the', 'nerves.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'get', 'wif', 'lelo', 'wand', 'anytim', 'go', 'look', 'on', 'display', 'model', 'alway', 'damn', 'battery\\n\\nampnbsp\\n\\nis', 'rot', 'vibr', 'magnet', 'want', 'get', 'on', 'on', 'leav', 'sup', 'weird', 'tingl', 'almost', 'unbear', 'touch', 'hand', 'leng', 'tim', 'due', 'sensit', 'nerv'], ['think', 'get', 'wife', 'lelo', 'wand', 'anytime', 'go', 'look', 'one', 'display', 'model', 'always', 'damn', 'battery\\n\\nampnbsp\\n\\nis', 'rotary', 'vibration', 'magnetic', 'want', 'get', 'one', 'ones', 'leave', 'super', 'weird', 'tingle', 'almost', 'unbearable', 'touch', 'hand', 'length', 'time', 'due', 'sensitivity', 'nerve'])\n",
      "original document: \n",
      "['As', 'fuck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck'], ['fuck'])\n",
      "original document: \n",
      "['Honestly', 'Im', 'suprised', 'people', 'still', 'think', 'Gamepedia', 'is', 'SOMEHOW', 'relevant.', 'They', 'seem', 'unreasonable', 'every', 'time', 'a', 'new', 'unit', 'gets', 'released.', 'Pure', 'Joke.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'im', 'supr', 'peopl', 'stil', 'think', 'gameped', 'somehow', 'relev', 'seem', 'unreason', 'every', 'tim', 'new', 'unit', 'get', 'releas', 'pur', 'jok'], ['honestly', 'im', 'suprised', 'people', 'still', 'think', 'gamepedia', 'somehow', 'relevant', 'seem', 'unreasonable', 'every', 'time', 'new', 'unit', 'get', 'release', 'pure', 'joke'])\n",
      "original document: \n",
      "['I', 'agree', 'with', 'what', \"you're\", 'saying,', 'but', 'at', 'the', 'same', 'time', 'in', 'this', 'situation', 'the', 'only', 'time', 'oxford', 'commas', 'would', 'ever', 'be', 'truly', 'necessary', 'would', 'be', 'either', 'in', 'a', 'complex', 'sentence', 'or', 'if', 'one', 'is', 'writing', 'to', 'a', 'person', 'who', 'lacks', 'the', 'ability', 'to', 'use', 'context', 'clues.\\n\\nIn', 'your', 'example,', \"we'd\", 'start', 'with', 'the', 'base', 'sentence:\\n\\n\"I', 'had', 'dinner', 'with', 'my', 'uncle', 'and', 'a', 'madman', 'and', 'an', 'accordion', 'player.\"\\n\\nThis', 'obviously', \"isn't\", 'proper', 'grammar,', 'so', \"we'd\", 'replace', 'the', 'first', '\"and\"', 'with', 'a', 'comma', 'since', \"we've\", 'created', 'a', 'list:\\n\\n\"I', 'had', 'dinner', 'with', 'my', 'uncle,', 'a', 'madman', 'and', 'an', 'accordion', 'player.\"\\n\\nIn', 'this', 'case', 'anyone', 'with', 'a', 'grasp', 'on', 'context', 'clues', 'would', 'understand', 'what', 'one', 'was', 'trying', 'to', 'say.', 'If', 'this', 'was', 'being', 'said', 'aloud', 'then', 'it', 'would', 'be', 'the', 'burden', 'of', 'the', 'person', 'pronouncing', 'it', 'to', 'clarify', 'what', 'they', 'were', 'saying.\\n\\nI', 'will', 'submit', 'that', 'there', 'are', 'situations', 'where', 'an', 'oxford', 'comma', 'is', 'necessary,', 'but', 'they', 'are', 'few', 'and', 'far', 'inbetween.', 'Meanwhile', 'our', 'society', 'expounds', 'into', 'our', 'heads', 'at', 'a', 'young', 'age', 'that', 'we', 'must', 'always', 'use', 'the', 'oxford', 'comma', 'whenever', 'we', 'write', 'regardless', 'of', 'the', 'situation.\\n\\nI', 'still', 'use', 'them,', 'but', 'only', 'because', 'of', \"I've\", 'been', 'oppressed', 'by', 'our', 'society', 'which', 'worships', 'them', 'to', 'the', 'point', 'where', 'that', 'oppression', 'is', 'now', 'internalized', 'in', 'all', 'within.', 'Break', 'the', 'cycle,', 'fight', 'the', 'Punctriarchy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'yo', 'say', 'tim', 'situ', 'tim', 'oxford', 'comma', 'would', 'ev', 'tru', 'necess', 'would', 'eith', 'complex', 'sent', 'on', 'writ', 'person', 'lack', 'abl', 'us', 'context', 'clues\\n\\nin', 'exampl', 'wed', 'start', 'bas', 'sentence\\n\\ni', 'din', 'unc', 'madm', 'accord', 'player\\n\\nthis', 'obvy', 'isnt', 'prop', 'gramm', 'wed', 'replac', 'first', 'comm', 'sint', 'wev', 'cre', 'list\\n\\ni', 'din', 'unc', 'madm', 'accord', 'player\\n\\nin', 'cas', 'anyon', 'grasp', 'context', 'clu', 'would', 'understand', 'on', 'try', 'say', 'said', 'aloud', 'would', 'burd', 'person', 'pronount', 'clar', 'saying\\n\\ni', 'submit', 'situ', 'oxford', 'comm', 'necess', 'far', 'inbetween', 'meanwhil', 'socy', 'expound', 'head', 'young', 'ag', 'must', 'alway', 'us', 'oxford', 'comm', 'whenev', 'writ', 'regardless', 'situation\\n\\ni', 'stil', 'us', 'iv', 'oppress', 'socy', 'wor', 'point', 'oppress', 'intern', 'within', 'break', 'cyc', 'fight', 'punctriarchy'], ['agree', 'youre', 'say', 'time', 'situation', 'time', 'oxford', 'commas', 'would', 'ever', 'truly', 'necessary', 'would', 'either', 'complex', 'sentence', 'one', 'write', 'person', 'lack', 'ability', 'use', 'context', 'clues\\n\\nin', 'example', 'wed', 'start', 'base', 'sentence\\n\\ni', 'dinner', 'uncle', 'madman', 'accordion', 'player\\n\\nthis', 'obviously', 'isnt', 'proper', 'grammar', 'wed', 'replace', 'first', 'comma', 'since', 'weve', 'create', 'list\\n\\ni', 'dinner', 'uncle', 'madman', 'accordion', 'player\\n\\nin', 'case', 'anyone', 'grasp', 'context', 'clue', 'would', 'understand', 'one', 'try', 'say', 'say', 'aloud', 'would', 'burden', 'person', 'pronounce', 'clarify', 'saying\\n\\ni', 'submit', 'situations', 'oxford', 'comma', 'necessary', 'far', 'inbetween', 'meanwhile', 'society', 'expound', 'head', 'young', 'age', 'must', 'always', 'use', 'oxford', 'comma', 'whenever', 'write', 'regardless', 'situation\\n\\ni', 'still', 'use', 'ive', 'oppress', 'society', 'worship', 'point', 'oppression', 'internalize', 'within', 'break', 'cycle', 'fight', 'punctriarchy'])\n",
      "original document: \n",
      "['Vocês', 'reclamam', 'de', 'tudo.', 'Esse', 'banner', 'tá', 'mil', 'vezes', 'melhor', 'que', 'aquelas', 'imagens', 'boring', 'de', 'cidades', 'cinzas.', 'Depois', 'que', 'a', 'pessoa', 'entende', 'onde', 'deve', 'clicar,', 'só', 'erra', 'se', 'tiver', 'probleminha', 'de', 'coordenação.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['voc', 'reclamam', 'de', 'tudo', 'ess', 'ban', 'ta', 'mil', 'vez', 'melh', 'que', 'aquela', 'im', 'bor', 'de', 'cidad', 'cinza', 'depo', 'que', 'pesso', 'entend', 'ond', 'dev', 'clic', 'err', 'se', 'tiv', 'probleminh', 'de', 'coordenacao'], ['voces', 'reclamam', 'de', 'tudo', 'esse', 'banner', 'ta', 'mil', 'vezes', 'melhor', 'que', 'aquelas', 'imagens', 'bore', 'de', 'cidades', 'cinzas', 'depois', 'que', 'pessoa', 'entende', 'onde', 'deve', 'clicar', 'erra', 'se', 'tiver', 'probleminha', 'de', 'coordenacao'])\n",
      "original document: \n",
      "['If', 'you', 'went', 'to', 'international', 'school,', \"I'd\", 'imagine', \"it's\", 'probably', 'passably', 'American', 'to', 'any', 'Brit,', 'and', 'to', 'at', 'least', 'a', 'substantial', 'chunk', 'of', 'Americans.', 'If', 'you', 'went', 'somewhere', 'with', 'regional', 'accents', '(Great', 'Lake', 'states', 'or', 'the', 'South)', 'they', 'may', 'notice', 'something', 'off', 'but', 'again', 'this', 'all', 'totally', 'speculation', 'and', 'guesswork.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['went', 'intern', 'school', 'id', 'imagin', 'prob', 'pass', 'am', 'brit', 'least', 'subst', 'chunk', 'am', 'went', 'somewh', 'reg', 'acc', 'gre', 'lak', 'stat', 'sou', 'may', 'not', 'someth', 'tot', 'spec', 'guesswork'], ['go', 'international', 'school', 'id', 'imagine', 'probably', 'passably', 'american', 'brit', 'least', 'substantial', 'chunk', 'americans', 'go', 'somewhere', 'regional', 'accent', 'great', 'lake', 'state', 'south', 'may', 'notice', 'something', 'totally', 'speculation', 'guesswork'])\n",
      "original document: \n",
      "['im', 'cool', 'with', 'that.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'cool'], ['im', 'cool'])\n",
      "original document: \n",
      "['Not', 'far,', 'maybe', 'a', '10', 'minute', 'drive', 'without', 'traffic', '(though', 'given', 'that', \"it's\", 'I4,', 'when', 'does', 'that', 'ever', 'happen?).', 'Like', 'I', 'said,', 'we', 'really', 'have', 'no', 'excuse', 'other', 'than', 'laziness', 'for', 'not', 'going', 'to', 'a', 'Solar', 'Bears', 'game.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['far', 'mayb', 'ten', 'minut', 'driv', 'without', 'traff', 'though', 'giv', 'i4', 'ev', 'hap', 'lik', 'said', 'real', 'excus', 'lazy', 'going', 'sol', 'bear', 'gam'], ['far', 'maybe', 'ten', 'minute', 'drive', 'without', 'traffic', 'though', 'give', 'i4', 'ever', 'happen', 'like', 'say', 'really', 'excuse', 'laziness', 'go', 'solar', 'bear', 'game'])\n",
      "original document: \n",
      "['Those', 'shirts', 'can', 'be', 'used', 'to', 'mop', 'up', \"Maria's\", 'waters.', '\\n\\n\\n\\n\\n\\nMaybe', 'that', 'is', 'their', 'plan', 'with', 'those', 'shirts...', 'hmmmm\\n\\n\\n\\n\\n\\nOh', 'nevermind.', \"It's\", 'definitely', 'Trump', 'being', 'racist\\n\\n\\n\\n\\n\\nProblems', 'would', 'be', 'gone', 'without', 'Trump', 'like', 'Obama', 'handled', 'the', 'floods', 'in', 'Louisiana', 'or', 'Sandy.\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['shirt', 'us', 'mop', 'maria', 'wat', '\\n\\n\\n\\n\\n\\nmaybe', 'plan', 'shirt', 'hmmmm\\n\\n\\n\\n\\n\\noh', 'nevermind', 'definit', 'trump', 'racist\\n\\n\\n\\n\\n\\nproblems', 'would', 'gon', 'without', 'trump', 'lik', 'obam', 'handl', 'flood', 'louisian', 'sandy\\n\\n\\n\\n\\n'], ['shirt', 'use', 'mop', 'marias', 'water', '\\n\\n\\n\\n\\n\\nmaybe', 'plan', 'shirt', 'hmmmm\\n\\n\\n\\n\\n\\noh', 'nevermind', 'definitely', 'trump', 'racist\\n\\n\\n\\n\\n\\nproblems', 'would', 'go', 'without', 'trump', 'like', 'obama', 'handle', 'flood', 'louisiana', 'sandy\\n\\n\\n\\n\\n'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Friend', 'of', \"mine's\", 'family', 'owns', 'a', 'pumpkin', 'patch.', 'They', 'just', 'opened', 'it', 'up', 'this', 'week.', 'Lo', 'was', 'in', 'heaven', 'with', 'all', 'the', '\"balls\"', 'everywhere.', '', '[Definately', 'one', 'for', 'the', 'memory', 'book.](http://imgur.com/a/O2g1I)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['friend', 'min', 'famy', 'own', 'pumpkin', 'patch', 'op', 'week', 'lo', 'heav', 'bal', 'everywh', 'defin', 'on', 'mem', 'bookhttpimgurcomao2g1i'], ['friend', 'mine', 'family', 'own', 'pumpkin', 'patch', 'open', 'week', 'lo', 'heaven', 'ball', 'everywhere', 'definately', 'one', 'memory', 'bookhttpimgurcomao2g1i'])\n",
      "original document: \n",
      "['Steam', 'treated', 'basins.', '', 'Then', 'off', 'to', 'the', 'slaughter.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['steam', 'tre', 'basin', 'slaught'], ['steam', 'treat', 'basins', 'slaughter'])\n",
      "original document: \n",
      "['Not', 'OP,', 'but', 'also', 'a', 'unicorn.', '', 'A', 'friend', 'and', 'coworker', 'was', 'part', 'of', 'a', 'couple', 'but', 'I', 'didn’t', 'know', 'her', 'very', 'well.', '', 'He', 'and', 'I', 'were', 'texting', 'one', 'night', 'and', 'he', 'jokingly', '(or', 'so', 'I', 'thought)', 'asked', 'if', 'I’d', 'join', 'them', 'for', 'a', 'threesome.', '', 'I', 'laughed', 'it', 'off,', 'saying', 'maybe', 'if', 'his', 'girl', 'asked', 'I’d', 'consider', 'it.', '', 'Five', 'seconds', 'later', 'she', 'fb', 'messages', 'me', 'saying', 'he', 'wasn’t', 'joking', 'but', 'we', 'should', 'all', 'get', 'to', 'know', 'each', 'other', 'first.', '', 'We', 'started', 'hanging', 'out', 'a', 'lot', 'and', 'they', 'told', 'me', 'about', 'the', 'lifestyle', 'and', 'I', 'met', 'a', 'lot', 'of', 'their', 'like-minded', 'friends.', '', 'The', 'opportunity', 'to', 'have', 'friendships', 'and', 'safe', 'sexual', 'relationships', 'without', 'the', 'burden', 'of', 'a', 'daily', 'relationship', 'was', 'heaven', 'to', 'me', 'after', 'a', 'divorce.', '', 'Three', 'years', 'later:', '', 'I', 'have', 'lots', 'of', 'fun,', 'lots', 'of', 'great', 'friends,', 'the', 'original', 'couple', 'and', 'I', 'still', 'talk', 'almost', 'daily', 'and', 'hang', 'out', 'in', 'some', 'form', 'every', 'week.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['op', 'also', 'unicorn', 'friend', 'cowork', 'part', 'coupl', 'didnt', 'know', 'wel', 'text', 'on', 'night', 'jok', 'thought', 'ask', 'id', 'join', 'threesome', 'laugh', 'say', 'mayb', 'girl', 'ask', 'id', 'consid', 'fiv', 'second', 'lat', 'fb', 'mess', 'say', 'wasnt', 'jok', 'get', 'know', 'first', 'start', 'hang', 'lot', 'told', 'lifestyl', 'met', 'lot', 'likemind', 'friend', 'opportun', 'friend', 'saf', 'sex', 'rel', 'without', 'burd', 'dai', 'rel', 'heav', 'divorc', 'three', 'year', 'lat', 'lot', 'fun', 'lot', 'gre', 'friend', 'origin', 'coupl', 'stil', 'talk', 'almost', 'dai', 'hang', 'form', 'every', 'week'], ['op', 'also', 'unicorn', 'friend', 'coworker', 'part', 'couple', 'didnt', 'know', 'well', 'texting', 'one', 'night', 'jokingly', 'think', 'ask', 'id', 'join', 'threesome', 'laugh', 'say', 'maybe', 'girl', 'ask', 'id', 'consider', 'five', 'second', 'later', 'fb', 'message', 'say', 'wasnt', 'joke', 'get', 'know', 'first', 'start', 'hang', 'lot', 'tell', 'lifestyle', 'meet', 'lot', 'likeminded', 'friends', 'opportunity', 'friendships', 'safe', 'sexual', 'relationships', 'without', 'burden', 'daily', 'relationship', 'heaven', 'divorce', 'three', 'years', 'later', 'lot', 'fun', 'lot', 'great', 'friends', 'original', 'couple', 'still', 'talk', 'almost', 'daily', 'hang', 'form', 'every', 'week'])\n",
      "original document: \n",
      "['[+NotClever](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnojicv/):\\n\\nI', 'was', 'going', 'to', 'say', 'that', 'the', 'parent', 'was', 'probably', 'talking', 'about', 'the', 'journey', 'to', 'finding', 'a', 'publisher', 'for', 'the', 'book,', 'but', \"it's\", 'not', 'entirely', 'clear', 'that', \"OP's\", 'book', 'is', 'not', 'self', 'published.', 'Clearly', 'there', 'are', 'paper', 'copies,', 'but', 'the', 'website', \"doesn't\", 'say', 'anything', 'about', 'a', 'publisher', 'and', 'a', 'google', 'search', \"doesn't\", 'turn', 'up', 'any', 'results', 'from', 'publisher', 'websites,', 'so', 'maybe', 'OP', 'is', 'just', 'self', 'published?\\n\\nEdit:', 'Further', 'down', 'OP', 'does', 'confirm', 'that', 'this', 'is', 'self', 'published.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['notcleverhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnojicv\\n\\ni', 'going', 'say', 'par', 'prob', 'talk', 'journey', 'find', 'publ', 'book', 'entir', 'clear', 'op', 'book', 'self', 'publ', 'clear', 'pap', 'cop', 'websit', 'doesnt', 'say', 'anyth', 'publ', 'googl', 'search', 'doesnt', 'turn', 'result', 'publ', 'websit', 'mayb', 'op', 'self', 'published\\n\\nedit', 'op', 'confirm', 'self', 'publ'], ['notcleverhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnojicv\\n\\ni', 'go', 'say', 'parent', 'probably', 'talk', 'journey', 'find', 'publisher', 'book', 'entirely', 'clear', 'ops', 'book', 'self', 'publish', 'clearly', 'paper', 'copy', 'website', 'doesnt', 'say', 'anything', 'publisher', 'google', 'search', 'doesnt', 'turn', 'result', 'publisher', 'websites', 'maybe', 'op', 'self', 'published\\n\\nedit', 'op', 'confirm', 'self', 'publish'])\n",
      "original document: \n",
      "['Definitely', '\"Piranha\"', '!!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['definit', 'piranh'], ['definitely', 'piranha'])\n",
      "original document: \n",
      "['Ok,', 'thanks', 'again!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'thank'], ['ok', 'thank'])\n",
      "original document: \n",
      "['Stay', 'strong,', 'my', 'friend.', 'What', 'I', 'found', 'helpful', 'when', 'I', 'start', 'to', 'get', 'urges', 'is', 'to', 'get', 'up,', 'wash', 'my', 'face', 'with', 'cold', 'water,', 'and', 'read', 'a', 'good', 'book.\\n\\nif', 'you', \"don't\", 'have', 'a', 'book', 'to', 'read,', 'My', 'Man', 'Jeeves', 'by', 'Wodestone', 'is', 'a', 'pretty', 'funny', 'book,', 'you', 'can', 'pull', 'it', 'off', 'of', 'gutenberg.org', 'and', 'have', 'a', 'read', '-', \"it'll\", 'certainly', 'take', 'your', 'mind', 'off', 'of', 'porn!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stay', 'strong', 'friend', 'found', 'help', 'start', 'get', 'urg', 'get', 'wash', 'fac', 'cold', 'wat', 'read', 'good', 'book\\n\\nif', 'dont', 'book', 'read', 'man', 'jeev', 'wodeston', 'pretty', 'funny', 'book', 'pul', 'gutenbergorg', 'read', 'itl', 'certain', 'tak', 'mind', 'porn'], ['stay', 'strong', 'friend', 'find', 'helpful', 'start', 'get', 'urge', 'get', 'wash', 'face', 'cold', 'water', 'read', 'good', 'book\\n\\nif', 'dont', 'book', 'read', 'man', 'jeeves', 'wodestone', 'pretty', 'funny', 'book', 'pull', 'gutenbergorg', 'read', 'itll', 'certainly', 'take', 'mind', 'porn'])\n",
      "original document: \n",
      "['Same', 'here', 'tbh,', 'I', 'always', 'make', 'sure', 'the', 'grammar', 'etc.', 'are', 'as', 'perfect', 'as', 'can', 'be.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tbh', 'alway', 'mak', 'sur', 'gramm', 'etc', 'perfect'], ['tbh', 'always', 'make', 'sure', 'grammar', 'etc', 'perfect'])\n",
      "original document: \n",
      "['I', \"don't\", 'know', 'what', \"you're\", 'talking', 'about,', '/u/budude2,', \"it's\", 'right', 'there', 'on', 'the', \"'hot'\", 'page.\\n\\n\\\\&gt;__&gt;']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'know', 'yo', 'talk', 'ubudude2', 'right', 'hot', 'page\\n\\ngt__gt'], ['dont', 'know', 'youre', 'talk', 'ubudude2', 'right', 'hot', 'page\\n\\ngt__gt'])\n",
      "original document: \n",
      "['haha', 'OH', 'that', 'makes', 'sense', ':P\\n\\nYeah', 'idk', 'where', 'another', 'option', 'is\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'oh', 'mak', 'sens', 'p\\n\\nyeah', 'idk', 'anoth', 'opt', 'is\\n'], ['haha', 'oh', 'make', 'sense', 'p\\n\\nyeah', 'idk', 'another', 'option', 'is\\n'])\n",
      "original document: \n",
      "['Nice', 'way', 'to', 'look', 'back', 'for', 'the', 'ball.', '', 'Hope', 'that', 'continues.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nic', 'way', 'look', 'back', 'bal', 'hop', 'continu'], ['nice', 'way', 'look', 'back', 'ball', 'hope', 'continue'])\n",
      "original document: \n",
      "['ALMOST.', 'ALMOST.', 'ALMOST.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['almost', 'almost', 'almost'], ['almost', 'almost', 'almost'])\n",
      "original document: \n",
      "['That', 'requires', 'effort', 'beyond', 'reading', 'a', 'price', 'tag.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['requir', 'effort', 'beyond', 'read', 'pric', 'tag'], ['require', 'effort', 'beyond', 'read', 'price', 'tag'])\n",
      "original document: \n",
      "['Okay,', 'så', 'er', 'der', 'åbenbart', 'en', 'del', 'jurister', 'på', 'samme', 'tid.', 'Godnat', ':o)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'sa', 'er', 'der', 'abenbart', 'en', 'del', 'jur', 'pa', 'sam', 'tid', 'godn'], ['okay', 'sa', 'er', 'der', 'abenbart', 'en', 'del', 'jurister', 'pa', 'samme', 'tid', 'godnat'])\n",
      "original document: \n",
      "['Depends', '-', 'pve', 'content', 'like', 'a', 'fractal', 'for', 'deadeye', 'is', 'a', 'big', 'no.', 'Chronmancer', 'is', 'always', 'needed', 'in', 'pve.', 'I', 'don’t', 'play', 'wvw', 'so', 'I', 'can’t', 'help', 'you', 'there,', 'but', 'I', 'love', 'my', 'deadeye', 'for', 'pvp.', 'For', 'looks,', 'you', 'can', 'always', 'by', 'outfits', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'pve', 'cont', 'lik', 'fract', 'deadey', 'big', 'chronmancer', 'alway', 'nee', 'pve', 'dont', 'play', 'wvw', 'cant', 'help', 'lov', 'deadey', 'pvp', 'look', 'alway', 'outfit'], ['depend', 'pve', 'content', 'like', 'fractal', 'deadeye', 'big', 'chronmancer', 'always', 'need', 'pve', 'dont', 'play', 'wvw', 'cant', 'help', 'love', 'deadeye', 'pvp', 'look', 'always', 'outfit'])\n",
      "original document: \n",
      "['StalkerSOUP', 'is', 'a', 'hardcore', 'overhaul', 'mod,', 'definitely', 'not', 'for', 'beginners.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stalkersoup', 'hardc', 'overha', 'mod', 'definit', 'begin'], ['stalkersoup', 'hardcore', 'overhaul', 'mod', 'definitely', 'beginners'])\n",
      "original document: \n",
      "['Be', 'careful', 'around', 'Termini', 'train', 'station', 'at', 'night.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['car', 'around', 'termin', 'train', 'stat', 'night'], ['careful', 'around', 'termini', 'train', 'station', 'night'])\n",
      "original document: \n",
      "['Thanks', 'man.', 'Good', 'day', 'to', 'you', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'man', 'good', 'day'], ['thank', 'man', 'good', 'day'])\n",
      "original document: \n",
      "['Its', 'not', 'gay', 'if', 'its', 'rape']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gay', 'rap'], ['gay', 'rape'])\n",
      "original document: \n",
      "['Well', 'that', 'was', 'time', 'well', 'spent.', 'How', 'far', 'have', 'you', 'gotten', 'in', 'F/SN?\\n\\n&gt;', 'Actually', 'I', 'underslept', 'so', 'much', 'I', 'am', 'currently', 'sick.\\n\\n[](#k-on-hug)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'tim', 'wel', 'spent', 'far', 'got', 'fsn\\n\\ngt', 'act', 'underslept', 'much', 'cur', 'sick\\n\\nkonhug'], ['well', 'time', 'well', 'spend', 'far', 'get', 'fsn\\n\\ngt', 'actually', 'underslept', 'much', 'currently', 'sick\\n\\nkonhug'])\n",
      "original document: \n",
      "[\"that's\", 'pretty', 'cheap,', 'the', 'whole', 'registration', 'and', 'inspection', 'process', 'cost', 'me', '~$300', '', '', '\\na', 'small', 'price', 'to', 'pay', 'muh', 'rooooooads']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'pretty', 'cheap', 'whol', 'reg', 'inspect', 'process', 'cost', 'three hundred', '\\na', 'smal', 'pric', 'pay', 'muh', 'rooooooad'], ['thats', 'pretty', 'cheap', 'whole', 'registration', 'inspection', 'process', 'cost', 'three hundred', '\\na', 'small', 'price', 'pay', 'muh', 'rooooooads'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['...and?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Go', 'Hokies.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'hoky'], ['go', 'hokies'])\n",
      "original document: \n",
      "['Sounds', 'like', 'an', 'easy', 'way', 'to', 'make', 'money.', '\\n\\nRich', 'guy:', 'are', 'the', 'satellites', 'up?\\n\\nEngineers:', 'uhh.....', 'yeah.', \"They'll\", 'be', 'there', 'in', '40', 'years.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sound', 'lik', 'easy', 'way', 'mak', 'money', '\\n\\nrich', 'guy', 'satellit', 'up\\n\\nengineers', 'uhh', 'yeah', 'theyl', 'forty', 'year'], ['sound', 'like', 'easy', 'way', 'make', 'money', '\\n\\nrich', 'guy', 'satellite', 'up\\n\\nengineers', 'uhh', 'yeah', 'theyll', 'forty', 'years'])\n",
      "original document: \n",
      "[\"I've\", 'seen', 'every', 'episode', 'at', 'least', '15', 'times', 'no', 'joke,', 'some', 'more.\\n\\n\"Stop', 'this', 'maadnesss!', 'LOOOOOK', 'AT', 'YOURSELVES!!!!\"\\n\\nClassic.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'seen', 'every', 'episod', 'least', 'fifteen', 'tim', 'jok', 'more\\n\\nstop', 'maadnesss', 'loooook', 'yourselves\\n\\nclassic'], ['ive', 'see', 'every', 'episode', 'least', 'fifteen', 'time', 'joke', 'more\\n\\nstop', 'maadnesss', 'loooook', 'yourselves\\n\\nclassic'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Yeah,', 'I', \"don't\", 'know', 'everything', 'that', 'went', 'on', 'there.', 'Just', 'that', 'they', 'implemented', 'the', 'policy,', 'most', 'people', 'stopped', 'going', 'there', 'and', 'after', 'a', 'few', 'months', 'they', 'changed', 'it', 'back.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'dont', 'know', 'everyth', 'went', 'impl', 'policy', 'peopl', 'stop', 'going', 'month', 'chang', 'back'], ['yeah', 'dont', 'know', 'everything', 'go', 'implement', 'policy', 'people', 'stop', 'go', 'months', 'change', 'back'])\n",
      "original document: \n",
      "[\"THAT'S\", 'MY', \"DOG'S\", 'NAME!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'dog', 'nam'], ['thats', 'dog', 'name'])\n",
      "original document: \n",
      "['looks', 'very', 'airy', 'for', 'an', 'Indica,', 'whats', 'the', 'density', 'like', 'overall?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'airy', 'indic', 'what', 'dens', 'lik', 'overal'], ['look', 'airy', 'indica', 'whats', 'density', 'like', 'overall'])\n",
      "original document: \n",
      "['This', 'would', 'assume', 'a', 'good', 'corner.', 'A', 'good', 'corner', 'would', 'be', 'closer', 'to', 'to', 'the', 'upper', 'of', 'my', 'price', 'range,', 'a', 'bad', 'corner', 'could', 'possible', 'go', 'below', 'my', 'price', 'range.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'assum', 'good', 'corn', 'good', 'corn', 'would', 'clos', 'up', 'pric', 'rang', 'bad', 'corn', 'could', 'poss', 'go', 'pric', 'rang'], ['would', 'assume', 'good', 'corner', 'good', 'corner', 'would', 'closer', 'upper', 'price', 'range', 'bad', 'corner', 'could', 'possible', 'go', 'price', 'range'])\n",
      "original document: \n",
      "[\"I've\", 'never', 'seen', 'it', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'nev', 'seen'], ['ive', 'never', 'see'])\n",
      "original document: \n",
      "['Alt', 'answer:', 'a', 'Tootsie', 'Roll']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alt', 'answ', 'tootsy', 'rol'], ['alt', 'answer', 'tootsie', 'roll'])\n",
      "original document: \n",
      "['Interesting', 'information.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['interest', 'inform'], ['interest', 'information'])\n",
      "original document: \n",
      "['go', 'to', 'the', 'bar....but', 'during', 'the', 'day!!!', '&gt;:D']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'barbut', 'day', 'gtd'], ['go', 'barbut', 'day', 'gtd'])\n",
      "original document: \n",
      "['Number', 'one.', \"Here's\", 'a', 'good', 'way', 'to', 'look', 'at', 'it:\\n\\nIf', 'you', 'were', 'starting', 'a', 'team', 'and', 'could', 'pick', 'any', 'QB', \"that's\", 'ever', 'played,', 'there', \"aren't\", 'many', 'you', 'could', 'put', 'above', 'him.', \"Elway's\", 'combination', 'of', 'mobility,', 'toughness,', 'a', 'rocket', 'arm', 'and', 'fierce', 'competitiveness', 'make', 'him', 'a', 'compelling', 'choice', 'regardless', 'of', 'who', \"he's\", 'compared', 'to.', \"Let's\", 'give', 'give', 'it', 'a', 'shot:\\n\\n**Brady**:', 'Simply', 'one', 'of', 'the', 'greatest', 'to', 'ever', 'play', 'the', 'game,', 'period.', \"He's\", 'also', 'had', 'the', 'benefit', 'of', 'playing', 'for', 'THE', 'greatest', 'head', 'coach', 'of', 'all', 'time.', 'That', 'has', 'to', 'be', 'factored', 'into', 'this', 'evaluation.', 'Would', 'Brady', 'have', 'been', 'as', 'great', 'under', 'multiple', 'head', 'coaches?', 'Although', \"we'll\", 'never', 'know', 'the', 'answer', 'definitively,', 'history', 'shows', 'that', \"it'd\", 'be', 'very', 'difficult', 'to', 'replicate', \"Brady's\", 'success', 'under', 'different', 'coaches.\\n\\nElway', 'however', 'went', 'to', 'five', 'Super', 'Bowls', 'under', 'two', 'different', 'head', 'coaches.', 'Neither', 'of', 'these', 'coaches', 'would', 'even', 'be', 'in', 'the', 'running', 'of', 'GOAT.', 'Also', 'if', 'you', 'compare', 'the', 'QBs', 'head-to-head,', 'Elway', 'is', 'more', 'athletic', 'and', 'mobile', 'than', 'Brady', 'while', 'being', 'just', 'as', 'competitive', \"(Brady's\", 'best', 'attribute).', 'Advantage', 'Elway\\n\\n**Favre**:', 'I', 'never', 'understood', 'why', 'Favre', 'was', 'treated', 'with', 'such', 'reverence', 'when', 'he', 'played.', 'Whenever', 'I', 'think', 'of', 'Favre,', 'I', 'think', 'of', 'back-breaking', 'picks', 'in', 'the', 'playoffs.', 'That', 'alone', 'disqualifies', 'him', 'from', 'consideration.', '\\n\\n**Peyton', 'Manning**', ':', 'Great', 'QB.', 'Gaudy', 'stats.', 'Four', 'Super', 'Bowl', 'appearances', 'with', 'four', 'different', 'head', 'coaches.', 'I', \"wouldn't\", 'start', 'him', 'over', 'Elway', 'though.', 'Elway', 'is', 'more', 'mobile', 'and', 'tougher,', 'but', 'his', 'biggest', 'advantage', 'is', 'that', 'he', 'played', 'his', 'best', 'ball', 'under', 'pressure.', \"That's\", \"Peyton's\", 'biggest', 'weakness.', \"That's\", 'a', 'big', 'weakness', 'of', 'most', 'QBs.', '\\n\\n**Marino**:', 'Well', 'he', 'was', 'a', 'great', 'QB', 'to', 'be', 'sure.', 'I', 'think', 'his', 'stats', 'were', 'better', 'than', 'Elway', 'overall.', 'But', 'Marino', 'had', 'a', 'downside.', 'I', 'read', 'recently', 'he', 'had', 'less', 'than', '100', 'yards', 'rushing', 'for', 'his', 'ENTIRE', 'career.', 'So', 'even', 'if', 'Marino', 'was', 'a', 'better', 'passer', 'than', 'Elway,', \"I'd\", 'consider', 'his', 'lack', 'of', 'mobility', 'to', 'be', 'a', 'significant', 'disadvantage.', 'Also--and', 'this', 'probably', \"isn't\", 'all', 'his', 'fault', 'necessarily--', 'he', 'played', 'for', 'two', 'Super', 'Bowl', 'winning', 'coaches', 'and', 'only', 'had', 'one', 'SB', 'appearance.', '', 'Elway', 'took', 'three', 'Reeves-coached', 'squads', 'to', 'the', 'Super', 'Bowl', 'that', 'had', 'no', 'business', 'being', 'there.', \"That's\", 'how', 'good', 'he', 'was.', '', 'Quick', 'who', 'was', \"Elway's\", 'top', 'receiver', 'on', 'those', 'teams?', 'Who', 'was', 'his', 'running', 'back?', 'I', \"can't\", 'remember', 'and', 'I', 'actually', 'WATCHED', 'those', 'teams!', '\\n\\n**Montana**', ':', 'Joe', 'Montana', 'was', 'a', 'fucking', 'great', 'QB', 'and', 'arguably', 'is', 'the', 'GOAT.', 'As', 'a', 'fan', 'of', 'both', 'the', 'Broncos', 'and', 'Cowboys', \"(don't\", 'ask),', 'I', 'had', 'the', 'pleasure', 'of', 'having', 'my', 'heart', 'ripped', 'out', 'by', 'this', 'muthafucker', 'for', 'over', 'a', 'decade.', 'Accurate,', 'cool', 'under', 'fire', 'with', 'the', 'heart', 'of', 'a', 'champion.', '', 'But', \"here's\", 'the', 'thing', 'about', 'Montana...dude', 'was', 'fragile', 'by', 'NFL', 'standards.', 'If', 'you', 'put', 'a', 'solid', 'lick', 'on', 'him,', 'he', \"wouldn't\", 'get', 'up.', \"That's\", 'why', 'he', \"didn't\", 'finish', 'his', 'career', 'as', 'a', '49er.', 'Toughness', 'is', 'important', 'in', 'this', 'position.', 'Elway', 'was', 'a', 'tough', 'sumamabitch.', '\\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['numb', 'on', 'her', 'good', 'way', 'look', 'it\\n\\nif', 'start', 'team', 'could', 'pick', 'qb', 'that', 'ev', 'play', 'ar', 'many', 'could', 'put', 'elway', 'combin', 'mobl', 'tough', 'rocket', 'arm', 'fierc', 'competit', 'mak', 'compel', 'cho', 'regardless', 'hes', 'comp', 'let', 'giv', 'giv', 'shot\\n\\nbrady', 'simply', 'on', 'greatest', 'ev', 'play', 'gam', 'period', 'hes', 'also', 'benefit', 'play', 'greatest', 'head', 'coach', 'tim', 'fact', 'evalu', 'would', 'brady', 'gre', 'multipl', 'head', 'coach', 'although', 'wel', 'nev', 'know', 'answ', 'definit', 'hist', 'show', 'itd', 'difficult', 'reply', 'brady', 'success', 'diff', 'coaches\\n\\nelway', 'howev', 'went', 'fiv', 'sup', 'bowl', 'two', 'diff', 'head', 'coach', 'neith', 'coach', 'would', 'ev', 'run', 'goat', 'also', 'comp', 'qbs', 'headtohead', 'elway', 'athlet', 'mobl', 'brady', 'competit', 'brady', 'best', 'attribut', 'adv', 'elway\\n\\nfavre', 'nev', 'understood', 'favr', 'tre', 'rev', 'play', 'whenev', 'think', 'favr', 'think', 'backbreak', 'pick', 'playoff', 'alon', 'disqual', 'consid', '\\n\\npeyton', 'man', 'gre', 'qb', 'gaudy', 'stat', 'four', 'sup', 'bowl', 'appear', 'four', 'diff', 'head', 'coach', 'wouldnt', 'start', 'elway', 'though', 'elway', 'mobl', 'tough', 'biggest', 'adv', 'play', 'best', 'bal', 'press', 'that', 'peyton', 'biggest', 'weak', 'that', 'big', 'weak', 'qbs', '\\n\\nmarino', 'wel', 'gre', 'qb', 'sur', 'think', 'stat', 'bet', 'elway', 'overal', 'marino', 'downsid', 'read', 'rec', 'less', 'one hundred', 'yard', 'rush', 'entir', 'car', 'ev', 'marino', 'bet', 'pass', 'elway', 'id', 'consid', 'lack', 'mobl', 'sign', 'disadv', 'alsoand', 'prob', 'isnt', 'fault', 'necess', 'play', 'two', 'sup', 'bowl', 'win', 'coach', 'on', 'sb', 'appear', 'elway', 'took', 'three', 'reevescoach', 'squad', 'sup', 'bowl', 'busy', 'that', 'good', 'quick', 'elway', 'top', 'receiv', 'team', 'run', 'back', 'cant', 'rememb', 'act', 'watch', 'team', '\\n\\nmontana', 'joe', 'montan', 'fuck', 'gre', 'qb', 'argu', 'goat', 'fan', 'bronco', 'cowboy', 'dont', 'ask', 'pleas', 'heart', 'rip', 'muthafuck', 'decad', 'acc', 'cool', 'fir', 'heart', 'champ', 'her', 'thing', 'montanadud', 'fragil', 'nfl', 'standard', 'put', 'solid', 'lick', 'wouldnt', 'get', 'that', 'didnt', 'fin', 'car', '49er', 'tough', 'import', 'posit', 'elway', 'tough', 'sumamabitch', '\\n'], ['number', 'one', 'heres', 'good', 'way', 'look', 'it\\n\\nif', 'start', 'team', 'could', 'pick', 'qb', 'thats', 'ever', 'play', 'arent', 'many', 'could', 'put', 'elways', 'combination', 'mobility', 'toughness', 'rocket', 'arm', 'fierce', 'competitiveness', 'make', 'compel', 'choice', 'regardless', 'hes', 'compare', 'let', 'give', 'give', 'shot\\n\\nbrady', 'simply', 'one', 'greatest', 'ever', 'play', 'game', 'period', 'hes', 'also', 'benefit', 'play', 'greatest', 'head', 'coach', 'time', 'factor', 'evaluation', 'would', 'brady', 'great', 'multiple', 'head', 'coach', 'although', 'well', 'never', 'know', 'answer', 'definitively', 'history', 'show', 'itd', 'difficult', 'replicate', 'bradys', 'success', 'different', 'coaches\\n\\nelway', 'however', 'go', 'five', 'super', 'bowl', 'two', 'different', 'head', 'coach', 'neither', 'coach', 'would', 'even', 'run', 'goat', 'also', 'compare', 'qbs', 'headtohead', 'elway', 'athletic', 'mobile', 'brady', 'competitive', 'bradys', 'best', 'attribute', 'advantage', 'elway\\n\\nfavre', 'never', 'understand', 'favre', 'treat', 'reverence', 'play', 'whenever', 'think', 'favre', 'think', 'backbreaking', 'pick', 'playoffs', 'alone', 'disqualify', 'consideration', '\\n\\npeyton', 'man', 'great', 'qb', 'gaudy', 'stats', 'four', 'super', 'bowl', 'appearances', 'four', 'different', 'head', 'coach', 'wouldnt', 'start', 'elway', 'though', 'elway', 'mobile', 'tougher', 'biggest', 'advantage', 'play', 'best', 'ball', 'pressure', 'thats', 'peytons', 'biggest', 'weakness', 'thats', 'big', 'weakness', 'qbs', '\\n\\nmarino', 'well', 'great', 'qb', 'sure', 'think', 'stats', 'better', 'elway', 'overall', 'marino', 'downside', 'read', 'recently', 'less', 'one hundred', 'yards', 'rush', 'entire', 'career', 'even', 'marino', 'better', 'passer', 'elway', 'id', 'consider', 'lack', 'mobility', 'significant', 'disadvantage', 'alsoand', 'probably', 'isnt', 'fault', 'necessarily', 'play', 'two', 'super', 'bowl', 'win', 'coach', 'one', 'sb', 'appearance', 'elway', 'take', 'three', 'reevescoached', 'squads', 'super', 'bowl', 'business', 'thats', 'good', 'quick', 'elways', 'top', 'receiver', 'team', 'run', 'back', 'cant', 'remember', 'actually', 'watch', 'team', '\\n\\nmontana', 'joe', 'montana', 'fuck', 'great', 'qb', 'arguably', 'goat', 'fan', 'broncos', 'cowboys', 'dont', 'ask', 'pleasure', 'heart', 'rip', 'muthafucker', 'decade', 'accurate', 'cool', 'fire', 'heart', 'champion', 'heres', 'thing', 'montanadude', 'fragile', 'nfl', 'standards', 'put', 'solid', 'lick', 'wouldnt', 'get', 'thats', 'didnt', 'finish', 'career', '49er', 'toughness', 'important', 'position', 'elway', 'tough', 'sumamabitch', '\\n'])\n",
      "original document: \n",
      "['143414170|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'qrXQpZdq)\\n\\n&gt;&gt;143413934\\nI', 'live', 'in', 'california', 'you', 'sensitive', 'faggot', 'clinton', 'was', 'winning', 'anyway\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and seventy', 'gt', 'unit', 'stat', 'anonym', 'id', 'qrxqpzdq\\n\\ngtgt143413934\\ni', 'liv', 'californ', 'sensit', 'faggot', 'clinton', 'win', 'anyway\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and seventy', 'gt', 'unite', 'state', 'anonymous', 'id', 'qrxqpzdq\\n\\ngtgt143413934\\ni', 'live', 'california', 'sensitive', 'faggot', 'clinton', 'win', 'anyway\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['God', 'dammit', 'our', 'O', 'Line', 'fucking', 'sucks.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'dammit', 'lin', 'fuck', 'suck'], ['god', 'dammit', 'line', 'fuck', 'suck'])\n",
      "original document: \n",
      "['/r/legaladvise', 'may', 'be', 'interested', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rlegaladv', 'may', 'interest'], ['rlegaladvise', 'may', 'interest'])\n",
      "original document: \n",
      "['Yup,', 'even', 'extremely', 'immigration', 'strict', 'Australia.\\n\\nI', 'heard', 'once', 'that', 'once', \"you're\", 'rich', 'enough,', 'immigration', 'is', 'not', 'really', 'something', 'you', 'have', 'to', 'worry', 'about,', \"it's\", 'taxes.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup', 'ev', 'extrem', 'immigr', 'strict', 'australia\\n\\ni', 'heard', 'yo', 'rich', 'enough', 'immigr', 'real', 'someth', 'worry', 'tax'], ['yup', 'even', 'extremely', 'immigration', 'strict', 'australia\\n\\ni', 'hear', 'youre', 'rich', 'enough', 'immigration', 'really', 'something', 'worry', 'tax'])\n",
      "original document: \n",
      "['jacob', 'couldnt', 'test', 'the', 'teradick', 'while', 'ice', 'wasnt', 'in', 'na?', 'jacob', 'couldnt', 'say', 'bring', 's8', 'backup', 'setup', 'since', 'ur', 'going', 'out', 'with', 'a', 'car?', 'what', 'did', 'jacob', 'plan', 'for', 'today?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jacob', 'couldnt', 'test', 'teradick', 'ic', 'wasnt', 'na', 'jacob', 'couldnt', 'say', 'bring', 's8', 'backup', 'setup', 'sint', 'ur', 'going', 'car', 'jacob', 'plan', 'today'], ['jacob', 'couldnt', 'test', 'teradick', 'ice', 'wasnt', 'na', 'jacob', 'couldnt', 'say', 'bring', 's8', 'backup', 'setup', 'since', 'ur', 'go', 'car', 'jacob', 'plan', 'today'])\n",
      "original document: \n",
      "['Fool', 'me', 'once,', 'fool', 'me', 'twice,', 'fool', 'me', 'chicken', 'soup', 'with', 'rice.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fool', 'fool', 'twic', 'fool', 'chick', 'soup', 'ric'], ['fool', 'fool', 'twice', 'fool', 'chicken', 'soup', 'rice'])\n",
      "original document: \n",
      "['Exactly', 'how', 'I', 'feel,', 'it', 'wasnt', 'his', 'strength', 'to', 'begin', 'with.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['exact', 'feel', 'wasnt', 'strength', 'begin'], ['exactly', 'feel', 'wasnt', 'strength', 'begin'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnok19y/):\\n\\nI', 'am,', 'in', 'fact,', 'self', 'published', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnok19y\\n\\ni', 'fact', 'self', 'publ'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnok19y\\n\\ni', 'fact', 'self', 'publish'])\n",
      "original document: \n",
      "['Hi', 'there!', 'Your', 'post', 'was', 'removed', 'because', 'it', 'uses', 'the', 'text', 'box.', 'Per', '[rule', '1](/r/AskReddit/wiki/index#wiki_-rule_1-),', 'use', 'of', 'the', 'text', 'box', 'is', 'prohibited.', 'You', 'can', 'resubmit', 'your', 'post', '[here](/r/askreddit/submit?selftext=true&amp;title=5', 'most', 'successful/least', 'successful', 'presidents', 'in', 'your', 'opinion)', 'without', 'the', 'textbox.\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/AskReddit)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hi', 'post', 'remov', 'us', 'text', 'box', 'per', 'rul', '1raskredditwikiindexwiki_rule_1', 'us', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitle5', 'successfulleast', 'success', 'presid', 'opin', 'without', 'textbox\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetoraskreddit', 'quest', 'concern'], ['hi', 'post', 'remove', 'use', 'text', 'box', 'per', 'rule', '1raskredditwikiindexwiki_rule_1', 'use', 'text', 'box', 'prohibit', 'resubmit', 'post', 'hereraskredditsubmitselftexttrueamptitle5', 'successfulleast', 'successful', 'presidents', 'opinion', 'without', 'textbox\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetoraskreddit', 'question', 'concern'])\n",
      "original document: \n",
      "['or', 'is', 'it', ';p']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['p'], ['p'])\n",
      "original document: \n",
      "['Give', 'Emmure', 'a', 'listen,', 'one', 'of', 'their', 'newer', 'songs', 'Flag', 'of', 'the', 'Beast', 'is', 'pretty', 'good.', 'Another', 'band', 'is', 'King', '810,', 'but', 'quite', 'a', 'few', 'people', 'here', 'do', 'not', 'like', 'them.', 'Try', 'their', 'song', 'Trauma', 'Model', 'or', 'Alpha', '&amp;', 'Omega', 'from', 'their', 'most', 'recent', 'album.', 'They', 'also', 'have', 'some', 'songs', 'similar', 'to', 'rapcore.', 'Deez', 'Nuts', 'is', 'a', 'good', 'rapcore', 'band.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['giv', 'em', 'list', 'on', 'new', 'song', 'flag', 'beast', 'pretty', 'good', 'anoth', 'band', 'king', 'eight hundred and ten', 'quit', 'peopl', 'lik', 'try', 'song', 'traum', 'model', 'alph', 'amp', 'omeg', 'rec', 'alb', 'also', 'song', 'simil', 'rapc', 'deez', 'nut', 'good', 'rapc', 'band'], ['give', 'emmure', 'listen', 'one', 'newer', 'songs', 'flag', 'beast', 'pretty', 'good', 'another', 'band', 'king', 'eight hundred and ten', 'quite', 'people', 'like', 'try', 'song', 'trauma', 'model', 'alpha', 'amp', 'omega', 'recent', 'album', 'also', 'songs', 'similar', 'rapcore', 'deez', 'nut', 'good', 'rapcore', 'band'])\n",
      "original document: \n",
      "['Yeah,', 'I', 'personally', 'support', 'it,', 'the', 'war', 'on', 'drugs', 'was', 'lost', 'long', 'ago.', 'I', 'say', 'tax', 'it', 'and', 'use', 'it', 'for', 'a', 'healthcare', 'system', 'where', 'I', \"don't\", 'have', 'a', '$6000', 'deductible.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'person', 'support', 'war', 'drug', 'lost', 'long', 'ago', 'say', 'tax', 'us', 'healthc', 'system', 'dont', 'six thousand', 'deduct'], ['yeah', 'personally', 'support', 'war', 'drug', 'lose', 'long', 'ago', 'say', 'tax', 'use', 'healthcare', 'system', 'dont', 'six thousand', 'deductible'])\n",
      "original document: \n",
      "[\"That's\", 'what', 'she', 'said.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'said'], ['thats', 'say'])\n",
      "original document: \n",
      "['&gt;', 'there', 'are', 'a', 'number', 'of', 'scientific', 'flaws', 'associated', 'with', 'that', 'method', 'that', 'I', \"won't\", 'go', 'into', 'right', 'now\\n\\nCould', 'you', 'though?', '', 'this', 'is', 'news', 'to', 'me.', '', 'I', 'was', 'planning', 'on', 'weighing', 'out', 'my', 'oil', 'or', 'measuring', 'with', 'a', 'syringe', 'as', \"I'm\", 'only', 'making', 'about', 'six', 'vials', 'the', 'first', 'time.', '', 'Could', 'you', 'just', 'measure', 'your', 'oil', 'in', 'a', 'volumetric', 'flask', 'and', 'then', 'pour', 'it', 'into', 'a', 'beaker?', '', 'I', 'feel', 'like', \"you're\", 'overthinking', 'this', 'somewhat', 'but', 'I', 'might', 'just', 'not', 'be', 'understanding', 'the', 'question.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'numb', 'sci', 'flaw', 'assocy', 'method', 'wont', 'go', 'right', 'now\\n\\ncould', 'though', 'new', 'plan', 'weigh', 'oil', 'meas', 'syr', 'im', 'mak', 'six', 'vial', 'first', 'tim', 'could', 'meas', 'oil', 'volumet', 'flask', 'pour', 'beak', 'feel', 'lik', 'yo', 'overthink', 'somewh', 'might', 'understand', 'quest'], ['gt', 'number', 'scientific', 'flaw', 'associate', 'method', 'wont', 'go', 'right', 'now\\n\\ncould', 'though', 'news', 'plan', 'weigh', 'oil', 'measure', 'syringe', 'im', 'make', 'six', 'vials', 'first', 'time', 'could', 'measure', 'oil', 'volumetric', 'flask', 'pour', 'beaker', 'feel', 'like', 'youre', 'overthinking', 'somewhat', 'might', 'understand', 'question'])\n",
      "original document: \n",
      "['2,581']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['two thousand, five hundred and eighty-one'], ['two thousand, five hundred and eighty-one'])\n",
      "original document: \n",
      "['Why', 'not', 'cut', 'out', 'the', 'middleman', 'and', 'just', 'get', 'it', 'on', 'with', 'the', 'ambulance?', '\\n\\nLike', \"I'm\", 'talking', 'hard,', 'rough,', 'sirens', 'on', 'full', 'blast', \"it's\", 'go', 'time', 'fucking.\\n', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cut', 'middlem', 'get', 'amb', '\\n\\nlike', 'im', 'talk', 'hard', 'rough', 'sir', 'ful', 'blast', 'go', 'tim', 'fucking\\n'], ['cut', 'middleman', 'get', 'ambulance', '\\n\\nlike', 'im', 'talk', 'hard', 'rough', 'sirens', 'full', 'blast', 'go', 'time', 'fucking\\n'])\n",
      "original document: \n",
      "['Why', 'is', 'it', 'that', 'some', 'of', 'us', 'become', 'suicidal?', 'Brains', 'are', 'weird', 'dude...\\n\\n\\n^^^^Okay', '^^^^not', '^^^^just', '^^^^some', '^^^^of', '^^^^us']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'becom', 'suicid', 'brain', 'weird', 'dude\\n\\n\\nokay', 'us'], ['us', 'become', 'suicidal', 'brain', 'weird', 'dude\\n\\n\\nokay', 'us'])\n",
      "original document: \n",
      "['You', \"can't\", 'suggest', 'democrats', \"aren't\", 'good', 'on', 'reddit.', 'The', 'circle', 'jerk', 'on', 'here', 'is', 'ridiculous.', 'No', 'one', 'wants', 'to', 'admit', 'faults', 'in', 'their', 'own', 'party', 'and', 'always', 'wind', 'up', 'pointing', 'fingers', 'when', 'people', 'say', 'bad', 'things', 'like', 'the', 'fact', 'the', 'other', 'party', 'does', 'it', 'excuses', 'them.', 'Both', 'parties', 'disgust', 'me.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'suggest', 'democr', 'ar', 'good', 'reddit', 'circ', 'jerk', 'ridic', 'on', 'want', 'admit', 'fault', 'party', 'alway', 'wind', 'point', 'fing', 'peopl', 'say', 'bad', 'thing', 'lik', 'fact', 'party', 'excus', 'party', 'disgust'], ['cant', 'suggest', 'democrats', 'arent', 'good', 'reddit', 'circle', 'jerk', 'ridiculous', 'one', 'want', 'admit', 'fault', 'party', 'always', 'wind', 'point', 'finger', 'people', 'say', 'bad', 'things', 'like', 'fact', 'party', 'excuse', 'party', 'disgust'])\n",
      "original document: \n",
      "['When', 'I', 'logged', 'onto', 'Reddit', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['log', 'onto', 'reddit'], ['log', 'onto', 'reddit'])\n",
      "original document: \n",
      "['After', 'you', 'get', 'unbanned', 'if', 'you', 'want', 'to', 'troll', 'to', 'spite', 'Riots', 'shit', 'system', 'hmu']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get', 'unban', 'want', 'trol', 'spit', 'riot', 'shit', 'system', 'hmu'], ['get', 'unbanned', 'want', 'troll', 'spite', 'riot', 'shit', 'system', 'hmu'])\n",
      "original document: \n",
      "[\"I'm\", 'so', 'mad', 'at', 'how', 'the', 'market', \"doesn't\", 'appreciate', 'Al', 'Ewing.\\n\\nAnd', 'how', 'Marvel', \"doesn't\", 'appreciate', 'Laura', 'Kinney', 'as', 'much', 'as', 'the', 'character', 'deserves.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'mad', 'market', 'doesnt', 'apprecy', 'al', 'ewing\\n\\nand', 'marvel', 'doesnt', 'apprecy', 'laur', 'kinney', 'much', 'charact', 'deserv'], ['im', 'mad', 'market', 'doesnt', 'appreciate', 'al', 'ewing\\n\\nand', 'marvel', 'doesnt', 'appreciate', 'laura', 'kinney', 'much', 'character', 'deserve'])\n",
      "original document: \n",
      "['So', 'if', 'you', 'could', 'force', 'a', 'quantum', 'state', 'without', 'breaking', 'the', 'entanglement', 'instead', 'of', 'just', 'reading', \"it's\", 'state,', 'then', \"you'd\", 'be', 'sending', 'information....', 'but', 'that', 'currently', 'is', 'not', 'possible?\\n\\nSo', 'instead', 'of', 'sending', 'information', 'as', 'a', 'transmission,', \"it's\", 'just', 'measuring', 'the', 'state', 'of', 'a', 'particle,', 'which', 'would', 'measure', 'the', 'same', 'for', 'anywhere', 'it', 'exist', 'entangled?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'forc', 'quant', 'stat', 'without', 'break', 'entangl', 'instead', 'read', 'stat', 'youd', 'send', 'inform', 'cur', 'possible\\n\\nso', 'instead', 'send', 'inform', 'transmit', 'meas', 'stat', 'partic', 'would', 'meas', 'anywh', 'ex', 'entangl'], ['could', 'force', 'quantum', 'state', 'without', 'break', 'entanglement', 'instead', 'read', 'state', 'youd', 'send', 'information', 'currently', 'possible\\n\\nso', 'instead', 'send', 'information', 'transmission', 'measure', 'state', 'particle', 'would', 'measure', 'anywhere', 'exist', 'entangle'])\n",
      "original document: \n",
      "['Most', 'any', 'site', 'that', 'sells', 'shoes', 'has', 'a', 'quick', 'and', 'easy', 'print-label', 'return', 'exchange', 'policy,', 'usually', '14', 'to', '30', 'days.', '', 'Check', 'the', \"site's\", 'return/exchange', 'policy.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sit', 'sel', 'sho', 'quick', 'easy', 'printlabel', 'return', 'exchang', 'policy', 'us', 'fourteen', 'thirty', 'day', 'check', 'sit', 'returnexchang', 'policy'], ['site', 'sell', 'shoe', 'quick', 'easy', 'printlabel', 'return', 'exchange', 'policy', 'usually', 'fourteen', 'thirty', 'days', 'check', 'sit', 'returnexchange', 'policy'])\n",
      "original document: \n",
      "['Ya', \"they're\", 'always', 'in', 'stock', 'at', 'my', 'local', 'target,', 'Walgreens', 'and', 'Walmart.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya', 'theyr', 'alway', 'stock', 'loc', 'target', 'walgreen', 'walmart'], ['ya', 'theyre', 'always', 'stock', 'local', 'target', 'walgreens', 'walmart'])\n",
      "original document: \n",
      "['Khuda', 'Qasam,', 'Awesome!!!', 'Where', 'was', 'this', 'diamond', 'content', 'hidden?', 'Great', 'job', 'OP.', 'Gave', 'me', 'a', 'good', 'amount', 'of', 'laugh.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['khud', 'qasam', 'awesom', 'diamond', 'cont', 'hid', 'gre', 'job', 'op', 'gav', 'good', 'amount', 'laugh'], ['khuda', 'qasam', 'awesome', 'diamond', 'content', 'hide', 'great', 'job', 'op', 'give', 'good', 'amount', 'laugh'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Colorgaurd', 'bear...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['colorgaurd', 'bear'], ['colorgaurd', 'bear'])\n",
      "original document: \n",
      "['Think', \"he's\", 'reading', 'into', \"Moreno's\", 'comments', 'a', 'bit', 'too', 'much.', 'Anyone', 'who', 'watches', 'us', 'closely', 'will', 'have', 'noticed', 'that', 'we', \"don't\", 'press', 'anywhere', 'near', 'as', 'much', 'anymore,', 'and', \"it's\", 'a', 'conscious', 'change.\\n\\nCould', 'be', 'many', 'reasons', 'for', 'it.', 'One', 'is', 'that', 'we', 'have', 'the', 'CL', 'this', 'season', 'so', 'we', \"can't\", 'afford', 'to', 'play', 'such', 'a', 'high', 'energy', 'game', 'every', 'week.', 'We', \"don't\", 'have', 'that', 'deep', 'of', 'a', 'squad,', 'and', 'Klopp', 'thus', 'far', 'has', 'only', 'really', 'rotated', 'in', 'the', 'full', 'back', 'areas.', 'City/Spurs', 'have', 'rotated', 'far', 'more', 'effectively', 'than', 'us.', 'Also', 'worth', 'remembering', 'we', 'have', 'the', 'CL', 'playoff', 'round', 'which', 'added', 'games', 'for', 'us,', 'while', 'having', 'to', 'deal', 'with', 'the', 'absence', 'of', 'Clyne,', 'Lallana,', 'Coutinho', 'and', 'dealing', 'with', 'bringing', 'back', 'players', 'from', 'injury', '(Hendo/Mane/Sturridge).\\n\\nAnother', 'is', 'that', 'pressing,', 'while', 'effective,', 'when', 'it', \"isn't\", 'working,', 'leaves', 'your', 'midfield', 'and', 'defence', 'exposed.', 'Notably', 'when', 'Lallana', 'is', 'missing,', 'we', \"don't\", 'have', 'a', 'natural', 'doggish', 'presser', 'aside', 'from', 'Firmino,', 'so', 'Klopp', 'is', 'being', 'pragmatic', 'in', 'the', 'absence', 'of', 'Lallana', 'in', 'order', 'to', 'protect', 'our', 'already', 'vulnerable', 'defence.', 'I', 'can', 'imagine', 'Klopp', 'remembers', 'playing', 'a', 'high', 'pressing', 'game', 'away', 'at', 'Bournemouth', 'or', 'Leicester', 'and', 'then', 'getting', 'caught', 'by', 'simple', 'long', 'balls', 'into', 'the', 'channels.\\n\\nI', 'remember', 'when', 'Klopp', 'first', 'arrived,', 'the', 'number', 'of', 'hamstring/calf', 'injuries', 'was', 'ridiculous.', 'These', 'injuries', 'have', 'slowed', 'down', 'somewhat,', 'but', 'a', 'less', 'intensive', 'pressing', 'game', 'will', 'have', 'helped.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'hes', 'read', 'moreno', 'com', 'bit', 'much', 'anyon', 'watch', 'us', 'clos', 'not', 'dont', 'press', 'anywh', 'near', 'much', 'anym', 'conscy', 'change\\n\\ncould', 'many', 'reason', 'on', 'cl', 'season', 'cant', 'afford', 'play', 'high', 'energy', 'gam', 'every', 'week', 'dont', 'deep', 'squad', 'klop', 'thu', 'far', 'real', 'rot', 'ful', 'back', 'area', 'citysp', 'rot', 'far', 'effect', 'us', 'also', 'wor', 'rememb', 'cl', 'playoff', 'round', 'ad', 'gam', 'us', 'deal', 'abs', 'clyn', 'lallan', 'coutinho', 'deal', 'bring', 'back', 'play', 'injury', 'hendomanesturridge\\n\\nanother', 'press', 'effect', 'isnt', 'work', 'leav', 'midfield', 'def', 'expos', 'not', 'lallan', 'miss', 'dont', 'nat', 'dog', 'press', 'asid', 'firmino', 'klop', 'pragm', 'abs', 'lallan', 'ord', 'protect', 'already', 'vuln', 'def', 'imagin', 'klop', 'rememb', 'play', 'high', 'press', 'gam', 'away', 'bournemou', 'leicest', 'get', 'caught', 'simpl', 'long', 'bal', 'channels\\n\\ni', 'rememb', 'klop', 'first', 'ar', 'numb', 'hamstringcalf', 'injury', 'ridic', 'injury', 'slow', 'somewh', 'less', 'intend', 'press', 'gam', 'help'], ['think', 'hes', 'read', 'morenos', 'comment', 'bite', 'much', 'anyone', 'watch', 'us', 'closely', 'notice', 'dont', 'press', 'anywhere', 'near', 'much', 'anymore', 'conscious', 'change\\n\\ncould', 'many', 'reason', 'one', 'cl', 'season', 'cant', 'afford', 'play', 'high', 'energy', 'game', 'every', 'week', 'dont', 'deep', 'squad', 'klopp', 'thus', 'far', 'really', 'rotate', 'full', 'back', 'areas', 'cityspurs', 'rotate', 'far', 'effectively', 'us', 'also', 'worth', 'remember', 'cl', 'playoff', 'round', 'add', 'game', 'us', 'deal', 'absence', 'clyne', 'lallana', 'coutinho', 'deal', 'bring', 'back', 'players', 'injury', 'hendomanesturridge\\n\\nanother', 'press', 'effective', 'isnt', 'work', 'leave', 'midfield', 'defence', 'expose', 'notably', 'lallana', 'miss', 'dont', 'natural', 'doggish', 'presser', 'aside', 'firmino', 'klopp', 'pragmatic', 'absence', 'lallana', 'order', 'protect', 'already', 'vulnerable', 'defence', 'imagine', 'klopp', 'remember', 'play', 'high', 'press', 'game', 'away', 'bournemouth', 'leicester', 'get', 'catch', 'simple', 'long', 'ball', 'channels\\n\\ni', 'remember', 'klopp', 'first', 'arrive', 'number', 'hamstringcalf', 'injuries', 'ridiculous', 'injuries', 'slow', 'somewhat', 'less', 'intensive', 'press', 'game', 'help'])\n",
      "original document: \n",
      "['The', 'Left', 'has', 'leaned', 'right', 'over', 'the', 'years.', 'Both', 'parties', 'have.', 'Obamacare', 'originated', 'with', 'the', 'Heritage', 'Foundation', 'in', 'the', \"80's.\", 'The', 'Left', 'absolutely', 'refused', 'to', 'pass', 'it', 'when', 'Gingrich', 'was', 'pushing', 'it', 'in', 'the', \"'90's.\", 'Once', 'Obama', 'was', 'president,', 'the', 'Left', 'loved', 'it', 'and', 'the', 'Right', 'hated', 'it.', '\\n\\nThe', \"NSA's\", 'domestic', 'spying', 'is', 'literally', 'fascist.', 'Not', 'the', \"Left's\", 'new', 'definition', 'which', 'is', '\"Trump', 'supports', 'it\",', 'but', 'actually', 'fascist.', 'Both', 'the', 'Right', 'and', 'the', 'Left', 'largely', 'support', 'it', 'out', 'of', 'fear,', 'but', \"it's\", 'a', 'very', 'far', 'right', 'program.', 'It', \"wouldn't\", 'have', 'been', 'allowed', 'before', '9/11.', '\\n\\nTrump', 'has', 'changed', 'his', 'position', 'on', 'things', 'over', 'the', 'years.', 'Most', 'people', 'do.', \"There's\", 'no', 'shame', 'in', 'that,', 'as', 'long', 'as', 'the', 'changes', \"don't\", 'happen', 'from', 'interview', 'to', 'interview.', 'Trump', 'has', 'been', 'consistently', 'leaning', 'farther', 'and', 'farther', 'to', 'the', 'right', 'over', 'the', 'last', 'several', 'decades.', '\\n\\nI', 'have', 'no', 'idea', 'why', 'you', 'think', 'the', 'country', 'has', 'been', 'moving', 'Left.', \"I've\", 'been', 'watching', 'politics', 'since', 'the', \"90's.\", \"We've\", 'moved', 'to', 'the', 'Right.', \"You'd\", 'think', 'people', 'on', 'the', 'Right', 'would', 'consider', 'that', 'a', 'victory,', 'but', 'somehow', \"y'all\", 'have', 'twisted', 'it', 'into', 'an', 'imaginary', 'defeat.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['left', 'lean', 'right', 'year', 'party', 'obamac', 'origin', 'herit', 'found', '80s', 'left', 'absolv', 'refus', 'pass', 'gingrich', 'push', '90s', 'obam', 'presid', 'left', 'lov', 'right', 'hat', '\\n\\nthe', 'nsa', 'domest', 'spy', 'lit', 'fasc', 'left', 'new', 'definit', 'trump', 'support', 'act', 'fasc', 'right', 'left', 'larg', 'support', 'fear', 'far', 'right', 'program', 'wouldnt', 'allow', 'nine hundred and eleven', '\\n\\ntrump', 'chang', 'posit', 'thing', 'year', 'peopl', 'ther', 'sham', 'long', 'chang', 'dont', 'hap', 'interview', 'interview', 'trump', 'consist', 'lean', 'farth', 'farth', 'right', 'last', 'sev', 'decad', '\\n\\ni', 'ide', 'think', 'country', 'mov', 'left', 'iv', 'watch', 'polit', 'sint', '90s', 'wev', 'mov', 'right', 'youd', 'think', 'peopl', 'right', 'would', 'consid', 'vict', 'somehow', 'yal', 'twist', 'imagin', 'def'], ['leave', 'lean', 'right', 'years', 'party', 'obamacare', 'originate', 'heritage', 'foundation', '80s', 'leave', 'absolutely', 'refuse', 'pass', 'gingrich', 'push', '90s', 'obama', 'president', 'leave', 'love', 'right', 'hat', '\\n\\nthe', 'nsas', 'domestic', 'spy', 'literally', 'fascist', 'lefts', 'new', 'definition', 'trump', 'support', 'actually', 'fascist', 'right', 'leave', 'largely', 'support', 'fear', 'far', 'right', 'program', 'wouldnt', 'allow', 'nine hundred and eleven', '\\n\\ntrump', 'change', 'position', 'things', 'years', 'people', 'theres', 'shame', 'long', 'change', 'dont', 'happen', 'interview', 'interview', 'trump', 'consistently', 'lean', 'farther', 'farther', 'right', 'last', 'several', 'decades', '\\n\\ni', 'idea', 'think', 'country', 'move', 'leave', 'ive', 'watch', 'politics', 'since', '90s', 'weve', 'move', 'right', 'youd', 'think', 'people', 'right', 'would', 'consider', 'victory', 'somehow', 'yall', 'twist', 'imaginary', 'defeat'])\n",
      "original document: \n",
      "['&gt;', 'I', 'mean', 'if', 'these', 'friends', 'are', 'so', 'far', 'away,', 'it', 'makes', 'sense', 'they', 'don’t', 'want', 'to', 'stay', 'so', 'far', 'from', 'home', 'for', 'a', 'long', \"time.\\n\\nI've\", 'offered', 'to', 'drive', 'by', 'them', 'but', 'still', 'they', \"don't\", 'seem', 'to', 'want', 'to', 'hang', 'out', 'with', 'any', 'frequency.', 'Granted,', 'I', 'only', 'do', 'that', 'once', 'or', 'twice', 'before', 'I', 'feel', 'too', 'rejected', 'to', 'keep', 'asking.', '\\n\\n&gt;', 'If', 'people', 'act', 'differently', 'in', 'real', 'life', 'than', 'texting,', 'it', 'might', 'have', 'to', 'do', 'with', 'your', 'person.', 'How’s', 'your', 'personal', 'hygiene?', 'You', 'take', 'care', 'of', 'yourself,', 'make', 'an', 'effort', 'to', 'look', 'nice?\\n\\nI', 'take', 'extremely', 'good', 'care', 'of', 'myself,', 'to', 'the', 'point', 'of', 'where', 'women', \"I've\", 'met', 'have', 'actually', 'told', 'me', 'they', 'assumed', 'I', 'was', 'a', 'b**ch', 'before', 'they', 'started', 'talking', 'to', 'me,', 'solely', 'because', 'I', 'am', '\"so', 'beautiful.\"', 'I', 'hate', 'even', 'saying', 'that', 'because', 'it', 'sounds', 'so', 'arrogant,', 'and', 'I', 'think', \"I'm\", 'a', 'really', 'down-to-earth,', 'kind', 'person.', ':(']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'mean', 'friend', 'far', 'away', 'mak', 'sens', 'dont', 'want', 'stay', 'far', 'hom', 'long', 'time\\n\\niv', 'off', 'driv', 'stil', 'dont', 'seem', 'want', 'hang', 'frequ', 'grant', 'twic', 'feel', 'reject', 'keep', 'ask', '\\n\\ngt', 'peopl', 'act', 'diff', 'real', 'lif', 'text', 'might', 'person', 'how', 'person', 'hygy', 'tak', 'car', 'mak', 'effort', 'look', 'nice\\n\\ni', 'tak', 'extrem', 'good', 'car', 'point', 'wom', 'iv', 'met', 'act', 'told', 'assum', 'bch', 'start', 'talk', 'sol', 'beauty', 'hat', 'ev', 'say', 'sound', 'arrog', 'think', 'im', 'real', 'downtoear', 'kind', 'person'], ['gt', 'mean', 'friends', 'far', 'away', 'make', 'sense', 'dont', 'want', 'stay', 'far', 'home', 'long', 'time\\n\\nive', 'offer', 'drive', 'still', 'dont', 'seem', 'want', 'hang', 'frequency', 'grant', 'twice', 'feel', 'reject', 'keep', 'ask', '\\n\\ngt', 'people', 'act', 'differently', 'real', 'life', 'texting', 'might', 'person', 'hows', 'personal', 'hygiene', 'take', 'care', 'make', 'effort', 'look', 'nice\\n\\ni', 'take', 'extremely', 'good', 'care', 'point', 'women', 'ive', 'meet', 'actually', 'tell', 'assume', 'bch', 'start', 'talk', 'solely', 'beautiful', 'hate', 'even', 'say', 'sound', 'arrogant', 'think', 'im', 'really', 'downtoearth', 'kind', 'person'])\n",
      "original document: \n",
      "['I', 'absolutely', 'hate', \"Venom's\", 'L3']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['absolv', 'hat', 'venom', 'l3'], ['absolutely', 'hate', 'venoms', 'l3'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['&gt;', 'It', 'makes', 'fun', 'of', 'people', 'that', 'are', 'totally', 'fine', 'with', 'animated', 'CP', 'because', 'somehow', 'that', 'makes', 'it', 'less', 'shameful.', 'It', \"doesn't.\\n\\nIt\", 'does', 'jerking', 'off', 'to', 'something', 'that', 'hurts', 'no', 'one', 'is', 'significantly', 'less', 'shameful', 'then', 'jerking', 'off', 'to', 'something', 'that', 'permanently', 'harms', 'a', 'kid.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'mak', 'fun', 'peopl', 'tot', 'fin', 'anim', 'cp', 'somehow', 'mak', 'less', 'sham', 'doesnt\\n\\nit', 'jerk', 'someth', 'hurt', 'on', 'sign', 'less', 'sham', 'jerk', 'someth', 'perm', 'harm', 'kid'], ['gt', 'make', 'fun', 'people', 'totally', 'fine', 'animate', 'cp', 'somehow', 'make', 'less', 'shameful', 'doesnt\\n\\nit', 'jerk', 'something', 'hurt', 'one', 'significantly', 'less', 'shameful', 'jerk', 'something', 'permanently', 'harm', 'kid'])\n",
      "original document: \n",
      "['Go', 'to', \"psyonix's\", 'twitter', 'for', 'the', 'answer.', '\\nOr', 'you', 'could', 'use', 'the', 'search', 'bar', 'because', 'this', 'has', 'been', 'asked', 'a', 'million', 'times', 'today.', '\\n\\nThe', 'trails', 'are', 'disabled', 'for', 'now', 'because', 'they', 'are', 'causing', 'crashes.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['go', 'psyonix', 'twit', 'answ', '\\nor', 'could', 'us', 'search', 'bar', 'ask', 'mil', 'tim', 'today', '\\n\\nthe', 'trail', 'dis', 'caus', 'crash'], ['go', 'psyonixs', 'twitter', 'answer', '\\nor', 'could', 'use', 'search', 'bar', 'ask', 'million', 'time', 'today', '\\n\\nthe', 'trail', 'disable', 'cause', 'crash'])\n",
      "original document: \n",
      "['They', \"weren't\", 'a', 'country', 'until', 'the', 'XIX', 'century.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wer', 'country', 'xix', 'century'], ['werent', 'country', 'xix', 'century'])\n",
      "original document: \n",
      "['Whoa,', 'you', 'really', 'are', 'a', 'kid.', 'Vote', 'Bernie', 'when', 'the', 'time', '', 'comes,', 'junior.\\n\\nThen', 'your', 'kids', 'may', 'know', 'some', 'basic', 'abundance', 'in', 'a', 'collapsing', 'world.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['who', 'real', 'kid', 'vot', 'berny', 'tim', 'com', 'junior\\n\\nthen', 'kid', 'may', 'know', 'bas', 'abund', 'collaps', 'world'], ['whoa', 'really', 'kid', 'vote', 'bernie', 'time', 'come', 'junior\\n\\nthen', 'kid', 'may', 'know', 'basic', 'abundance', 'collapse', 'world'])\n",
      "original document: \n",
      "['On', 'one', 'hand,', 'It', 'only', 'looks', 'like', 'a', 'small', 'cut,', 'yes', 'there', 'was', 'blood', 'but', 'it', 'doesnt', 'have', 'to', 'be', 'painful.', 'I', '', 'think', 'that', 'many', 'of', 'us', 'have', 'bled', 'a', 'lot', 'more', 'after', 'shaving,', 'and', 'is', 'it', 'not', 'necessary', 'need', 'to', 'be', 'painful,', 'is', 'just', 'annoying.\\nOn', 'the', 'other', 'hand,', 'a', 'proper', 'healing', 'should', 'have', 'been', 'done', 'after', 'they', 'noticed', 'he', 'was', 'bleeding.', 'I', 'dont', 'care', 'about', 'their', 'schedule,', 'if', 'it', 'is', 'not', 'flexible', 'enough', 'for', 'them', '', 'to', 'have', 'a', '10/20', 'minutes', 'pause', 'to', 'heal', 'Daehwi,', 'YMC', 'sucks.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'hand', 'look', 'lik', 'smal', 'cut', 'ye', 'blood', 'doesnt', 'pain', 'think', 'many', 'us', 'bled', 'lot', 'shav', 'necess', 'nee', 'pain', 'annoying\\non', 'hand', 'prop', 'heal', 'don', 'not', 'blee', 'dont', 'car', 'schedule', 'flex', 'enough', 'one thousand and twenty', 'minut', 'paus', 'heal', 'daehw', 'ymc', 'suck'], ['one', 'hand', 'look', 'like', 'small', 'cut', 'yes', 'blood', 'doesnt', 'painful', 'think', 'many', 'us', 'bleed', 'lot', 'shave', 'necessary', 'need', 'painful', 'annoying\\non', 'hand', 'proper', 'heal', 'do', 'notice', 'bleed', 'dont', 'care', 'schedule', 'flexible', 'enough', 'one thousand and twenty', 'minutes', 'pause', 'heal', 'daehwi', 'ymc', 'suck'])\n",
      "original document: \n",
      "['And', 'it’s', 'not', 'as', 'if', 'he', 'didn’t', 'get', 'looks', 'in', 'our', 'system\\n\\nIt', 'seemed', 'like', 'he', 'was', 'open', 'constantly', 'but', 'just', 'bricked', 'almost', 'every', 'single', 'shot', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'get', 'look', 'system\\n\\nit', 'seem', 'lik', 'op', 'const', 'brick', 'almost', 'every', 'singl', 'shot'], ['didnt', 'get', 'look', 'system\\n\\nit', 'seem', 'like', 'open', 'constantly', 'bricked', 'almost', 'every', 'single', 'shoot'])\n",
      "original document: \n",
      "['Still', \"haven't\", 'answered', 'my', 'question.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stil', 'hav', 'answ', 'quest'], ['still', 'havent', 'answer', 'question'])\n",
      "original document: \n",
      "['\\nadobe.com/go/edu-validate\\n\\nbcbla', 'Redacted', 'for', 'privacy', \"(it's\", 'where', 'I', 'work)\\n\\ncc.com/shows/the-daily-show-with-trevor-noah\\n\\ndrivetexas.org\\n\\nexpedia.com\\n\\nnothin', 'for', 'f\\n\\ngoogle', '(duh)\\n\\nhulu\\n\\naaaand', \"I'm\", 'bored']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\nadobecomgoeduvalidate\\n\\nbcbla', 'redact', 'priv', 'work\\n\\ncccomshowsthedailyshowwithtrevornoah\\n\\ndrivetexasorg\\n\\nexpediacom\\n\\nnothin', 'f\\n\\ngoogle', 'duh\\n\\nhulu\\n\\naaaand', 'im', 'bor'], ['\\nadobecomgoeduvalidate\\n\\nbcbla', 'redact', 'privacy', 'work\\n\\ncccomshowsthedailyshowwithtrevornoah\\n\\ndrivetexasorg\\n\\nexpediacom\\n\\nnothin', 'f\\n\\ngoogle', 'duh\\n\\nhulu\\n\\naaaand', 'im', 'bore'])\n",
      "original document: \n",
      "['But', 'this', 'is', 'presumably', 'also', 'while', 'Frank', 'is', 'shooting', 'back', 'at', 'her.', 'Someone', 'who', 'tagged', 'Spider-Man,', 'twice.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['presum', 'also', 'frank', 'shoot', 'back', 'someon', 'tag', 'spiderm', 'twic'], ['presumably', 'also', 'frank', 'shoot', 'back', 'someone', 'tag', 'spiderman', 'twice'])\n",
      "original document: \n",
      "['Even', 'hollow', 'moon', 'cannot', 'be', 'compared', 'to', 'flat', 'earth.', 'The', 'moon', '[rings', 'like', 'a', 'bell](https://www.popsci.com/does-moon-sound-like-bell),', 'and', 'craters', 'should', 'be', 'deeper.', 'So,', 'why', 'not?', 'It', 'is', 'not', 'falsified', 'per', 'say.', 'Flat', 'earth', 'is', 'falsified', 'every', 'time', 'you', 'look', 'at', 'the', 'sky.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ev', 'hollow', 'moon', 'cannot', 'comp', 'flat', 'ear', 'moon', 'ring', 'lik', 'bellhttpswwwpopscicomdoesmoonsoundlikebel', 'crat', 'deep', 'fals', 'per', 'say', 'flat', 'ear', 'fals', 'every', 'tim', 'look', 'sky'], ['even', 'hollow', 'moon', 'cannot', 'compare', 'flat', 'earth', 'moon', 'ring', 'like', 'bellhttpswwwpopscicomdoesmoonsoundlikebell', 'craters', 'deeper', 'falsify', 'per', 'say', 'flat', 'earth', 'falsify', 'every', 'time', 'look', 'sky'])\n",
      "original document: \n",
      "[\"I'd\", 'pay', 'good', 'money', 'to', 'see', 'Huma', 'on', 'roller', 'skates.', 'Amirite?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'pay', 'good', 'money', 'see', 'hum', 'rol', 'skat', 'amirit'], ['id', 'pay', 'good', 'money', 'see', 'huma', 'roller', 'skate', 'amirite'])\n",
      "original document: \n",
      "['Traffic', 'is', 'really', 'fucked', 'today.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['traff', 'real', 'fuck', 'today'], ['traffic', 'really', 'fuck', 'today'])\n",
      "original document: \n",
      "['Kyrtasaurus', 'and', 'ShishkaBobSaget', 'both', 'with', 'multiple', 'clears.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kyrtasaur', 'shishkabobsaget', 'multipl', 'clear'], ['kyrtasaurus', 'shishkabobsaget', 'multiple', 'clear'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'an', 'ocelot', 'and', \"you're\", 'not', 'a', 'snake.', \"We're\", 'men,', 'with', 'names.', \"\\n\\n\\nI'm\", 'Adamska...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'ocelot', 'yo', 'snak', 'men', 'nam', '\\n\\n\\nim', 'adamsk'], ['im', 'ocelot', 'youre', 'snake', 'men', 'name', '\\n\\n\\nim', 'adamska'])\n",
      "original document: \n",
      "['Fun', 'fact', 'apparently', 'Mr', \"Satan's\", 'his', 'gimmick', 'name.', 'The', 'creator', 'has', 'stated', 'his', 'real', 'name', 'is', 'Mark.\\n\\nAnd', 'I', 'just', 'realised', 'while', 'typing', 'that', 'sentence', 'in', 'this', 'subreddit', 'how', 'hilarious', 'that', 'is.\\n\\nhttp://www.kanzenshuu.com/2009/04/21/mr-satans-real-name-revealed/']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fun', 'fact', 'app', 'mr', 'sat', 'gimmick', 'nam', 'cre', 'stat', 'real', 'nam', 'mark\\n\\nand', 'real', 'typ', 'sent', 'subreddit', 'hil', 'is\\n\\nhttpwwwkanzenshuucom20090421mrsatansrealnamerevealed'], ['fun', 'fact', 'apparently', 'mr', 'satans', 'gimmick', 'name', 'creator', 'state', 'real', 'name', 'mark\\n\\nand', 'realise', 'type', 'sentence', 'subreddit', 'hilarious', 'is\\n\\nhttpwwwkanzenshuucom20090421mrsatansrealnamerevealed'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['no']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['ive', 'not', 'lost', 'any', 'data', 'ive', 'bounced', 'between', 'xbox', 'and', 'pc', 'but', 'pc', 'mainly']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'lost', 'dat', 'iv', 'bount', 'xbox', 'pc', 'pc', 'main'], ['ive', 'lose', 'data', 'ive', 'bounce', 'xbox', 'pc', 'pc', 'mainly'])\n",
      "original document: \n",
      "['woops,', 'thanks']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['woop', 'thank'], ['woops', 'thank'])\n",
      "original document: \n",
      "['Too', 'many', 'ground', 'balls', 'on', 'our', 'end.', '', 'Come', 'on', 'guys.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'ground', 'bal', 'end', 'com', 'guy'], ['many', 'grind', 'ball', 'end', 'come', 'guy'])\n",
      "original document: \n",
      "['Too', 'many', 'people', \"don't\", 'want', 'to', 'see', 'us', 'get', 'pooped', 'on', 'by', 'OSU.', 'It', 'sucks', 'but', 'I', 'get', 'why', 'people', \"don't\", 'wanna', 'go', 'to', 'this', 'game.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['many', 'peopl', 'dont', 'want', 'see', 'us', 'get', 'poop', 'osu', 'suck', 'get', 'peopl', 'dont', 'wann', 'go', 'gam'], ['many', 'people', 'dont', 'want', 'see', 'us', 'get', 'pooped', 'osu', 'suck', 'get', 'people', 'dont', 'wanna', 'go', 'game'])\n",
      "original document: \n",
      "['I', 'gotchu,', 'msg:', 'The', 'Sweed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gotchu', 'msg', 'swee'], ['gotchu', 'msg', 'sweed'])\n",
      "original document: \n",
      "['I', 'just', 'made', 'one', 'of', 'these', 'the', 'other', 'day,', 'but', 'max', 'weight.', 'I', 'really', 'like', 'that', 'this', 'build', 'has', 'decent', 'mid', 'range', 'so', 'you', 'can', 'still', 'dominate', 'in', 'the', 'paint', 'and', 'stretch', 'the', 'floor', 'a', 'little', 'bit', 'if', 'you’re', 'on', 'a', 'team', 'with', 'other', 'bigs.', '\\n\\nBtw', 'do', 'you', 'know', 'what', 'the', 'rebounding', 'badges', 'go', 'up', 'to?', '', 'I', 'was', 'hoping', 'gold', 'for', 'hustle', 'rebounder', 'and', 'putback', 'king', 'but', 'I', 'don’t', 'think', 'so.', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mad', 'on', 'day', 'max', 'weight', 'real', 'lik', 'build', 'dec', 'mid', 'rang', 'stil', 'domin', 'paint', 'stretch', 'flo', 'littl', 'bit', 'yo', 'team', 'big', '\\n\\nbtw', 'know', 'rebound', 'badg', 'go', 'hop', 'gold', 'hustl', 'rebound', 'putback', 'king', 'dont', 'think', '\\n'], ['make', 'one', 'day', 'max', 'weight', 'really', 'like', 'build', 'decent', 'mid', 'range', 'still', 'dominate', 'paint', 'stretch', 'floor', 'little', 'bite', 'youre', 'team', 'bigs', '\\n\\nbtw', 'know', 'rebound', 'badge', 'go', 'hop', 'gold', 'hustle', 'rebounder', 'putback', 'king', 'dont', 'think', '\\n'])\n",
      "original document: \n",
      "['Some', 'of', 'us', 'do', 'realize', 'it.\\n\\nAnd', 'it', 'is', 'weird.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'real', 'it\\n\\nand', 'weird'], ['us', 'realize', 'it\\n\\nand', 'weird'])\n",
      "original document: \n",
      "['Bitch', \"can't\", 'even', 'stand', 'for', 'a', 'picture.', 'Unfit!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bitch', 'cant', 'ev', 'stand', 'pict', 'unfit'], ['bitch', 'cant', 'even', 'stand', 'picture', 'unfit'])\n",
      "original document: \n",
      "['no,', 'we', 'regret', 'hiring', 'coach', 'O']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['regret', 'hir', 'coach'], ['regret', 'hire', 'coach'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Gotta', 'love', 'the', 'the', 'bender', 'musings!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gott', 'lov', 'bend', 'mus'], ['gotta', 'love', 'bender', 'muse'])\n",
      "original document: \n",
      "['Funny.', '', 'This', 'was', 'a', 'common', 'sight', 'in', 'the', \"70's.\", 'Only', 'they', 'were', 'rolled', 'higher-', 'as', 'high', 'as', 'the', 'could', 'reach.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['funny', 'common', 'sight', '70s', 'rol', 'high', 'high', 'could', 'reach'], ['funny', 'common', 'sight', '70s', 'roll', 'higher', 'high', 'could', 'reach'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['It', 'is', 'listed', 'under', 'payouts,', 'but', 'is', 'under', 'the', 'minimum', 'payout', 'amount', 'so', 'no.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['list', 'payout', 'minim', 'payout', 'amount'], ['list', 'payouts', 'minimum', 'payout', 'amount'])\n",
      "original document: \n",
      "['No,', 'but', 'it', 'changes', 'how', 'much', 'they', 'score,', 'are', 'you', 'looking', 'for', 'big', 'potential', 'or', 'someone', 'who', 'has', 'a', 'safe', 'floor.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['chang', 'much', 'scor', 'look', 'big', 'pot', 'someon', 'saf', 'flo'], ['change', 'much', 'score', 'look', 'big', 'potential', 'someone', 'safe', 'floor'])\n",
      "original document: \n",
      "['Pretty', 'sure', 'he', 'did.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pretty', 'sur'], ['pretty', 'sure'])\n",
      "original document: \n",
      "['Do', 'it.', 'Eat', 'your', 'vegetables.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['eat', 'veget'], ['eat', 'vegetables'])\n",
      "original document: \n",
      "['Have', 'pink', 'spiralis', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pink', 'spir'], ['pink', 'spiralis'])\n",
      "original document: \n",
      "['I', 'want', 'really', 'want', 'Ice', 'to', 'crash', 'his', 'car', 'so', 'he', 'can', 'learn', 'a', 'lesson', 'of', 'not', 'looking', 'at', 'phone/chat', 'while', 'driving.', 'Dont', 'want', 'anyone', 'to', 'get', 'hurt', 'or', 'anything...maybe', 'just', 'crash', 'into', 'a', 'pole', 'or', 'something', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'real', 'want', 'ic', 'crash', 'car', 'learn', 'lesson', 'look', 'phonech', 'driv', 'dont', 'want', 'anyon', 'get', 'hurt', 'anythingmayb', 'crash', 'pol', 'someth'], ['want', 'really', 'want', 'ice', 'crash', 'car', 'learn', 'lesson', 'look', 'phonechat', 'drive', 'dont', 'want', 'anyone', 'get', 'hurt', 'anythingmaybe', 'crash', 'pole', 'something'])\n",
      "original document: \n",
      "['I', 'am', 'now', 'going', 'to', 'write', 'a', 'poem', 'based', 'on', 'that', 'because', 'I', 'find', 'it', 'intetesting.', \"I'll\", 'edit', 'this', 'comment', 'when', 'finished', 'if', \"you're\", 'curious']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['going', 'writ', 'poem', 'bas', 'find', 'intetest', 'il', 'edit', 'com', 'fin', 'yo', 'cury'], ['go', 'write', 'poem', 'base', 'find', 'intetesting', 'ill', 'edit', 'comment', 'finish', 'youre', 'curious'])\n",
      "original document: \n",
      "['Killing', 'your', 'literacy', 'eating', 'into', 'Spain', 'bro.', 'Should', 'have', 'taken', 'Belgium/Netherlands', 'instead', 'if', 'you', 'want', 'to', 'blob', 'in', 'Europe.', 'Steal', 'Quebec', 'from', 'the', 'Brits', 'and', 'return', 'the', 'French-Canadians', 'to', 'their', 'sisterland.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['kil', 'lit', 'eat', 'spain', 'bro', 'tak', 'belgiumnetherland', 'instead', 'want', 'blob', 'europ', 'ste', 'quebec', 'brit', 'return', 'frenchcanad', 'sisterland'], ['kill', 'literacy', 'eat', 'spain', 'bro', 'take', 'belgiumnetherlands', 'instead', 'want', 'blob', 'europe', 'steal', 'quebec', 'brits', 'return', 'frenchcanadians', 'sisterland'])\n",
      "original document: \n",
      "['Reasonable', 'skepticism,', 'I', 'can', 'respect', 'that.', 'It', 'says', \"it's\", '3000', 'lm,', '500', 'ANSI', 'lm.', '[link.](https://www.amazon.com/Nebula-Portable-Multi-media-supported-Projector/dp/B073P3JHTH)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['reason', 'skept', 'respect', 'say', 'three thousand', 'lm', 'five hundred', 'ans', 'lm', 'linkhttpswwwamazoncomnebulaportablemultimediasupportedprojectordpb073p3jhth'], ['reasonable', 'skepticism', 'respect', 'say', 'three thousand', 'lm', 'five hundred', 'ansi', 'lm', 'linkhttpswwwamazoncomnebulaportablemultimediasupportedprojectordpb073p3jhth'])\n",
      "original document: \n",
      "['[+LFAH94](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnonalv/):\\n\\nBrianna,', 'as', 'someone', \"who's\", 'trying', 'to', 'do', 'the', 'same', '(while', 'racing', 'against', 'the', 'college', 'graduation', 'clock),', 'how', 'did', 'you', 'manage', 'to', 'self-publish', 'without', 'paying', 'anything?\\nThere', 'are', 'decent', 'self', 'publishers', 'here', 'in', 'Canada', 'but', 'even', 'a', 'short', 'run', 'of', '10', 'paperbacks', 'is', 'going', 'to', 'set', 'me', 'back', 'at', 'least', 'a', 'few', 'hundred', 'bucks.', '\\nAny', 'suggestions?', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lfah94httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnonalv\\n\\nbrianna', 'someon', 'who', 'try', 'rac', 'colleg', 'gradu', 'clock', 'man', 'selfpubl', 'without', 'pay', 'anything\\nthere', 'dec', 'self', 'publ', 'canad', 'ev', 'short', 'run', 'ten', 'paperback', 'going', 'set', 'back', 'least', 'hundr', 'buck', '\\nany', 'suggest'], ['lfah94httpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnonalv\\n\\nbrianna', 'someone', 'whos', 'try', 'race', 'college', 'graduation', 'clock', 'manage', 'selfpublish', 'without', 'pay', 'anything\\nthere', 'decent', 'self', 'publishers', 'canada', 'even', 'short', 'run', 'ten', 'paperbacks', 'go', 'set', 'back', 'least', 'hundred', 'buck', '\\nany', 'suggestions'])\n",
      "original document: \n",
      "['143413955|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'msELbNeP)\\n\\n&gt;&gt;143413905\\nDual', 'citizenship?\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, nine hundred and fifty-fiv', 'gt', 'unit', 'stat', 'anonym', 'id', 'mselbnep\\n\\ngtgt143413905\\ndual', 'citizenship\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, nine hundred and fifty-five', 'gt', 'unite', 'state', 'anonymous', 'id', 'mselbnep\\n\\ngtgt143413905\\ndual', 'citizenship\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Accessorize', 'is', 'good', 'for', 'nothing', 'besides', 'looks', 'but', 'I', 'think', 'you', 'want', 'maybe', 'more', 'an', 'example', 'where', 'the', 'function', 'is', 'being', 'disused', 'or', 'not', 'used', 'at', 'all', 'like', 'with', 'outdoor', 'wear', '(North', 'Face', 'etc)', 'as', 'fashion.', 'Actually', 'I', \"can't\", 'really', 'think', 'of', 'any', 'good', 'examples', 'right', 'now,', 'but', 'feel', 'that', 'they', 'are', 'many,', 'hm.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['access', 'good', 'noth', 'besid', 'look', 'think', 'want', 'mayb', 'exampl', 'funct', 'disus', 'us', 'lik', 'outdo', 'wear', 'nor', 'fac', 'etc', 'fash', 'act', 'cant', 'real', 'think', 'good', 'exampl', 'right', 'feel', 'many', 'hm'], ['accessorize', 'good', 'nothing', 'besides', 'look', 'think', 'want', 'maybe', 'example', 'function', 'disused', 'use', 'like', 'outdoor', 'wear', 'north', 'face', 'etc', 'fashion', 'actually', 'cant', 'really', 'think', 'good', 'examples', 'right', 'feel', 'many', 'hm'])\n",
      "original document: \n",
      "['This!\\n\\nEvery', 'player', 'passes', 'in', 'the', 'same', 'pattern', 'and', 'it', 'takes', '2-3', 'passes', 'to', 'get', 'to', 'goal', 'and', 'score.', '\\n\\nWtf!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['this\\n\\nevery', 'play', 'pass', 'pattern', 'tak', 'twenty-three', 'pass', 'get', 'goal', 'scor', '\\n\\nwtf'], ['this\\n\\nevery', 'player', 'pass', 'pattern', 'take', 'twenty-three', 'pass', 'get', 'goal', 'score', '\\n\\nwtf'])\n",
      "original document: \n",
      "['*Who', 'is', 'a', 'Pineapple', 'under', 'the', 'sea*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pineappl', 'sea'], ['pineapple', 'sea'])\n",
      "original document: \n",
      "['i', 'know', \"you're\", 'trolling', '.', 'it', 'is', 'all', 'good', 'my', 'friend.', 'I', 'too', 'like', 'to', 'gun', 'down', 'uppity', '12', 'year', 'olds.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['know', 'yo', 'trol', 'good', 'friend', 'lik', 'gun', 'up', 'twelv', 'year', 'old'], ['know', 'youre', 'troll', 'good', 'friend', 'like', 'gun', 'uppity', 'twelve', 'year', 'olds'])\n",
      "original document: \n",
      "['I', 'mean', 'most', 'of', 'the', 'time', 'Athel', 'Loren', 'and', 'Ulthuan', 'are', 'more', 'or', 'less', 'safe', 'for', 'it´s', 'inhabitants.', 'I', 'don´t', 'know', 'how', 'many', 'Invasions', 'Malekith', 'had,', 'but', 'compared', 'to', 'the', 'Millenia', 'he´s', 'been', 'the', 'Witch', 'King', 'I´d', 'say', 'it´s', 'still', 'a', 'relatively', 'quiet', 'life.', 'Same', 'goes', 'for', 'Athel', 'Loren.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mean', 'tim', 'athel', 'lor', 'ulthu', 'less', 'saf', 'it s', 'inhabit', 'don t', 'know', 'many', 'invas', 'maleki', 'comp', 'millen', 'he s', 'witch', 'king', 'i d', 'say', 'it s', 'stil', 'rel', 'quiet', 'lif', 'goe', 'athel', 'lor'], ['mean', 'time', 'athel', 'loren', 'ulthuan', 'less', 'safe', 'it s', 'inhabitants', 'don t', 'know', 'many', 'invasions', 'malekith', 'compare', 'millenia', 'he s', 'witch', 'king', 'i d', 'say', 'it s', 'still', 'relatively', 'quiet', 'life', 'go', 'athel', 'loren'])\n",
      "original document: \n",
      "['&gt;', '\"When', 'I', 'visited', 'your', 'campus,', 'it', 'felt', 'magical.\\n\\nYou', 'do', 'realize', 'that', 'these', 'kids', 'have', 'to', 'write', 'ridiculously', 'long', 'essays', 'and', 'multiple', 'application', 'forms', 'during', 'their', 'senior', 'year,', 'and', \"you're\", 'asking', 'them', 'to', 'customize', 'each', 'application', 'form.\\n\\nUh,', 'no.\\n\\nSo', 'trying', 'to', 'make', 'a', 'more', 'or', 'less', '\"generic\"', 'application', 'that', 'can', 'be', '\"tuned\"', 'to', 'each', 'college', 'is', 'hardly', 'a', 'sin.', '', 'If', 'anything,', \"it's\", 'clever', 'thinking.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'visit', 'camp', 'felt', 'magical\\n\\nyou', 'real', 'kid', 'writ', 'ridic', 'long', 'essay', 'multipl', 'apply', 'form', 'seny', 'year', 'yo', 'ask', 'custom', 'apply', 'form\\n\\nuh', 'no\\n\\nso', 'try', 'mak', 'less', 'gen', 'apply', 'tun', 'colleg', 'hard', 'sin', 'anyth', 'clev', 'think'], ['gt', 'visit', 'campus', 'felt', 'magical\\n\\nyou', 'realize', 'kid', 'write', 'ridiculously', 'long', 'essay', 'multiple', 'application', 'form', 'senior', 'year', 'youre', 'ask', 'customize', 'application', 'form\\n\\nuh', 'no\\n\\nso', 'try', 'make', 'less', 'generic', 'application', 'tune', 'college', 'hardly', 'sin', 'anything', 'clever', 'think'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['It', 'could', 'be', 'we', 'got', 'more', 'enjoyment', 'out', 'of', 'it', 'playing', 'cooperatively', 'in', 'the', 'same', 'room...but', 'it', 'IS', 'the', 'sort', 'of', 'game', 'that', 'if', \"it's\", 'not', 'a', 'persons', 'genre', 'type,', 'no', 'matter', 'who', 'you', 'are', 'playing', 'with', 'it', 'would', 'be', 'boring.', 'Thanks', 'for', 'the', 'comment', 'mehoff88', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'got', 'enjoy', 'play', 'coop', 'roombut', 'sort', 'gam', 'person', 'genr', 'typ', 'mat', 'play', 'would', 'bor', 'thank', 'com', 'mehoff88'], ['could', 'get', 'enjoyment', 'play', 'cooperatively', 'roombut', 'sort', 'game', 'persons', 'genre', 'type', 'matter', 'play', 'would', 'bore', 'thank', 'comment', 'mehoff88'])\n",
      "original document: \n",
      "['That’s', 'the', 'one.', 'I’ve', 'got', 'pics', 'of', 'him', 'in', 'his', 'cage', 'somewhere.', 'It', 'was', 'quite', 'sad', 'actually.', '\\n\\n\\nMy', 'wonderful', 'mom', 'used', 'to', 'give', 'my', 'brothers', 'and', 'I(8,', '6,', 'and', '4)', 'a', 'lit', 'cigarette', 'and', 'a', 'handful', 'of', 'blackcats', 'to', 'light', 'and', 'throw', 'at', 'each', 'other', 'on', 'the', 'beach.', 'Ah', 'yes,', 'the', 'good', 'ol', 'days.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'on', 'iv', 'got', 'pic', 'cag', 'somewh', 'quit', 'sad', 'act', '\\n\\n\\nmy', 'wond', 'mom', 'us', 'giv', 'broth', 'i8', 'six', 'four', 'lit', 'cigaret', 'hand', 'blackc', 'light', 'throw', 'beach', 'ah', 'ye', 'good', 'ol', 'day'], ['thats', 'one', 'ive', 'get', 'pics', 'cage', 'somewhere', 'quite', 'sad', 'actually', '\\n\\n\\nmy', 'wonderful', 'mom', 'use', 'give', 'brothers', 'i8', 'six', 'four', 'light', 'cigarette', 'handful', 'blackcats', 'light', 'throw', 'beach', 'ah', 'yes', 'good', 'ol', 'days'])\n",
      "original document: \n",
      "['Was', 'there', 'a', 'game', 'delay?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gam', 'delay'], ['game', 'delay'])\n",
      "original document: \n",
      "[\"He's\", 'clearly', 'someone', 'that', \"doesn't\", 'understand', 'that', 'half', 'of', 'the', 'world', 'still', 'has', 'to', 'be', 'more', 'stupid', 'than', 'the', 'average', 'person.', \"He's\", 'much', 'below', 'the', 'average', 'here', 'lol']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'clear', 'someon', 'doesnt', 'understand', 'half', 'world', 'stil', 'stupid', 'av', 'person', 'hes', 'much', 'av', 'lol'], ['hes', 'clearly', 'someone', 'doesnt', 'understand', 'half', 'world', 'still', 'stupid', 'average', 'person', 'hes', 'much', 'average', 'lol'])\n",
      "original document: \n",
      "['Hong', 'Kong', 'moves', 'at', 'twice', 'the', 'speed,', 'and', 'sad', 'to', 'say', 'a', '\"princess', 'syndrome\"', 'exist', 'and', 'they', 'are', 'far', 'too', 'picky,', 'so', 'I', 'hear', 'a', 'lot', 'of', 'hk', 'guys', 'end', 'up', 'shacking', 'up', 'with', 'mainlander', 'girls,', 'yet', 'the', 'irony', 'HK', 'girls', 'then', 'blame', 'HK', 'men', 'for', 'doing', 'this...like', 'wat!?\\n\\nBecause', 'of', 'these', 'high', 'expectations,', 'there', 'are', 'quite', 'a', 'few', '\"loa', 'gaw', 'por\"', 'or', 'old', 'tiger', 'women,', 'they', 'expect', 'a', 'man', 'to', 'earn', 'way', 'more', 'than', 'them,', 'they', 'want', 'a', 'tool.', 'Yet,', 'the', 'sad', 'state', 'of', 'if', 'a', 'wm', 'enters,', 'he', 'can', 'be', 'a', 'bum', 'and', 'they', 'go', 'for', 'it.', 'Aaaand', 'LKF', 'is', 'a', 'shithole', 'too,', 'but', 'I', 'would', 'like', 'to', 'think', 'there', 'is', 'hope', 'in', 'HK', 'at', 'least,', 'compared', 'to', 'the', 'garbage', 'UK', 'offers.', '\\n\\nI', 'have', 'been', 'thinking', 'lately', 'too,', 'working', 'hard', 'but', 'you', 'need', 'other', 'parts', 'in', 'life', 'to', 'make', 'you', 'happy.', 'I', 'hear', 'stories', 'of', 'people', 'who', 'just', 'travel', 'a', 'lot', 'when', 'they', 'are', 'still', 'studying,', 'I', 'wish', 'I', 'did', 'this', 'when', 'I', 'was', 'younger', '(thus', 'have', 'less', 'commitment', 'to', 'things)', 'I', 'still', 'could', 'but', 'I', 'am', 'in', 'a', 'different', 'situation', 'whereby', 'I', 'need', 'to', 'fend', 'for', 'myself.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hong', 'kong', 'mov', 'twic', 'spee', 'sad', 'say', 'princess', 'syndrom', 'ex', 'far', 'picky', 'hear', 'lot', 'hk', 'guy', 'end', 'shack', 'mainland', 'girl', 'yet', 'irony', 'hk', 'girl', 'blam', 'hk', 'men', 'thislik', 'wat\\n\\nbecause', 'high', 'expect', 'quit', 'loa', 'gaw', 'por', 'old', 'tig', 'wom', 'expect', 'man', 'earn', 'way', 'want', 'tool', 'yet', 'sad', 'stat', 'wm', 'ent', 'bum', 'go', 'aaaand', 'lkf', 'shithol', 'would', 'lik', 'think', 'hop', 'hk', 'least', 'comp', 'garb', 'uk', 'off', '\\n\\ni', 'think', 'lat', 'work', 'hard', 'nee', 'part', 'lif', 'mak', 'happy', 'hear', 'story', 'peopl', 'travel', 'lot', 'stil', 'study', 'wish', 'young', 'thu', 'less', 'commit', 'thing', 'stil', 'could', 'diff', 'situ', 'whereby', 'nee', 'fend'], ['hong', 'kong', 'move', 'twice', 'speed', 'sad', 'say', 'princess', 'syndrome', 'exist', 'far', 'picky', 'hear', 'lot', 'hk', 'guy', 'end', 'shack', 'mainlander', 'girls', 'yet', 'irony', 'hk', 'girls', 'blame', 'hk', 'men', 'thislike', 'wat\\n\\nbecause', 'high', 'expectations', 'quite', 'loa', 'gaw', 'por', 'old', 'tiger', 'women', 'expect', 'man', 'earn', 'way', 'want', 'tool', 'yet', 'sad', 'state', 'wm', 'enter', 'bum', 'go', 'aaaand', 'lkf', 'shithole', 'would', 'like', 'think', 'hope', 'hk', 'least', 'compare', 'garbage', 'uk', 'offer', '\\n\\ni', 'think', 'lately', 'work', 'hard', 'need', 'part', 'life', 'make', 'happy', 'hear', 'stories', 'people', 'travel', 'lot', 'still', 'study', 'wish', 'younger', 'thus', 'less', 'commitment', 'things', 'still', 'could', 'different', 'situation', 'whereby', 'need', 'fend'])\n",
      "original document: \n",
      "['Think', 'of', 'Oregeron', 'at', 'Ole', 'Miss.', \"It's\", '2.0', 'of', 'that']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'oregeron', 'ol', 'miss', 'twenty'], ['think', 'oregeron', 'ole', 'miss', 'twenty'])\n",
      "original document: \n",
      "['&gt;Sweden', 'has', 'much', 'fewer', 'people', 'walking', 'outside', 'of', 'designated', 'cross', 'walks.\\n\\nAre', 'you', 'Swedish?', 'If', 'not', 'how', 'can', 'you', 'possibly', 'know', 'this?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtsweden', 'much', 'few', 'peopl', 'walk', 'outsid', 'design', 'cross', 'walks\\n\\nare', 'swed', 'poss', 'know'], ['gtsweden', 'much', 'fewer', 'people', 'walk', 'outside', 'designate', 'cross', 'walks\\n\\nare', 'swedish', 'possibly', 'know'])\n",
      "original document: \n",
      "['I', \"don't\", 'think', 'this', 'series', 'has', 'made', 'be', 'this', 'emotional', 'before..', \"DR2's\", 'second', 'chapter', 'topped', 'the', 'list', 'for', 'a', 'while,', 'but', 'this', 'fucked', 'me', 'up.', 'When', 'Kaito', 'started', 'coughing', 'up', 'blood', 'again,', 'I', 'honestly', \"couldn't\", 'help', 'but', 'cry.', 'I', \"don't\", 'want', 'him', 'to', 'fucking', 'die,', 'he', 'needs', 'to', 'survive', 'this', 'god', 'damn', 'game.', 'Gonta', 'being', 'executed', 'because', 'of', 'something', 'he', \"can't\", 'even', 'remember,', \"it's\", 'gotta', 'be', 'the', 'most', 'messed', 'up', \"'motive'\", 'for', 'a', 'killer', 'thus', 'far.', \"Kokichi's\", 'obviously', 'planning', 'some', 'fucked', 'up', 'shit,', 'and', 'quite', 'frankly', \"I'm\", 'kinda', 'scared', 'to', 'see', 'where', 'this', 'is', 'going.', 'Just,', \"don't\", 'die', 'Kaito.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'think', 'sery', 'mad', 'emot', 'dr2s', 'second', 'chapt', 'top', 'list', 'fuck', 'kaito', 'start', 'cough', 'blood', 'honest', 'couldnt', 'help', 'cry', 'dont', 'want', 'fuck', 'die', 'nee', 'surv', 'god', 'damn', 'gam', 'gont', 'execut', 'someth', 'cant', 'ev', 'rememb', 'gott', 'mess', 'mot', 'kil', 'thu', 'far', 'kokich', 'obvy', 'plan', 'fuck', 'shit', 'quit', 'frank', 'im', 'kind', 'scar', 'see', 'going', 'dont', 'die', 'kaito'], ['dont', 'think', 'series', 'make', 'emotional', 'dr2s', 'second', 'chapter', 'top', 'list', 'fuck', 'kaito', 'start', 'cough', 'blood', 'honestly', 'couldnt', 'help', 'cry', 'dont', 'want', 'fuck', 'die', 'need', 'survive', 'god', 'damn', 'game', 'gonta', 'execute', 'something', 'cant', 'even', 'remember', 'gotta', 'mess', 'motive', 'killer', 'thus', 'far', 'kokichis', 'obviously', 'plan', 'fuck', 'shit', 'quite', 'frankly', 'im', 'kinda', 'scar', 'see', 'go', 'dont', 'die', 'kaito'])\n",
      "original document: \n",
      "['Ill', 'do', 'that,', 'GT:', 'Huf', 'AP\\nIll', 'be', 'on', 'in', 'about', 'an', 'hour', 'if', 'thats', 'cool?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'gt', 'huf', 'ap\\nill', 'hour', 'that', 'cool'], ['ill', 'gt', 'huf', 'ap\\nill', 'hour', 'thats', 'cool'])\n",
      "original document: \n",
      "['Today', 'I', 'got', 'there', 'for', 'rope', 'drop.', 'Did', 'guardians', 'twice', '(once', 'was', 'a', 'fast', 'pass).', 'Cars,', 'screamin,', 'then', 'went', 'over', 'to', 'Disneyland', 'and', 'did', 'Space', 'Mountain', 'and', 'Haunted', 'Mansion.', 'Left', 'park', 'at', '10:10am.', 'Rope', 'drops', 'are', 'far', 'and', 'away', 'the', 'best', 'time.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['today', 'got', 'rop', 'drop', 'guard', 'twic', 'fast', 'pass', 'car', 'screamin', 'went', 'disneyland', 'spac', 'mountain', 'haunt', 'mand', 'left', 'park', '1010am', 'rop', 'drop', 'far', 'away', 'best', 'tim'], ['today', 'get', 'rope', 'drop', 'guardians', 'twice', 'fast', 'pass', 'cars', 'screamin', 'go', 'disneyland', 'space', 'mountain', 'haunt', 'mansion', 'leave', 'park', '1010am', 'rope', 'drop', 'far', 'away', 'best', 'time'])\n",
      "original document: \n",
      "['I', 'also', 'love', 'how', 'he', 'Robbie', 'starts', 'a', 'calm', 'stroll', 'towards', 'a', 'knocked', 'down', 'Rory.', 'As', 'if', 'saying', '\"okay,', 'I', 'knocked', 'him', 'down,', 'now', \"let's\", 'go', 'for', 'the', 'finish.\"', 'Mark', 'Hunt', 'did', 'the', 'same', 'relaxed', 'walk', 'toward', 'Bigfoot', 'in', 'their', 'first', 'fight.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'lov', 'robby', 'start', 'calm', 'stroll', 'toward', 'knock', 'rory', 'say', 'okay', 'knock', 'let', 'go', 'fin', 'mark', 'hunt', 'relax', 'walk', 'toward', 'bigfoot', 'first', 'fight'], ['also', 'love', 'robbie', 'start', 'calm', 'stroll', 'towards', 'knock', 'rory', 'say', 'okay', 'knock', 'let', 'go', 'finish', 'mark', 'hunt', 'relax', 'walk', 'toward', 'bigfoot', 'first', 'fight'])\n",
      "original document: \n",
      "['I', 'wish', 'more', \"SM's\", 'cared', 'more', 'about', 'their', 'financial', 'future.', 'I', 'talk', 'to', 'my', 'friends', 'all', 'the', 'time', 'about', 'it', 'and', 'I', 'can', 'tell', 'they', 'get', 'annoyed', 'or', 'just', \"don't\", 'give', 'a', 'fuck.', \"It's\", 'sad', 'when', \"I'm\", 'explaining', 'why', 'the', 'TSP', 'is', 'great', 'to', 'a', '40', 'year', 'old', 'who', 'thinks', 'gold', 'is', 'the', 'number', 'one', 'best', 'investment', 'known', 'to', 'man.', 'But', 'whatever.', \"I'm\", 'gonna', 'be', 'squared', 'away', 'so', 'fuck', 'em.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wish', 'sms', 'car', 'fin', 'fut', 'talk', 'friend', 'tim', 'tel', 'get', 'annoy', 'dont', 'giv', 'fuck', 'sad', 'im', 'explain', 'tsp', 'gre', 'forty', 'year', 'old', 'think', 'gold', 'numb', 'on', 'best', 'invest', 'known', 'man', 'whatev', 'im', 'gonn', 'squ', 'away', 'fuck', 'em'], ['wish', 'sms', 'care', 'financial', 'future', 'talk', 'friends', 'time', 'tell', 'get', 'annoy', 'dont', 'give', 'fuck', 'sad', 'im', 'explain', 'tsp', 'great', 'forty', 'year', 'old', 'think', 'gold', 'number', 'one', 'best', 'investment', 'know', 'man', 'whatever', 'im', 'gonna', 'square', 'away', 'fuck', 'em'])\n",
      "original document: \n",
      "['I', 'wonder', 'too.', 'The', 'hourly', 'on', 'GDAX', 'seems', 'like', 'it', 'is', 'reaching', 'the', 'tipping', 'point', 'of', 'a', 'rising', 'wedge,', 'and', 'Asia', 'is', 'just', 'waking', 'up.', 'I', 'have', 'a', 'tight', 'stop', 'loss', 'set', 'just', 'in', 'case.\\n\\nEDIT:', 'welp,', 'there', 'someone', 'just', 'sold', '700', 'coins', 'at', 'once...\\nEDIT', '2:', 'I', 'wonder', 'how', 'low', 'this', 'one', 'will', 'go,', 'guessing', 'near', '$50', 'since', 'that', 'is', 'the', 'support', 'where', 'we', 'had', 'the', 'most', 'volume']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wond', 'hour', 'gdax', 'seem', 'lik', 'reach', 'tip', 'point', 'ris', 'wedg', 'as', 'wak', 'tight', 'stop', 'loss', 'set', 'case\\n\\nedit', 'welp', 'someon', 'sold', 'seven hundred', 'coin', 'once\\nedit', 'two', 'wond', 'low', 'on', 'go', 'guess', 'near', 'fifty', 'sint', 'support', 'volum'], ['wonder', 'hourly', 'gdax', 'seem', 'like', 'reach', 'tip', 'point', 'rise', 'wedge', 'asia', 'wake', 'tight', 'stop', 'loss', 'set', 'case\\n\\nedit', 'welp', 'someone', 'sell', 'seven hundred', 'coin', 'once\\nedit', 'two', 'wonder', 'low', 'one', 'go', 'guess', 'near', 'fifty', 'since', 'support', 'volume'])\n",
      "original document: \n",
      "['I', 'have', 'to', 'agree', 'with', 'a', 'lot', 'of', 'the', 'critiques', 'here.', '', 'The', 'pilot', 'showed', 'a', 'lot', 'of', 'promise,', 'but', 'the', 'show', 'just', 'went', 'down', 'hill', 'from', 'there.', '', 'The', 'show', 'had', 'so', 'much', 'potential', 'to', 'be', 'an', 'accurate', 'portrayal', 'of', 'the', 'origins', 'of', 'hip', 'hop', 'and', 'how', 'the', 'culture', 'of', 'the', '70s', 'in', 'NYC', 'really', 'influenced', 'not', 'only', 'a', 'generation', 'but', 'changed', 'the', 'landscape', 'of', 'the', 'music', 'industry', 'in', 'general.', '', '\\n\\nBut', 'sadly', 'they', 'just', \"didn't\", 'portray', 'it', 'very', 'realistically', 'and', 'it', \"wasn't,\", 'like', 'someone', 'else', 'said,', 'raw', 'or', 'real', 'enough.', '', 'And', \"don't\", 'get', 'me', 'started', 'on', 'the', 'whole', 'rape', 'scene', 'where', 'Leslie', 'Lesgold', 'forces', 'Jackie', 'Moreno', 'to', 'go', 'down', 'on', 'her', 'because', 'he', 'once', 'coerced', 'her', 'into', 'giving', 'him', 'a', 'blowjob', 'and', 'then', 'fired', 'her', 'way', 'back', 'when', 'she', 'interned', 'for', 'him.', '', 'Really?', '', 'Such', 'BS', 'and', 'so', 'completely', 'unrealistic.', '', 'I', 'lived', 'through', 'that', 'era,', 'women', 'got', 'used', 'and', 'sexually', 'exploited', 'all', 'the', 'time', 'and', 'not', 'just', 'in', 'the', 'music', 'industry.', '', 'Back', 'then', 'it', 'was', 'the', 'unofficial', 'and', 'deeply', 'repulsive', 'status', 'quo', 'that', 'in', 'order', 'for', 'a', 'woman', 'to', 'get', 'a', 'ahead,', 'pardon', 'the', 'pun,', 'she', 'had', 'to', 'give', 'head', 'and', 'much', 'more.', 'If', 'she', \"didn't\", 'play', 'that', 'game', 'she', 'got', 'black', 'listed', 'and', 'her', 'career', 'was', 'over.', '', '\\n\\nOnce', 'a', 'woman', 'gained', 'stardom', 'then', 'it', 'was', 'different', 'story,', 'but', 'even', 'then', 'the', 'male', 'power', 'players', 'of', 'the', 'day', 'ran', 'the', 'show', 'and', 'their', 'word', 'was', 'law.', '', 'Back', 'then', 'no', 'woman', 'would', 'dare', 'pull', 'some', 'revenge', 'crap', 'like', 'Leslie', 'did,', 'because', 'no', 'male', 'power', 'player', 'then', 'would', 'ever', 'allow', 'a', 'woman', 'to', 'have', 'that', 'kind', 'of', 'power.', '', 'Sorry', 'ladies', 'but', 'rape', 'revenge', 'like', 'what', 'Leslie', 'did,', 'however', 'empowering', 'and', 'entertaining,', 'was', 'not', 'only', 'not', 'practical', 'but', 'due', 'to', 'the', 'culture', 'of', 'the', 'time', 'would', 'never', 'have', 'happened.', 'So', 'yeah', 'that', 'was', 'the', 'big', 'BS', 'exclamation', 'point', 'on', 'the', 'ending', 'of', 'that', 'show,', 'that', 'and', 'the', 'ridiculous', 'amount', 'of', 'money', 'Netflix', 'spent', 'on', 'it.', '', 'The', 'Get', 'Down', 'deserved', 'to', 'get', 'cancelled.', '', 'Overall', 'though,', 'with', 'all', 'of', \"Netflix's\", 'other', 'hit', 'shows,', 'one', 'miss', 'like', 'the', 'get', 'down', \"shouldn't\", 'hurt', 'their', 'overhead', 'much', 'at', 'all.', '', 'They', 'are', 'still', 'hitting', 'it', 'out', 'of', 'the', 'park', 'with', 'shows', 'like', 'Luke', 'Cage', 'and', 'Stranger', 'Things,', 'as', 'well', 'as', 'a', 'lot', 'of', 'their', 'other', 'original', 'shows', 'and', 'the', 'ones', 'they', 'currently', 'got', 'in', 'development.', '', 'The', 'Get', 'Down', 'will', 'barely', 'end', 'up', 'as', 'a', 'hiccup', 'in', 'what', 'has', 'become', \"Netflix's\", 'original', 'show', 'money', 'making', 'machine.', '', '\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['agr', 'lot', 'crit', 'pilot', 'show', 'lot', 'prom', 'show', 'went', 'hil', 'show', 'much', 'pot', 'acc', 'portray', 'origin', 'hip', 'hop', 'cult', '70s', 'nyc', 'real', 'influ', 'gen', 'chang', 'landscap', 'mus', 'industry', 'gen', '\\n\\nbut', 'sad', 'didnt', 'portray', 'real', 'wasnt', 'lik', 'someon', 'els', 'said', 'raw', 'real', 'enough', 'dont', 'get', 'start', 'whol', 'rap', 'scen', 'les', 'lesgold', 'forc', 'jacky', 'moreno', 'go', 'coerc', 'giv', 'blowjob', 'fir', 'way', 'back', 'intern', 'real', 'bs', 'complet', 'unr', 'liv', 'er', 'wom', 'got', 'us', 'sex', 'exploit', 'tim', 'mus', 'industry', 'back', 'unoff', 'deeply', 'repuls', 'stat', 'quo', 'ord', 'wom', 'get', 'ahead', 'pardon', 'pun', 'giv', 'head', 'much', 'didnt', 'play', 'gam', 'got', 'black', 'list', 'car', '\\n\\nonce', 'wom', 'gain', 'stardom', 'diff', 'story', 'ev', 'mal', 'pow', 'play', 'day', 'ran', 'show', 'word', 'law', 'back', 'wom', 'would', 'dar', 'pul', 'reveng', 'crap', 'lik', 'les', 'mal', 'pow', 'play', 'would', 'ev', 'allow', 'wom', 'kind', 'pow', 'sorry', 'lady', 'rap', 'reveng', 'lik', 'les', 'howev', 'empow', 'entertain', 'pract', 'due', 'cult', 'tim', 'would', 'nev', 'hap', 'yeah', 'big', 'bs', 'exclam', 'point', 'end', 'show', 'ridic', 'amount', 'money', 'netflix', 'spent', 'get', 'deserv', 'get', 'cancel', 'overal', 'though', 'netflix', 'hit', 'show', 'on', 'miss', 'lik', 'get', 'shouldnt', 'hurt', 'overhead', 'much', 'stil', 'hit', 'park', 'show', 'lik', 'luk', 'cag', 'stranger', 'thing', 'wel', 'lot', 'origin', 'show', 'on', 'cur', 'got', 'develop', 'get', 'bar', 'end', 'hiccup', 'becom', 'netflix', 'origin', 'show', 'money', 'mak', 'machin', '\\n'], ['agree', 'lot', 'critique', 'pilot', 'show', 'lot', 'promise', 'show', 'go', 'hill', 'show', 'much', 'potential', 'accurate', 'portrayal', 'origins', 'hip', 'hop', 'culture', '70s', 'nyc', 'really', 'influence', 'generation', 'change', 'landscape', 'music', 'industry', 'general', '\\n\\nbut', 'sadly', 'didnt', 'portray', 'realistically', 'wasnt', 'like', 'someone', 'else', 'say', 'raw', 'real', 'enough', 'dont', 'get', 'start', 'whole', 'rape', 'scene', 'leslie', 'lesgold', 'force', 'jackie', 'moreno', 'go', 'coerce', 'give', 'blowjob', 'fire', 'way', 'back', 'intern', 'really', 'bs', 'completely', 'unrealistic', 'live', 'era', 'women', 'get', 'use', 'sexually', 'exploit', 'time', 'music', 'industry', 'back', 'unofficial', 'deeply', 'repulsive', 'status', 'quo', 'order', 'woman', 'get', 'ahead', 'pardon', 'pun', 'give', 'head', 'much', 'didnt', 'play', 'game', 'get', 'black', 'list', 'career', '\\n\\nonce', 'woman', 'gain', 'stardom', 'different', 'story', 'even', 'male', 'power', 'players', 'day', 'run', 'show', 'word', 'law', 'back', 'woman', 'would', 'dare', 'pull', 'revenge', 'crap', 'like', 'leslie', 'male', 'power', 'player', 'would', 'ever', 'allow', 'woman', 'kind', 'power', 'sorry', 'ladies', 'rape', 'revenge', 'like', 'leslie', 'however', 'empower', 'entertain', 'practical', 'due', 'culture', 'time', 'would', 'never', 'happen', 'yeah', 'big', 'bs', 'exclamation', 'point', 'end', 'show', 'ridiculous', 'amount', 'money', 'netflix', 'spend', 'get', 'deserve', 'get', 'cancel', 'overall', 'though', 'netflixs', 'hit', 'show', 'one', 'miss', 'like', 'get', 'shouldnt', 'hurt', 'overhead', 'much', 'still', 'hit', 'park', 'show', 'like', 'luke', 'cage', 'stranger', 'things', 'well', 'lot', 'original', 'show', 'ones', 'currently', 'get', 'development', 'get', 'barely', 'end', 'hiccup', 'become', 'netflixs', 'original', 'show', 'money', 'make', 'machine', '\\n'])\n",
      "original document: \n",
      "[\"He's\", 'funny', 'but', 'man', 'from', 'everything', \"I've\", 'seen', 'this', 'guy', 'is', 'an', 'insufferable', 'douche']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'funny', 'man', 'everyth', 'iv', 'seen', 'guy', 'insuff', 'douch'], ['hes', 'funny', 'man', 'everything', 'ive', 'see', 'guy', 'insufferable', 'douche'])\n",
      "original document: \n",
      "[\"He's\", 'black,', 'even', 'in', 'daylight', \"couldn't\", 'spot', 'the', 'fucker.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hes', 'black', 'ev', 'daylight', 'couldnt', 'spot', 'fuck'], ['hes', 'black', 'even', 'daylight', 'couldnt', 'spot', 'fucker'])\n",
      "original document: \n",
      "['God', 'dammit']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'dammit'], ['god', 'dammit'])\n",
      "original document: \n",
      "['Happy', 'Bottle!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['happy', 'bottl'], ['happy', 'bottle'])\n",
      "original document: \n",
      "['**SHOWTIME**']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['showtim'], ['showtime'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "[\"Ishizu's\", 'scream', 'in', 'Japanese', 'freaked', 'me', 'out.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ishiz', 'scream', 'japanes', 'freak'], ['ishizus', 'scream', 'japanese', 'freak'])\n",
      "original document: \n",
      "['already', 'got', 'piper.', 'im', 'very', 'unlucky', 'with', 'brawl', 'boxes.', 'i', 'got', 'mortis', 'at', 'level', '10,', 'though.', 'i', 'have', 'piper', 'at', '42', 'trophies', 'and', '3', 'upgrades,', 'aiming', 'to', 'get', '6', 'upgrades', 'by', 'the', 'end', 'of', 'today']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['already', 'got', 'pip', 'im', 'unlucky', 'brawl', 'box', 'got', 'mort', 'level', 'ten', 'though', 'pip', 'forty-two', 'troph', 'three', 'upgrad', 'aim', 'get', 'six', 'upgrad', 'end', 'today'], ['already', 'get', 'piper', 'im', 'unlucky', 'brawl', 'box', 'get', 'mortis', 'level', 'ten', 'though', 'piper', 'forty-two', 'trophies', 'three', 'upgrade', 'aim', 'get', 'six', 'upgrade', 'end', 'today'])\n",
      "original document: \n",
      "['I', 'needed', 'to', 'see', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'see'], ['need', 'see'])\n",
      "original document: \n",
      "['I', \"don't\", 'have', 'black', 'rlcs']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'black', 'rlcs'], ['dont', 'black', 'rlcs'])\n",
      "original document: \n",
      "[\"Isn't\", 'there', 'some', 'saying', 'about', 'just', 'renting', 'their', 'food?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['isnt', 'say', 'rent', 'food'], ['isnt', 'say', 'rent', 'food'])\n",
      "original document: \n",
      "['\"no\"\\n\\n\"wrong\"\\n\\n\"puppet\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['no\\n\\nwrong\\n\\npuppet'], ['no\\n\\nwrong\\n\\npuppet'])\n",
      "original document: \n",
      "['Are', 'these', 'posts', 'allowed?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'allow'], ['post', 'allow'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnop6dj/):\\n\\nGo', 'through', 'CreateSpace', 'and', 'Amazon.', \"That's\", 'what', 'I', 'did!', 'You', 'just', 'have', 'to', 'buy', 'the', 'ISBN', 'number']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop6dj\\n\\ngo', 'createspac', 'amazon', 'that', 'buy', 'isbn', 'numb'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnop6dj\\n\\ngo', 'createspace', 'amazon', 'thats', 'buy', 'isbn', 'number'])\n",
      "original document: \n",
      "[\"I've\", 'never', 'heard', 'of', 'anyone', 'saying', 'that.', 'Do', 'you', 'have', 'a', 'link', 'to', 'who', 'said', 'it', 'happened', 'to', 'them?', 'With', 'that', 'as', 'well', 'this', 'would', 'be', 'a', 'very', 'hard', 'thing', 'to', 'prove/collect', 'data', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'nev', 'heard', 'anyon', 'say', 'link', 'said', 'hap', 'wel', 'would', 'hard', 'thing', 'provecollect', 'dat'], ['ive', 'never', 'hear', 'anyone', 'say', 'link', 'say', 'happen', 'well', 'would', 'hard', 'thing', 'provecollect', 'data'])\n",
      "original document: \n",
      "['&gt;And', 'yeah,', 'if', 'I', 'walk', 'into', 'my', \"school's\", 'libertarian', 'club', 'meetings,', 'or', 'even', 'conservative', 'meetings,', 'and', 'say', '\"hey', 'guys', 'let', 'me', 'tell', 'you', \"what's\", 'great', 'about', 'the', 'other', 'side,\"', \"they'll\", 'be', 'happy', 'to', 'spend', 'forever', 'and', 'a', 'day', 'telling', 'me', 'why', \"I'm\", 'wrong.', 'I', \"can't\", 'say', 'the', 'same', 'about', 'the', 'left\\n\\nthen', \"you're\", 'clearly', 'doing', 'it', 'wrong,', 'because', 'the', 'biggest', 'complaint', 'about', 'leftists', 'amongst', 'leftists', 'is', 'that', 'we', 'spend', 'too', 'much', 'time', 'yelling', 'at', 'each', 'other', 'about', 'how', 'the', 'other', 'person', 'is', 'doing', 'leftism', 'wrong\\n\\ni', 'dunno', 'what', 'you', 'want', 'me', 'to', 'say', '-', 'none', 'of', 'what', \"you're\", 'saying', 'here', 'aligns', 'with', 'the', 'real', 'world.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtand', 'yeah', 'walk', 'schools', 'libert', 'club', 'meet', 'ev', 'conserv', 'meet', 'say', 'hey', 'guy', 'let', 'tel', 'what', 'gre', 'sid', 'theyl', 'happy', 'spend', 'forev', 'day', 'tel', 'im', 'wrong', 'cant', 'say', 'left\\n\\nthen', 'yo', 'clear', 'wrong', 'biggest', 'complaint', 'left', 'amongst', 'left', 'spend', 'much', 'tim', 'yel', 'person', 'left', 'wrong\\n\\ni', 'dunno', 'want', 'say', 'non', 'yo', 'say', 'align', 'real', 'world'], ['gtand', 'yeah', 'walk', 'school', 'libertarian', 'club', 'meet', 'even', 'conservative', 'meet', 'say', 'hey', 'guy', 'let', 'tell', 'whats', 'great', 'side', 'theyll', 'happy', 'spend', 'forever', 'day', 'tell', 'im', 'wrong', 'cant', 'say', 'left\\n\\nthen', 'youre', 'clearly', 'wrong', 'biggest', 'complaint', 'leftists', 'amongst', 'leftists', 'spend', 'much', 'time', 'yell', 'person', 'leftism', 'wrong\\n\\ni', 'dunno', 'want', 'say', 'none', 'youre', 'say', 'align', 'real', 'world'])\n",
      "original document: \n",
      "['Well', 'sorry', 'for', 'trying.', 'Mr.Akkuron']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'sorry', 'try', 'mrakkuron'], ['well', 'sorry', 'try', 'mrakkuron'])\n",
      "original document: \n",
      "['No']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Among', 'their', '10', 'fans,', '3', 'of', 'them', 'are', 'toxic', 'downvoters']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['among', 'ten', 'fan', 'three', 'tox', 'downvot'], ['among', 'ten', 'fan', 'three', 'toxic', 'downvoters'])\n",
      "original document: \n",
      "['They', \"wouldn't\", 'let', 'me', 'donate', 'platelets', 'today', 'because', 'they', 'had', 'shutdown', 'the', 'login', 'computer', 'already.', 'Lame']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wouldnt', 'let', 'don', 'platelet', 'today', 'shutdown', 'login', 'comput', 'already', 'lam'], ['wouldnt', 'let', 'donate', 'platelets', 'today', 'shutdown', 'login', 'computer', 'already', 'lame'])\n",
      "original document: \n",
      "['Lol.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['First', 'one', 'on', 'the', 'floor?', 'Hope', 'he', 'keeps', 'it', 'up.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'on', 'flo', 'hop', 'keep'], ['first', 'one', 'floor', 'hope', 'keep'])\n",
      "original document: \n",
      "['[deleted]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Can’t', 'protest', 'during', 'the', 'anthem', 'in', 'hockey.', 'Your', 'knees', 'would', 'get', 'cold.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'protest', 'anthem', 'hockey', 'kne', 'would', 'get', 'cold'], ['cant', 'protest', 'anthem', 'hockey', 'knees', 'would', 'get', 'cold'])\n",
      "original document: \n",
      "['Goat.', 'I', 'saw', 'a', 'goat.', 'I', 'need', 'help']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['goat', 'saw', 'goat', 'nee', 'help'], ['goat', 'saw', 'goat', 'need', 'help'])\n",
      "original document: \n",
      "['The', 'ones', 'released', 'in', 'Australia', '(the', 'mini', 'version)', 'seem', 'to', 'be', 'selling', 'on', 'eBay', 'way', 'cheaper', 'than', 'the', 'NES', 'classic', 'was.', 'I', 'missed', 'out', 'on', 'the', 'NES', 'classic', 'mini', 'and', 'the', 'listings', 'were', 'around', '$300-$500', 'each.', 'So', 'obviously', 'I', \"didn't\", 'get', 'one.\\n\\nFor', 'the', 'SNES', 'Classic', 'mini,', 'I', 'missed', 'out', 'on', 'preorder', 'again,', 'went', 'on', 'eBay', 'and', 'they', 'were', 'selling', 'on', 'preorder', 'for', 'around', '$40-$50', 'more', 'than', 'the', 'release', 'price.', 'Which', \"isn't\", '*too*', 'bad,', 'I', 'guess.', 'Maybe', 'no', 'one', 'bought', 'the', 'ridiculously', 'overpriced', 'ones', 'so', 'they', 'had', 'to', 'lower', 'them,', 'who', 'knows.\\n\\nBut', 'that', 'said,', 'I', 'went', 'to', 'the', 'local', 'department', 'store', '6', 'hours', 'after', 'they', 'opened', 'expecting', 'them', 'to', 'have', 'sold', 'out', 'hours', 'earlier,', 'but', 'they', 'still', 'had', 'plenty', 'so', 'I', 'was', 'able', 'to', 'get', 'one', '(they', 'said', 'they', 'received', 'about', '50-60,', 'and', 'they', 'had', 'a', 'line', 'up', 'in', 'the', 'morning', 'but', \"it's\", 'a', 'small', 'town.', 'Some', 'major', 'stores', 'got', 'hundreds', 'and', 'hundreds.', 'When', 'the', 'NES', 'was', 'released', 'they', 'got', '12.', 'And', 'a', 'lineup', 'of', 'furious', 'people', 'yelling', 'at', 'the', 'employees', 'for', 'not', 'having', 'enough)\\n\\nMaybe', 'Nintendo', 'learned', 'after', 'the', 'NES', 'fiasco,', 'but', 'it', 'was', 'still', 'cool', 'to', 'get', 'one', 'and', 'not', 'have', 'to', 'pay', 'stupid', 'amounts', 'for', 'it.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'releas', 'austral', 'min', 'vert', 'seem', 'sel', 'ebay', 'way', 'cheap', 'nes', 'class', 'miss', 'nes', 'class', 'min', 'list', 'around', 'three hundred thousand, five hundred', 'obvy', 'didnt', 'get', 'one\\n\\nfor', 'sne', 'class', 'min', 'miss', 'preord', 'went', 'ebay', 'sel', 'preord', 'around', 'four thousand and fifty', 'releas', 'pric', 'isnt', 'bad', 'guess', 'mayb', 'on', 'bought', 'ridic', 'overpr', 'on', 'low', 'knows\\n\\nbut', 'said', 'went', 'loc', 'depart', 'stor', 'six', 'hour', 'op', 'expect', 'sold', 'hour', 'ear', 'stil', 'plenty', 'abl', 'get', 'on', 'said', 'receiv', 'five thousand and sixty', 'lin', 'morn', 'smal', 'town', 'maj', 'stor', 'got', 'hundr', 'hundr', 'nes', 'releas', 'got', 'twelv', 'lineup', 'fury', 'peopl', 'yel', 'employ', 'enough\\n\\nmaybe', 'nintendo', 'learn', 'nes', 'fiasco', 'stil', 'cool', 'get', 'on', 'pay', 'stupid', 'amount', 'it\\n'], ['ones', 'release', 'australia', 'mini', 'version', 'seem', 'sell', 'ebay', 'way', 'cheaper', 'nes', 'classic', 'miss', 'nes', 'classic', 'mini', 'list', 'around', 'three hundred thousand, five hundred', 'obviously', 'didnt', 'get', 'one\\n\\nfor', 'snes', 'classic', 'mini', 'miss', 'preorder', 'go', 'ebay', 'sell', 'preorder', 'around', 'four thousand and fifty', 'release', 'price', 'isnt', 'bad', 'guess', 'maybe', 'one', 'buy', 'ridiculously', 'overprice', 'ones', 'lower', 'knows\\n\\nbut', 'say', 'go', 'local', 'department', 'store', 'six', 'hours', 'open', 'expect', 'sell', 'hours', 'earlier', 'still', 'plenty', 'able', 'get', 'one', 'say', 'receive', 'five thousand and sixty', 'line', 'morning', 'small', 'town', 'major', 'store', 'get', 'hundreds', 'hundreds', 'nes', 'release', 'get', 'twelve', 'lineup', 'furious', 'people', 'yell', 'employees', 'enough\\n\\nmaybe', 'nintendo', 'learn', 'nes', 'fiasco', 'still', 'cool', 'get', 'one', 'pay', 'stupid', 'amount', 'it\\n'])\n",
      "original document: \n",
      "['He', 'said', 'not', 'complex']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['said', 'complex'], ['say', 'complex'])\n",
      "original document: \n",
      "['&gt;', \"it's\", 'funny', 'that', 'people', 'can', 'be', 'trolled', 'into', 'basically', 'believing', '*anything*', 'is', 'a', 'Nazi', 'symbol\\n\\nYeah,', 'LOL!', '[What', \"could've\", 'given', 'them', '*that*', 'idea?!](https://rationalwiki.org/wiki/Alt-right_glossary#Kekistan)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gt', 'funny', 'peopl', 'trol', 'bas', 'believ', 'anyth', 'naz', 'symbol\\n\\nyeah', 'lol', 'couldv', 'giv', 'ideahttpsrationalwikiorgwikialtright_glossarykekistan'], ['gt', 'funny', 'people', 'troll', 'basically', 'believe', 'anything', 'nazi', 'symbol\\n\\nyeah', 'lol', 'couldve', 'give', 'ideahttpsrationalwikiorgwikialtright_glossarykekistan'])\n",
      "original document: \n",
      "['Turns', 'out', '“the', 'Cave”', 'is', 'what', 'Kit', 'and', 'Rose', 'call', 'their', 'bedroom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turn', 'cav', 'kit', 'ros', 'cal', 'bedroom'], ['turn', 'cave', 'kit', 'rise', 'call', 'bedroom'])\n",
      "original document: \n",
      "['The', 'end', 'of', 'the', 'video', 'had', 'it', 'seem', 'like', 'its', 'the', 'making', 'of', 'a', 'JAV', 'Porno', 'and', 'I', 'was', 'super', 'stoked', 'to', 'see', 'what', 'happened', 'next.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['end', 'video', 'seem', 'lik', 'mak', 'jav', 'porno', 'sup', 'stok', 'see', 'hap', 'next'], ['end', 'video', 'seem', 'like', 'make', 'jav', 'porno', 'super', 'stoke', 'see', 'happen', 'next'])\n",
      "original document: \n",
      "['I', 'fucking', 'love', 'KFC']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'lov', 'kfc'], ['fuck', 'love', 'kfc'])\n",
      "original document: \n",
      "[\"I'm\", 'not', 'sure', \"what's\", 'causing', 'the', 'popping', 'issue', 'although', 'my', 'guess', 'would', 'be', 'disk.', 'Make', 'sure', \"you're\", 'not', 'running', 'anything', 'in', 'the', 'background', 'that', 'could', 'be', 'using', 'your', 'system', 'resources.\\n\\nAlso', 'with', 'a', '1080', 'you', 'should', 'definitely', 'be', 'able', 'to', 'run', 'Ultra-', 'You', 'can', 'hit', 'almost', '60fps', 'on', 'a', '1070']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'sur', 'what', 'caus', 'pop', 'issu', 'although', 'guess', 'would', 'disk', 'mak', 'sur', 'yo', 'run', 'anyth', 'background', 'could', 'us', 'system', 'resources\\n\\nalso', 'one thousand and eighty', 'definit', 'abl', 'run', 'ultr', 'hit', 'almost', '60fps', 'one thousand and seventy'], ['im', 'sure', 'whats', 'cause', 'pop', 'issue', 'although', 'guess', 'would', 'disk', 'make', 'sure', 'youre', 'run', 'anything', 'background', 'could', 'use', 'system', 'resources\\n\\nalso', 'one thousand and eighty', 'definitely', 'able', 'run', 'ultra', 'hit', 'almost', '60fps', 'one thousand and seventy'])\n",
      "original document: \n",
      "[\"I'll\", 'drink', 'to', 'that!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['il', 'drink'], ['ill', 'drink'])\n",
      "original document: \n",
      "[\"it's\", 'not', 'like', 'there', \"aren't\", 'a', 'dozen', 'other', 'cabinet', 'members', 'doing', 'the', 'same', 'thing.', '', 'And', \"trump's\", 'maralago', 'bills', 'are', '3x', \"price's\", 'charges\\n', 'in', 'a', 'weekend.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'ar', 'doz', 'cabinet', 'memb', 'thing', 'trump', 'maralago', 'bil', '3x', 'pric', 'charges\\n', 'weekend'], ['like', 'arent', 'dozen', 'cabinet', 'members', 'thing', 'trump', 'maralago', 'bill', '3x', 'price', 'charges\\n', 'weekend'])\n",
      "original document: \n",
      "['She', 'could', 'have', 'won', 'that,', '1', 'more', 'hit', 'would', 'have', 'been', 'enough', 'but', 'for', 'some', 'reason', 'she', 'turned', 'around', 'for', 'a', 'second']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'on', 'hit', 'would', 'enough', 'reason', 'turn', 'around', 'second'], ['could', 'one', 'hit', 'would', 'enough', 'reason', 'turn', 'around', 'second'])\n",
      "original document: \n",
      "['If', 'you', 'didnt', 'already,', 'try', 'tank-karma.', 'Best', 'thing', 'in', 'the', 'world.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'already', 'try', 'tankkarm', 'best', 'thing', 'world'], ['didnt', 'already', 'try', 'tankkarma', 'best', 'thing', 'world'])\n",
      "original document: \n",
      "[\"It's\", 'only', 'forfiet', 'for', 'threatening', 'or', 'infringing', 'on', 'someones', 'rights.', 'The', 'sort', 'of', 'speech', 'that', 'needs', 'protecting', 'is', 'precisely', 'tge', 'sort', 'that', 'someone', 'is', 'going', 'to', 'find', '\"disrespectful\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['forfiet', 'threatening', 'infr', 'someon', 'right', 'sort', 'speech', 'nee', 'protect', 'prec', 'tge', 'sort', 'someon', 'going', 'find', 'disrespect'], ['forfiet', 'threaten', 'infringe', 'someones', 'right', 'sort', 'speech', 'need', 'protect', 'precisely', 'tge', 'sort', 'someone', 'go', 'find', 'disrespectful'])\n",
      "original document: \n",
      "['Pic?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pic'], ['pic'])\n",
      "original document: \n",
      "['Google', 'search', 'says', 'yes,', 'though', \"I've\", 'never', 'had', 'sprout', 'pods', 'before.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['googl', 'search', 'say', 'ye', 'though', 'iv', 'nev', 'sprout', 'pod'], ['google', 'search', 'say', 'yes', 'though', 'ive', 'never', 'sprout', 'pod'])\n",
      "original document: \n",
      "['He', 'is', 'a', 'rookie', 'too.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['rooky'], ['rookie'])\n",
      "original document: \n",
      "['And', 'tasty', 'for', 'sure', '😋']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tasty', 'sur'], ['tasty', 'sure'])\n",
      "original document: \n",
      "['Trying', 'to', 'overturn', 'the', '1st', 'ammendment.', '\\n\\nAs', 'much', 'as', 'these', 'idiots', 'want', 'things', 'like', 'hate', 'speech', 'laws,', 'if', 'such', 'a', 'law', 'were', 'to', 'get', 'passed,', \"that'd\", 'be', \"it.\\n\\nThat's\", 'what', 'would', 'cause', 'the', '2nd', 'Civil', 'War.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['try', 'overturn', '1st', 'ammend', '\\n\\nas', 'much', 'idiot', 'want', 'thing', 'lik', 'hat', 'speech', 'law', 'law', 'get', 'pass', 'thatd', 'it\\n\\nthats', 'would', 'caus', '2nd', 'civil', 'war'], ['try', 'overturn', '1st', 'ammendment', '\\n\\nas', 'much', 'idiots', 'want', 'things', 'like', 'hate', 'speech', 'laws', 'law', 'get', 'pass', 'thatd', 'it\\n\\nthats', 'would', 'cause', '2nd', 'civil', 'war'])\n",
      "original document: \n",
      "['Apenas', 'o', 'socialismo', 'pode', 'libertar', 'o', 'país', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['apena', 'socialismo', 'pod', 'libert', 'pai'], ['apenas', 'socialismo', 'pode', 'libertar', 'pais'])\n",
      "original document: \n",
      "['Feedback', 'for', 'you', 'guys', 'straight', 'from', 'your', 'neighbouring', 'city', 'Roosendaal!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['feedback', 'guy', 'straight', 'neighbo', 'city', 'roosenda'], ['feedback', 'guy', 'straight', 'neighbour', 'city', 'roosendaal'])\n",
      "original document: \n",
      "['Can', 'you', 'post', 'the', 'original', 'picture', 'without', 'the', 'desktop', 'shortcuts?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['post', 'origin', 'pict', 'without', 'desktop', 'shortcut'], ['post', 'original', 'picture', 'without', 'desktop', 'shortcuts'])\n",
      "original document: \n",
      "['http://www.ishtar-collective.net/categories/books-of-sorrow\\n\\nAt', 'the', 'bottom', 'listed', 'in', 'order']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpwwwishtarcollectivenetcategoriesbooksofsorrow\\n\\nat', 'bottom', 'list', 'ord'], ['httpwwwishtarcollectivenetcategoriesbooksofsorrow\\n\\nat', 'bottom', 'list', 'order'])\n",
      "original document: \n",
      "['As', 'a', 'new', 'dad,', 'this', 'hit', 'me', 'right', 'in', 'the', 'feels.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['new', 'dad', 'hit', 'right', 'feel'], ['new', 'dad', 'hit', 'right', 'feel'])\n",
      "original document: \n",
      "['I', 'voted,', 'but', 'better', 'not', 'to', 'vote', 'than', 'to', 'vote', 'for', 'him']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['vot', 'bet', 'vot', 'vot'], ['vote', 'better', 'vote', 'vote'])\n",
      "original document: \n",
      "['143412679|', '&gt;', 'None', 'Anonymous', '(ID:', 'eG6B9PIj)\\n\\n&gt;&gt;143412250', '(OP)\\n2008:', \"didn't\", 'vote\\n2012:', 'Ron', 'Paul\\n2016:', \"didn't\", 'vote\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and twelve thousand, six hundred and seventy-nin', 'gt', 'non', 'anonym', 'id', 'eg6b9pij\\n\\ngtgt143412250', 'op\\n2008', 'didnt', 'vote\\n2012', 'ron', 'paul\\n2016', 'didnt', 'vote\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and twelve thousand, six hundred and seventy-nine', 'gt', 'none', 'anonymous', 'id', 'eg6b9pij\\n\\ngtgt143412250', 'op\\n2008', 'didnt', 'vote\\n2012', 'ron', 'paul\\n2016', 'didnt', 'vote\\n\\t\\t\\t'])\n",
      "original document: \n",
      "[\"It's\", 'a', 'walking', 'mop', 'too.', 'I', 'always', 'think', 'poodles', 'are', 'best', 'for', 'not', 'shedding,', 'but', 'after', 'boarding', 's', 'couple,', 'they', 'really', 'hang', 'onto', 'scents', 'and', 'dirt', 'after', 'just', 'a', 'couple', 'days', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['walk', 'mop', 'alway', 'think', 'poodl', 'best', 'shed', 'board', 'coupl', 'real', 'hang', 'onto', 'scent', 'dirt', 'coupl', 'day'], ['walk', 'mop', 'always', 'think', 'poodles', 'best', 'shed', 'board', 'couple', 'really', 'hang', 'onto', 'scent', 'dirt', 'couple', 'days'])\n",
      "original document: \n",
      "['That', 'factors', 'in,', 'but', \"it's\", 'the', 'opposite', 'side', 'of', 'the', 'same', 'coin', 'that', 'causes', '(Or', 'at', 'least', 'used', 'to)', 'her', 'to', 'get', 'a', 'ton', 'of', 'upvotes', 'on', 'every', 'post', 'with', 'images.\\n\\nShitty', 'treatment', 'is', 'shitty,', 'her', 'builds', 'getting', 'upvoted', 'past', 'other', 'more', 'imaganitive', '/', 'better', 'builds', 'is', 'also', 'shitty.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fact', 'opposit', 'sid', 'coin', 'caus', 'least', 'us', 'get', 'ton', 'upvot', 'every', 'post', 'images\\n\\nshitty', 'tre', 'shitty', 'build', 'get', 'upvot', 'past', 'imaganit', 'bet', 'build', 'also', 'shitty'], ['factor', 'opposite', 'side', 'coin', 'cause', 'least', 'use', 'get', 'ton', 'upvotes', 'every', 'post', 'images\\n\\nshitty', 'treatment', 'shitty', 'build', 'get', 'upvoted', 'past', 'imaganitive', 'better', 'build', 'also', 'shitty'])\n",
      "original document: \n",
      "['&gt;To', 'stop', 'Catalonia’s', 'self-determination,', 'Madrid', 'has', 'gone', 'so', 'far', 'as', 'to', 'suspend', 'its', 'autonomy', 'and', 'freeze', 'all', 'its', 'financial', 'assets.\\n\\nkinda', 'like', 'when', 'Abe', 'Lincoln', 'suspended', 'the', 'right', 'of', 'habeus', 'corpus', 'and', 'locked', 'up', 'the', 'Maryland', 'legislature', 'specifically', 'to', 'stop', 'them', 'from', 'voting', 'to', 'secede', 'from', 'the', 'union?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gtto', 'stop', 'catalonia', 'selfdetermin', 'madrid', 'gon', 'far', 'suspend', 'autonom', 'freez', 'fin', 'assets\\n\\nkinda', 'lik', 'ab', 'lincoln', 'suspend', 'right', 'habe', 'corp', 'lock', 'maryland', 'legisl', 'spec', 'stop', 'vot', 'sec', 'un'], ['gtto', 'stop', 'catalonias', 'selfdetermination', 'madrid', 'go', 'far', 'suspend', 'autonomy', 'freeze', 'financial', 'assets\\n\\nkinda', 'like', 'abe', 'lincoln', 'suspend', 'right', 'habeus', 'corpus', 'lock', 'maryland', 'legislature', 'specifically', 'stop', 'vote', 'secede', 'union'])\n",
      "original document: \n",
      "['[Original', 'post](https://www.reddit.com/r/60fpsporn/comments/73i1ob/knees_together/)', '-', '[Subreddit](https://www.reddit.com/r/60fpsporn)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['origin', 'posthttpswwwredditcomr60fpsporncomments73i1obknees_togeth', 'subreddithttpswwwredditcomr60fpsporn'], ['original', 'posthttpswwwredditcomr60fpsporncomments73i1obknees_together', 'subreddithttpswwwredditcomr60fpsporn'])\n",
      "original document: \n",
      "['omg', 'lol', 'i', 'laughed', 'so', 'hard', 'at', 'your', 'title.', 'def', 'my', 'kinda', 'lesbo']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['omg', 'lol', 'laugh', 'hard', 'titl', 'def', 'kind', 'lesbo'], ['omg', 'lol', 'laugh', 'hard', 'title', 'def', 'kinda', 'lesbo'])\n",
      "original document: \n",
      "['You', 'can', 'also', 'use', 'nickel', 'and', 'dimes', 'to', 'buy', 'Hippy', 'Army', 'MPE', 'and', 'Frat', 'Army', 'FGF.', '', 'I', 'may', 'have', 'actually', 'done', 'that', 'back', 'before', 'I', 'stockpiled', 'the', 'foods.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['also', 'us', 'nickel', 'dim', 'buy', 'hippy', 'army', 'mpe', 'frat', 'army', 'fgf', 'may', 'act', 'don', 'back', 'stockpil', 'food'], ['also', 'use', 'nickel', 'dim', 'buy', 'hippy', 'army', 'mpe', 'frat', 'army', 'fgf', 'may', 'actually', 'do', 'back', 'stockpile', 'foods'])\n",
      "original document: \n",
      "['The', 'only', 'writer', 'to', 'try', 'to', 'write', 'a', 'utopian', 'culture', 'that', 'works.', 'These', 'are', 'wonderful,', 'fantastic', 'books.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['writ', 'try', 'writ', 'utop', 'cult', 'work', 'wond', 'fantast', 'book'], ['writer', 'try', 'write', 'utopian', 'culture', 'work', 'wonderful', 'fantastic', 'book'])\n",
      "original document: \n",
      "['Without', 'EDA', 'bch', 'would', 'already', 'be', 'gone.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['without', 'ed', 'bch', 'would', 'already', 'gon'], ['without', 'eda', 'bch', 'would', 'already', 'go'])\n",
      "original document: \n",
      "['I', 'laughed', 'at', 'that', 'one,', 'as', 'well.\\n\\n\"Wait,', 'you', 'can', 'change', 'stuff', 'at', 'halftime?\"', '\\n\\n-Dan', 'Mullen']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['laugh', 'on', 'well\\n\\nwait', 'chang', 'stuff', 'halftim', '\\n\\ndan', 'mul'], ['laugh', 'one', 'well\\n\\nwait', 'change', 'stuff', 'halftime', '\\n\\ndan', 'mullen'])\n",
      "original document: \n",
      "['https://www.youtube.com/watch?v=IfXMN3VhikA']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpswwwyoutubecomwatchvifxmn3vhika'], ['httpswwwyoutubecomwatchvifxmn3vhika'])\n",
      "original document: \n",
      "['Oooh,', \"y'all\", 'got', '$1.74', 'billion', 'today', 'I', 'just', 'heard!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oooh', 'yal', 'got', 'one hundred and seventy-four', 'bil', 'today', 'heard'], ['oooh', 'yall', 'get', 'one hundred and seventy-four', 'billion', 'today', 'hear'])\n",
      "original document: \n",
      "['Man', 'you', 'aren’t', 'even', 'op', 'and', 'your', 'responding', 'to', 'everyone', 'great', 'work!!']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['man', 'ar', 'ev', 'op', 'respond', 'everyon', 'gre', 'work'], ['man', 'arent', 'even', 'op', 'respond', 'everyone', 'great', 'work'])\n",
      "original document: \n",
      "['The', 'Amity', 'Affliction']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['am', 'afflict'], ['amity', 'affliction'])\n",
      "original document: \n",
      "['U', '2']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['u', 'two'], ['u', 'two'])\n",
      "original document: \n",
      "['Yup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup'], ['yup'])\n",
      "original document: \n",
      "['Lol.', 'Making', 'me', 'feel', 'old.', 'Back', 'in', 'the', 'day', 'when', 'we', 'still', 'used', 'film', 'in', 'cameras,', 'we', 'had', 'to', 'keep', 'them', 'safe', 'when', 'in', 'the', 'camera', 'and', 'out', 'of', 'it.', 'So', 'we', 'put', 'them', 'in', 'film', 'canisters.\\nhttps://www.bhphotovideo.com/images/images2500x2500/General_Brand_Plastic_Film_Canisters_With_263775.jpg']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'mak', 'feel', 'old', 'back', 'day', 'stil', 'us', 'film', 'camera', 'keep', 'saf', 'camer', 'put', 'film', 'canisters\\nhttpswwwbhphotovideocomimagesimages2500x2500general_brand_plastic_film_canisters_with_263775jpg'], ['lol', 'make', 'feel', 'old', 'back', 'day', 'still', 'use', 'film', 'cameras', 'keep', 'safe', 'camera', 'put', 'film', 'canisters\\nhttpswwwbhphotovideocomimagesimages2500x2500general_brand_plastic_film_canisters_with_263775jpg'])\n",
      "original document: \n",
      "['Unless', \"you've\", 'cycled', 'on', 'it,', \"it's\", 'hard', 'to', 'say', 'whether', 'or', 'not', 'it', 'is', 'perfectly', 'good.', 'There', 'was', 'one', 'I', 'used', 'to', 'ride', 'on', 'which', 'I', 'gave', 'up', 'on', 'after', 'having', 'several', 'punctures.', 'The', 'ones', 'adjacent', 'to', 'roads', 'tend', 'to', 'end', 'up', 'accumulating', 'broken', 'glass', 'and', 'other', 'debris', 'which', 'is', 'brushed', 'onto', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['unless', 'youv', 'cyc', 'hard', 'say', 'wheth', 'perfect', 'good', 'on', 'us', 'rid', 'gav', 'sev', 'punct', 'on', 'adjac', 'road', 'tend', 'end', 'accum', 'brok', 'glass', 'debr', 'brush', 'onto'], ['unless', 'youve', 'cycle', 'hard', 'say', 'whether', 'perfectly', 'good', 'one', 'use', 'ride', 'give', 'several', 'puncture', 'ones', 'adjacent', 'roads', 'tend', 'end', 'accumulate', 'break', 'glass', 'debris', 'brush', 'onto'])\n",
      "original document: \n",
      "['Congratulations,', 'your', 'skill', 'attempt', 'was', 'successful!\\n\\nPlease', 'make', 'sure', 'to', 'edit', 'your', 'original', 'bio', 'with', 'a', 'note', 'that', 'states', 'that', \"you've\", 'learned', 'this', 'skill.', 'Thanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['congrat', 'skil', 'attempt', 'successful\\n\\nplease', 'mak', 'sur', 'edit', 'origin', 'bio', 'not', 'stat', 'youv', 'learn', 'skil', 'thank'], ['congratulations', 'skill', 'attempt', 'successful\\n\\nplease', 'make', 'sure', 'edit', 'original', 'bio', 'note', 'state', 'youve', 'learn', 'skill', 'thank'])\n",
      "original document: \n",
      "['yep']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep'], ['yep'])\n",
      "original document: \n",
      "['Stop', \"you're\", 'stressing', 'me', 'out']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'yo', 'stressing'], ['stop', 'youre', 'stress'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Could', 'have', 'wished', 'that', 'was', 'real.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['could', 'wish', 'real'], ['could', 'wish', 'real'])\n",
      "original document: \n",
      "['This', 'submission', 'was', 'automatically', 'removed', 'because', 'it', 'did', 'not', 'have', 'the', 'correct', 'title', 'format.\\n\\nTitles', 'need', 'to', 'follow', 'the', 'format:', '**Gender/Age/Height', '[Weight', 'Before', '&gt;', 'Weight', 'After', '=', 'Total', 'Amount', 'Lost]', 'Personal', 'title**\\n\\nFor', 'example:\\n\\n**F/23/5\\'5\"', '[189lbs', '&gt;', '169lbs', '=', '20lbs]', 'Weight', 'loss', 'progress**', '', '\\n\\n-', 'Gender', 'must', 'be', '**ONE', 'letter**', '\"M\",', '\"F\",', '\"T\",', 'etc.', '', 'Do', 'Not', 'write', 'out', '\"Male\"', '', '\\n-', 'Do', 'not', 'add', 'extra', 'spaces.\\n-', 'Do', 'not', 'use', 'the', '\"~\"', 'character\\n-', 'Use', 'Brackets', '[', '],', '', '', '', 'DO', 'NOT', 'USE', '(', ')', 'Parentheses', '\\n-', 'For', 'best', 'results,', 'copy/paste', 'the', 'above', 'example,', 'and', 'replace', 'the', 'information', 'with', 'your', 'own.', '', '\\n\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/progresspics)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'autom', 'remov', 'correct', 'titl', 'format\\n\\ntitles', 'nee', 'follow', 'form', 'genderageheight', 'weight', 'gt', 'weight', 'tot', 'amount', 'lost', 'person', 'title\\n\\nfor', 'example\\n\\nf2355', '189lbs', 'gt', '169lbs', '20lbs', 'weight', 'loss', 'progress', '\\n\\n', 'gend', 'must', 'on', 'let', 'f', 'etc', 'writ', 'mal', '\\n', 'ad', 'extr', 'spaces\\n', 'us', 'character\\n', 'us', 'bracket', 'us', 'parenthes', '\\n', 'best', 'result', 'copypast', 'exampl', 'replac', 'inform', '\\n\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorprogressp', 'quest', 'concern'], ['submission', 'automatically', 'remove', 'correct', 'title', 'format\\n\\ntitles', 'need', 'follow', 'format', 'genderageheight', 'weight', 'gt', 'weight', 'total', 'amount', 'lose', 'personal', 'title\\n\\nfor', 'example\\n\\nf2355', '189lbs', 'gt', '169lbs', '20lbs', 'weight', 'loss', 'progress', '\\n\\n', 'gender', 'must', 'one', 'letter', 'f', 'etc', 'write', 'male', '\\n', 'add', 'extra', 'spaces\\n', 'use', 'character\\n', 'use', 'bracket', 'use', 'parentheses', '\\n', 'best', 'result', 'copypaste', 'example', 'replace', 'information', '\\n\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorprogresspics', 'question', 'concern'])\n",
      "original document: \n",
      "['Why', 'would', 'Gohan', 'suddenly', 'know', 'Kaioken?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'goh', 'sud', 'know', 'kaiok'], ['would', 'gohan', 'suddenly', 'know', 'kaioken'])\n",
      "original document: \n",
      "['They', 'turned', 'them', 'down', 'but', 'it', 'was', 'not', 'with', 'a', '\"We\\'re', 'not', 'interested', 'in', 'selling.\"', 'reason', 'it', 'was', 'with', 'a', '\"The', 'premium', 'is', 'too', 'small.\"', '', 'Which', 'translates', 'to', '\"Come', 'up', 'with', 'more', 'money', 'and', 'we', 'will', 'listen.\"\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['turn', 'interest', 'sel', 'reason', 'prem', 'smal', 'transl', 'com', 'money', 'listen\\n'], ['turn', 'interest', 'sell', 'reason', 'premium', 'small', 'translate', 'come', 'money', 'listen\\n'])\n",
      "original document: \n",
      "['I', \"can't\", 'say', 'as', 'though', 'I', 'would', 'be', 'particularly', 'interested', 'in', 'going', 'to', 'my', \"ex's\", 'wedding', 'invited.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'say', 'though', 'would', 'particul', 'interest', 'going', 'ex', 'wed', 'invit'], ['cant', 'say', 'though', 'would', 'particularly', 'interest', 'go', 'exs', 'wed', 'invite'])\n",
      "original document: \n",
      "['Some', 'peopke', 'honestly', \"can't\", 'handle', 'that', 'truth', ',', 'use', 'to', 'though', 'it', 'was', 'not', 'so', 'easy', 'to', 'self', 'publish', '(well', 'she', 'has', 'spent', 'thousands', 'of', 'dollars)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopk', 'honest', 'cant', 'handl', 'tru', 'us', 'though', 'easy', 'self', 'publ', 'wel', 'spent', 'thousand', 'doll'], ['peopke', 'honestly', 'cant', 'handle', 'truth', 'use', 'though', 'easy', 'self', 'publish', 'well', 'spend', 'thousands', 'dollars'])\n",
      "original document: \n",
      "['I', 'too', 'would', 'like', 'to', 'know', \"what's\", 'the', 'other', 'guy', 'is', 'asking', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'lik', 'know', 'what', 'guy', 'ask'], ['would', 'like', 'know', 'whats', 'guy', 'ask'])\n",
      "original document: \n",
      "['Hey', 'fuck', 'you,', 'he', 'was', 'ours', 'first', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hey', 'fuck', 'first'], ['hey', 'fuck', 'first'])\n",
      "original document: \n",
      "['I', 'saw', 'Young', 'Sheldon,', 'it', 'was', 'total', 'garbage,', 'and', \"I'v\", 'been', 'a', 'Big', 'Bang', 'Theory', 'fan', 'for', 'years', 'now--little', 'kid', 'is', 'an', 'insulting', 'ass,and', 'the', 'supporting', 'cast', 'is', 'woefully', 'anemic']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['saw', 'young', 'sheldon', 'tot', 'garb', 'iv', 'big', 'bang', 'the', 'fan', 'year', 'nowlittl', 'kid', 'insult', 'assand', 'support', 'cast', 'woe', 'anem'], ['saw', 'young', 'sheldon', 'total', 'garbage', 'iv', 'big', 'bang', 'theory', 'fan', 'years', 'nowlittle', 'kid', 'insult', 'assand', 'support', 'cast', 'woefully', 'anemic'])\n",
      "original document: \n",
      "['This', 'is', 'so', 'true.', 'Also,', 'makes', 'me', 'happy', 'to', 'have', 'pulled', 'a', 'single', 'Reinhardt', 'in', 'my', '5', 'months', 'of', 'play.', '4★,', 'but', '+atk', 'and', 'upgraded', 'him', 'to', '5★.', 'Now', 'he', 'carries', 'me!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tru', 'also', 'mak', 'happy', 'pul', 'singl', 'reinhardt', 'fiv', 'month', 'play', 'four', 'atk', 'upgrad', 'fiv', 'carry'], ['true', 'also', 'make', 'happy', 'pull', 'single', 'reinhardt', 'five', 'months', 'play', 'four', 'atk', 'upgrade', 'five', 'carry'])\n",
      "original document: \n",
      "['Oh', 'boy', 'I', 'went', 'to', 'read', 'a', 'bit', 'that', 'subreddit', 'to', 'see', 'how', 'bad', 'was', 'it', 'and', 'I', 'was', 'not', 'dissapointed', '!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'boy', 'went', 'read', 'bit', 'subreddit', 'see', 'bad', 'dissapoint'], ['oh', 'boy', 'go', 'read', 'bite', 'subreddit', 'see', 'bad', 'dissapointed'])\n",
      "original document: \n",
      "['Hoeneß', 'and', 'his', 'idea', 'of', 'who', 'to', 'put', 'in', 'charge', 'of', 'certain', 'positions', 'is', 'unprofessional.', \"I'm\", 'all', 'for', 'ex-players', 'remaining', 'with', 'the', 'club,', 'but', 'if', 'they', 'occupy', 'an', 'important', 'position,', 'they', 'have', 'to', 'be', 'somewhat', 'qualified.', 'The', 'people', 'we', 'have', 'lost', 'and', 'not', 'adequately', 'replaced', 'in', 'the', 'last', 'year', 'or', 'so', 'is', 'disappointing.', '\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hoen', 'ide', 'put', 'charg', 'certain', 'posit', 'unprofess', 'im', 'explay', 'remain', 'club', 'occupy', 'import', 'posit', 'somewh', 'qual', 'peopl', 'lost', 'adequ', 'replac', 'last', 'year', 'disappoint', '\\n\\n'], ['hoene', 'idea', 'put', 'charge', 'certain', 'position', 'unprofessional', 'im', 'explayers', 'remain', 'club', 'occupy', 'important', 'position', 'somewhat', 'qualify', 'people', 'lose', 'adequately', 'replace', 'last', 'year', 'disappoint', '\\n\\n'])\n",
      "original document: \n",
      "['Username', 'checks', 'out.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['usernam', 'check'], ['username', 'check'])\n",
      "original document: \n",
      "['You', 'think', 'Trump', 'will', 'be', 'charged', 'with', 'treason', 'and', \"I'm\", 'the', 'stupid', 'one....']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'trump', 'charg', 'treason', 'im', 'stupid', 'on'], ['think', 'trump', 'charge', 'treason', 'im', 'stupid', 'one'])\n",
      "original document: \n",
      "['[+SharkBaitDLS](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoqth8/):\\n\\nYou', 'actually', 'don’t', 'even', 'need', 'to', 'buy', 'an', 'ISBN', 'if', 'you', 'don’t', 'mind', 'your', 'publisher', 'being', 'listed', 'as', '“Independently', 'Published”.', 'So', 'it’s', 'possible', 'to', 'do', 'with', 'zero', 'cost.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sharkbaitdlshttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoqth8\\n\\nyou', 'act', 'dont', 'ev', 'nee', 'buy', 'isbn', 'dont', 'mind', 'publ', 'list', 'independ', 'publ', 'poss', 'zero', 'cost'], ['sharkbaitdlshttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoqth8\\n\\nyou', 'actually', 'dont', 'even', 'need', 'buy', 'isbn', 'dont', 'mind', 'publisher', 'list', 'independently', 'publish', 'possible', 'zero', 'cost'])\n",
      "original document: \n",
      "['Pics', '4', 'and', '6', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pic', 'four', 'six'], ['pics', 'four', 'six'])\n",
      "original document: \n",
      "['N']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['n'], ['n'])\n",
      "original document: \n",
      "['Sometimes', 'They', 'Come', 'Back']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sometim', 'com', 'back'], ['sometimes', 'come', 'back'])\n",
      "original document: \n",
      "['Currently:\\n\\n•LJ', 'Peretti', 'Park', 'Sq.\\n\\n•OGS\\n\\n•Lane', 'Black', 'Raspberry\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['currently\\n\\nlj', 'perett', 'park', 'sq\\n\\nogs\\n\\nlane', 'black', 'raspberry\\n\\n'], ['currently\\n\\nlj', 'peretti', 'park', 'sq\\n\\nogs\\n\\nlane', 'black', 'raspberry\\n\\n'])\n",
      "original document: \n",
      "['So', 'you', 'have', 'no', 'evidence', 'at', 'all', 'is', 'what', \"you're\", 'saying?', \"You're\", 'just', 'making', 'wild,', 'baseless', 'assumptions', 'that', 'have', 'no', 'grounding', 'in', 'reality,', 'strictly', 'to', 'push', 'a', 'narrative', 'against', 'a', 'politician', 'you', \"don't\", 'like?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['evid', 'yo', 'say', 'yo', 'mak', 'wild', 'baseless', 'assum', 'ground', 'real', 'strictly', 'push', 'nar', 'polit', 'dont', 'lik'], ['evidence', 'youre', 'say', 'youre', 'make', 'wild', 'baseless', 'assumptions', 'ground', 'reality', 'strictly', 'push', 'narrative', 'politician', 'dont', 'like'])\n",
      "original document: \n",
      "['Yeah,', \"I've\", 'got', 'that', 'down.', 'I', 'just', \"don't\", 'really', 'know', 'how', 'you', 'would', 'go', 'about', 'actually', 'making', 'a', 'realistic', 'looking', 'one.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'iv', 'got', 'dont', 'real', 'know', 'would', 'go', 'act', 'mak', 'real', 'look', 'on'], ['yeah', 'ive', 'get', 'dont', 'really', 'know', 'would', 'go', 'actually', 'make', 'realistic', 'look', 'one'])\n",
      "original document: \n",
      "['Read', 'a', 'bumper', 'sticker', 'once', 'that', 'said', '\"dress', 'for', 'the', 'slide', 'not', 'door', 'the', 'ride\"', 'made', 'a', 'lot', 'of', 'sense.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['read', 'bump', 'stick', 'said', 'dress', 'slid', 'door', 'rid', 'mad', 'lot', 'sens'], ['read', 'bumper', 'sticker', 'say', 'dress', 'slide', 'door', 'ride', 'make', 'lot', 'sense'])\n",
      "original document: \n",
      "['Haha', 'whatever', 'gets', 'us', 'through', 'the', 'raid', 'my', 'friend.', '', 'We', 'all', 'have', 'to', 'learn', 'somehow!', 'The', 'amount', 'of', 'times', \"I've\", 'gone', 'flying', 'over', 'those', 'ramps...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hah', 'whatev', 'get', 'us', 'raid', 'friend', 'learn', 'somehow', 'amount', 'tim', 'iv', 'gon', 'fly', 'ramp'], ['haha', 'whatever', 'get', 'us', 'raid', 'friend', 'learn', 'somehow', 'amount', 'time', 'ive', 'go', 'fly', 'ramp'])\n",
      "original document: \n",
      "[\"That's\", 'their', 'placeholder', 'art:', '', 'Its', 'Studio', 'MDHR', 'lead', 'designer', 'and', 'founder', 'Jared', 'Moldenhauer.\\n\\nhttps://twitter.com/tonycoculuzzi/status/665156098610065408\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'placehold', 'art', 'studio', 'mdhr', 'lead', 'design', 'found', 'jar', 'moldenhauer\\n\\nhttpstwittercomtonycoculuzzistatus665156098610065408\\n'], ['thats', 'placeholder', 'art', 'studio', 'mdhr', 'lead', 'designer', 'founder', 'jar', 'moldenhauer\\n\\nhttpstwittercomtonycoculuzzistatus665156098610065408\\n'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['In', 'the', 'US', 'it', 'varies', 'from', 'state', 'to', 'state.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'vary', 'stat', 'stat'], ['us', 'vary', 'state', 'state'])\n",
      "original document: \n",
      "['God', 'forbit', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['god', 'forbit'], ['god', 'forbit'])\n",
      "original document: \n",
      "['Why', 'the', 'long', 'face?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['long', 'fac'], ['long', 'face'])\n",
      "original document: \n",
      "['And', 'more', 'content', 'is', 'ON', 'PTP.', 'Almost', 'twice', 'as', 'much', 'in', 'fact,', 'and', \"that's\", 'without', 'even', 'subtracting', 'non-movie', 'content', 'on', 'HDB.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cont', 'ptp', 'almost', 'twic', 'much', 'fact', 'that', 'without', 'ev', 'subtract', 'nonmovy', 'cont', 'hdb'], ['content', 'ptp', 'almost', 'twice', 'much', 'fact', 'thats', 'without', 'even', 'subtract', 'nonmovie', 'content', 'hdb'])\n",
      "original document: \n",
      "['What', 'I', 'generally', 'do', 'is', 'roll', 'my', 'eyes', 'and', 'scoff', 'under', 'my', 'breath.', 'Not', 'very', 'effective', 'as', 'far', 'as', 'activism', 'goes', 'but', 'it', 'makes', 'me', 'feel', 'better.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gen', 'rol', 'ey', 'scoff', 'brea', 'effect', 'far', 'act', 'goe', 'mak', 'feel', 'bet'], ['generally', 'roll', 'eye', 'scoff', 'breath', 'effective', 'far', 'activism', 'go', 'make', 'feel', 'better'])\n",
      "original document: \n",
      "['Oh', 'sweet.', '', 'That', 'was', 'nice', 'of', 'her.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'sweet', 'nic'], ['oh', 'sweet', 'nice'])\n",
      "original document: \n",
      "['I', \"don't\", 'like', 'either', 'person', 'very', 'much,', 'if', 'I', 'was', 'American', 'I', 'would', 'have', 'voted', 'for', 'Hillary,', 'but', 'even', 'I', 'think', 'Pence', 'would', 'be', 'a', 'better', 'president', 'than', 'Trump.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'lik', 'eith', 'person', 'much', 'am', 'would', 'vot', 'hil', 'ev', 'think', 'pent', 'would', 'bet', 'presid', 'trump'], ['dont', 'like', 'either', 'person', 'much', 'american', 'would', 'vote', 'hillary', 'even', 'think', 'pence', 'would', 'better', 'president', 'trump'])\n",
      "original document: \n",
      "['Why', \"don't\", 'you', 'just', '\"rent\"', 'one', 'and', 'never', 'return', 'it.', '', 'Worst', 'case', 'scenario,', 'they', 'make', 'you', 'pay', 'for', 'it', 'which', 'is', 'what', 'you', 'wanted', 'all', 'along.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'rent', 'on', 'nev', 'return', 'worst', 'cas', 'scenario', 'mak', 'pay', 'want', 'along'], ['dont', 'rent', 'one', 'never', 'return', 'worst', 'case', 'scenario', 'make', 'pay', 'want', 'along'])\n",
      "original document: \n",
      "['Sure', 'is', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur'], ['sure'])\n",
      "original document: \n",
      "['Yup']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yup'], ['yup'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['It’s', 'the', 'least', 'that', 'I', 'can', 'do', 'after', 'all', 'that', 'this', 'sub', 'gave', 'to', 'me', '😂']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['least', 'sub', 'gav'], ['least', 'sub', 'give'])\n",
      "original document: \n",
      "['I', 'honestly', \"don't\", 'know.', \"I've\", 'heard', 'a', 'few', 'names', 'thrown', 'around', 'but', 'none', 'of', 'them', 'seem', 'realistic.', 'Thankfully', \"I'm\", 'not', 'being', 'paid', 'to', 'worry', 'about', 'this', 'problem', 'so', \"I'll\", 'let', 'those', 'who', 'are', 'figure', 'it', 'out.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['honest', 'dont', 'know', 'iv', 'heard', 'nam', 'thrown', 'around', 'non', 'seem', 'real', 'thank', 'im', 'paid', 'worry', 'problem', 'il', 'let', 'fig'], ['honestly', 'dont', 'know', 'ive', 'hear', 'name', 'throw', 'around', 'none', 'seem', 'realistic', 'thankfully', 'im', 'pay', 'worry', 'problem', 'ill', 'let', 'figure'])\n",
      "original document: \n",
      "['https://streamable.com/chgmu\\n\\nIs', 'this', 'your', 'own', 'footage', 'as', 'live?', 'Please', 'if', 'you', 'could', 'download', 'periscope,', 'everyone', 'would', 'love', 'to', 'watch', 'this', 'and', 'you', 'could', 'really', 'change', 'the', 'world', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsstreamablecomchgmu\\n\\nis', 'foot', 'liv', 'pleas', 'could', 'download', 'periscop', 'everyon', 'would', 'lov', 'watch', 'could', 'real', 'chang', 'world'], ['httpsstreamablecomchgmu\\n\\nis', 'footage', 'live', 'please', 'could', 'download', 'periscope', 'everyone', 'would', 'love', 'watch', 'could', 'really', 'change', 'world'])\n",
      "original document: \n",
      "['Son', 'of', 'a', 'trailer', 'park', 'electrician']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['son', 'trail', 'park', 'elect'], ['son', 'trailer', 'park', 'electrician'])\n",
      "original document: \n",
      "['Why']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['Wonderful', 'picture,', 'she', 'looks', 'like', 'a', 'meme']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wond', 'pict', 'look', 'lik', 'mem'], ['wonderful', 'picture', 'look', 'like', 'meme'])\n",
      "original document: \n",
      "['Ya', 'go', 'get', 'swabbed']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya', 'go', 'get', 'swab'], ['ya', 'go', 'get', 'swab'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['It’s', 'so', 'surprising', 'how', 'little', 'caps', 'he', 'has.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['surpr', 'littl', 'cap'], ['surprise', 'little', 'cap'])\n",
      "original document: \n",
      "['And', 'who', 'says', 'the', 'Metro', 'just', \"isn't\", 'classy?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['say', 'metro', 'isnt', 'classy'], ['say', 'metro', 'isnt', 'classy'])\n",
      "original document: \n",
      "['747']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seven hundred and forty-sev'], ['seven hundred and forty-seven'])\n",
      "original document: \n",
      "['Your', 'framing', 'is', 'pretty', 'good', 'for', 'an', 'accidental', 'photo.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fram', 'pretty', 'good', 'accid', 'photo'], ['frame', 'pretty', 'good', 'accidental', 'photo'])\n",
      "original document: \n",
      "['Way', 'to', 'stick', 'with', 'it', 'Ward.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'stick', 'ward'], ['way', 'stick', 'ward'])\n",
      "original document: \n",
      "['Classic', 'D-Frag', 'perfection.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['class', 'dfrag', 'perfect'], ['classic', 'dfrag', 'perfection'])\n",
      "original document: \n",
      "['[Here](https://gist.github.com/GenuineMP5/b65c481fbc3a164f29ade6943f3df82f)', 'is', 'the', 'file.', 'I', 'used', 'the', 'default', 'as', 'a', 'base', 'and', 'modified', 'from', 'there.', 'I', 'used', 'the', 'function', 'that', 'originally', 'moved', 'to', 'the', 'Plover', 'layer', 'as', 'my', 'game', 'function,', 'but', 'I', 'may', 'have', 'accidentally', 'changed', 'something.', 'Everything', 'works', 'except', 'for', 'going', 'to', 'that', 'layer', '(which', 'does', 'work,', 'just', 'only', 'after', 'the', 'second', 'key', 'press).', '\\n\\nThanks!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['herehttpsgistgithubcomgenuinemp5b65c481fbc3a164f29ade6943f3df82f', 'fil', 'us', 'default', 'bas', 'mod', 'us', 'funct', 'origin', 'mov', 'plov', 'lay', 'gam', 'funct', 'may', 'accid', 'chang', 'someth', 'everyth', 'work', 'exceiv', 'going', 'lay', 'work', 'second', 'key', 'press', '\\n\\nthanks'], ['herehttpsgistgithubcomgenuinemp5b65c481fbc3a164f29ade6943f3df82f', 'file', 'use', 'default', 'base', 'modify', 'use', 'function', 'originally', 'move', 'plover', 'layer', 'game', 'function', 'may', 'accidentally', 'change', 'something', 'everything', 'work', 'except', 'go', 'layer', 'work', 'second', 'key', 'press', '\\n\\nthanks'])\n",
      "original document: \n",
      "['8h', 'a', 'day.', '5', 'days', 'a', 'week.', '5', 'days', '(sorry', 'I', 'counted', 'it', 'wrong)', 'a', 'month', 'I', 'have', 'to', 'work', 'the', 'night', 'shift.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['8h', 'day', 'fiv', 'day', 'week', 'fiv', 'day', 'sorry', 'count', 'wrong', 'mon', 'work', 'night', 'shift'], ['8h', 'day', 'five', 'days', 'week', 'five', 'days', 'sorry', 'count', 'wrong', 'month', 'work', 'night', 'shift'])\n",
      "original document: \n",
      "['Well,', 'there', 'probably', 'is,', 'but', 'not', 'the', 'same', 'number', 'of', 'people', 'as', 'say', 'Chicago.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wel', 'prob', 'numb', 'peopl', 'say', 'chicago'], ['well', 'probably', 'number', 'people', 'say', 'chicago'])\n",
      "original document: \n",
      "['Since', 'you', \"aren't\", 'taking', 'a', 'substantial', 'part', 'of', 'the', 'source', 'material,', 'and', 'you', 'are', 'transforming', 'it', 'by', 'creating', 'your', 'own', '\"performance\"', 'for', 'comedic', 'effect,', 'you', 'should', 'be', 'fine.', 'You', 'are', 'in', 'no', 'way', 'usurping', 'or', 'replacing', 'the', 'original', 'work,', 'which', 'weighs', 'greatly', 'in', 'your', 'favor', 'for', 'fair', 'use.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sint', 'ar', 'tak', 'subst', 'part', 'sourc', 'mat', 'transform', 'cre', 'perform', 'com', 'effect', 'fin', 'way', 'usurp', 'replac', 'origin', 'work', 'weigh', 'gre', 'fav', 'fair', 'us'], ['since', 'arent', 'take', 'substantial', 'part', 'source', 'material', 'transform', 'create', 'performance', 'comedic', 'effect', 'fine', 'way', 'usurp', 'replace', 'original', 'work', 'weigh', 'greatly', 'favor', 'fair', 'use'])\n",
      "original document: \n",
      "['Yes,', 'it', 'is', 'a', 'male', 'sex', 'organ.', '', 'Being', 'attracted', 'to', 'a', 'woman', 'who', 'has', 'non-standard', 'sex', 'organs', \"doesn't\", 'make', 'you', 'gay.', '', 'I', 'mean,', 'if', 'someone', 'is', 'solely', 'fixated', 'on', 'the', 'penis,', 'and', \"doesn't\", 'care', 'for', 'the', 'rest', 'of', 'the', 'person,', 'that', 'might', 'mean', 'something', 'different.', '', 'But', \"that's\", 'not', 'what', 'people', 'are', 'typically', 'doing.', '', 'Bailey', 'Jay', 'is', 'considered', 'by', 'a', 'lot', 'of', 'people', 'to', 'be', 'good', 'looking', 'and', '*also*', 'she', 'had', 'a', 'penis.', '', 'Random', 'penises', 'are', 'not', 'what', 'these', 'people', 'are', 'into', '-', \"they're\", 'just', 'more', 'open', 'about', 'non-standard', 'genitalia.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'mal', 'sex', 'org', 'attract', 'wom', 'nonstandard', 'sex', 'org', 'doesnt', 'mak', 'gay', 'mean', 'someon', 'sol', 'fix', 'pen', 'doesnt', 'car', 'rest', 'person', 'might', 'mean', 'someth', 'diff', 'that', 'peopl', 'typ', 'bailey', 'jay', 'consid', 'lot', 'peopl', 'good', 'look', 'also', 'pen', 'random', 'pen', 'peopl', 'theyr', 'op', 'nonstandard', 'genital'], ['yes', 'male', 'sex', 'organ', 'attract', 'woman', 'nonstandard', 'sex', 'organs', 'doesnt', 'make', 'gay', 'mean', 'someone', 'solely', 'fixate', 'penis', 'doesnt', 'care', 'rest', 'person', 'might', 'mean', 'something', 'different', 'thats', 'people', 'typically', 'bailey', 'jay', 'consider', 'lot', 'people', 'good', 'look', 'also', 'penis', 'random', 'penises', 'people', 'theyre', 'open', 'nonstandard', 'genitalia'])\n",
      "original document: \n",
      "['I', 'think', \"it's\", 'the', 'horn', \"that's\", 'giving', 'him', 'smarts.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['think', 'horn', 'that', 'giv', 'smart'], ['think', 'horn', 'thats', 'give', 'smart'])\n",
      "original document: \n",
      "['Ok.', 'I', 'fixed', 'the', 'typo.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ok', 'fix', 'typo'], ['ok', 'fix', 'typo'])\n",
      "original document: \n",
      "['Délicieuse', '!!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delicieus'], ['delicieuse'])\n",
      "original document: \n",
      "['[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoteu8/):\\n\\nThis', 'is', 'very', 'true']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoteu8\\n\\nth', 'tru'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoteu8\\n\\nthis', 'true'])\n",
      "original document: \n",
      "['he', 'likes', 'raw', 'fish']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lik', 'raw', 'fish'], ['like', 'raw', 'fish'])\n",
      "original document: \n",
      "['Literally', 'any', 'church.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lit', 'church'], ['literally', 'church'])\n",
      "original document: \n",
      "['Diarrhea.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['diarrhe'], ['diarrhea'])\n",
      "original document: \n",
      "['You', 'need', 'to', 'get', 'the', 'three', 'override', 'holotapes', 'but', 'that', 'is', 'a', 'matter', 'of', 'fighting', 'through', 'everything', 'until', 'the', 'last', 'bit', 'before', 'the', 'boss', 'fight.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nee', 'get', 'three', 'overrid', 'holotap', 'mat', 'fight', 'everyth', 'last', 'bit', 'boss', 'fight'], ['need', 'get', 'three', 'override', 'holotapes', 'matter', 'fight', 'everything', 'last', 'bite', 'boss', 'fight'])\n",
      "original document: \n",
      "['This', 'submission', 'has', 'been', 'removed.\\r\\n\\r\\nViolation:\\r\\n\\r\\n##', '[Rule', '0:', 'No', 'threads', 'that', 'are', 'answered', 'by', 'the', 'Wiki,', 'Searching', 'r/Fitness,', 'or', 'Google](https://www.reddit.com/r/Fitness/wiki/rules#wiki_rule_.230)\\r\\n\\r\\n[/r/Fitness', 'Rules](http://www.reddit.com/r/Fitness/wiki/rules)', '|', '[/r/Fitness', 'Wiki](http://www.reddit.com/r/fitness/wiki/index)\\r\\n\\r\\n**PLEASE', 'NOTE:', 'I', 'am', 'a', 'bot', 'account,', 'but', 'I', 'have', 'removed', 'this', 'thread', 'by', 'request', 'from', 'a', 'human', 'moderator.', 'If', 'you', 'have', 'a', 'question', 'about', 'this', 'removal,', 'please', '[send', 'a', 'message', 'to', 'ModMail](https://www.reddit.com/message/compose?to=%2Fr%2FFitness)', 'and', 'include', 'a', 'link', 'to', 'your', 'thread.**']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['submit', 'removed\\r\\n\\r\\nviolation\\r\\n\\r\\n', 'rul', 'zero', 'threads', 'answ', 'wik', 'search', 'rfit', 'googlehttpswwwredditcomrfitnesswikiruleswiki_rule_230\\r\\n\\r\\nrfitness', 'ruleshttpwwwredditcomrfitnesswikir', 'rfit', 'wikihttpwwwredditcomrfitnesswikiindex\\r\\n\\r\\nplease', 'not', 'bot', 'account', 'remov', 'thread', 'request', 'hum', 'mod', 'quest', 'remov', 'pleas', 'send', 'mess', 'modmailhttpswwwredditcommessagecomposeto2fr2ffitness', 'includ', 'link', 'thread'], ['submission', 'removed\\r\\n\\r\\nviolation\\r\\n\\r\\n', 'rule', 'zero', 'thread', 'answer', 'wiki', 'search', 'rfitness', 'googlehttpswwwredditcomrfitnesswikiruleswiki_rule_230\\r\\n\\r\\nrfitness', 'ruleshttpwwwredditcomrfitnesswikirules', 'rfitness', 'wikihttpwwwredditcomrfitnesswikiindex\\r\\n\\r\\nplease', 'note', 'bot', 'account', 'remove', 'thread', 'request', 'human', 'moderator', 'question', 'removal', 'please', 'send', 'message', 'modmailhttpswwwredditcommessagecomposeto2fr2ffitness', 'include', 'link', 'thread'])\n",
      "original document: \n",
      "['YA', 'BUT', 'MY', 'IDEOLOGUES', 'COULD', 'BEAT', 'UP', 'YOUR', 'IDEOLOGUES!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ya', 'ideolog', 'could', 'beat', 'ideolog'], ['ya', 'ideologues', 'could', 'beat', 'ideologues'])\n",
      "original document: \n",
      "['\"Oh', 'fuck', 'the', \"acid's\", 'really', 'kicking', 'in.', 'Act', 'like', 'a', 'human.', 'What', 'does', 'that', '*even', 'mean*?\"', '', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['oh', 'fuck', 'acid', 'real', 'kick', 'act', 'lik', 'hum', 'ev', 'mean'], ['oh', 'fuck', 'acids', 'really', 'kick', 'act', 'like', 'human', 'even', 'mean'])\n",
      "original document: \n",
      "['I', 'was', 'so', 'young', 'and', 'just', 'really', 'dug', 'the', '\"techno\"', 'song', 'from', 'the', 'movie', 'and', 'playing', 'as', 'Sonya', 'on', 'Sega,', 'lol.', 'Then', 'I', 'went', 'to', \"Spencer's\", 'and', 'got', 'a', 'full', 'body', 'condom', 'and', 'we', 'ran', 'around', 'the', 'mall', 'wearing', 'them', 'like', 'idiots.', 'Gotta', 'love', \"90's\", 'slumber', 'parties.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['young', 'real', 'dug', 'techno', 'song', 'movy', 'play', 'sony', 'seg', 'lol', 'went', 'spent', 'got', 'ful', 'body', 'condom', 'ran', 'around', 'mal', 'wear', 'lik', 'idiot', 'gott', 'lov', '90s', 'slumb', 'party'], ['young', 'really', 'dig', 'techno', 'song', 'movie', 'play', 'sonya', 'sega', 'lol', 'go', 'spencers', 'get', 'full', 'body', 'condom', 'run', 'around', 'mall', 'wear', 'like', 'idiots', 'gotta', 'love', '90s', 'slumber', 'party'])\n",
      "original document: \n",
      "['Bro', 'ur', 'a', 'lover', 'not', 'a', 'fighter.', 'You', 'got', 'flower', 'shirts', 'on.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bro', 'ur', 'lov', 'fight', 'got', 'flow', 'shirt'], ['bro', 'ur', 'lover', 'fighter', 'get', 'flower', 'shirt'])\n",
      "original document: \n",
      "['I', 'wish', 'i', 'had', 'your', 'problem', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['wish', 'problem'], ['wish', 'problem'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['my', 'b', 'lol', 'oh', 'well']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['b', 'lol', 'oh', 'wel'], ['b', 'lol', 'oh', 'well'])\n",
      "original document: \n",
      "['Looks', 'a', 'bit', 'Scarlet', 'Crusadey.\\nEither', 'way,', 'hot', 'as', 'fuck.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'bit', 'scarlet', 'crusadey\\neither', 'way', 'hot', 'fuck'], ['look', 'bite', 'scarlet', 'crusadey\\neither', 'way', 'hot', 'fuck'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['143413374|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', 'lXCJyWOh)\\n\\n&gt;&gt;143413305\\nthanks', 'leaf\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and seventy-four', 'gt', 'unit', 'stat', 'anonym', 'id', 'lxcjywoh\\n\\ngtgt143413305\\nthanks', 'leaf\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and thirteen thousand, three hundred and seventy-four', 'gt', 'unite', 'state', 'anonymous', 'id', 'lxcjywoh\\n\\ngtgt143413305\\nthanks', 'leaf\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Playing', 'on', 'a', 'team', 'is', 'a', 'privilege,', 'though.', 'Nobody', 'is', 'entitled', 'a', 'spot', 'on', 'a', 'team,', 'and', 'coaches', 'kick', 'people', 'off', 'of', 'teams', 'for', 'whatever', 'they', 'want']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['play', 'team', 'privileg', 'though', 'nobody', 'entitl', 'spot', 'team', 'coach', 'kick', 'peopl', 'team', 'whatev', 'want'], ['play', 'team', 'privilege', 'though', 'nobody', 'entitle', 'spot', 'team', 'coach', 'kick', 'people', 'team', 'whatever', 'want'])\n",
      "original document: \n",
      "['Just', 'disable', 'it.', 'Then', 'when', 'it', 'is', 'up', 'and', 'running', 'reenact', 'windows', 'defender.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dis', 'run', 'reenact', 'window', 'defend'], ['disable', 'run', 'reenact', 'windows', 'defender'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Amusingly,', \"it's\", 'not', 'even', 'Roman.', 'The', 'earliest', 'attestation', 'of', 'the', 'Bellamy', 'Salute', 'is', '*The', 'Oath', 'of', 'the', 'Horatii*', 'in', '1784.', 'Hitler', 'and', 'Mussolini', \"couldn't\", 'even', 'get', 'their', 'fucking', 'Roman', 'cosplaying', 'right.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['amus', 'ev', 'rom', 'earliest', 'attest', 'bellamy', 'salut', 'oa', 'horati', 'one thousand, seven hundred and eighty-four', 'hitl', 'mussolin', 'couldnt', 'ev', 'get', 'fuck', 'rom', 'cosplay', 'right'], ['amusingly', 'even', 'roman', 'earliest', 'attestation', 'bellamy', 'salute', 'oath', 'horatii', 'one thousand, seven hundred and eighty-four', 'hitler', 'mussolini', 'couldnt', 'even', 'get', 'fuck', 'roman', 'cosplaying', 'right'])\n",
      "original document: \n",
      "['How', 'about', 'one', 'at', '7:00?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['on', 'seven hundred'], ['one', 'seven hundred'])\n",
      "original document: \n",
      "['Sorry,', 'your', 'Karma', 'is', 'less', 'than', '0.', 'This', 'could', 'mean', \"you're\", 'a', 'bot', 'or', 'a', 'troll.\\n\\n*I', 'am', 'a', 'bot,', 'and', 'this', 'action', 'was', 'performed', 'automatically.', 'Please', '[contact', 'the', 'moderators', 'of', 'this', 'subreddit](/message/compose/?to=/r/lfg)', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns.*']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sorry', 'karm', 'less', 'zero', 'could', 'mean', 'yo', 'bot', 'troll\\n\\ni', 'bot', 'act', 'perform', 'autom', 'pleas', 'contact', 'mod', 'subredditmessagecomposetorlfg', 'quest', 'concern'], ['sorry', 'karma', 'less', 'zero', 'could', 'mean', 'youre', 'bot', 'troll\\n\\ni', 'bot', 'action', 'perform', 'automatically', 'please', 'contact', 'moderators', 'subredditmessagecomposetorlfg', 'question', 'concern'])\n",
      "original document: \n",
      "['I', 'hope', 'they', 'can', 'get', 'you', 'straightened', 'out.', '', '', 'Good', 'luck!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'get', 'straightened', 'good', 'luck'], ['hope', 'get', 'straighten', 'good', 'luck'])\n",
      "original document: \n",
      "['You', 'are', 'right', 'my', 'friend.', 'And', 'it', 'could', 'also', 'be', 'neither', 'too', ':)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['right', 'friend', 'could', 'also', 'neith'], ['right', 'friend', 'could', 'also', 'neither'])\n",
      "original document: \n",
      "[\"I've\", 'had', 'an', 'experience', 'like', 'that', 'before.', 'The', 'passenger', 'was', 'upset', 'because', 'he', \"couldn't\", 'drink', 'his', 'Bud', 'Lite', 'in', 'my', 'car.', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'expery', 'lik', 'passeng', 'upset', 'couldnt', 'drink', 'bud', 'lit', 'car'], ['ive', 'experience', 'like', 'passenger', 'upset', 'couldnt', 'drink', 'bud', 'lite', 'car'])\n",
      "original document: \n",
      "['Red', 'Wings', 'are', 'notorious', 'for', 'a', 'hard', 'break', 'in', 'period.', 'But', 'when', \"it's\", 'done', 'they', 'create', 'an', 'exact', 'mold', 'to', 'your', 'foot']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['red', 'wing', 'not', 'hard', 'break', 'period', 'don', 'cre', 'exact', 'mold', 'foot'], ['red', 'wing', 'notorious', 'hard', 'break', 'period', 'do', 'create', 'exact', 'mold', 'foot'])\n",
      "original document: \n",
      "['Depends', 'on', 'who', 'you', 'ask', 'LOL!', 'I', 'like', 'some', 'of', 'it', 'but', 'it', \"doesn't\", 'taste', 'like', 'the', 'real', 'thing']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['depend', 'ask', 'lol', 'lik', 'doesnt', 'tast', 'lik', 'real', 'thing'], ['depend', 'ask', 'lol', 'like', 'doesnt', 'taste', 'like', 'real', 'thing'])\n",
      "original document: \n",
      "['Okay', \"I'll\", 'try', 'that,', 'thank', 'you', 'very', 'jukmifgguggh', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'il', 'try', 'thank', 'jukmifgguggh'], ['okay', 'ill', 'try', 'thank', 'jukmifgguggh'])\n",
      "original document: \n",
      "['Sad', 'story', 'time.', '\\n\\nI', 'was', '6', 'years', 'old', 'and', 'had', 'family', 'friends', 'over', 'just', 'before', 'Christmas.', 'My', 'mom', 'was', 'single', 'and', 'working', 'a', 'ton', 'to', 'support', 'my', 'sister', 'and', 'I', 'at', 'the', 'time.', '\\n\\nAnyways,', 'we', \"didn't\", 'want', 'our', 'friends', 'to', 'leave', 'so', 'we', 'hid', 'in', 'the', 'laundry', 'room.', '', 'Low', 'and', 'behold,', 'I', 'stumble', 'across', 'the', 'technodrome', 'hidden', 'behind', 'the', 'furnace.', 'We', 'book', 'it', 'out', 'of', 'there,', 'and', 'agree', 'not', 'to', 'say', 'anything.', '\\n\\nSo', 'they', 'leave,', 'and', \"it's\", 'just', 'the', '3', 'of', 'us', 'at', 'the', 'kitchen', 'table.', 'I', 'was', 'so', 'excited', 'about', 'it', 'that', 'I', 'kept', 'dropping', 'hints', 'until', 'it', 'was', 'obvious', 'and', 'she', 'just', 'broke', 'down', 'crying.', 'I', 'had', 'never', 'seen', 'her', 'cry', 'before,', 'and', 'granted', 'she', 'was', 'stressed', 'from', 'everything', 'but', 'its', 'probably', 'the', 'worst', 'memory', 'I', 'have', 'with', 'her.', 'She', 'tried', 'so', 'hard', 'to', 'make', 'a', 'perfect', 'Christmas', 'and', 'circumstances', '(as', 'I', 'know', 'I', 'was', 'a', 'kid', 'and', \"didn't\", 'do', 'it', 'on', 'purpose)', 'just', 'took', 'that', 'away', 'from', 'her,', 'as', 'was', 'common', 'in', 'general', 'for', 'her.', '\\n\\nI', \"didn't\", 'even', 'ask', 'for', 'that', 'present', 'but', 'she', 'knew', 'I', 'loved', 'TMNT', 'and', 'got', 'me', 'the', 'best', 'present', 'ever.', 'It', 'was', 'my', 'last', 'Christmas', 'with', 'her.', '\\n\\n', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sad', 'story', 'tim', '\\n\\ni', 'six', 'year', 'old', 'famy', 'friend', 'christmas', 'mom', 'singl', 'work', 'ton', 'support', 'sist', 'tim', '\\n\\nanyways', 'didnt', 'want', 'friend', 'leav', 'hid', 'laundry', 'room', 'low', 'behold', 'stumbl', 'across', 'technodrom', 'hid', 'behind', 'furnac', 'book', 'agr', 'say', 'anyth', '\\n\\nso', 'leav', 'three', 'us', 'kitch', 'tabl', 'excit', 'kept', 'drop', 'hint', 'obvy', 'brok', 'cry', 'nev', 'seen', 'cry', 'grant', 'stressed', 'everyth', 'prob', 'worst', 'mem', 'tri', 'hard', 'mak', 'perfect', 'christmas', 'circumst', 'know', 'kid', 'didnt', 'purpos', 'took', 'away', 'common', 'gen', '\\n\\ni', 'didnt', 'ev', 'ask', 'pres', 'knew', 'lov', 'tmnt', 'got', 'best', 'pres', 'ev', 'last', 'christmas', '\\n\\n'], ['sad', 'story', 'time', '\\n\\ni', 'six', 'years', 'old', 'family', 'friends', 'christmas', 'mom', 'single', 'work', 'ton', 'support', 'sister', 'time', '\\n\\nanyways', 'didnt', 'want', 'friends', 'leave', 'hide', 'laundry', 'room', 'low', 'behold', 'stumble', 'across', 'technodrome', 'hide', 'behind', 'furnace', 'book', 'agree', 'say', 'anything', '\\n\\nso', 'leave', 'three', 'us', 'kitchen', 'table', 'excite', 'keep', 'drop', 'hint', 'obvious', 'break', 'cry', 'never', 'see', 'cry', 'grant', 'stress', 'everything', 'probably', 'worst', 'memory', 'try', 'hard', 'make', 'perfect', 'christmas', 'circumstances', 'know', 'kid', 'didnt', 'purpose', 'take', 'away', 'common', 'general', '\\n\\ni', 'didnt', 'even', 'ask', 'present', 'know', 'love', 'tmnt', 'get', 'best', 'present', 'ever', 'last', 'christmas', '\\n\\n'])\n",
      "original document: \n",
      "['\"I', \"don't\", 'like', 'cake\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'lik', 'cak'], ['dont', 'like', 'cake'])\n",
      "original document: \n",
      "['I', 'appreciate', 'you', 'cursing', 'the', 'top', 'players,', 'one', 'god', 'already', 'fallen', 'into', 'the', 'depth', 'of', 'losers.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['apprecy', 'curs', 'top', 'play', 'on', 'god', 'already', 'fal', 'dep', 'los'], ['appreciate', 'curse', 'top', 'players', 'one', 'god', 'already', 'fall', 'depth', 'losers'])\n",
      "original document: \n",
      "['Check', 'out', 'this', '[r/bestof', 'of', 'some', 'guys', 'experience', 'of', 'Burning', 'Man](https://www.reddit.com/r/bestof/comments/6yt3zs/redditor_explains_the_logistics_behind_and_what/)', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['check', 'rbestof', 'guy', 'expery', 'burn', 'manhttpswwwredditcomrbestofcomments6yt3zsredditor_explains_the_logistics_behind_and_what'], ['check', 'rbestof', 'guy', 'experience', 'burn', 'manhttpswwwredditcomrbestofcomments6yt3zsredditor_explains_the_logistics_behind_and_what'])\n",
      "original document: \n",
      "['I', 'love', 'your', 'breasts...', 'I', 'want', 'to', 'put', 'my', 'face', 'in-between', 'them']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'breast', 'want', 'put', 'fac', 'inbetween'], ['love', 'breast', 'want', 'put', 'face', 'inbetween'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Such', 'sagacity', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sagac'], ['sagacity'])\n",
      "original document: \n",
      "['So', 'just', 'to', 'be', 'clear,', '\"lift,\"', '\"timing,\"', 'and', '\"angle,\"', 'all', 'ultimately', 'refer', 'to', 'the', 'amount', 'of', 'valve', 'overlap', 'caused', 'by', 'the', 'cams?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['clear', 'lift', 'tim', 'angl', 'ultim', 'ref', 'amount', 'valv', 'overlap', 'caus', 'cam'], ['clear', 'lift', 'time', 'angle', 'ultimately', 'refer', 'amount', 'valve', 'overlap', 'cause', 'cams'])\n",
      "original document: \n",
      "['Lol', 'I', 'factored', 'it', 'in,', 'but', \"didn't\", 'realize', 'this', 'bike', 'had', 'a', '12k', 'service', 'coming', 'up.', 'I', 'have', 'the', '$', 'but', 'that', \"doesn't\", 'mean', \"I'm\", 'not', 'broke.', 'Buy', 'it', 'and', 'almost', 'immediately', 'spend', '$1,200?', 'That', 'sucks', 'no', 'matter', 'who', 'you', 'are.', '\\n\\nMy', 'fault', 'for', 'not', 'researching', 'that,', 'but', 'I', 'only', 'expected', 'tires', 'and', 'normal', 'fluids', 'for', 'awhile']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol', 'fact', 'didnt', 'real', 'bik', '12k', 'serv', 'com', 'doesnt', 'mean', 'im', 'brok', 'buy', 'almost', 'immedy', 'spend', 'one thousand, two hundred', 'suck', 'mat', '\\n\\nmy', 'fault', 'research', 'expect', 'tir', 'norm', 'fluid', 'awhil'], ['lol', 'factor', 'didnt', 'realize', 'bike', '12k', 'service', 'come', 'doesnt', 'mean', 'im', 'break', 'buy', 'almost', 'immediately', 'spend', 'one thousand, two hundred', 'suck', 'matter', '\\n\\nmy', 'fault', 'research', 'expect', 'tire', 'normal', 'fluids', 'awhile'])\n",
      "original document: \n",
      "['Seize', 'information', 'technology.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['seiz', 'inform', 'technolog'], ['seize', 'information', 'technology'])\n",
      "original document: \n",
      "[\"It's\", 'possible', 'to', 'stream', 'it,', 'but', \"it's\", 'not', 'the', 'same,', 'you', 'know?', 'Unfortunately', 'we', 'are', 'in', 'different', 'states,', 'so', 'getting', 'married', 'in', 'front', 'of', 'her', 'is', 'not', 'really', 'an', 'option', 'unless', 'we', 'just', 'say', 'our', 'vows', 'in', 'front', 'of', 'her.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['poss', 'stream', 'know', 'unfortun', 'diff', 'stat', 'get', 'marry', 'front', 'real', 'opt', 'unless', 'say', 'vow', 'front'], ['possible', 'stream', 'know', 'unfortunately', 'different', 'state', 'get', 'marry', 'front', 'really', 'option', 'unless', 'say', 'vow', 'front'])\n",
      "original document: \n",
      "['I', \"don't\", 'watch', 'basketball,', 'but', 'are', 'you', 'serious?', \"That's\", 'laughably', 'bad.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'watch', 'basketbal', 'sery', 'that', 'laugh', 'bad'], ['dont', 'watch', 'basketball', 'serious', 'thats', 'laughably', 'bad'])\n",
      "original document: \n",
      "['redline', 'concealment', 'DCR', '2.0\\n\\nhttp://redlineconcealmentholsters.com/product.sc?productId=14']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['redlin', 'cont', 'dcr', '20\\n\\nhttpredlineconcealmentholsterscomproductscproductid14'], ['redline', 'concealment', 'dcr', '20\\n\\nhttpredlineconcealmentholsterscomproductscproductid14'])\n",
      "original document: \n",
      "['To', 'save', 'her', 'life', 'if', 'she', 'was', 'on', 'fire.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sav', 'lif', 'fir'], ['save', 'life', 'fire'])\n",
      "original document: \n",
      "['sure', 'thing', 'Day', 'Day']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sur', 'thing', 'day', 'day'], ['sure', 'thing', 'day', 'day'])\n",
      "original document: \n",
      "['Would', 'you', 'be', 'able', 'to', 'elaborate', 'how', 'you', 'got', 'it', 'under', 'control?', 'Currently', 'struggling', 'with', 'the', 'same', 'thing.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'abl', 'elab', 'got', 'control', 'cur', 'struggling', 'thing'], ['would', 'able', 'elaborate', 'get', 'control', 'currently', 'struggle', 'thing'])\n",
      "original document: \n",
      "['Showalter', 'would', 'play', 'a', 'better', 'Vince', 'though.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['showalt', 'would', 'play', 'bet', 'vint', 'though'], ['showalter', 'would', 'play', 'better', 'vince', 'though'])\n",
      "original document: \n",
      "['But', \"it's\", 'actually', 'kinda', 'cool', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['act', 'kind', 'cool', 'though'], ['actually', 'kinda', 'cool', 'though'])\n",
      "original document: \n",
      "['I', 'used', 'to', 'rock', 'the', 'psp', 'version', 'of', 'this', 'shit', 'back', 'in', 'the', 'day', 'and', 'I', 'loved', 'it.', 'Regardless', 'of', 'the', 'quality,', 'I', 'just', 'enjoyed', 'that', 'a', 'crash', 'racing', 'game', 'had', 'some', 'platforming', 'in', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['us', 'rock', 'psp', 'vert', 'shit', 'back', 'day', 'lov', 'regardless', 'qual', 'enjoy', 'crash', 'rac', 'gam', 'platform'], ['use', 'rock', 'psp', 'version', 'shit', 'back', 'day', 'love', 'regardless', 'quality', 'enjoy', 'crash', 'race', 'game', 'platforming'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['His', 'momma', 'listens', 'to', 'AM', 'radio.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['momm', 'list', 'radio'], ['momma', 'listen', 'radio'])\n",
      "original document: \n",
      "['Damn,People', 'can', 'turn', 'into', 'real', 'assholes', 'while', 'on', 'the', 'karts', 'with', 'bumping', 'etc.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['damnpeopl', 'turn', 'real', 'asshol', 'kart', 'bump', 'etc'], ['damnpeople', 'turn', 'real', 'assholes', 'karts', 'bump', 'etc'])\n",
      "original document: \n",
      "['anyone', 'have', 'a', 'stream?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['anyon', 'stream'], ['anyone', 'stream'])\n",
      "original document: \n",
      "['After', 'the', 'first', '4', 'or', '5', 'days,', 'I', 'just', 'stopped', 'using', 'up', 'the', 'raid', 'orbs.', '', '3', 'a', 'day', 'for', 'the', 'daily', 'and', 'thats', 'it.', '', '\\n\\nIf', 'I', 'tried', 'to', 'use', 'every', 'orb,', 'I', 'think', 'I', 'would', 'of', 'ended', 'up', 'quitting', 'the', 'game', 'before', 'the', 'event', 'was', 'over.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['first', 'four', 'fiv', 'day', 'stop', 'us', 'raid', 'orb', 'three', 'day', 'dai', 'that', '\\n\\nif', 'tri', 'us', 'every', 'orb', 'think', 'would', 'end', 'quit', 'gam', 'ev'], ['first', 'four', 'five', 'days', 'stop', 'use', 'raid', 'orb', 'three', 'day', 'daily', 'thats', '\\n\\nif', 'try', 'use', 'every', 'orb', 'think', 'would', 'end', 'quit', 'game', 'event'])\n",
      "original document: \n",
      "['Making', 'a', 'big', 'batch', 'of', 'soup,', 'sipping', 'a', 'hot', 'toddy', 'with', \"Bushmill's\", 'Black', 'Bush', '(highly', 'recommend),', 'listening', 'to', 'some', '[Mike', 'Love](https://youtu.be/SheUtYVritw)', '(also', 'highly', 'recommend),', 'switching', 'my', 'plans', 'from', 'seeing', 'It', '(the', 'movie)', 'to', 'hanging', 'at', 'my', 'place,', 'lining', 'up', 'weekend', 'plans', 'for', 'Oct.', 'to', 'see', 'friends', 'in', 'NYC', 'and', 'DC,', 'avoiding', 'going', 'to', 'the', 'gym,', 'giving', 'my', 'dog', 'some', 'attention,', 'washing', 'clothes,', 'the', 'usual', 'stuff.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'big', 'batch', 'soup', 'sip', 'hot', 'toddy', 'bushmil', 'black', 'bush', 'high', 'recommend', 'list', 'mik', 'lovehttpsyoutubesheutyvritw', 'also', 'high', 'recommend', 'switch', 'plan', 'see', 'movy', 'hang', 'plac', 'lin', 'weekend', 'plan', 'oct', 'see', 'friend', 'nyc', 'dc', 'avoid', 'going', 'gym', 'giv', 'dog', 'at', 'wash', 'cloth', 'us', 'stuff'], ['make', 'big', 'batch', 'soup', 'sip', 'hot', 'toddy', 'bushmills', 'black', 'bush', 'highly', 'recommend', 'listen', 'mike', 'lovehttpsyoutubesheutyvritw', 'also', 'highly', 'recommend', 'switch', 'plan', 'see', 'movie', 'hang', 'place', 'line', 'weekend', 'plan', 'oct', 'see', 'friends', 'nyc', 'dc', 'avoid', 'go', 'gym', 'give', 'dog', 'attention', 'wash', 'clothe', 'usual', 'stuff'])\n",
      "original document: \n",
      "['stop', 'editing', 'the', 'amount', 'of', 'upvotes', 'you', 'autistic', 'karma', 'fuck.', 'ban', 'the', 'man']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['stop', 'edit', 'amount', 'upvot', 'aut', 'karm', 'fuck', 'ban', 'man'], ['stop', 'edit', 'amount', 'upvotes', 'autistic', 'karma', 'fuck', 'ban', 'man'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Amen.', 'Hoping', 'Bitcoin', 'can', 'prove', 'the', 'wonders', 'of', 'decentralization', 'rather', 'than', 'proving', 'that', 'we,', 'as', 'humans,', 'do', 'in', 'fact', 'need', 'a', 'central', 'authority', 'to', 'tell', 'us', 'what', 'to', 'do.', '', 'Pretty', 'sad', 'IMO.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['am', 'hop', 'bitcoin', 'prov', 'wond', 'dec', 'rath', 'prov', 'hum', 'fact', 'nee', 'cent', 'auth', 'tel', 'us', 'pretty', 'sad', 'imo'], ['amen', 'hop', 'bitcoin', 'prove', 'wonder', 'decentralization', 'rather', 'prove', 'humans', 'fact', 'need', 'central', 'authority', 'tell', 'us', 'pretty', 'sad', 'imo'])\n",
      "original document: \n",
      "['I', 'was', 'worried', 'there', 'for', 'a', 'bit', 'that', 'either', 'your', 'super', 'would', 'run', 'out', 'before', 'you', 'made', 'it', 'to', 'the', 'drop', 'or', 'you', 'would', 'smash', 'the', 'edge', 'by', 'accident.', 'Thankfully', 'I', 'was', 'wrong.', '\\n\\nAs', 'a', 'fellow', 'smashbro,', 'I', 'salute', 'you.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['worry', 'bit', 'eith', 'sup', 'would', 'run', 'mad', 'drop', 'would', 'smash', 'edg', 'accid', 'thank', 'wrong', '\\n\\nas', 'fellow', 'smashbro', 'salut'], ['worry', 'bite', 'either', 'super', 'would', 'run', 'make', 'drop', 'would', 'smash', 'edge', 'accident', 'thankfully', 'wrong', '\\n\\nas', 'fellow', 'smashbro', 'salute'])\n",
      "original document: \n",
      "['Looking', 'for', 'Mark', 'Messier', 'and', 'Mark', 'Recchi']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'mark', 'messy', 'mark', 'recch'], ['look', 'mark', 'messier', 'mark', 'recchi'])\n",
      "original document: \n",
      "['I’m', 'afraid', 'there', 'are', 'even', 'worst...', 'some', 'women', 'show...', 'THEIR', 'HANDS', 'honestly', 'wtf', 'is', 'wrong', 'with', 'those', 'people!!!', 'Kids', 'are', 'around', 'you', 'know!!??']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'afraid', 'ev', 'worst', 'wom', 'show', 'hand', 'honest', 'wtf', 'wrong', 'peopl', 'kid', 'around', 'know'], ['im', 'afraid', 'even', 'worst', 'women', 'show', 'hand', 'honestly', 'wtf', 'wrong', 'people', 'kid', 'around', 'know'])\n",
      "original document: \n",
      "['Or', 'maybe', 'I', 'have', 'real', 'life', 'ties', 'to', 'both', 'institutions...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mayb', 'real', 'lif', 'tie', 'institut'], ['maybe', 'real', 'life', 'tie', 'institutions'])\n",
      "original document: \n",
      "['Private', 'Property:', 'Man', 'claims', 'woman', 'as', 'his', '', 'property.', '', 'Drinks', 'both', 'sodas.', '', 'Complains', 'bitterly', 'that', 'life', 'was', 'better', 'when', 'Black', 'people', 'were', 'also', 'his', 'property.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['priv', 'property', 'man', 'claim', 'wom', 'property', 'drink', 'soda', 'complain', 'bit', 'lif', 'bet', 'black', 'peopl', 'also', 'property'], ['private', 'property', 'man', 'claim', 'woman', 'property', 'drink', 'sodas', 'complain', 'bitterly', 'life', 'better', 'black', 'people', 'also', 'property'])\n",
      "original document: \n",
      "['This', 'is', 'seriously', 'one', 'of', 'those', 'cases', 'in', 'which', 'I', 'think', 'OP', 'needs', 'to', 'ask', 'himself,', '\"why', 'do', 'I', 'want', 'to', 'maintain', 'fatherhood', 'and', 'how', 'does', 'it', 'benefit', 'the', 'kids?\".', '', 'I', \"don't\", 'know', 'how', 'long', \"it's\", 'been', 'but', 'kids', 'deserve', 'a', 'man', 'who', 'will', 'support', 'them', 'financially,', 'AND', 'emotionally.', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sery', 'on', 'cas', 'think', 'op', 'nee', 'ask', 'want', 'maintain', 'fath', 'benefit', 'kid', 'dont', 'know', 'long', 'kid', 'deserv', 'man', 'support', 'fin', 'emot'], ['seriously', 'one', 'case', 'think', 'op', 'need', 'ask', 'want', 'maintain', 'fatherhood', 'benefit', 'kid', 'dont', 'know', 'long', 'kid', 'deserve', 'man', 'support', 'financially', 'emotionally'])\n",
      "original document: \n",
      "['Good', 'bot!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['good', 'bot'], ['good', 'bot'])\n",
      "original document: \n",
      "['Cue', 'me', 'going', 'to', 'play', 'fallout', '3.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cue', 'going', 'play', 'fallout', 'three'], ['cue', 'go', 'play', 'fallout', 'three'])\n",
      "original document: \n",
      "['Thank', 'you', 'for', 'bringing', 'this', 'to', 'my', 'attention.', 'I', 'go', 'to', 'this', 'sub', 'every', 'few', 'days', 'and', 'actually', 'care', 'about', 'the', 'alert', 'slider', 'a', 'lot', 'but', 'I', \"didn't\", 'notice', 'the', 'stickied', 'thread.', 'I', 'think', 'the', 'sub', 'had', 'the', 'same', 'stickied', 'thread', 'before', 'for', 'weeks', 'so', 'I', 'never', 'paid', 'attention', 'to', 'it.\\n\\nThey', 'still', 'seem', 'to', 'be', 'a', 'bit', 'out', 'of', 'touch', 'with', 'their', 'survey', 'but', 'at', 'least', 'a', 'change', 'is', 'coming.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'bring', 'at', 'go', 'sub', 'every', 'day', 'act', 'car', 'alert', 'slid', 'lot', 'didnt', 'not', 'sticky', 'thread', 'think', 'sub', 'sticky', 'thread', 'week', 'nev', 'paid', 'at', 'it\\n\\nthey', 'stil', 'seem', 'bit', 'touch', 'survey', 'least', 'chang', 'com'], ['thank', 'bring', 'attention', 'go', 'sub', 'every', 'days', 'actually', 'care', 'alert', 'slider', 'lot', 'didnt', 'notice', 'stickied', 'thread', 'think', 'sub', 'stickied', 'thread', 'weeks', 'never', 'pay', 'attention', 'it\\n\\nthey', 'still', 'seem', 'bite', 'touch', 'survey', 'least', 'change', 'come'])\n",
      "original document: \n",
      "['Okay,', 'but', 'you', 'missed', 'a', 'point,', 'they', 'use', 'the', 'availability', 'of', 'a', 'Pharaoh', 'deck', 'for', 'the', 'Griffin', 'as', 'a', 'selling', 'point.', 'They', 'advertise', 'it', 'as', 'having', 'a', 'Pharaoh-style', 'deck.', 'Look', 'at', 'the', \"tank's\", 'page.', 'And', \"that's\", 'why', 'I', 'ordered', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['okay', 'miss', 'point', 'us', 'avail', 'pharaoh', 'deck', 'griffin', 'sel', 'point', 'advert', 'pharaohstyl', 'deck', 'look', 'tank', 'pag', 'that', 'ord'], ['okay', 'miss', 'point', 'use', 'availability', 'pharaoh', 'deck', 'griffin', 'sell', 'point', 'advertise', 'pharaohstyle', 'deck', 'look', 'tank', 'page', 'thats', 'order'])\n",
      "original document: \n",
      "['No,', 'she', \"can't\", 'post', 'mouth', 'sounds', 'on', 'youtube', 'anymore', 'so', \"she's\", 'just', 'going', 'to', 'post', 'them', 'on', 'vidme.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['cant', 'post', 'mou', 'sound', 'youtub', 'anym', 'she', 'going', 'post', 'vidm'], ['cant', 'post', 'mouth', 'sound', 'youtube', 'anymore', 'shes', 'go', 'post', 'vidme'])\n",
      "original document: \n",
      "['is']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['They', 'looked', 'at', 'one', 'from', 'Scotty', 'B', 'maybe', '10-15', 'phases', 'before']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'on', 'scotty', 'b', 'mayb', 'one thousand and fifteen', 'phas'], ['look', 'one', 'scotty', 'b', 'maybe', 'one thousand and fifteen', 'phase'])\n",
      "original document: \n",
      "['misadventure\\nmɪsədˈvɛntʃə/Submit\\nnoun\\n1.\\nENGLISH', 'LAW\\ndeath', 'caused', 'by', 'a', 'person', 'accidentally', 'while', 'performing', 'a', 'legal', 'act', 'without', 'negligence', 'or', 'intent', 'to', 'harm.\\n\"the', 'coroner', 'recorded', 'a', 'verdict', 'of', 'death', 'by', 'misadventure\"\\n2.\\nan', 'unfortunate', 'incident;', 'a', 'mishap.\\n\"the', 'petty', 'misdemeanours', 'and', 'misadventures', 'of', 'childhood\"\\nsynonyms:\\taccident,', 'problem,', 'difficulty,', 'misfortune,', 'mishap,', 'mischance;', 'More']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['misadventure\\nmsdvntsubmit\\nnoun\\n1\\nenglish', 'law\\ndeath', 'caus', 'person', 'accid', 'perform', 'leg', 'act', 'without', 'neglig', 'int', 'harm\\nthe', 'coron', 'record', 'verdict', 'dea', 'misadventure\\n2\\nan', 'unfortun', 'incid', 'mishap\\nthe', 'petty', 'misdemeano', 'misadv', 'childhood\\nsynonyms\\taccident', 'problem', 'difficul', 'misfortun', 'mishap', 'misch'], ['misadventure\\nmsdvntsubmit\\nnoun\\n1\\nenglish', 'law\\ndeath', 'cause', 'person', 'accidentally', 'perform', 'legal', 'act', 'without', 'negligence', 'intent', 'harm\\nthe', 'coroner', 'record', 'verdict', 'death', 'misadventure\\n2\\nan', 'unfortunate', 'incident', 'mishap\\nthe', 'petty', 'misdemeanours', 'misadventures', 'childhood\\nsynonyms\\taccident', 'problem', 'difficulty', 'misfortune', 'mishap', 'mischance'])\n",
      "original document: \n",
      "['Geniunely', 'curious,', 'why', 'do', 'you', 'think', 'complying', 'with', 'someone', 'threatening', 'you', 'with', 'a', 'deadly', 'weapon', 'would', 'decrease', 'the', 'chances', 'of', 'you', 'getting', 'killed?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['geniun', 'cury', 'think', 'comply', 'someon', 'threatening', 'dead', 'weapon', 'would', 'decreas', 'chant', 'get', 'kil'], ['geniunely', 'curious', 'think', 'comply', 'someone', 'threaten', 'deadly', 'weapon', 'would', 'decrease', 'chance', 'get', 'kill'])\n",
      "original document: \n",
      "['Take', 'it', 'easy', 'Chris', 'Brown.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['tak', 'easy', 'chris', 'brown'], ['take', 'easy', 'chris', 'brown'])\n",
      "original document: \n",
      "[\"It's\", 'always', 'nice', 'to', 'see', 'famous', 'people', 'putting', 'their', 'money', 'where', 'their', 'mouth', 'is.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['alway', 'nic', 'see', 'fam', 'peopl', 'put', 'money', 'mou'], ['always', 'nice', 'see', 'famous', 'people', 'put', 'money', 'mouth'])\n",
      "original document: \n",
      "['&gt;how', 'is', 'celebrating', 'togetherness', 'being', 'political?', '\\n\\nExplain', 'to', 'me', 'how', 'bringing', 'up', 'segregation', \"isn't\", 'a', 'commentary', 'on', 'politics?', \"\\n\\nI'm\", 'not', 'agreeing', 'or', 'disagreeing', 'about', 'the', 'message,', 'but', 'whether', 'or', 'not', 'the', 'SEC', 'should', 'be', 'funding', 'that', 'shit.', '\\n\\nThe', 'point', 'of', 'the', 'SEC', 'is', 'athletics,', 'and', \"shouldn't\", 'be', 'delving', 'into', 'social', 'issues.', 'Look', 'at', 'the', 'NFL,', 'they', 'decided', 'to', 'get', 'political,', 'and', 'now', 'you', \"can't\", 'hardly', 'enjoy', 'a', 'game', 'without', 'getting', 'hit', 'over', 'the', 'head', 'with', 'politics.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gthow', 'celebr', 'togeth', 'polit', '\\n\\nexplain', 'bring', 'segreg', 'isnt', 'com', 'polit', '\\n\\nim', 'agr', 'disagr', 'mess', 'wheth', 'sec', 'fund', 'shit', '\\n\\nthe', 'point', 'sec', 'athlet', 'shouldnt', 'delv', 'soc', 'issu', 'look', 'nfl', 'decid', 'get', 'polit', 'cant', 'hard', 'enjoy', 'gam', 'without', 'get', 'hit', 'head', 'polit'], ['gthow', 'celebrate', 'togetherness', 'political', '\\n\\nexplain', 'bring', 'segregation', 'isnt', 'commentary', 'politics', '\\n\\nim', 'agree', 'disagree', 'message', 'whether', 'sec', 'fund', 'shit', '\\n\\nthe', 'point', 'sec', 'athletics', 'shouldnt', 'delve', 'social', 'issue', 'look', 'nfl', 'decide', 'get', 'political', 'cant', 'hardly', 'enjoy', 'game', 'without', 'get', 'hit', 'head', 'politics'])\n",
      "original document: \n",
      "['[+frenchbritchick](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnoohze/):\\n\\nCongratulations', '!\\n\\nI', 'guess', 'I', 'only', 'have', 'one', 'question:', 'how', 'did', 'you', 'afford', 'this?', 'And', 'by', 'this', 'I', 'mean', 'the', 'professional', 'editor,', 'graphic', 'designer,', 'and', 'world', 'class', 'marketer?', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['frenchbritchickhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoohze\\n\\ncongratulation', '\\n\\ni', 'guess', 'on', 'quest', 'afford', 'mean', 'profess', 'edit', 'graph', 'design', 'world', 'class', 'market'], ['frenchbritchickhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnoohze\\n\\ncongratulations', '\\n\\ni', 'guess', 'one', 'question', 'afford', 'mean', 'professional', 'editor', 'graphic', 'designer', 'world', 'class', 'marketer'])\n",
      "original document: \n",
      "[\"\\nI'll\", 'buy', 'it', 'on', 'PC', 'as', 'soon', 'as', 'they', 'release', 'it', 'on', 'PC.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\nill', 'buy', 'pc', 'soon', 'releas', 'pc'], ['\\nill', 'buy', 'pc', 'soon', 'release', 'pc'])\n",
      "original document: \n",
      "['NYC.', 'My', 'company', 'focused', 'a', 'lot', 'on', 'high', 'end', 'luxury', 'residential,', 'so', 'when', 'the', 'economy', 'tanked,', 'we', 'were', 'hit', 'especially', 'hard.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nyc', 'company', 'focus', 'lot', 'high', 'end', 'luxury', 'resid', 'econom', 'tank', 'hit', 'espec', 'hard'], ['nyc', 'company', 'focus', 'lot', 'high', 'end', 'luxury', 'residential', 'economy', 'tank', 'hit', 'especially', 'hard'])\n",
      "original document: \n",
      "[\"It's\", 'the', '\"You', 'just', \"don't\", 'understand', 'it\"', 'argument', 'Hindus', 'use.', '\\n\\n\"Why', \"didn't\", 'I', 'find', 'the', 'answers', 'to', 'all', 'my', 'questions', 'in', 'Geeta?\"\\n\\n\"Well,', 'because', 'you', \"don't\", 'understand', 'it\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'understand', 'argu', 'hind', 'us', '\\n\\nwhy', 'didnt', 'find', 'answ', 'quest', 'geeta\\n\\nwell', 'dont', 'understand'], ['dont', 'understand', 'argument', 'hindus', 'use', '\\n\\nwhy', 'didnt', 'find', 'answer', 'question', 'geeta\\n\\nwell', 'dont', 'understand'])\n",
      "original document: \n",
      "['Merciless', 'is', 'really', 'good', 'for', 'it.', 'Or', 'pulse', 'nade', 'if', \"you're\", 'a', 'titan']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['merciless', 'real', 'good', 'puls', 'nad', 'yo', 'tit'], ['merciless', 'really', 'good', 'pulse', 'nade', 'youre', 'titan'])\n",
      "original document: \n",
      "['This', 'annoys', 'me', 'too.', 'I', 'usually', 'take', 'a', 'couple', 'quick', 'picks', 'to', 'share', 'with', 'people', 'but', 'spend', 'the', 'rest', 'of', 'the', 'time', 'actually', 'enjoying', 'what', 'I', 'went', 'there', 'to', 'see..']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['annoy', 'us', 'tak', 'coupl', 'quick', 'pick', 'shar', 'peopl', 'spend', 'rest', 'tim', 'act', 'enjoy', 'went', 'see'], ['annoy', 'usually', 'take', 'couple', 'quick', 'pick', 'share', 'people', 'spend', 'rest', 'time', 'actually', 'enjoy', 'go', 'see'])\n",
      "original document: \n",
      "['Why', 'the', 'fuck', 'would', 'he', 'admit', 'to', 'that', 'on', 'commentary?', 'Is', 'he', 'not', 'aware', 'that', 'hardware', 'mods', 'are', 'banned?', 'Jfc.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fuck', 'would', 'admit', 'com', 'aw', 'hardw', 'mod', 'ban', 'jfc'], ['fuck', 'would', 'admit', 'commentary', 'aware', 'hardware', 'mods', 'ban', 'jfc'])\n",
      "original document: \n",
      "[\"i'm\", 'the', 'kind', 'of', 'guy', \"who'd\", 'keep', 'his', 'scrap.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'kind', 'guy', 'whod', 'keep', 'scrap'], ['im', 'kind', 'guy', 'whod', 'keep', 'scrap'])\n",
      "original document: \n",
      "['Every', 'time', 'I', 'do', 'this', 'though', 'the', 'POTG', 'is', 'always', 'some', 'dumb', 'mess', 'like', 'a', 'Soldier', 'solo-ulting', 'a', 'Pharah', 'and', 'then', 'shooting', 'a', 'helix', 'into', 'the', 'grav', 'and', 'getting', 'like', '50', 'damage', 'worth', 'of', 'last-hits', 'on', '3', 'enemies.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['every', 'tim', 'though', 'potg', 'alway', 'dumb', 'mess', 'lik', 'soldy', 'soloult', 'pharah', 'shoot', 'helix', 'grav', 'get', 'lik', 'fifty', 'dam', 'wor', 'lasthit', 'three', 'enemy'], ['every', 'time', 'though', 'potg', 'always', 'dumb', 'mess', 'like', 'soldier', 'soloulting', 'pharah', 'shoot', 'helix', 'grav', 'get', 'like', 'fifty', 'damage', 'worth', 'lasthits', 'three', 'enemies'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "['Yeah,', \"Han's\", 'down', 'it', 'was', 'the', 'worst', 'call', 'he', 'could', 'make.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yeah', 'han', 'worst', 'cal', 'could', 'mak'], ['yeah', 'hans', 'worst', 'call', 'could', 'make'])\n",
      "original document: \n",
      "[\"Didn't\", 'think', 'about', 'it', 'that', 'way,', 'makes', 'sense...its', 'just', 'that', 'i', 'wanted', 'to', 'get', 'proper', 'instruction', 'on', 'at', 'least', 'how', 'to', 'throw', 'the', 'basic', 'punches', 'and', 'block/bob.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['didnt', 'think', 'way', 'mak', 'senseit', 'want', 'get', 'prop', 'instruct', 'least', 'throw', 'bas', 'punch', 'blockbob'], ['didnt', 'think', 'way', 'make', 'senseits', 'want', 'get', 'proper', 'instruction', 'least', 'throw', 'basic', 'punch', 'blockbob'])\n",
      "original document: \n",
      "['Looks', 'like', 'OP', 'is', 'a', '\"boyfriend', 'of', 'instagram\"', 'too.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'op', 'boyfriend', 'instagram'], ['look', 'like', 'op', 'boyfriend', 'instagram'])\n",
      "original document: \n",
      "['Did', 'they', 'announce', 'anything', 'interesting', 'regarding', 'the', 'game?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['annount', 'anyth', 'interest', 'regard', 'gam'], ['announce', 'anything', 'interest', 'regard', 'game'])\n",
      "original document: \n",
      "['later', ';)']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lat'], ['later'])\n",
      "original document: \n",
      "['asking', '5']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ask', 'fiv'], ['ask', 'five'])\n",
      "original document: \n",
      "[\"You're\", 'assuming', 'way', 'too', 'much', 'about', 'the', 'average', 'player', 'here', 'when', \"it's\", 'a', 'huge', 'player', 'base,', \"there's\", 'no', 'way', 'everyone', 'started', 'when', 'the', 'game', 'started.', 'People', 'still', 'complain', 'about', 'no', 'Camus/Xander', 'repeat', 'because', 'they', 'started', 'after', 'that', 'point.', \"There's\", 'no', 'way', 'everyone', 'has', 'the', 'same', 'goal', 'of', 'collecting', 'every', 'unit', 'at', '4*', 'level', '40', 'when', \"there's\", 'literally', 'no', 'point', 'to', 'that', 'either.', 'The', 'only', 'thing', 'the', 'game', 'actually', 'tracks', 'is', '5*', 'level', '40s.', 'F2P', 'have', 'limited', 'barrack', 'space', 'and', 'limited', 'supplies', 'of', 'feathers,', 'so', \"there's\", 'nothing', 'in', 'game', 'even', 'wanting', 'you', 'to', 'do', 'this.', \"I've\", 'upgraded', 'literally', '7', 'total', 'units', 'to', '5*', 'with', 'the', 'amount', 'of', 'Feathers', \"I've\", 'gathered', 'since', 'I', 'started', 'playing', 'and', 'I', \"don't\", 'have', 'enough', 'to', 'upgrade', 'someone', 'to', '5*', 'right', 'now.', 'And', 'two', 'of', 'those', 'were', '4*', 's', 'I', 'pulled', 'and', 'wanted', 'to', 'build', 'up,', 'like', 'a', 'ton', 'of', 'other', 'users', 'would', 'want', 'to', 'do', 'with', 'their', 'favorite', 'units,', 'which', 'would', 'be', 'a', 'much', 'more', 'important', 'goal', 'for', 'them', 'than', 'raising', 'their', 'Virion', 'to', '4*', 'level', '40', 'using', 'the', 'Feathers', 'and', 'stamina', 'they', \"would've\", 'otherwise', 'put', 'into', 'a', '+10', 'Selena', 'or', 'something.', '\\nEven', 'if', 'somehow', 'there', 'was', 'several', 'thousand', 'like', 'minded', 'individuals', 'who', 'did', 'that,', \"you'd\", 'still', 'not', 'be', 'able', 'to', 'follow', 'a', 'lot', 'of', 'guides', 'because', 'they', 'require', '3*', 's,', 'and', 'throwing', 'off', 'stats', 'even', 'IV', 'wise', 'entirely', 'messes', 'up', 'strategies.', 'Some', 'strategies', 'need', 'people', 'like', 'Arthur', 'or', 'Stahl', 'or', 'Serena', 'there', 'at', '3*', 'because', 'they', 'can', 'soak', 'hits,', 'but', \"won't\", 'kill', 'a', 'specific', 'unit', 'at', 'a', 'choke', 'point.', 'So', 'bringing', 'your', '4*', 'Serena/Arthur/Stahl', 'means', 'you', 'mess', 'up', 'the', 'strategy,', 'the', 'Axe', 'you', 'were', 'stalling', 'dies,', 'and', 'the', 'Lance', 'behind', 'it', 'kills', 'Serena/Stahl', 'instead.', '\\n', '\\nBesides', 'the', 'whole', '\"yeah', 'but', 'what', 'if', 'this', 'or', 'what', 'if', 'that\",', 'the', 'entire', 'point', 'is', '\"what', 'if', 'people', \"don't\", 'have', 'this\",', 'your', 'argument', 'here', 'is', '\"well,', 'they', 'should\",', 'my', 'point', 'is', '\"yeah,', 'but', 'they', 'don\\'t\".', 'How', 'does', 'your', 'argument', 'actually', 'fix', 'the', 'situation', 'those', 'people', 'are', 'in', 'who', 'see', 'that', 'video', 'about', 'the', 'easy', '4*', 'Oboro', 'level', '40', 'wall', 'strategy,', 'and', 'then', 'realize', \"they'd\", 'have', 'to', 'spend', 'like', '150', 'stamina', 'and', '2000', 'Feathers', 'just', 'for', 'this', 'one', 'orb', \"they're\", 'wanting', 'to', 'earn?', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yo', 'assum', 'way', 'much', 'av', 'play', 'hug', 'play', 'bas', 'ther', 'way', 'everyon', 'start', 'gam', 'start', 'peopl', 'stil', 'complain', 'camusxand', 'rep', 'start', 'point', 'ther', 'way', 'everyon', 'goal', 'collect', 'every', 'unit', 'four', 'level', 'forty', 'ther', 'lit', 'point', 'eith', 'thing', 'gam', 'act', 'track', 'fiv', 'level', '40s', 'f2p', 'limit', 'barrack', 'spac', 'limit', 'supply', 'feath', 'ther', 'noth', 'gam', 'ev', 'want', 'iv', 'upgrad', 'lit', 'sev', 'tot', 'unit', 'fiv', 'amount', 'feath', 'iv', 'gath', 'sint', 'start', 'play', 'dont', 'enough', 'upgrad', 'someon', 'fiv', 'right', 'two', 'four', 'pul', 'want', 'build', 'lik', 'ton', 'us', 'would', 'want', 'favorit', 'unit', 'would', 'much', 'import', 'goal', 'rais', 'vir', 'four', 'level', 'forty', 'us', 'feath', 'stamin', 'wouldv', 'otherw', 'put', 'ten', 'selen', 'someth', '\\neven', 'somehow', 'sev', 'thousand', 'lik', 'mind', 'individ', 'youd', 'stil', 'abl', 'follow', 'lot', 'guid', 'requir', 'three', 'throwing', 'stat', 'ev', 'iv', 'wis', 'entir', 'mess', 'strategies', 'strategies', 'nee', 'peopl', 'lik', 'arth', 'stahl', 'seren', 'three', 'soak', 'hit', 'wont', 'kil', 'spec', 'unit', 'chok', 'point', 'bring', 'four', 'serenaarthurstahl', 'mean', 'mess', 'strategy', 'ax', 'stal', 'die', 'lant', 'behind', 'kil', 'serenastahl', 'instead', '\\n', '\\nbesides', 'whol', 'yeah', 'entir', 'point', 'peopl', 'dont', 'argu', 'wel', 'point', 'yeah', 'dont', 'argu', 'act', 'fix', 'situ', 'peopl', 'see', 'video', 'easy', 'four', 'oboro', 'level', 'forty', 'wal', 'strategy', 'real', 'theyd', 'spend', 'lik', 'one hundred and fifty', 'stamin', 'two thousand', 'feath', 'on', 'orb', 'theyr', 'want', 'earn'], ['youre', 'assume', 'way', 'much', 'average', 'player', 'huge', 'player', 'base', 'theres', 'way', 'everyone', 'start', 'game', 'start', 'people', 'still', 'complain', 'camusxander', 'repeat', 'start', 'point', 'theres', 'way', 'everyone', 'goal', 'collect', 'every', 'unit', 'four', 'level', 'forty', 'theres', 'literally', 'point', 'either', 'thing', 'game', 'actually', 'track', 'five', 'level', '40s', 'f2p', 'limit', 'barrack', 'space', 'limit', 'supply', 'feather', 'theres', 'nothing', 'game', 'even', 'want', 'ive', 'upgrade', 'literally', 'seven', 'total', 'units', 'five', 'amount', 'feather', 'ive', 'gather', 'since', 'start', 'play', 'dont', 'enough', 'upgrade', 'someone', 'five', 'right', 'two', 'four', 'pull', 'want', 'build', 'like', 'ton', 'users', 'would', 'want', 'favorite', 'units', 'would', 'much', 'important', 'goal', 'raise', 'virion', 'four', 'level', 'forty', 'use', 'feather', 'stamina', 'wouldve', 'otherwise', 'put', 'ten', 'selena', 'something', '\\neven', 'somehow', 'several', 'thousand', 'like', 'mind', 'individuals', 'youd', 'still', 'able', 'follow', 'lot', 'guide', 'require', 'three', 'throw', 'stats', 'even', 'iv', 'wise', 'entirely', 'mess', 'strategies', 'strategies', 'need', 'people', 'like', 'arthur', 'stahl', 'serena', 'three', 'soak', 'hit', 'wont', 'kill', 'specific', 'unit', 'choke', 'point', 'bring', 'four', 'serenaarthurstahl', 'mean', 'mess', 'strategy', 'axe', 'stall', 'die', 'lance', 'behind', 'kill', 'serenastahl', 'instead', '\\n', '\\nbesides', 'whole', 'yeah', 'entire', 'point', 'people', 'dont', 'argument', 'well', 'point', 'yeah', 'dont', 'argument', 'actually', 'fix', 'situation', 'people', 'see', 'video', 'easy', 'four', 'oboro', 'level', 'forty', 'wall', 'strategy', 'realize', 'theyd', 'spend', 'like', 'one hundred and fifty', 'stamina', 'two thousand', 'feather', 'one', 'orb', 'theyre', 'want', 'earn'])\n",
      "original document: \n",
      "['♥️']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "([], [])\n",
      "original document: \n",
      "['The', 'fact', 'that', 'she', 'tweeted', 'an', 'actual', 'commonsense', 'recommendation', '(sending', 'the', 'Navy', 'and', 'the', 'hospital', 'ship)', 'and', 'it', 'got', 'done', 'is', 'kind', 'of', 'evidence', 'of', 'that.', 'She', 'knew', 'what', 'needed', 'to', 'be', 'done.', 'Thing', 'is', \"she's\", 'also', 'done', 'this', 'with', 'healthcare', 'and', 'now', \"she's\", 'putting', 'pressure', 'on', 'them', 'about', 'CHIP.', 'She', 'is', 'using', 'her', 'visibility', 'to', 'pressure', 'the', 'administration.\\n\\nThere', 'are', 'many', 'politicians', 'who', 'would', 'be', 'content', 'with', 'some', 'empty', 'platitude', 'and', 'move', 'on.', 'We', 'should', 'credit', 'politicians', 'who', 'offer', 'help', '(like', 'those', 'who', 'have', 'flown', 'to', 'Puerto', 'Rico)', 'and', 'those', 'who', 'offer', 'solutions', 'to', 'problems.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fact', 'tweet', 'act', 'commonsens', 'recommend', 'send', 'navy', 'hospit', 'ship', 'got', 'don', 'kind', 'evid', 'knew', 'nee', 'don', 'thing', 'she', 'also', 'don', 'healthc', 'she', 'put', 'press', 'chip', 'us', 'vis', 'press', 'administration\\n\\nthere', 'many', 'polit', 'would', 'cont', 'empty', 'platitud', 'mov', 'credit', 'polit', 'off', 'help', 'lik', 'flown', 'puerto', 'rico', 'off', 'solv', 'problem'], ['fact', 'tweet', 'actual', 'commonsense', 'recommendation', 'send', 'navy', 'hospital', 'ship', 'get', 'do', 'kind', 'evidence', 'know', 'need', 'do', 'thing', 'shes', 'also', 'do', 'healthcare', 'shes', 'put', 'pressure', 'chip', 'use', 'visibility', 'pressure', 'administration\\n\\nthere', 'many', 'politicians', 'would', 'content', 'empty', 'platitude', 'move', 'credit', 'politicians', 'offer', 'help', 'like', 'fly', 'puerto', 'rico', 'offer', 'solutions', 'problems'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['143414176|', '&gt;', 'United', 'States', 'Anonymous', '(ID:', '5XaVjx/n)\\n\\n&gt;&gt;143412250', '(OP)\\nWhy', 'not', 'just', 'run', 'down', 'a', 'crowded', 'public', 'street', 'screaming;', '\"I\\'m', 'a', 'stupid', 'asshole!\"\\n\\nThat', 'way', 'everyone', 'knows', 'your', 'a', 'Democrat.\\n\\t\\t\\t']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and seventy-six', 'gt', 'unit', 'stat', 'anonym', 'id', '5xavjxn\\n\\ngtgt143412250', 'op\\nwhy', 'run', 'crowd', 'publ', 'street', 'screaming', 'im', 'stupid', 'asshole\\n\\nthat', 'way', 'everyon', 'know', 'democrat\\n\\t\\t\\t'], ['one hundred and forty-three million, four hundred and fourteen thousand, one hundred and seventy-six', 'gt', 'unite', 'state', 'anonymous', 'id', '5xavjxn\\n\\ngtgt143412250', 'op\\nwhy', 'run', 'crowd', 'public', 'street', 'scream', 'im', 'stupid', 'asshole\\n\\nthat', 'way', 'everyone', 'know', 'democrat\\n\\t\\t\\t'])\n",
      "original document: \n",
      "['Yep,', 'same', 'thing', 'happens', 'as', 'High', 'Elves', 'for', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['yep', 'thing', 'hap', 'high', 'elv'], ['yep', 'thing', 'happen', 'high', 'elves'])\n",
      "original document: \n",
      "['Brewers', 'fans', 'be', 'like', '\"Stop', 'sucking', 'and', 'beat', 'the', 'Rockies\"\\n\\nMe:', 'Nigga', 'worry', 'about', 'your...', 'Better', 'luck', 'next', 'year.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['brew', 'fan', 'lik', 'stop', 'suck', 'beat', 'rockies\\n\\nme', 'nigg', 'worry', 'bet', 'luck', 'next', 'year'], ['brewers', 'fan', 'like', 'stop', 'suck', 'beat', 'rockies\\n\\nme', 'nigga', 'worry', 'better', 'luck', 'next', 'year'])\n",
      "original document: \n",
      "['lol', 'for', 'what?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lol'], ['lol'])\n",
      "original document: \n",
      "['Gotcha,', 'just', 'reply', 'to', 'any', 'of', 'my', 'comment', 'when', \"you're\", 'ready', 'and', \"we'll\", 'get', 'this', 'going!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gotch', 'reply', 'com', 'yo', 'ready', 'wel', 'get', 'going'], ['gotcha', 'reply', 'comment', 'youre', 'ready', 'well', 'get', 'go'])\n",
      "original document: \n",
      "['People', 'talk', 'about', 'the', 'success', 'stories', 'of', 'orthopaedic', 'surgery.', 'But', 'have', 'you', 'ever', 'seen', 'the', 'failures?', 'I', 'have.', 'I', 'have', 'wheeled', 'bodies', 'into', 'the', 'morgue', 'of', 'people', 'who', 'died', 'from', 'complications', 'due', 'to', 'surgery.', \"I've\", 'seen', 'people', 'tied', 'up', 'in', 'bed', 'for', '8', 'months', 'due', 'to', 'infection.', \"\\n\\nThere's\", 'a', 'study', 'where', 'the', 'surgeon', 'randomly', 'assigned', 'patients', 'to', 'a', 'fake', 'knee', 'arthroscopy', 'vs', 'a', 'real', 'one,', 'without', 'telling', 'the', 'patients.', 'Pain', 'was', 'reduced', 'equally', 'in', 'both', 'groups...', '\\n\\n[This', 'multicenter,', 'randomized,', 'sham-controlled', 'trial', 'involving', 'patients', 'with', 'a', 'degenerative', 'medial', 'meniscus', 'tear', 'showed', 'that', 'arthroscopic', 'partial', 'meniscectomy', 'was', 'not', 'superior', 'to', 'sham', 'surgery,', 'with', 'regard', 'to', 'outcomes', 'assessed', 'during', 'a', '12-month', 'follow-up', 'period.', 'Although', 'both', 'groups', 'had', 'significant', 'improvement', 'in', 'all', 'primary', 'outcomes,', 'the', 'patients', 'assigned', 'to', 'arthroscopic', 'partial', 'meniscectomy', 'had', 'no', 'greater', 'improvement', 'than', 'those', 'assigned', 'to', 'sham', 'surgery.](http://www.nejm.org/doi/full/10.1056/NEJMoa1305189#t=article)\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['peopl', 'talk', 'success', 'story', 'orthopa', 'surgery', 'ev', 'seen', 'fail', 'wheel', 'body', 'morgu', 'peopl', 'died', 'comply', 'due', 'surgery', 'iv', 'seen', 'peopl', 'tied', 'bed', 'eight', 'month', 'due', 'infect', '\\n\\ntheres', 'study', 'surgeon', 'random', 'assign', 'paty', 'fak', 'kne', 'arthroscop', 'vs', 'real', 'on', 'without', 'tel', 'paty', 'pain', 'reduc', 'eq', 'group', '\\n\\nthis', 'mult', 'random', 'shamcontrol', 'tri', 'involv', 'paty', 'deg', 'med', 'menisc', 'tear', 'show', 'arthroscop', 'part', 'meniscectom', 'supery', 'sham', 'surgery', 'regard', 'outcom', 'assess', '12month', 'followup', 'period', 'although', 'group', 'sign', 'improv', 'prim', 'outcom', 'paty', 'assign', 'arthroscop', 'part', 'meniscectom', 'gre', 'improv', 'assign', 'sham', 'surgeryhttpwwwnejmorgdoifull101056nejmoa1305189tarticle\\n'], ['people', 'talk', 'success', 'stories', 'orthopaedic', 'surgery', 'ever', 'see', 'failures', 'wheel', 'body', 'morgue', 'people', 'die', 'complications', 'due', 'surgery', 'ive', 'see', 'people', 'tie', 'bed', 'eight', 'months', 'due', 'infection', '\\n\\ntheres', 'study', 'surgeon', 'randomly', 'assign', 'patients', 'fake', 'knee', 'arthroscopy', 'vs', 'real', 'one', 'without', 'tell', 'patients', 'pain', 'reduce', 'equally', 'group', '\\n\\nthis', 'multicenter', 'randomize', 'shamcontrolled', 'trial', 'involve', 'patients', 'degenerative', 'medial', 'meniscus', 'tear', 'show', 'arthroscopic', 'partial', 'meniscectomy', 'superior', 'sham', 'surgery', 'regard', 'outcomes', 'assess', '12month', 'followup', 'period', 'although', 'group', 'significant', 'improvement', 'primary', 'outcomes', 'patients', 'assign', 'arthroscopic', 'partial', 'meniscectomy', 'greater', 'improvement', 'assign', 'sham', 'surgeryhttpwwwnejmorgdoifull101056nejmoa1305189tarticle\\n'])\n",
      "original document: \n",
      "['My', 'preference', 'for', 'a', 'kinslayer', 'was', '[Ragathiel](https://pathfinderwiki.com/wiki/Ragathiel),', 'he', 'was', 'born', 'from', 'evil', 'and', 'dedicated', 'his', 'life', 'to', 'good', 'and', 'opposing', 'evil.', 'Plus', 'he', 'uses', 'the', 'bastard', 'sword.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['pref', 'kinslay', 'ragathielhttpspathfinderwikicomwikiragathiel', 'born', 'evil', 'ded', 'lif', 'good', 'oppos', 'evil', 'plu', 'us', 'bastard', 'sword'], ['preference', 'kinslayer', 'ragathielhttpspathfinderwikicomwikiragathiel', 'bear', 'evil', 'dedicate', 'life', 'good', 'oppose', 'evil', 'plus', 'use', 'bastard', 'sword'])\n",
      "original document: \n",
      "['Just', 'run', 'the', 'fucking', 'ball', 'up', 'the', 'middle.', 'Mond', 'sucks', 'so', 'hard']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['run', 'fuck', 'bal', 'middl', 'mond', 'suck', 'hard'], ['run', 'fuck', 'ball', 'middle', 'mond', 'suck', 'hard'])\n",
      "original document: \n",
      "['I', 'set', '2', 'savings', 'goals', 'for', 'March', '2018', 'back', 'in', 'Feb', 'of', 'this', 'year.', \"I'm\", '$5000', 'away', 'and', \"I've\", 'saved', 'the', 'most', 'I', 'ever', 'had', 'in', 'my', 'life!', '\\n\\nUnfortunately', 'I', 'estimate', 'to', 'fall', 'short', 'for', 'my', 'emergency', 'savings.', 'However', 'I', 'may', 'have', 'set', 'it', 'a', 'little', 'high', '(I', 'have', 'a', 'big', \"dog...I'm\", 'scared', 'of', 'surprise', 'surgeries).', 'But', 'will', 'be', 'doing', 'my', 'best', 'and', 'saving', 'as', 'much', 'as', 'I', 'can!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['set', 'two', 'sav', 'goal', 'march', 'two thousand and eighteen', 'back', 'feb', 'year', 'im', 'five thousand', 'away', 'iv', 'sav', 'ev', 'lif', '\\n\\nunfortunately', 'estim', 'fal', 'short', 'emerg', 'sav', 'howev', 'may', 'set', 'littl', 'high', 'big', 'dogim', 'scar', 'surpr', 'surgery', 'best', 'sav', 'much'], ['set', 'two', 'save', 'goals', 'march', 'two thousand and eighteen', 'back', 'feb', 'year', 'im', 'five thousand', 'away', 'ive', 'save', 'ever', 'life', '\\n\\nunfortunately', 'estimate', 'fall', 'short', 'emergency', 'save', 'however', 'may', 'set', 'little', 'high', 'big', 'dogim', 'scar', 'surprise', 'surgeries', 'best', 'save', 'much'])\n",
      "original document: \n",
      "['Apparently', 'it', 'is', 'all', 'from', 'a', 'recent', 'update', 'to', 'Safari.', 'Apple', 'added', 'in', 'a', 'bunch', 'of', 'calculations', 'of', 'their', 'own', 'for', 'it.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['app', 'rec', 'upd', 'safar', 'appl', 'ad', 'bunch', 'calc'], ['apparently', 'recent', 'update', 'safari', 'apple', 'add', 'bunch', 'calculations'])\n",
      "original document: \n",
      "['Why', 'would', 'anyone', 'want', 'to', 'drive', 'that?', \"It's\", 'a', 'limo,', 'if', 'I', 'pay', 'to', 'be', 'in', 'a', 'limo', \"it'll\", 'be', 'in', 'the', 'back.', 'If', 'I', 'pay', 'to', 'drive', 'a', 'Ferrari,', \"it'll\", 'be', 'a', 'normal', 'one.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['would', 'anyon', 'want', 'driv', 'limo', 'pay', 'limo', 'itl', 'back', 'pay', 'driv', 'ferrar', 'itl', 'norm', 'on'], ['would', 'anyone', 'want', 'drive', 'limo', 'pay', 'limo', 'itll', 'back', 'pay', 'drive', 'ferrari', 'itll', 'normal', 'one'])\n",
      "original document: \n",
      "['Start', 'watching', 'at', '1:23']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['start', 'watch', 'one hundred and twenty-thr'], ['start', 'watch', 'one hundred and twenty-three'])\n",
      "original document: \n",
      "[\"I've\", 'tried', 'a', 'few', 'dozen', 'games', 'of', 'Overload', 'Aggro', 'Shaman', 'and', 'Evolve', 'Shaman', 'at', 'Wild.', 'Aggro', 'Shaman', 'is', 'the', 'king', 'of', 'aggro', 'in', 'the', 'format', 'and', 'is', 'more', 'skill-testing', 'than', 'people', 'give', 'it', 'credit', 'for.', 'Evolve', 'Shaman', 'seemed', 'underwhelming.', 'Unfortunately,', 'Control', 'Shaman', 'is', 'dead', 'in', 'both', 'formats.', \"I've\", 'tried', 'very', 'hard', 'to', 'make', 'use', 'of', 'my', 'Kalimos', 'but', 'crumbled', 'in', 'front', 'of', 'Priests.', 'Sad,', 'since', 'Elemental', 'Control', 'is', 'the', 'most', 'consistent', 'Shaman', 'archetype.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['iv', 'tri', 'doz', 'gam', 'overload', 'aggro', 'sham', 'evolv', 'sham', 'wild', 'aggro', 'sham', 'king', 'aggro', 'form', 'skilltest', 'peopl', 'giv', 'credit', 'evolv', 'sham', 'seem', 'underwhelm', 'unfortun', 'control', 'sham', 'dead', 'form', 'iv', 'tri', 'hard', 'mak', 'us', 'kalimo', 'crumbl', 'front', 'priest', 'sad', 'sint', 'el', 'control', 'consist', 'sham', 'archetyp'], ['ive', 'try', 'dozen', 'game', 'overload', 'aggro', 'shaman', 'evolve', 'shaman', 'wild', 'aggro', 'shaman', 'king', 'aggro', 'format', 'skilltesting', 'people', 'give', 'credit', 'evolve', 'shaman', 'seem', 'underwhelming', 'unfortunately', 'control', 'shaman', 'dead', 'format', 'ive', 'try', 'hard', 'make', 'use', 'kalimos', 'crumble', 'front', 'priests', 'sad', 'since', 'elemental', 'control', 'consistent', 'shaman', 'archetype'])\n",
      "original document: \n",
      "['He', 'neglects', 'to', 'mention', 'that', 'the', 'so', 'called', '\"Champion\"', 'teams', 'of', 'Brisbane', 'and', 'West', 'Coast', 'back', 'in', 'the', 'day', 'were', 'propped', 'up', 'by', 'major', 'salary', 'cap', 'concessions.', 'Not', 'every', 'club', 'can', 'afford', 'to', 'assemble', 'a', 'star', 'studded', 'midfield', 'like', 'that,', 'let', 'alone', 'keep', 'them', 'around.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['neglect', 'ment', 'cal', 'champ', 'team', 'brisb', 'west', 'coast', 'back', 'day', 'prop', 'maj', 'sal', 'cap', 'concess', 'every', 'club', 'afford', 'assembl', 'star', 'stud', 'midfield', 'lik', 'let', 'alon', 'keep', 'around'], ['neglect', 'mention', 'call', 'champion', 'team', 'brisbane', 'west', 'coast', 'back', 'day', 'prop', 'major', 'salary', 'cap', 'concessions', 'every', 'club', 'afford', 'assemble', 'star', 'stud', 'midfield', 'like', 'let', 'alone', 'keep', 'around'])\n",
      "original document: \n",
      "['If', \"I'm\", 'not', 'mistaken,', \"wasn't\", 'the', 'costume', 'made', 'for', 'him?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['im', 'mistak', 'wasnt', 'costum', 'mad'], ['im', 'mistake', 'wasnt', 'costume', 'make'])\n",
      "original document: \n",
      "['he', 'was', 'way', 'too', 'respectful', 'of', 'peach', 'on', 'the', 'ground,', 'stuck', 'to', 'either', 'nairing', 'on', 'shield', 'or', 'zoning', 'too', 'far', 'and', 'samsora', 'picked', 'up', 'on', 'that\\n\\nhope', 'he', 'finds', 'his', 'footing', 'in', 'losers']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['way', 'respect', 'peach', 'ground', 'stuck', 'eith', 'nair', 'shield', 'zon', 'far', 'samsor', 'pick', 'that\\n\\nhope', 'find', 'foot', 'los'], ['way', 'respectful', 'peach', 'grind', 'stick', 'either', 'nairing', 'shield', 'zone', 'far', 'samsora', 'pick', 'that\\n\\nhope', 'find', 'foot', 'losers'])\n",
      "original document: \n",
      "['[+get8bit](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnp4w6d/):\\n\\nHi,', 'I', 'designed', 'the', 'cover.', '[My', 'portfolio.](http://devinholmesdesign.com/)', 'Kinda', 'wished', \"I'd\", 'asked', 'for', 'a', 'credit', 'given', 'the', 'AMA', 'blowing', 'up', 'like', 'this.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['get8bithttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnp4w6d\\n\\nhi', 'design', 'cov', 'portfoliohttpdevinholmesdesigncom', 'kind', 'wish', 'id', 'ask', 'credit', 'giv', 'am', 'blow', 'lik'], ['get8bithttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnp4w6d\\n\\nhi', 'design', 'cover', 'portfoliohttpdevinholmesdesigncom', 'kinda', 'wish', 'id', 'ask', 'credit', 'give', 'ama', 'blow', 'like'])\n",
      "original document: \n",
      "['Fork,', 'no', 'ketchup', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fork', 'ketchup'], ['fork', 'ketchup'])\n",
      "original document: \n",
      "['I', 'hope', 'it’ll', 'support', '2', 'simultaneous', 'languages', 'like', 'Google', 'Now', 'does', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hop', 'itl', 'support', 'two', 'simult', 'langu', 'lik', 'googl'], ['hope', 'itll', 'support', 'two', 'simultaneous', 'languages', 'like', 'google'])\n",
      "original document: \n",
      "['Because', 'once', 'you', 'understand', 'what', 'the', 'bugs', 'are', 'you', 'can', 'avoid', 'them', 'entirely.\\n\\nI', 'have', 'a', 'blast', 'in', 'this', 'game', 'and', \"can't\", 'remember', 'the', 'last', 'time', 'I', 'was', 'legitimately', 'caught', 'off', 'guard', 'or', 'frustrated', 'by', 'a', 'bug.\\n\\nServer', 'downtime', 'is', 'par', 'for', 'the', 'course', 'when', \"you're\", 'playing', 'a', 'new', 'game', 'experiencing', 'growing', 'pains.\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['understand', 'bug', 'avoid', 'entirely\\n\\ni', 'blast', 'gam', 'cant', 'rememb', 'last', 'tim', 'legitim', 'caught', 'guard', 'frust', 'bug\\n\\nserver', 'downtim', 'par', 'cours', 'yo', 'play', 'new', 'gam', 'expery', 'grow', 'pains\\n'], ['understand', 'bug', 'avoid', 'entirely\\n\\ni', 'blast', 'game', 'cant', 'remember', 'last', 'time', 'legitimately', 'catch', 'guard', 'frustrate', 'bug\\n\\nserver', 'downtime', 'par', 'course', 'youre', 'play', 'new', 'game', 'experience', 'grow', 'pains\\n'])\n",
      "original document: \n",
      "['[removed]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['remov'], ['remove'])\n",
      "original document: \n",
      "[\"I'd\", 'rather', 'see', 'a', 'TF1', 'Megatron', '[jet](https://goo.gl/images/6tYqM1)', 'or', 'a', 'TFRotF', 'Megatron', '[tank](https://goo.gl/images/sCrHHQ)', 'or', 'a', 'G1', 'Megatron-style', '[gun](https://goo.gl/images/qgyp8m).\\n\\nSeriously', 'Megatron', 'has', 'turned', 'into', 'a', 'lot', 'of', 'stuff.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['id', 'rath', 'see', 'tf1', 'megatron', 'jethttpsgooglimages6tyqm1', 'tfrotf', 'megatron', 'tankhttpsgooglimagesscrhhq', 'g1', 'megatronstyl', 'gunhttpsgooglimagesqgyp8m\\n\\nseriously', 'megatron', 'turn', 'lot', 'stuff'], ['id', 'rather', 'see', 'tf1', 'megatron', 'jethttpsgooglimages6tyqm1', 'tfrotf', 'megatron', 'tankhttpsgooglimagesscrhhq', 'g1', 'megatronstyle', 'gunhttpsgooglimagesqgyp8m\\n\\nseriously', 'megatron', 'turn', 'lot', 'stuff'])\n",
      "original document: \n",
      "['Is', 'Gears', '2', 'in', 'the', 'original', 'case?', 'If', 'so,', 'can', 'you', 'provide', 'pictures?']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['gear', 'two', 'origin', 'cas', 'provid', 'pict'], ['gear', 'two', 'original', 'case', 'provide', 'picture'])\n",
      "original document: \n",
      "['That’s', 'awesome', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['that', 'awesom'], ['thats', 'awesome'])\n",
      "original document: \n",
      "['Frisk', 'me,', 'plz!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['frisk', 'plz'], ['frisk', 'plz'])\n",
      "original document: \n",
      "['how', 'about', 'the', '\\nLure', 'ball', 'Herracross\\n\\nlevel', 'ball', 'cyndaquil', 'with', 'flare', 'blitz\\n\\naaaand', 'the', 'safari', 'ball', 'Skorupi', 'i', 'guess', 'unless', 'you', 'have', 'another', 'safari', 'ball', 'chansey', 'kicking', 'about.\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['\\nlure', 'bal', 'herracross\\n\\nlevel', 'bal', 'cyndaquil', 'flar', 'blitz\\n\\naaaand', 'safar', 'bal', 'skorup', 'guess', 'unless', 'anoth', 'safar', 'bal', 'chansey', 'kick', 'about\\n\\n'], ['\\nlure', 'ball', 'herracross\\n\\nlevel', 'ball', 'cyndaquil', 'flare', 'blitz\\n\\naaaand', 'safari', 'ball', 'skorupi', 'guess', 'unless', 'another', 'safari', 'ball', 'chansey', 'kick', 'about\\n\\n'])\n",
      "original document: \n",
      "['Thank', 'you.', 'I', 'searched', 'the', 'sub', 'for', '\"trails\"', 'but', \"didn't\", 'find', 'anything.', 'I', 'appreciate', 'it.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['thank', 'search', 'sub', 'trail', 'didnt', 'find', 'anyth', 'apprecy'], ['thank', 'search', 'sub', 'trail', 'didnt', 'find', 'anything', 'appreciate'])\n",
      "original document: \n",
      "['She', 'looks', 'like', 'Emma', 'Stone', 'here']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'lik', 'emm', 'ston'], ['look', 'like', 'emma', 'stone'])\n",
      "original document: \n",
      "['I', 'love', 'Supernatural.\\n\\nI', 'love', 'the', 'Pizza', 'Man', 'stuff', 'from', 'season', '6,', 'where', 'Cass', 'is', 'watching', 'porn', 'and', 'he', 'says,', '\"if', 'the', 'pizza', 'man', 'loves', 'this', 'babysitter,', 'why', 'is', 'he', 'spanking', 'her', 'bottom?\"\\n\\nLater', 'he', 'kisses', 'Meg', 'passionately', 'and', 'then', 'looks', 'at', 'a', 'shocked', 'Sam', 'and', 'Dean', 'and', 'says,', '\"I', 'learned', 'that', 'from', 'the', 'Pizza', 'Man.\"']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['lov', 'supernatural\\n\\ni', 'lov', 'pizz', 'man', 'stuff', 'season', 'six', 'cass', 'watch', 'porn', 'say', 'pizz', 'man', 'lov', 'babysit', 'spank', 'bottom\\n\\nlater', 'kiss', 'meg', 'pass', 'look', 'shock', 'sam', 'dean', 'say', 'learn', 'pizz', 'man'], ['love', 'supernatural\\n\\ni', 'love', 'pizza', 'man', 'stuff', 'season', 'six', 'cass', 'watch', 'porn', 'say', 'pizza', 'man', 'love', 'babysitter', 'spank', 'bottom\\n\\nlater', 'kiss', 'meg', 'passionately', 'look', 'shock', 'sam', 'dean', 'say', 'learn', 'pizza', 'man'])\n",
      "original document: \n",
      "['https://m.youtube.com/watch?v=GRglOSBV2VE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['httpsmyoutubecomwatchvgrglosbv2ve'], ['httpsmyoutubecomwatchvgrglosbv2ve'])\n",
      "original document: \n",
      "['lets', 'see', 'how', 'deep', 'this', 'rabbit', 'hole', 'goes...']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['let', 'see', 'deep', 'rabbit', 'hol', 'goe'], ['let', 'see', 'deep', 'rabbit', 'hole', 'go'])\n",
      "original document: \n",
      "['Yes,', 'to', 'buy', 'brand', 'new', 'authentic', 'pairs', 'of', 'both', 'shoes', 'will', 'cost', 'you', 'close', 'to', '2,000', 'USD', 'total.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ye', 'buy', 'brand', 'new', 'auth', 'pair', 'sho', 'cost', 'clos', 'two thousand', 'usd', 'tot'], ['yes', 'buy', 'brand', 'new', 'authentic', 'pair', 'shoe', 'cost', 'close', 'two thousand', 'usd', 'total'])\n",
      "original document: \n",
      "['**POTD', 'Record:**', '7-0-1', '(W-P-L)\\n\\nStreak:', '6W\\n\\nPick', 'record:', '23-2-16\\n\\nBonus', 'Picks', 'record:', '17-1-23\\n***\\n\\n*Had', 'a', 'decent', 'day', 'yesterday.', 'The', 'POTD', 'pick', '-', 'Coventry', 'won,', 'and', 'I', 'got', '4/6', 'picks', 'as', 'winners', 'with', 'one', 'other', 'as', 'a', 'push*\\n\\n**POTD:**\\n\\n**Double:', 'Torino', 'to', 'Win', '&amp;', 'Inter', 'Milan', 'to', 'Win', '-', '1.60', '(L', '-', '2-2', '&amp;', '1-2.', 'Torino', 'concede', '2', 'goals', 'in', 'the', 'last', '6', 'minutes', 'to', 'lose', 'the', 'bet,', 'unbelievable!)**\\n\\nNow', 'I', 'like', 'to', 'stick', 'to', 'singles,', 'especially', 'for', 'POTD', 'but', 'after', 'researching', 'through', 'all', 'the', 'games', 'I', 'like', 'today', \"there's\", 'nothing', 'that', 'puts', 'me', 'over', 'the', 'edge', 'for', 'certainly', 'other', 'than', 'these', 'picks.', 'I', 'was', 'thinking', 'of', 'doing', 'the', 'Lazio', 'game', 'O2.5', 'goals', 'as', 'I', 'can', 'see', 'both', 'teams', 'scoring', 'there', 'or', 'even', 'Lazio', 'scoring', '3', 'just', 'themselves', 'but', \"I'm\", 'not', '100%', 'confident', 'and', 'a', '2-0', 'win', 'in', 'that', 'game', \"wouldn't\", 'surprise', 'me.', '\\n\\nSo,', 'into', 'detail', 'about', \"Torino's\", 'game.', 'They', 'host', 'Verona', 'today', 'who', \"I've\", 'talked', 'about', 'in', 'previous', 'picks', 'about', 'how', 'utterly', 'terrible', 'they', 'are', 'and', 'if', 'reports', 'are', 'to', 'be', 'believed', 'their', 'manager', 'is', 'close', 'to', 'the', 'sack.', 'Verona', 'have', 'looked', 'hopeless', 'through', '6', 'games', 'with', 'a', 'record', 'of', '0-2-4,', 'only', 'scoring', '1', '(from', 'a', 'penalty)', 'and', 'conceding', '14.', '\\n\\nTorino', 'have', 'been', 'decent', 'so', 'far', 'this', 'season.', 'They', 'find', 'themselves', '7th', 'with', 'a', 'record', 'of', '3-2-1,', 'goals', '10:9.', 'Attackers', 'Belotti', 'and', 'Ljajic', 'have', 'been', 'in', 'good', 'form', 'with', '3', 'goals', 'each', 'and', 'are', 'very', 'likely', 'to', 'score', 'today,', 'Belotti', 'has', 'scored', 'in', 'the', 'two', 'home', 'games', 'Torino', 'have', 'played', 'so', 'far.', 'With', \"Verona's\", 'horrible', 'scoring', 'record', 'I', 'doubt', \"they'll\", 'score', 'today', 'but', 'if', 'they', 'do', \"it'll\", 'only', 'be', '1', 'and', 'Torino', 'have', 'more', 'than', 'enough', 'quality', 'to', 'get', '2+.', 'Defensive', 'mid', 'Baselli', 'will', 'be', 'missing', 'after', 'being', 'sent', 'off', 'last', 'week,', 'Obi', '&amp;', 'Barreca', 'will', 'also', 'miss', 'out', 'for', 'the', 'hosts', 'but', \"they'll\", 'be', 'able', 'to', 'cope', 'without', 'them.', '\\n\\nIn', 'the', 'other', 'game,', 'Inter', 'play', 'away', 'to', 'Benevento', 'who', 'are', 'equally', 'as', 'bad', 'as', 'Verona', 'and', 'sit', 'in', 'last', 'place,', 'also', 'just', 'scoring', 'the', '1', 'goal', 'and', 'have', 'conceded', '16', 'goals', 'already', 'after', 'being', 'smashed', 'by', 'Napoli', '6-0', 'and', 'Roma', '4-0.', 'It', \"won't\", 'get', 'any', 'easier', 'for', 'them', 'here', 'against', 'Inter', 'who', 'have', 'won', '5', 'and', 'drawn', 'once', 'in', 'their', 'games', 'and', 'have', 'the', 'best', 'defensive', 'record', 'in', 'the', 'league.', '\\n\\nInter', 'were', 'lucky', 'to', 'win', 'last', 'week', 'against', 'Genoa,', 'but', 'they', 'continued', 'their', 'record', 'of', 'scoring', 'in', 'every', 'game.', 'That', 'comes', 'mostly', 'thanks', 'to', 'Icardi', 'who', 'is', 'up', 'to', '6', 'goals', 'already', 'and', 'will', 'like', 'his', 'chances', 'of', 'bagging', 'a', 'couple', 'today,', 'I', 'might', 'put', 'a', 'little', 'on', 'him', 'to', 'even', 'get', 'a', 'hat', 'trick.', 'As', 'far', 'as', 'injuries', 'go', 'Inter', 'are', 'only', 'missing', 'Cancelo', 'who', \"hasn't\", 'played', 'in', 'a', 'month', 'anyway.', 'It', 'could', 'be', 'a', 'rout', 'here', 'after', 'some', 'of', \"Benevento's\", 'performances', 'but', 'Inter', \"don't\", 'tend', 'to', 'be', 'a', 'big', 'scoring', 'team', 'so', \"I'm\", 'expecting', '2-0/3-0', 'here.', '\\n\\n***\\n*Quite', 'a', 'lot', 'of', 'games', 'I', 'like', 'today,', 'so', \"I'll\", 'try', 'to', 'whittle', 'them', 'down', 'to', 'just', 'the', 'better', 'looking', 'ones', 'for', 'picks*\\n\\n**Picks:**\\n\\n•', '**Arsenal', 'v', 'Brighton', '-', 'Arsenal', 'Clean', 'Sheet', 'YES', '-', '1.80', '(W', '-', '2-0)**\\n\\nOriginally', 'this', 'pick', 'was', 'going', 'to', 'be', 'Arsenal', 'to', 'Win', 'to', 'Nil', 'but', 'the', 'odds', 'so', 'similar', 'for', 'just', 'a', 'clean', 'sheet', 'that', 'it', 'seems', 'much', 'more', 'worth', 'it', 'in', 'case', 'Arsenal', \"can't\", 'score', 'somehow.', '\\n\\nArsenal', 'have', 'kept', 'their', 'last', '6/8', 'home', 'matches', 'as', 'clean', 'sheets.', \"They've\", 'shown', 'how', 'impressive', 'at', 'home', 'they', 'are', 'winning', 'all', 'their', 'games', 'so', 'far', 'conceding', 'all', 'their', 'only', 'hole', 'goals', 'let', 'in', 'in', 'that', 'early', '4-3', 'thriller', 'against', 'Leicester.', '\\n\\nBrighton', 'have', 'struggled', 'for', 'goals', 'this', 'season,', \"they're\", 'scored', '5', 'in', '6', '(3', 'goals', 'coming', 'against', 'West', 'Brom)', 'and', 'have', 'only', 'scored', '1', 'away', 'from', 'home', 'so', 'far.', 'To', 'get', 'a', 'better', 'look', 'at', 'how', 'I', 'expect', 'this', 'game', 'to', 'play', 'out', 'I', 'looked', 'at', 'how', 'they', 'faired', 'against', 'their', 'only', 'big', 'team', 'they', 'faced', 'yet', 'Man', 'City', 'and', 'the', 'stats', 'show', 'a', '79/21%', 'possession', 'to', 'Man', 'City,', 'showing', 'that', 'Brighton', 'are', 'likely', 'to', 'sit', 'back', 'and', 'try', 'to', 'grind', 'out', 'a', 'draw', 'today', 'against', 'Arsenal.', '\\n\\n•', '**Everton', 'v', 'Burnley', '-', 'Burnley', 'or', 'Draw', '-', '2.25', '(W', '-', '0-1)**\\n\\nI', 'fucked', 'up', 'here,', 'looking', 'at', 'the', 'odds', 'Burnley', '+1.0AH', '@1.65', 'are', 'a', 'much', 'better', 'pick', 'than', 'this', 'so', 'if', \"you're\", 'reading', 'this', 'pick', 'late', 'I', 'would', 'take', 'that,', 'I', 'still', 'feel', 'Burnley', 'can', 'draw', 'here', 'and', 'I', \"won't\", 'change', 'my', 'pick', 'after', 'the', 'fact', 'so', 'here', 'are', 'the', 'stats.', '\\n\\nBurnley', 'have', 'performed', 'well', 'this', 'season', 'with', 'a', 'record', 'of', '2-3-1,', 'goals', '6:5.', 'Away', 'from', 'home', 'Burnley', 'have', 'really', 'turned', 'it', 'on', 'after', 'a', 'terrible', 'record', 'last', 'season,', 'this', 'season', 'they', 'won', 'at', 'Chelsea', '2-1,', 'as', 'well', 'as', 'getting', '1-1', 'draws', 'at', 'Spurs', '&amp;', 'Liverpool.', 'If', 'Burnley', 'do', 'lose', 'it', 'tends', 'to', 'be', 'only', 'by', 'a', 'goal,', 'although', 'last', 'season', 'Everton', 'were', 'one', 'of', 'the', 'only', 'teams', 'able', 'to', 'beat', 'by', 'that', 'margin', 'winning', '3-1', 'but', 'this', 'is', 'a', 'different', 'Everton', 'this', 'year.', '\\n\\nEverton', 'have', 'struggled', 'for', 'goals.', \"They've\", 'scored', '4', 'in', '6,', '2', 'of', 'them', 'coming', 'last', 'game', 'where', 'they', 'were', 'fortunate', 'to', 'get', 'away', 'with', 'a', 'win.', 'Koeman', 'is', 'under', 'pressure', 'at', 'the', 'moment', 'and', 'will', 'want', 'to', 'continue', \"Everton's\", 'winning', 'streak.', 'If', 'Burnley', 'are', 'able', 'to', 'get', 'the', 'first', 'goal', 'then', 'the', 'Goodison', 'Park', 'crowd', 'will', 'be', 'booing', 'and', 'they', 'will', 'get', 'on', 'their', 'players', 'backs,', 'that', 'pressure', 'might', 'be', 'too', 'much.', '\\n\\n•', '**Atalanta', 'v', 'Juventus', '--', 'BTTS', '-', '1.95', '(W', '-', '2-2)**\\n\\nThe', 'head', 'to', 'head', 'record', 'here', 'has', 'BTTS', 'in', 'the', 'last', '4/6.', 'Atalanta', 'have', 'had', '5/6', 'BTTS', 'too.', 'Both', 'teams', 'have', 'scored', 'in', 'all', 'of', 'their', 'games', 'so', 'far', '(except', 'for', 'when', 'Juve', 'played', 'Barca).', 'I', 'think', 'both', 'teams', 'will', 'fancy', 'their', 'chances', 'and', 'have', 'inform', 'players', 'on', 'either', 'side', 'in', 'Gomez', 'and', 'Dybala.', 'I', 'like', 'the', 'O2.5', 'goals', 'here', 'too,', 'it', 'was', '2-2', 'in', 'this', 'game', 'last', 'season', 'and', 'it', 'could', 'be', 'agin', 'today.', '\\n\\n•', '**Torino', 'v', 'Verona', '--', 'Torino', 'Clean', 'Sheet', 'YES', '-', '2.20', '(L', '-', '2-2)**\\n\\nExplained', 'in', 'POTD.', '\\n\\n•', '**Benevento', 'v', 'Inter', 'Milan', '--', 'Inter', '-2.0', 'Asian', 'Handicap', '-', '2.20', '(L', '-', '1-2)**\\n\\nExplained', 'in', 'POTD.', '\\n\\n•', '**Benevento', 'v', 'Inter', 'Milan', '--', 'Inter', 'to', 'Win', 'to', 'Nil', '-', '1.95', '(L', '-', '0-2)**\\n\\nExplained', 'in', 'POTD.', '\\n\\n•', '**Hertha', 'Berlin', 'v', 'Bayern', 'Munich', '--', 'Hertha', 'Berlin', 'to', 'Score', '1+', 'goals', '-', '1.90', '(W', '-', \"2-2)**\\n\\nI've\", 'explained', 'this', 'pick', 'in', 'a', 'comment', 'below.', '\\n\\n•', '**Koln', 'v', 'RB', 'Leipzig', '--', 'Leipzig', 'to', 'Win', '-', '2.00', '(W', '-', '1-2)**\\n\\n•', '**Nice', 'v', 'Marseille', '--', 'Nice', 'Draw', 'No', 'Bet', '-', '1.92', '(L', '-', '2-4.', 'Nice', \"we're\", '2-0', 'up!', 'That', 'would', 'have', 'paid', 'out', 'with', 'some', 'bookies', 'Ina', 'straight', 'win,', 'how', 'annoying)**\\n\\n•', '**Maritimo', 'v', 'Benfica', '--', 'Maritimo', '+1.0', 'Asian', 'Handicap', '-', '2.06', '(W', '-', '1-1)**\\n\\n*Lastly,', 'I', 'would', 'not', 'advise', 'putting', 'these', 'all', 'into', 'an', 'acca.', 'As', 'much', 'as', \"I'd\", 'like', 'them', 'all', 'to', 'win,', \"it's\", 'always', 'unlikely', 'with', 'this', 'many', 'picks', 'so', 'if', 'you', 'do', 'feel', 'you', 'have', 'to', 'combine', 'them', 'in', 'some', 'way', 'please', 'choose', 'a', 'double', 'of', 'your', 'favs', 'or', 'even', 'a', 'treble.', 'Anything', 'more', 'and', \"you're\", 'asking', 'for', 'trouble.*\\n\\n**Bonus', 'Picks:', '(Odds', 'of', 'Evens', 'or', 'greater)**\\n\\n•', 'Everton', 'v', 'Burnley', '--', 'Draw', '-', '3.80', '(L)\\n\\n•', 'Newcastle', 'v', 'Liverpool', '--', 'BTTS', '&amp;', 'O2.5', 'goals', '-', '2.00', '(L)\\n\\n•', 'Benevento', 'v', 'Inter', 'Milan', '--', 'Inter', '-3.0', 'Asian', 'Handicap', '-', '4.40', '(L)\\n\\n•', 'Hertha', 'Berlin', 'or', 'Draw', '-', '3.50', '(W)\\n\\n•', 'Hertha', 'Berlin', 'to', 'Win', '-', '11.00', '(small', 'units)', '[L]\\n\\n•', 'Heerenveen', 'or', 'Draw', '-', '2.20', '(L)\\n\\n•', 'Maritimo', 'Draw', 'No', 'Bet', '-', '5.50', '(P)\\n\\n*Apologies', 'for', 'any', 'spelling', '&amp;', 'grammar', 'errors,', 'I', 'am', 'writing', 'this', 'all', 'on', 'my', 'phone*']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['potd', 'record', 'seven hundred and one', 'wpl\\n\\nstreak', '6w\\n\\npick', 'record', '23216\\n\\nbonus', 'pick', 'record', '17123\\n\\n\\nhad', 'dec', 'day', 'yesterday', 'potd', 'pick', 'coventry', 'got', 'forty-six', 'pick', 'win', 'on', 'push\\n\\npotd\\n\\ndouble', 'torino', 'win', 'amp', 'int', 'mil', 'win', 'one hundred and sixty', 'l', 'twenty-two', 'amp', 'twelv', 'torino', 'cont', 'two', 'goal', 'last', 'six', 'minut', 'los', 'bet', 'unbelievable\\n\\nnow', 'lik', 'stick', 'singl', 'espec', 'potd', 'research', 'gam', 'lik', 'today', 'ther', 'noth', 'put', 'edg', 'certain', 'pick', 'think', 'lazio', 'gam', 'o25', 'goal', 'see', 'team', 'scor', 'ev', 'lazio', 'scor', 'three', 'im', 'one hundred', 'confid', 'twenty', 'win', 'gam', 'wouldnt', 'surpr', '\\n\\nso', 'detail', 'torino', 'gam', 'host', 'veron', 'today', 'iv', 'talk', 'prevy', 'pick', 'ut', 'terr', 'report', 'believ', 'man', 'clos', 'sack', 'veron', 'look', 'hopeless', 'six', 'gam', 'record', 'twenty-four', 'scor', 'on', 'penal', 'cont', 'fourteen', '\\n\\ntorino', 'dec', 'far', 'season', 'find', '7th', 'record', 'three hundred and twenty-one', 'goal', 'one hundred and nin', 'attack', 'belott', 'ljas', 'good', 'form', 'three', 'goal', 'lik', 'scor', 'today', 'belott', 'scor', 'two', 'hom', 'gam', 'torino', 'play', 'far', 'verona', 'horr', 'scor', 'record', 'doubt', 'theyl', 'scor', 'today', 'itl', 'on', 'torino', 'enough', 'qual', 'get', 'two', 'defend', 'mid', 'basell', 'miss', 'sent', 'last', 'week', 'ob', 'amp', 'barrec', 'also', 'miss', 'host', 'theyl', 'abl', 'cop', 'without', '\\n\\nin', 'gam', 'int', 'play', 'away', 'benevento', 'eq', 'bad', 'veron', 'sit', 'last', 'plac', 'also', 'scor', 'on', 'goal', 'cont', 'sixteen', 'goal', 'already', 'smash', 'napol', 'sixty', 'rom', 'forty', 'wont', 'get', 'easy', 'int', 'fiv', 'drawn', 'gam', 'best', 'defend', 'record', 'leagu', '\\n\\ninter', 'lucky', 'win', 'last', 'week', 'geno', 'continu', 'record', 'scor', 'every', 'gam', 'com', 'most', 'thank', 'icard', 'six', 'goal', 'already', 'lik', 'chant', 'bag', 'coupl', 'today', 'might', 'put', 'littl', 'ev', 'get', 'hat', 'trick', 'far', 'injury', 'go', 'int', 'miss', 'cancelo', 'hasnt', 'play', 'mon', 'anyway', 'could', 'rout', 'benevento', 'perform', 'int', 'dont', 'tend', 'big', 'scor', 'team', 'im', 'expect', 'two thousand and thirty', '\\n\\n\\nquite', 'lot', 'gam', 'lik', 'today', 'il', 'try', 'whittl', 'bet', 'look', 'on', 'picks\\n\\npicks\\n\\n', 'ars', 'v', 'brighton', 'ars', 'cle', 'sheet', 'ye', 'one hundred and eighty', 'w', '20\\n\\noriginally', 'pick', 'going', 'ars', 'win', 'nil', 'od', 'simil', 'cle', 'sheet', 'seem', 'much', 'wor', 'cas', 'ars', 'cant', 'scor', 'somehow', '\\n\\narsenal', 'kept', 'last', 'sixty-eight', 'hom', 'match', 'cle', 'sheet', 'theyv', 'shown', 'impress', 'hom', 'win', 'gam', 'far', 'cont', 'hol', 'goal', 'let', 'ear', 'forty-three', 'thriller', 'leicest', '\\n\\nbrighton', 'struggled', 'goal', 'season', 'theyr', 'scor', 'fiv', 'six', 'three', 'goal', 'com', 'west', 'brom', 'scor', 'on', 'away', 'hom', 'far', 'get', 'bet', 'look', 'expect', 'gam', 'play', 'look', 'fair', 'big', 'team', 'fac', 'yet', 'man', 'city', 'stat', 'show', 'seven thousand, nine hundred and twenty-one', 'possess', 'man', 'city', 'show', 'brighton', 'lik', 'sit', 'back', 'try', 'grind', 'draw', 'today', 'ars', '\\n\\n', 'everton', 'v', 'burnley', 'burnley', 'draw', 'two hundred and twenty-five', 'w', '01\\n\\ni', 'fuck', 'look', 'od', 'burnley', '10ah', 'one hundred and sixty-fiv', 'much', 'bet', 'pick', 'yo', 'read', 'pick', 'lat', 'would', 'tak', 'stil', 'feel', 'burnley', 'draw', 'wont', 'chang', 'pick', 'fact', 'stat', '\\n\\nburnley', 'perform', 'wel', 'season', 'record', 'two hundred and thirty-one', 'goal', 'sixty-five', 'away', 'hom', 'burnley', 'real', 'turn', 'terr', 'record', 'last', 'season', 'season', 'chelse', 'twenty-one', 'wel', 'get', 'elev', 'draw', 'spur', 'amp', 'liverpool', 'burnley', 'los', 'tend', 'goal', 'although', 'last', 'season', 'everton', 'on', 'team', 'abl', 'beat', 'margin', 'win', 'thirty-one', 'diff', 'everton', 'year', '\\n\\neverton', 'struggled', 'goal', 'theyv', 'scor', 'four', 'six', 'two', 'com', 'last', 'gam', 'fortun', 'get', 'away', 'win', 'koem', 'press', 'mom', 'want', 'continu', 'everton', 'win', 'streak', 'burnley', 'abl', 'get', 'first', 'goal', 'goodison', 'park', 'crowd', 'boo', 'get', 'play', 'back', 'press', 'might', 'much', '\\n\\n', 'atalant', 'v', 'juvent', 'btts', 'one hundred and ninety-fiv', 'w', '22\\n\\nthe', 'head', 'head', 'record', 'btts', 'last', 'forty-six', 'atalant', 'fifty-six', 'btts', 'team', 'scor', 'gam', 'far', 'exceiv', 'juv', 'play', 'barc', 'think', 'team', 'fant', 'chant', 'inform', 'play', 'eith', 'sid', 'gomez', 'dybal', 'lik', 'o25', 'goal', 'twenty-two', 'gam', 'last', 'season', 'could', 'agin', 'today', '\\n\\n', 'torino', 'v', 'veron', 'torino', 'cle', 'sheet', 'ye', 'two hundred and twenty', 'l', '22\\n\\nexplained', 'potd', '\\n\\n', 'benevento', 'v', 'int', 'mil', 'int', 'twenty', 'as', 'handicap', 'two hundred and twenty', 'l', '12\\n\\nexplained', 'potd', '\\n\\n', 'benevento', 'v', 'int', 'mil', 'int', 'win', 'nil', 'one hundred and ninety-fiv', 'l', '02\\n\\nexplained', 'potd', '\\n\\n', 'herth', 'berlin', 'v', 'bayern', 'munich', 'herth', 'berlin', 'scor', 'on', 'goal', 'one hundred and ninety', 'w', '22\\n\\nive', 'explain', 'pick', 'com', '\\n\\n', 'koln', 'v', 'rb', 'leipzig', 'leipzig', 'win', 'two hundred', 'w', '12\\n\\n', 'nic', 'v', 'marseil', 'nic', 'draw', 'bet', 'one hundred and ninety-two', 'l', 'twenty-four', 'nic', 'twenty', 'would', 'paid', 'booky', 'in', 'straight', 'win', 'annoying\\n\\n', 'maritimo', 'v', 'benfic', 'maritimo', 'ten', 'as', 'handicap', 'two hundred and six', 'w', '11\\n\\nlastly', 'would', 'adv', 'put', 'acc', 'much', 'id', 'lik', 'win', 'alway', 'unlik', 'many', 'pick', 'feel', 'combin', 'way', 'pleas', 'choos', 'doubl', 'fav', 'ev', 'trebl', 'anyth', 'yo', 'ask', 'trouble\\n\\nbonus', 'pick', 'od', 'ev', 'greater\\n\\n', 'everton', 'v', 'burnley', 'draw', 'three hundred and eighty', 'l\\n\\n', 'newcastl', 'v', 'liverpool', 'btts', 'amp', 'o25', 'goal', 'two hundred', 'l\\n\\n', 'benevento', 'v', 'int', 'mil', 'int', 'thirty', 'as', 'handicap', 'four hundred and forty', 'l\\n\\n', 'herth', 'berlin', 'draw', 'three hundred and fifty', 'w\\n\\n', 'herth', 'berlin', 'win', 'one thousand, one hundred', 'smal', 'unit', 'l\\n\\n', 'heerenveen', 'draw', 'two hundred and twenty', 'l\\n\\n', 'maritimo', 'draw', 'bet', 'five hundred and fifty', 'p\\n\\napologies', 'spel', 'amp', 'gramm', 'er', 'writ', 'phon'], ['potd', 'record', 'seven hundred and one', 'wpl\\n\\nstreak', '6w\\n\\npick', 'record', '23216\\n\\nbonus', 'pick', 'record', '17123\\n\\n\\nhad', 'decent', 'day', 'yesterday', 'potd', 'pick', 'coventry', 'get', 'forty-six', 'pick', 'winners', 'one', 'push\\n\\npotd\\n\\ndouble', 'torino', 'win', 'amp', 'inter', 'milan', 'win', 'one hundred and sixty', 'l', 'twenty-two', 'amp', 'twelve', 'torino', 'concede', 'two', 'goals', 'last', 'six', 'minutes', 'lose', 'bet', 'unbelievable\\n\\nnow', 'like', 'stick', 'single', 'especially', 'potd', 'research', 'game', 'like', 'today', 'theres', 'nothing', 'put', 'edge', 'certainly', 'pick', 'think', 'lazio', 'game', 'o25', 'goals', 'see', 'team', 'score', 'even', 'lazio', 'score', 'three', 'im', 'one hundred', 'confident', 'twenty', 'win', 'game', 'wouldnt', 'surprise', '\\n\\nso', 'detail', 'torinos', 'game', 'host', 'verona', 'today', 'ive', 'talk', 'previous', 'pick', 'utterly', 'terrible', 'report', 'believe', 'manager', 'close', 'sack', 'verona', 'look', 'hopeless', 'six', 'game', 'record', 'twenty-four', 'score', 'one', 'penalty', 'concede', 'fourteen', '\\n\\ntorino', 'decent', 'far', 'season', 'find', '7th', 'record', 'three hundred and twenty-one', 'goals', 'one hundred and nine', 'attackers', 'belotti', 'ljajic', 'good', 'form', 'three', 'goals', 'likely', 'score', 'today', 'belotti', 'score', 'two', 'home', 'game', 'torino', 'play', 'far', 'veronas', 'horrible', 'score', 'record', 'doubt', 'theyll', 'score', 'today', 'itll', 'one', 'torino', 'enough', 'quality', 'get', 'two', 'defensive', 'mid', 'baselli', 'miss', 'send', 'last', 'week', 'obi', 'amp', 'barreca', 'also', 'miss', 'host', 'theyll', 'able', 'cope', 'without', '\\n\\nin', 'game', 'inter', 'play', 'away', 'benevento', 'equally', 'bad', 'verona', 'sit', 'last', 'place', 'also', 'score', 'one', 'goal', 'concede', 'sixteen', 'goals', 'already', 'smash', 'napoli', 'sixty', 'roma', 'forty', 'wont', 'get', 'easier', 'inter', 'five', 'draw', 'game', 'best', 'defensive', 'record', 'league', '\\n\\ninter', 'lucky', 'win', 'last', 'week', 'genoa', 'continue', 'record', 'score', 'every', 'game', 'come', 'mostly', 'thank', 'icardi', 'six', 'goals', 'already', 'like', 'chance', 'bag', 'couple', 'today', 'might', 'put', 'little', 'even', 'get', 'hat', 'trick', 'far', 'injuries', 'go', 'inter', 'miss', 'cancelo', 'hasnt', 'play', 'month', 'anyway', 'could', 'rout', 'beneventos', 'performances', 'inter', 'dont', 'tend', 'big', 'score', 'team', 'im', 'expect', 'two thousand and thirty', '\\n\\n\\nquite', 'lot', 'game', 'like', 'today', 'ill', 'try', 'whittle', 'better', 'look', 'ones', 'picks\\n\\npicks\\n\\n', 'arsenal', 'v', 'brighton', 'arsenal', 'clean', 'sheet', 'yes', 'one hundred and eighty', 'w', '20\\n\\noriginally', 'pick', 'go', 'arsenal', 'win', 'nil', 'odds', 'similar', 'clean', 'sheet', 'seem', 'much', 'worth', 'case', 'arsenal', 'cant', 'score', 'somehow', '\\n\\narsenal', 'keep', 'last', 'sixty-eight', 'home', 'match', 'clean', 'sheet', 'theyve', 'show', 'impressive', 'home', 'win', 'game', 'far', 'concede', 'hole', 'goals', 'let', 'early', 'forty-three', 'thriller', 'leicester', '\\n\\nbrighton', 'struggle', 'goals', 'season', 'theyre', 'score', 'five', 'six', 'three', 'goals', 'come', 'west', 'brom', 'score', 'one', 'away', 'home', 'far', 'get', 'better', 'look', 'expect', 'game', 'play', 'look', 'fair', 'big', 'team', 'face', 'yet', 'man', 'city', 'stats', 'show', 'seven thousand, nine hundred and twenty-one', 'possession', 'man', 'city', 'show', 'brighton', 'likely', 'sit', 'back', 'try', 'grind', 'draw', 'today', 'arsenal', '\\n\\n', 'everton', 'v', 'burnley', 'burnley', 'draw', 'two hundred and twenty-five', 'w', '01\\n\\ni', 'fuck', 'look', 'odds', 'burnley', '10ah', 'one hundred and sixty-five', 'much', 'better', 'pick', 'youre', 'read', 'pick', 'late', 'would', 'take', 'still', 'feel', 'burnley', 'draw', 'wont', 'change', 'pick', 'fact', 'stats', '\\n\\nburnley', 'perform', 'well', 'season', 'record', 'two hundred and thirty-one', 'goals', 'sixty-five', 'away', 'home', 'burnley', 'really', 'turn', 'terrible', 'record', 'last', 'season', 'season', 'chelsea', 'twenty-one', 'well', 'get', 'eleven', 'draw', 'spur', 'amp', 'liverpool', 'burnley', 'lose', 'tend', 'goal', 'although', 'last', 'season', 'everton', 'one', 'team', 'able', 'beat', 'margin', 'win', 'thirty-one', 'different', 'everton', 'year', '\\n\\neverton', 'struggle', 'goals', 'theyve', 'score', 'four', 'six', 'two', 'come', 'last', 'game', 'fortunate', 'get', 'away', 'win', 'koeman', 'pressure', 'moment', 'want', 'continue', 'evertons', 'win', 'streak', 'burnley', 'able', 'get', 'first', 'goal', 'goodison', 'park', 'crowd', 'boo', 'get', 'players', 'back', 'pressure', 'might', 'much', '\\n\\n', 'atalanta', 'v', 'juventus', 'btts', 'one hundred and ninety-five', 'w', '22\\n\\nthe', 'head', 'head', 'record', 'btts', 'last', 'forty-six', 'atalanta', 'fifty-six', 'btts', 'team', 'score', 'game', 'far', 'except', 'juve', 'play', 'barca', 'think', 'team', 'fancy', 'chance', 'inform', 'players', 'either', 'side', 'gomez', 'dybala', 'like', 'o25', 'goals', 'twenty-two', 'game', 'last', 'season', 'could', 'agin', 'today', '\\n\\n', 'torino', 'v', 'verona', 'torino', 'clean', 'sheet', 'yes', 'two hundred and twenty', 'l', '22\\n\\nexplained', 'potd', '\\n\\n', 'benevento', 'v', 'inter', 'milan', 'inter', 'twenty', 'asian', 'handicap', 'two hundred and twenty', 'l', '12\\n\\nexplained', 'potd', '\\n\\n', 'benevento', 'v', 'inter', 'milan', 'inter', 'win', 'nil', 'one hundred and ninety-five', 'l', '02\\n\\nexplained', 'potd', '\\n\\n', 'hertha', 'berlin', 'v', 'bayern', 'munich', 'hertha', 'berlin', 'score', 'one', 'goals', 'one hundred and ninety', 'w', '22\\n\\nive', 'explain', 'pick', 'comment', '\\n\\n', 'koln', 'v', 'rb', 'leipzig', 'leipzig', 'win', 'two hundred', 'w', '12\\n\\n', 'nice', 'v', 'marseille', 'nice', 'draw', 'bet', 'one hundred and ninety-two', 'l', 'twenty-four', 'nice', 'twenty', 'would', 'pay', 'bookies', 'ina', 'straight', 'win', 'annoying\\n\\n', 'maritimo', 'v', 'benfica', 'maritimo', 'ten', 'asian', 'handicap', 'two hundred and six', 'w', '11\\n\\nlastly', 'would', 'advise', 'put', 'acca', 'much', 'id', 'like', 'win', 'always', 'unlikely', 'many', 'pick', 'feel', 'combine', 'way', 'please', 'choose', 'double', 'favs', 'even', 'treble', 'anything', 'youre', 'ask', 'trouble\\n\\nbonus', 'pick', 'odds', 'even', 'greater\\n\\n', 'everton', 'v', 'burnley', 'draw', 'three hundred and eighty', 'l\\n\\n', 'newcastle', 'v', 'liverpool', 'btts', 'amp', 'o25', 'goals', 'two hundred', 'l\\n\\n', 'benevento', 'v', 'inter', 'milan', 'inter', 'thirty', 'asian', 'handicap', 'four hundred and forty', 'l\\n\\n', 'hertha', 'berlin', 'draw', 'three hundred and fifty', 'w\\n\\n', 'hertha', 'berlin', 'win', 'one thousand, one hundred', 'small', 'units', 'l\\n\\n', 'heerenveen', 'draw', 'two hundred and twenty', 'l\\n\\n', 'maritimo', 'draw', 'bet', 'five hundred and fifty', 'p\\n\\napologies', 'spell', 'amp', 'grammar', 'errors', 'write', 'phone'])\n",
      "original document: \n",
      "['But', 'they', \"aren't\", 'making', 'money', 'on', 'it,', 'least', 'I', \"don't\", 'think', 'so.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ar', 'mak', 'money', 'least', 'dont', 'think'], ['arent', 'make', 'money', 'least', 'dont', 'think'])\n",
      "original document: \n",
      "[\"There's\", 'a', \"Bra'tac\", 'quote', 'from', 'a', 'book', 'that', 'often', 'gets', 'confused', 'with', 'an', 'episode', 'as', \"it's\", 'probably', 'the', 'most', 'quoted', 'line', 'from', 'any', 'of', 'the', 'books,', 'simply', 'because', 'of', 'this', 'question.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ther', 'bratac', 'quot', 'book', 'oft', 'get', 'confus', 'episod', 'prob', 'quot', 'lin', 'book', 'simply', 'quest'], ['theres', 'bratac', 'quote', 'book', 'often', 'get', 'confuse', 'episode', 'probably', 'quote', 'line', 'book', 'simply', 'question'])\n",
      "original document: \n",
      "['Such', 'sagacity', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sagac'], ['sagacity'])\n",
      "original document: \n",
      "['JoJo', \"OP/ED's\\n\\nInitial\", \"D's\", 'Entire', 'OST']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['jojo', 'opeds\\n\\ninitial', 'ds', 'entir', 'ost'], ['jojo', 'opeds\\n\\ninitial', 'ds', 'entire', 'ost'])\n",
      "original document: \n",
      "['Few', 'huge', 'tackles', 'there', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hug', 'tackl'], ['huge', 'tackle'])\n",
      "original document: \n",
      "['If', 'Obama', 'could', 'deal', 'with', 'anything,', \"it's\", 'kids.', \"He's\", 'a', 'natural', 'with', 'them.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['obam', 'could', 'deal', 'anyth', 'kid', 'hes', 'nat'], ['obama', 'could', 'deal', 'anything', 'kid', 'hes', 'natural'])\n",
      "original document: \n",
      "[\"[+BriannaMGoux](https://www.reddit.com/r/IAmA/comments/737yli/i_am_brianna_goux_a_19_year_old_redditor_who_has/dnp8saw/):\\n\\nI'm\", 'sorry', 'for', 'not', 'giving', 'your', 'proper', 'dues.', 'That', 'is', 'my', 'fault', ':(', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnp8saw\\n\\nim', 'sorry', 'giv', 'prop', 'due', 'fault'], ['briannamgouxhttpswwwredditcomriamacomments737ylii_am_brianna_goux_a_19_year_old_redditor_who_hasdnp8saw\\n\\nim', 'sorry', 'give', 'proper', 'dues', 'fault'])\n",
      "original document: \n",
      "['What', 'is', 'with', 'NRG', 'and', 'conceding', 'kick', 'off', 'goals', 'today?', \"It's\", 'sorta', 'embarrassing', 'at', 'this', 'point', 'when', '1', 'goal', 'games', 'seem', 'to', 'be', 'decided', 'by', 'a', 'kick', 'off', 'goal.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['nrg', 'cont', 'kick', 'goal', 'today', 'sort', 'embarrass', 'point', 'on', 'goal', 'gam', 'seem', 'decid', 'kick', 'goal'], ['nrg', 'concede', 'kick', 'goals', 'today', 'sorta', 'embarrass', 'point', 'one', 'goal', 'game', 'seem', 'decide', 'kick', 'goal'])\n",
      "original document: \n",
      "['psn', 'chaimazing,', 'hunter', '296']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['psn', 'chaimaz', 'hunt', 'two hundred and ninety-six'], ['psn', 'chaimazing', 'hunter', 'two hundred and ninety-six'])\n",
      "original document: \n",
      "['You', 'want', 'to', 'acquire', 'a', 'Jager?', 'So', '...', \"that's\", 'what', 'I', 'am', 'offering.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['want', 'acquir', 'jag', 'that', 'off'], ['want', 'acquire', 'jager', 'thats', 'offer'])\n",
      "original document: \n",
      "['Part', '', '2', 'pls', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['part', 'two', 'pls'], ['part', 'two', 'pls'])\n",
      "original document: \n",
      "['Heroic', 'd', 'by', 'dmac']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['hero', 'dmac'], ['heroic', 'dmac'])\n",
      "original document: \n",
      "['[deleted]']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['delet'], ['delete'])\n",
      "original document: \n",
      "['Estro', 'was', 'top', 'of', 'range', 'but', 'still', 'in', 'range', 'at', '0.5adex', 'eod.', '\\nI', 'upped', 'it', 'to', '0.5', 'ed', 'to', 'bring', 'myself', 'lower.', '\\nNo', 'other', 'ancillaries', 'have', 'been', 'needed', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['estro', 'top', 'rang', 'stil', 'rang', '05adex', 'eod', '\\ni', 'up', 'fiv', 'ed', 'bring', 'low', '\\nno', 'ancil', 'nee'], ['estro', 'top', 'range', 'still', 'range', '05adex', 'eod', '\\ni', 'up', 'five', 'ed', 'bring', 'lower', '\\nno', 'ancillaries', 'need'])\n",
      "original document: \n",
      "['That', 'was', 'some', 'bullshit.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['bullshit'], ['bullshit'])\n",
      "original document: \n",
      "[\"Don't\", 'forget', '\"I', 'love', 'to', 'laugh!\"\\n\\nWho', 'the', 'fuck', 'dislikes', 'laughing?!']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['dont', 'forget', 'lov', 'laugh\\n\\nwho', 'fuck', 'dislik', 'laugh'], ['dont', 'forget', 'love', 'laugh\\n\\nwho', 'fuck', 'dislike', 'laugh'])\n",
      "original document: \n",
      "['Those', 'are', 'super', 'fun!', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['sup', 'fun'], ['super', 'fun'])\n",
      "original document: \n",
      "['They', 'look', 'exactly', 'like', 'Marijuana', 'seeds', 'to', 'me.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['look', 'exact', 'lik', 'marijuan', 'see'], ['look', 'exactly', 'like', 'marijuana', 'seed'])\n",
      "original document: \n",
      "['Of', 'the', 'file', 'name', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['fil', 'nam'], ['file', 'name'])\n",
      "original document: \n",
      "['Its', 'being', 'warmed', 'up', 'by', 'Florida', 'for', 'us']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['warm', 'florid', 'us'], ['warm', 'florida', 'us'])\n",
      "original document: \n",
      "['this', 'makes', 'it', 'easier', 'for', 'things', 'like', 'the', 'goto', 'fail', 'bug', 'to', 'happen,', 'though.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['mak', 'easy', 'thing', 'lik', 'goto', 'fail', 'bug', 'hap', 'though'], ['make', 'easier', 'things', 'like', 'goto', 'fail', 'bug', 'happen', 'though'])\n",
      "original document: \n",
      "['Here', 'at', 'the', 'end', 'of', 'the', 'fight.', '', 'https://youtu.be/navB1r96_do']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['end', 'fight', 'httpsyoutubenavb1r96_do'], ['end', 'fight', 'httpsyoutubenavb1r96_do'])\n",
      "original document: \n",
      "['The', 'idea', 'that', \"you're\", 'ok', 'with', 'him', 'despite', 'what', \"he's\", 'done', 'to', 'degrade', 'the', 'sanctity', 'of', 'your', \"'absolutely'\", 'favorite', 'amendment', 'invalidates', 'any', 'desperate', 'virtue', 'signalling', 'you', 'want', 'to', 'pull', 'to', 'excuse', 'it.', 'The', 'president', 'hates', 'freedom', 'of', 'the', 'press,', 'freedom', 'of', 'speech,', 'freedom', 'of', 'assembly', 'and', 'freedom', 'of', 'religion.', \"You're\", 'not', 'a', 'first', 'amendment', 'absolutist', 'and', 'you', 'never', 'will', 'be', 'until', 'you', 'see', 'the', 'error', 'of', 'your', 'ways.', '', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['ide', 'yo', 'ok', 'despit', 'hes', 'don', 'degrad', 'sanct', 'absolv', 'favorit', 'amend', 'invalid', 'desp', 'virtu', 'signal', 'want', 'pul', 'excus', 'presid', 'hat', 'freedom', 'press', 'freedom', 'speech', 'freedom', 'assembl', 'freedom', 'relig', 'yo', 'first', 'amend', 'absolv', 'nev', 'see', 'er', 'way'], ['idea', 'youre', 'ok', 'despite', 'hes', 'do', 'degrade', 'sanctity', 'absolutely', 'favorite', 'amendment', 'invalidate', 'desperate', 'virtue', 'signal', 'want', 'pull', 'excuse', 'president', 'hat', 'freedom', 'press', 'freedom', 'speech', 'freedom', 'assembly', 'freedom', 'religion', 'youre', 'first', 'amendment', 'absolutist', 'never', 'see', 'error', 'ways'])\n",
      "original document: \n",
      "['Being', 'a', 'member', 'of', 'the', 'Austrian', 'Armed', 'Forces,', 'I', 'highly', 'doubt', \"he's\", 'even', 'been', 'in', 'a', 'fire', 'fight,', 'let', 'alone', 'killed', 'anyone.', '']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "(['memb', 'aust', 'arm', 'forc', 'high', 'doubt', 'hes', 'ev', 'fir', 'fight', 'let', 'alon', 'kil', 'anyon'], ['member', 'austrian', 'arm', 'force', 'highly', 'doubt', 'hes', 'even', 'fire', 'fight', 'let', 'alone', 'kill', 'anyone'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open('result.json', 'r') as rf:\n",
    "    data = json.load(rf)\n",
    "    for entry in data:\n",
    "        doc_sample = data[entry]['body']\n",
    "        print('original document: ')\n",
    "        words = []\n",
    "        for word in doc_sample.split(' '):\n",
    "            words.append(word)\n",
    "        print(words)\n",
    "        \n",
    "        words = normalize(words)\n",
    "        print('\\n\\n tokenized and lemmatized document: ')\n",
    "        print(stem_and_lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
