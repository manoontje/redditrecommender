{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het begin van wat ik gevolgd heb van https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24 is nog wel solid. Later gaat er iets niet goed, dus dat moet nog even gefixt worden, maar volgens mij is dit al semi de basis van onze recommender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import numpy as np\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "import inflect\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-912a8eb1d46d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RC_2015-01/test_data.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mline_short\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_short\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mcomment_id\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mjson_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcomment_body\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taras\\appdata\\local\\programs\\python\\python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taras\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\taras\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 490 (char 489)"
     ],
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 490 (char 489)",
     "output_type": "error"
    }
   ],
   "source": [
    "comment_dict = {}\n",
    "\n",
    "# filtering data\n",
    "for line in open('RC_2015-01/test_data.txt'):\n",
    "    line_short = line[:-2]\n",
    "    json_file = json.loads(line_short)\n",
    "    comment_id =  json_file['id']\n",
    "    comment_body = json_file['body']\n",
    "    comment_sub = json_file['subreddit']\n",
    "    comment = {\"body\": comment_body, \"subreddit\": comment_sub}\n",
    "    comment_dict[comment_id] = comment\n",
    "\n",
    "\n",
    "#writing filtered data to file\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(comment_dict, fp)\n",
    "    \n",
    "#sanity check\n",
    "with open('result.json', 'r') as rf:\n",
    "    data = json.load(rf)\n",
    "    for entry in data:\n",
    "        print(data[entry])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copied from Manon's Text Mining project\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    #\"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    #\"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    #\"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    #\"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    #\"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    #\"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    #\"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "def preprocess(words):\n",
    "    normalize(words)\n",
    "    stem_words(words)\n",
    "    lemmatize_verbs(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d8d04ab368ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Pre-processing sanity check (words are weird)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result.json'"
     ],
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result.json'",
     "output_type": "error"
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "# Pre-processing sanity check (words are weird)\n",
    "with open('result.json', 'r') as rf:\n",
    "    data = json.load(rf)\n",
    "    for entry in data:\n",
    "        doc_sample = data[entry]['body']\n",
    "       # print('original document: ')\n",
    "        words = []\n",
    "        for word in doc_sample.split(' '):\n",
    "            words.append(word)\n",
    "      #  print(words)\n",
    "        \n",
    "        words = normalize(words)\n",
    "        words = stem_words(words)\n",
    "        words = lemmatize_verbs(words)\n",
    "      #  print('\\n\\n tokenized and lemmatized document: ')\n",
    "      #  print(words)\n",
    "        \n",
    "        all_words.append(words)\n",
    "print(all_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 quarry\n",
      "1 \n",
      "\n",
      "2 im\n",
      "3 saidhttpimgurcom9ttainh\n",
      "4 salut\n",
      "5 sur\n",
      "6 basebal\n",
      "7 bite\n",
      "8 cain\n",
      "9 crazy\n",
      "10 get\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4, 1), (12, 1), (85, 2), (152, 1), (162, 1), (163, 1), (164, 1), (165, 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        \n",
    "dictionary = gensim.corpora.Dictionary(all_words)\n",
    "        \n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "                \n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in all_words]\n",
    "bow_corpus[28]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"get\") appears 1 time.\n",
      "Word 12 (\"fuck\") appears 1 time.\n",
      "Word 85 (\"good\") appears 2 time.\n",
      "Word 152 (\"pretty\") appears 1 time.\n",
      "Word 162 (\"job\") appears 1 time.\n",
      "Word 163 (\"laugh\") appears 1 time.\n",
      "Word 164 (\"lol\") appears 1 time.\n",
      "Word 165 (\"wel\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_28 = bow_corpus[28]\n",
    "for i in range(len(bow_doc_28)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_28[i][0], \n",
    "                                               dictionary[bow_doc_28[i][0]], \n",
    "bow_doc_28[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break\n",
    "    \n",
    "# Dit doet dus nie, weet niet waarom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.021*\"thread\" + 0.019*\"ev\" + 0.018*\"bot\" + 0.018*\"act\" + 0.016*\"post\" + 0.015*\"yo\" + 0.015*\"lik\" + 0.015*\"pleas\" + 0.015*\"fuck\" + 0.013*\"bite\"\n",
      "Topic: 1 \n",
      "Words: 0.032*\"good\" + 0.030*\"say\" + 0.023*\"peopl\" + 0.022*\"us\" + 0.019*\"that\" + 0.018*\"lov\" + 0.018*\"man\" + 0.014*\"iv\" + 0.014*\"get\" + 0.013*\"form\"\n",
      "Topic: 2 \n",
      "Words: 0.060*\"see\" + 0.038*\"let\" + 0.036*\"goe\" + 0.035*\"deep\" + 0.035*\"hol\" + 0.035*\"rabbit\" + 0.028*\"bal\" + 0.023*\"day\" + 0.023*\"peopl\" + 0.018*\"wel\"\n",
      "Topic: 3 \n",
      "Words: 0.029*\"lik\" + 0.023*\"didnt\" + 0.021*\"get\" + 0.019*\"thing\" + 0.016*\"\n",
      "\n",
      "i\" + 0.015*\"think\" + 0.015*\"mak\" + 0.014*\"would\" + 0.013*\"want\" + 0.012*\"tim\"\n",
      "Topic: 4 \n",
      "Words: 0.054*\"delet\" + 0.030*\"know\" + 0.028*\"dont\" + 0.023*\"kid\" + 0.021*\"team\" + 0.019*\"on\" + 0.019*\"want\" + 0.015*\"play\" + 0.015*\"understand\" + 0.014*\"im\"\n",
      "Topic: 5 \n",
      "Words: 0.035*\"get\" + 0.026*\"polit\" + 0.019*\"lov\" + 0.016*\"gam\" + 0.015*\"us\" + 0.015*\"peopl\" + 0.014*\"part\" + 0.014*\"don\" + 0.013*\"due\" + 0.012*\"on\"\n",
      "Topic: 6 \n",
      "Words: 0.046*\"lik\" + 0.031*\"us\" + 0.029*\"remov\" + 0.024*\"go\" + 0.020*\"see\" + 0.018*\"work\" + 0.017*\"mak\" + 0.014*\"high\" + 0.013*\"plan\" + 0.013*\"look\"\n",
      "Topic: 7 \n",
      "Words: 0.021*\"sav\" + 0.020*\"would\" + 0.018*\"id\" + 0.017*\"ev\" + 0.015*\"im\" + 0.014*\"stat\" + 0.014*\"gt\" + 0.014*\"gam\" + 0.013*\"on\" + 0.012*\"unit\"\n",
      "Topic: 8 \n",
      "Words: 0.023*\"point\" + 0.020*\"lik\" + 0.020*\"play\" + 0.018*\"real\" + 0.017*\"get\" + 0.015*\"four\" + 0.015*\"would\" + 0.014*\"want\" + 0.013*\"peopl\" + 0.013*\"fiv\"\n",
      "Topic: 9 \n",
      "Words: 0.032*\"gam\" + 0.027*\"win\" + 0.024*\"pick\" + 0.022*\"get\" + 0.022*\"on\" + 0.020*\"lik\" + 0.019*\"int\" + 0.018*\"last\" + 0.018*\"draw\" + 0.017*\"play\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.019*\"lik\" + 0.015*\"say\" + 0.015*\"lol\" + 0.014*\"lov\" + 0.011*\"yo\" + 0.011*\"see\" + 0.010*\"fuck\" + 0.009*\"what\" + 0.009*\"get\" + 0.009*\"feel\"\n",
      "Topic: 1 Word: 0.014*\"think\" + 0.012*\"that\" + 0.011*\"ev\" + 0.008*\"stat\" + 0.008*\"would\" + 0.008*\"tru\" + 0.007*\"unit\" + 0.007*\"bas\" + 0.007*\"dont\" + 0.007*\"id\"\n",
      "Topic: 2 Word: 0.151*\"delet\" + 0.087*\"remov\" + 0.015*\"ye\" + 0.009*\"lik\" + 0.008*\"want\" + 0.007*\"on\" + 0.006*\"good\" + 0.006*\"suck\" + 0.006*\"put\" + 0.005*\"peopl\"\n",
      "Topic: 3 Word: 0.022*\"delet\" + 0.016*\"go\" + 0.012*\"know\" + 0.012*\"look\" + 0.011*\"good\" + 0.010*\"post\" + 0.010*\"oh\" + 0.010*\"dont\" + 0.009*\"get\" + 0.008*\"thing\"\n",
      "Topic: 4 Word: 0.012*\"lik\" + 0.011*\"get\" + 0.011*\"two\" + 0.011*\"im\" + 0.010*\"on\" + 0.009*\"spot\" + 0.008*\"year\" + 0.008*\"gam\" + 0.008*\"us\" + 0.008*\"tim\"\n",
      "Topic: 5 Word: 0.015*\"much\" + 0.015*\"play\" + 0.012*\"us\" + 0.010*\"dont\" + 0.010*\"sorry\" + 0.009*\"ok\" + 0.009*\"im\" + 0.009*\"day\" + 0.009*\"also\" + 0.009*\"try\"\n",
      "Topic: 6 Word: 0.037*\"rabbit\" + 0.037*\"let\" + 0.037*\"deep\" + 0.037*\"hol\" + 0.036*\"goe\" + 0.033*\"see\" + 0.017*\"delet\" + 0.016*\"sur\" + 0.011*\"im\" + 0.008*\"stat\"\n",
      "Topic: 7 Word: 0.016*\"get\" + 0.013*\"fuck\" + 0.012*\"lik\" + 0.012*\"on\" + 0.009*\"think\" + 0.009*\"gam\" + 0.008*\"us\" + 0.008*\"gre\" + 0.007*\"would\" + 0.007*\"pretty\"\n",
      "Topic: 8 Word: 0.013*\"see\" + 0.010*\"pleas\" + 0.010*\"real\" + 0.010*\"get\" + 0.009*\"dont\" + 0.009*\"bet\" + 0.009*\"nee\" + 0.009*\"on\" + 0.008*\"would\" + 0.008*\"mak\"\n",
      "Topic: 9 Word: 0.018*\"on\" + 0.017*\"thank\" + 0.010*\"man\" + 0.009*\"get\" + 0.008*\"im\" + 0.008*\"think\" + 0.007*\"gt\" + 0.007*\"com\" + 0.007*\"diff\" + 0.007*\"could\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.589565634727478\t \n",
      "Topic: 0.032*\"good\" + 0.030*\"say\" + 0.023*\"peopl\" + 0.022*\"us\" + 0.019*\"that\" + 0.018*\"lov\" + 0.018*\"man\" + 0.014*\"iv\" + 0.014*\"get\" + 0.013*\"form\"\n",
      "\n",
      "Score: 0.19771523773670197\t \n",
      "Topic: 0.035*\"get\" + 0.026*\"polit\" + 0.019*\"lov\" + 0.016*\"gam\" + 0.015*\"us\" + 0.015*\"peopl\" + 0.014*\"part\" + 0.014*\"don\" + 0.013*\"due\" + 0.012*\"on\"\n",
      "\n",
      "Score: 0.14270007610321045\t \n",
      "Topic: 0.060*\"see\" + 0.038*\"let\" + 0.036*\"goe\" + 0.035*\"deep\" + 0.035*\"hol\" + 0.035*\"rabbit\" + 0.028*\"bal\" + 0.023*\"day\" + 0.023*\"peopl\" + 0.018*\"wel\"\n",
      "\n",
      "Score: 0.010003894567489624\t \n",
      "Topic: 0.029*\"lik\" + 0.023*\"didnt\" + 0.021*\"get\" + 0.019*\"thing\" + 0.016*\"\n",
      "\n",
      "i\" + 0.015*\"think\" + 0.015*\"mak\" + 0.014*\"would\" + 0.013*\"want\" + 0.012*\"tim\"\n",
      "\n",
      "Score: 0.010003355331718922\t \n",
      "Topic: 0.021*\"sav\" + 0.020*\"would\" + 0.018*\"id\" + 0.017*\"ev\" + 0.015*\"im\" + 0.014*\"stat\" + 0.014*\"gt\" + 0.014*\"gam\" + 0.013*\"on\" + 0.012*\"unit\"\n",
      "\n",
      "Score: 0.010003204457461834\t \n",
      "Topic: 0.021*\"thread\" + 0.019*\"ev\" + 0.018*\"bot\" + 0.018*\"act\" + 0.016*\"post\" + 0.015*\"yo\" + 0.015*\"lik\" + 0.015*\"pleas\" + 0.015*\"fuck\" + 0.013*\"bite\"\n",
      "\n",
      "Score: 0.010002684779465199\t \n",
      "Topic: 0.054*\"delet\" + 0.030*\"know\" + 0.028*\"dont\" + 0.023*\"kid\" + 0.021*\"team\" + 0.019*\"on\" + 0.019*\"want\" + 0.015*\"play\" + 0.015*\"understand\" + 0.014*\"im\"\n",
      "\n",
      "Score: 0.010002470575273037\t \n",
      "Topic: 0.023*\"point\" + 0.020*\"lik\" + 0.020*\"play\" + 0.018*\"real\" + 0.017*\"get\" + 0.015*\"four\" + 0.015*\"would\" + 0.014*\"want\" + 0.013*\"peopl\" + 0.013*\"fiv\"\n",
      "\n",
      "Score: 0.010002123191952705\t \n",
      "Topic: 0.032*\"gam\" + 0.027*\"win\" + 0.024*\"pick\" + 0.022*\"get\" + 0.022*\"on\" + 0.020*\"lik\" + 0.019*\"int\" + 0.018*\"last\" + 0.018*\"draw\" + 0.017*\"play\"\n",
      "\n",
      "Score: 0.010001279413700104\t \n",
      "Topic: 0.046*\"lik\" + 0.031*\"us\" + 0.029*\"remov\" + 0.024*\"go\" + 0.020*\"see\" + 0.018*\"work\" + 0.017*\"mak\" + 0.014*\"high\" + 0.013*\"plan\" + 0.013*\"look\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[28]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5499863028526306\t Topic: 0.032*\"good\" + 0.030*\"say\" + 0.023*\"peopl\" + 0.022*\"us\" + 0.019*\"that\"\n",
      "Score: 0.05000265687704086\t Topic: 0.029*\"lik\" + 0.023*\"didnt\" + 0.021*\"get\" + 0.019*\"thing\" + 0.016*\"\n",
      "\n",
      "i\"\n",
      "Score: 0.05000166967511177\t Topic: 0.021*\"thread\" + 0.019*\"ev\" + 0.018*\"bot\" + 0.018*\"act\" + 0.016*\"post\"\n",
      "Score: 0.05000150203704834\t Topic: 0.046*\"lik\" + 0.031*\"us\" + 0.029*\"remov\" + 0.024*\"go\" + 0.020*\"see\"\n",
      "Score: 0.05000144615769386\t Topic: 0.035*\"get\" + 0.026*\"polit\" + 0.019*\"lov\" + 0.016*\"gam\" + 0.015*\"us\"\n",
      "Score: 0.05000137537717819\t Topic: 0.023*\"point\" + 0.020*\"lik\" + 0.020*\"play\" + 0.018*\"real\" + 0.017*\"get\"\n",
      "Score: 0.050001367926597595\t Topic: 0.054*\"delet\" + 0.030*\"know\" + 0.028*\"dont\" + 0.023*\"kid\" + 0.021*\"team\"\n",
      "Score: 0.05000133439898491\t Topic: 0.060*\"see\" + 0.038*\"let\" + 0.036*\"goe\" + 0.035*\"deep\" + 0.035*\"hol\"\n",
      "Score: 0.050001226365566254\t Topic: 0.032*\"gam\" + 0.027*\"win\" + 0.024*\"pick\" + 0.022*\"get\" + 0.022*\"on\"\n",
      "Score: 0.05000109225511551\t Topic: 0.021*\"sav\" + 0.020*\"would\" + 0.018*\"id\" + 0.017*\"ev\" + 0.015*\"im\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = ['How', 'a', 'Pentagon', 'deal', 'became','an', 'identity', 'crisis', 'for', 'Google'] \n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}